from neo4j import GraphDatabase
import pandas as pd

# 1. è¿æ¥ Neo4j =========
uri = "bolt://localhost:7687"  # ä¿®æ”¹ä¸ºä½ çš„åœ°å€
username = "neo4j"
password = "Aa123456"     # ä¿®æ”¹ä¸ºä½ çš„å¯†ç 

driver = GraphDatabase.driver(uri, auth=(username, password))
attributes = ['Title', 'SrcDatabase','CountryName',  'PubTime', 'Summary', 'Claims']

name2id = {}    # æ‰€æœ‰å®ä½“çš„ç¼–å·å­—å…¸
no2label = {}   # æ‰€æœ‰å®ä½“çš„ç±»å‹å­—å…¸
texts_features = []
# ======== æ•è·å®ä½“åŠå…¶å±æ€§ç‰¹å¾å‘é‡å‡½æ•° =========
def get_node_index_map(tx):
    attr_str = ", ".join([f"n.{attr} AS {attr}" for attr in attributes])
    query = f"MATCH (n) RETURN n.name AS name, labels(n) AS label, {attr_str}"
    result = tx.run(query)
    name_to_id = {}
    no_to_label = {}
    for i, record in enumerate(result):
        name_to_id[record["name"]] = i
        no_to_label[i] = record["label"]
    return name_to_id, no_to_label

# ======== æ•è·è¾¹å‡½æ•° =========
def get_edges(tx, name_to_id, relation_to_id):
    result = tx.run("MATCH (a)-[r]->(b) RETURN a.name AS head, type(r) AS rel, b.name AS tail")
    edge_index = []
    edge_type = []
    for record in result:
        head = name_to_id[record["head"]]
        tail = name_to_id[record["tail"]]
        rel = relation_to_id.setdefault(record["rel"], len(relation_to_id))  # ç¼–å·ä»0å¼€å§‹
        edge_index.append([head, tail])
        edge_type.append(rel)
    return edge_index, edge_type, relation_to_id

edge_index = []
edge_type = []
name2id = {}
relation2id = {}
with driver.session(database="final") as session:
    name2id, no2label = session.execute_read(get_node_index_map)
    edge_index, edge_type, relation2id = session.execute_read(get_edges, name2id, relation2id)

driver.close()

keys_with_pubno = [k for k, v in no2label.items() if "PubNo" in v]# è·å–æ‰€æœ‰ PubNoå®ä½“ç‚¹ çš„ç´¢å¼•
pubno_labels = [k for k, v in name2id.items() if v in keys_with_pubno]# è·å–æ‰€æœ‰ PubNoå®ä½“ç‚¹ çš„key

y_true = []
# è¯»å– CSV æ–‡ä»¶
df = pd.read_csv("Patent.csv")

for i in pubno_labels:
    # æŸ¥æ‰¾å¯¹åº” 'PubNo' çš„ 'label' å€¼
    label_value = df.loc[df['PubNo-å…¬å¼€å·'] == i, 'Label-æ ‡ç­¾'].values
    y_true.append(label_value[0])

for i in range(len(y_true)):
    value_to_find = keys_with_pubno[i]
    # ä½¿ç”¨å­—å…¸éå†æ¥æ‰¾åˆ°å¯¹åº”çš„ key
    keys = [k for k, v in name2id.items() if v == value_to_find]
    #print(keys,y_true[i])
#input()
# å®ä½“ç‚¹æ•°é‡
node_num = len(name2id)
rel_num = len(relation2id)

import torch
import numpy as np
from torch import nn
from torch_geometric.nn import RGCNConv
from torch.nn.parameter import Parameter
import torch.nn.functional as F
import math
from torch.nn.init import xavier_normal_, xavier_uniform_
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
import random
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
from scipy.optimize import linear_sum_assignment
from sklearn.metrics import accuracy_score

# ========== è®­ç»ƒ RGCN ==========
'''
class RGCN(nn.Module):
    def __init__(self, num_nodes, num_relations, hidden_dim=128, dropout=0.3):
        super(RGCN, self).__init__()
        self.embedding = nn.Embedding(num_nodes, hidden_dim)
        self.conv1 = RGCNConv(hidden_dim, hidden_dim, num_relations, num_bases=6)
        self.conv2 = RGCNConv(hidden_dim, hidden_dim, num_relations, num_bases=6)
        self.dropout = dropout

    def forward(self, edge_index, edge_type):
        x = self.embedding.weight
        x = self.conv1(x, edge_index, edge_type)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)  # ğŸ”¥ åŠ åœ¨ç¬¬ä¸€å±‚æ¿€æ´»å
        x = self.conv2(x, edge_index, edge_type)
        return x
'''

class RGCN(nn.Module):
    def __init__(self, num_nodes, num_relations,
                 hidden_dim=128, num_bases=6, dropout=0.3):
        super().__init__()

        # âš¡ 2.1  ç‰¹å¾å¯é€‰ï¼šé¢„åŠ è½½å±æ€§å‘é‡ / Glorot init æ›´ç¨³å®š
        self.embedding = nn.Embedding(num_nodes, hidden_dim)
        nn.init.xavier_uniform_(self.embedding.weight)

        # âš¡ 2.2  RGCNConv å±‚åŠ  bias=Falseï¼Œåé¢æ¥ LayerNorm
        self.conv1 = RGCNConv(hidden_dim, hidden_dim,
                              num_relations, num_bases=num_bases, bias=False)
        self.norm1 = nn.LayerNorm(hidden_dim)

        # âš¡ 2.3  ç¬¬äºŒå±‚ residualï¼Œé¿å…è¿‡å¹³æ»‘
        self.conv2 = RGCNConv(hidden_dim, hidden_dim,
                              num_relations, num_bases=num_bases, bias=False)
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.res_scale = nn.Parameter(torch.tensor(0.5))  # å¯å­¦ä¹ æ®‹å·®æ¯”ä¾‹

        self.dropout = dropout

    def forward(self, edge_index, edge_type, x=None):
        if x is None:
            x = self.embedding.weight  # é»˜è®¤ç”¨æ¨¡å‹å†…éƒ¨embedding
        h = self.conv1(x, edge_index, edge_type)
        h = self.norm1(h)
        h = F.relu(h)
        h = F.dropout(h, p=self.dropout, training=self.training)

        h2 = self.conv2(h, edge_index, edge_type)
        h2 = self.norm2(h2)

        out = h2 + self.res_scale * h
        return out

# æ‰€æœ‰å…³ç³»çš„[[headset],[tailset]]
edge_index = torch.tensor(edge_index, dtype=torch.long).t()      # shape: [2, num_edges]
# æ‰€æœ‰å…³ç³»çš„[rel_set]
edge_type = torch.LongTensor(edge_type)        # shape: [num_edges]

print(node_num)
print(edge_index.shape)
print(edge_type.shape)
print(len(relation2id))

model = RGCN(num_nodes=node_num, num_relations=len(relation2id), hidden_dim=128, dropout=0.3)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
temperature = 0.5  # Ï„ è¶…å‚æ•°

for epoch in range(100):
    model.train()
    out = model(edge_index, edge_type)  # shape: [num_nodes, hidden_dim]

    num_edges = edge_index.shape[1]
    loss = 0

    # ---------------- æ­£è¾¹ (çœŸå®é‚»æ¥è¾¹) ----------------
    src = edge_index[0]
    dst = edge_index[1]

    # ---------------- è´Ÿè¾¹ (éšæœºè´Ÿé‡‡æ ·) ----------------
    # éšæœºæ‰“ä¹± dst å¾—åˆ°è´Ÿä¾‹ï¼ˆä¹Ÿå¯ä»¥æ›´å¤æ‚è´Ÿé‡‡æ ·ï¼‰
    perm = torch.randperm(num_edges)
    neg_dst = dst[perm]

    # å–å‡ºåµŒå…¥å‘é‡
    h_src = out[src]
    h_pos = out[dst]
    h_neg = out[neg_dst]

    # è®¡ç®— cosine ç›¸ä¼¼åº¦
    sim_pos = F.cosine_similarity(h_src, h_pos, dim=1)   # shape: [num_edges]
    sim_neg = F.cosine_similarity(h_src, h_neg, dim=1)   # shape: [num_edges]

    # InfoNCE å¯¹æ¯”æŸå¤±
    logits = torch.exp(sim_pos / temperature) / (torch.exp(sim_pos / temperature) + torch.exp(sim_neg / temperature))
    loss = -torch.log(logits + 1e-8).mean()

    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch}: Loss = {loss.item():.4f}")

# è®¾å®š
model.eval()
with torch.no_grad():
    all_embeddings = model(edge_index, edge_type)  # è¾“å‡º shape: [num_entities, hidden_dim]

# å‡è®¾ä½ è¦å¯¹æŸä¸€ç±»å®ä½“è¿›è¡Œèšç±»ï¼ˆå·²çŸ¥å…¶ç´¢å¼•ï¼‰
selected_idx = torch.tensor(keys_with_pubno)  # æŒ‡å®šè¦èšç±»çš„å®ä½“ç´¢å¼•ï¼Œæ¯”å¦‚ä½œè€…ç±»ã€ä¸“åˆ©ç±»
selected_embeddings = all_embeddings[selected_idx]

# å‡è®¾ä½ æœ‰çœŸå®æ ‡ç­¾
y_true = np.array(y_true)  # shape: [num_selected_entities]

# ---------------- KMeans èšç±» ----------------
num_clusters = len(np.unique(y_true))
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
y_pred = kmeans.fit_predict(selected_embeddings.cpu().numpy())

# ---------------- t-SNE å¯è§†åŒ– ----------------
tsne = TSNE(n_components=2, random_state=42)
tsne_result = tsne.fit_transform(selected_embeddings.cpu().numpy())

plt.figure(figsize=(8, 6))
scatter = plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=y_pred, cmap='tab10', s=10)
plt.title("t-SNE Visualization of R-GCN Entity Embeddings")
plt.colorbar(scatter)
plt.show()

# ---------------- èšç±»è¯„ä¼° ----------------
def clustering_accuracy(y_true, y_pred):
    # ç”¨åŒˆç‰™åˆ©ç®—æ³•å¯¹èšç±»æ ‡ç­¾é‡æ–°ç¼–å·
    y_true = y_true.astype(np.int64)
    D = max(y_pred.max(), y_true.max()) + 1
    w = np.zeros((D, D), dtype=np.int64)
    for i in range(len(y_pred)):
        w[y_pred[i], y_true[i]] += 1
    row_ind, col_ind = linear_sum_assignment(w.max() - w)
    return sum([w[i, j] for i, j in zip(row_ind, col_ind)]) / len(y_pred)

acc = clustering_accuracy(y_true, y_pred)
nmi = normalized_mutual_info_score(y_true, y_pred)
ari = adjusted_rand_score(y_true, y_pred)

print(f"ACC: {acc:.4f}")
print(f"NMI: {nmi:.4f}")
print(f"ARI: {ari:.4f}")


def add_new_node_index_map(new_nodes_dict):
    global name2id
    global no2label

    for node_name, node_label in new_nodes_dict.items():
        if node_name not in name2id:
            new_id = len(name2id)
            name2id[node_name] = new_id
            no2label[new_id] = node_label

#KeywordIs, CLCIs, AuthorIs
def add_new_edges(new_edges):
    edge_index_new = []
    pass

def get_patent(edge_index_old, edge_index_new, edge_type_old, edge_type_new, num_new_nodes):
    global model, name2id, relation2id
    #name2id.setdefault(record["rel"], len(relation_to_id))  # ç¼–å·ä»0å¼€å§‹
    # 1.æ‹¼æ¥æ–°æ—§è¾¹
    # edge_index_old: æ—§è¾¹ç´¢å¼•
    # edge_index_new: æ–°è¾¹ç´¢å¼•
    edge_index = torch.cat([edge_index_old, edge_index_new], dim=1)
    edge_type = torch.cat([edge_type_old, edge_type_new], dim=0)

    # 2.æ›´æ–°ç½‘ç»œ
    # 2.1 æ›´æ–° embedding å±‚
    # æ›¿æ¢ embedding å±‚ï¼ˆä¿æŒæ—§å‚æ•°ä¸å˜ï¼Œæ–°å¢çš„éšæœºåˆå§‹åŒ–ï¼‰
    old_weight = model.embedding.weight.data
    num_old_nodes, hidden_dim = old_weight.size()

    new_embedding = nn.Embedding(num_new_nodes, hidden_dim)
    nn.init.xavier_uniform_(new_embedding.weight)
    new_embedding.weight.data[:num_old_nodes] = old_weight  # ä¿ç•™æ—§èŠ‚ç‚¹çš„å‚æ•°
    model.embedding = new_embedding.to(model.embedding.weight.device)

    # optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    model.train()

    for epoch in range(10):
        out = model(edge_index, edge_type)  # shape: [num_nodes, hidden_dim]

        num_edges = edge_index.shape[1]
        loss = 0

        # ---------------- æ­£è¾¹ (çœŸå®é‚»æ¥è¾¹) ----------------
        src = edge_index[0]
        dst = edge_index[1]

        # ---------------- è´Ÿè¾¹ (éšæœºè´Ÿé‡‡æ ·) ----------------
        # éšæœºæ‰“ä¹± dst å¾—åˆ°è´Ÿä¾‹ï¼ˆä¹Ÿå¯ä»¥æ›´å¤æ‚è´Ÿé‡‡æ ·ï¼‰
        perm = torch.randperm(num_edges)
        neg_dst = dst[perm]

        # å–å‡ºåµŒå…¥å‘é‡
        h_src = out[src]
        h_pos = out[dst]
        h_neg = out[neg_dst]

        # è®¡ç®— cosine ç›¸ä¼¼åº¦
        sim_pos = F.cosine_similarity(h_src, h_pos, dim=1)   # shape: [num_edges]
        sim_neg = F.cosine_similarity(h_src, h_neg, dim=1)   # shape: [num_edges]

        # InfoNCE å¯¹æ¯”æŸå¤±
        logits = torch.exp(sim_pos / temperature) / (torch.exp(sim_pos / temperature) + torch.exp(sim_neg / temperature))
        loss = -torch.log(logits + 1e-8).mean()

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        print(f"Epoch {epoch}: Loss = {loss.item():.4f}")

    return 

