<html xmlns:o='urn:schemas-microsoft-com:office:office' xmlns:x='urn:schemas-microsoft-com:office:excel' xmlns='http://www.w3.org/TR/REC-html40'><head><style>table td,th{vnd.ms-excel.numberformat:@;text-align: center;} table th{color:red}</style><title></title><meta http-equiv='Content-Type' content="text/html; charset=utf-8"></head><body><table cellspacing='0' border='1'>     <tr>   <td style='vnd.ms-excel.numberformat:@'>SrcDatabase-来源库</td>   <td style='vnd.ms-excel.numberformat:@'>Author-作者</td>   <td style='vnd.ms-excel.numberformat:@'>Applicant-申请人</td>   <td style='vnd.ms-excel.numberformat:@'>Title-题名</td>   <td style='vnd.ms-excel.numberformat:@'>CountryName-国省名称</td>   <td style='vnd.ms-excel.numberformat:@'>PubNo-公开号</td>   <td style='vnd.ms-excel.numberformat:@'>PubTime-公开日期</td>   <td style='vnd.ms-excel.numberformat:@'>Summary-摘要</td>   <td style='vnd.ms-excel.numberformat:@'>Claims-主权项</td>   <td style='vnd.ms-excel.numberformat:@'>CLC-中图分类号</td>  </tr>  <tr>   <td>中国专利</td>   <td>         邱建秀;              蔡霁初;              王大刚;              王振刚;                   陈建耀       </td>   <td>中山大学</td>   <td>一种基于土壤水分的森林火灾预测方法、装置及存储介质</td>   <td>广东省</td>   <td>CN112016744B</td>   <td>2021-06-01</td>   <td>本发明公开了一种基于土壤水分的森林火灾预测方法、装置及存储介质,所述方法包括：获取监测区域的观测数据；其中,所述观测数据包括不同时间尺度的土壤水分数据以及不同时间尺度的气象观测数据；根据预测时间尺度,选取对应的森林火灾预测模型；根据所述预测时间尺度以及所选取的森林火灾预测模型,从所述观测数据中提取对应的土壤水分数据和气象观测数据；将所提取的土壤水分数据和气象观测数据输入所选取的森林火灾预测模型中,以使所述森林火灾预测模型对所述监测区域发生森林火灾的情况进行预测。本发明技术方案能够提高森林火灾预测的准确性。</td>   <td>1.一种基于土壤水分的森林火灾预测方法,其特征在于,包括：获取监测区域的观测数据；其中,所述观测数据包括不同时间尺度的土壤水分数据以及不同时间尺度的气象观测数据；根据预测时间尺度,选取对应的森林火灾预测模型；根据所述预测时间尺度以及所选取的森林火灾预测模型,从所述观测数据中提取对应的土壤水分数据和气象观测数据；将所提取的土壤水分数据和气象观测数据输入所选取的森林火灾预测模型中,以使所述森林火灾预测模型对所述监测区域发生森林火灾的情况进行预测。</td>   <td>G06Q10/04;G06Q10/06;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;                   何智通       </td>   <td>中山大学</td>   <td>一种基于实例分割和图像修复的带遮挡行人重识别方法</td>   <td>广东省</td>   <td>CN112861785A</td>   <td>2021-05-28</td>   <td>本发明公开了一种基于实例分割和图像修复的带遮挡行人重识别方法,该方法包括：获取待查图像并对待查图像进行实例分割和图像修复处理,得到修复后行人图像；基于预训练的实例分割模型对行人图像库中的行人图像进行实例分割,得到分割后行人图像；分别对修复后行人图像和分割后行人图像进行特征提取并将提取的特征进行相似性度量,检索得到同一行人的其他图像。本发明方法通过对行人缺失部位进行检测并修复的方式,提供更大区域更加完整的行人信息,帮助后续网络获得更具判别性的特征表示,从而提高行人重识别效果。本发明作为一种基于实例分割和图像修复的带遮挡行人重识别方法,可广泛应用于行人重识别领域。</td>   <td>1.一种基于实例分割和图像修复的带遮挡行人重识别方法,其特征在于,包括以下步骤：获取待查图像并对待查图像进行实例分割和图像修复处理,得到修复后行人图像；基于预训练的实例分割模型对行人图像库中的行人图像进行实例分割,得到分割后行人图像；分别对修复后行人图像和分割后行人图像进行特征提取并将提取的特征进行相似性度量,检索得到同一行人的其他图像。</td>   <td>G06K9/00;G06K9/32;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨猛;              叶林彬;                   刘俊峰       </td>   <td>中山大学</td>   <td>一种基于内容特征和风格特征的人脸图像生成方法</td>   <td>广东省</td>   <td>CN112861805A</td>   <td>2021-05-28</td>   <td>本发明提供一种基于内容特征和风格特征的人脸图像生成方法,包括以下步骤：S1：获取人脸图像数据集,并构建双路径生成式对抗网络模型；S2：从原始域人脸图像中提取原始域的内容特征和风格特征,从目标域人脸图像中提取目标域的内容特征和风格特征；S3：通过对内容特征和风格特征进行监督学习建立特征关联损失函数；S4：根据特征关联损失函数建立双路径生成式对抗网络模型的价值函数；S5：通过对抗学习得到价值函数的全局最优解,从而得到优化好的双路径生成式对抗网络模型进行人脸图像生成。本发明提供一种基于内容特征和风格特征的人脸图像生成方法,解决了现有的人脸图像生成技术无法保证生成的人脸图像保持输入人脸图像的身份的问题。</td>   <td>1.一种基于内容特征和风格特征的人脸图像生成方法,其特征在于,包括以下步骤：S1：获取人脸图像数据集,并构建双路径生成式对抗网络模型,其中所述人脸图像数据集包括原始域人脸图像和目标域人脸图像；S2：利用双路径生成式对抗网络模型从原始域人脸图像中提取原始域的内容特征和风格特征,从目标域人脸图像中提取目标域的内容特征和风格特征；S3：通过对内容特征和风格特征进行监督学习建立特征关联损失函数；S4：根据特征关联损失函数建立双路径生成式对抗网络模型的价值函数；S5：通过对抗学习得到价值函数的全局最优解,从而得到优化好的双路径生成式对抗网络模型进行人脸图像生成。</td>   <td>G06K9/00;G06K9/32;G06K9/62;G06N20/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈川;              张梓旸;                   郑子彬       </td>   <td>中山大学</td>   <td>一种多标签图像分类方法、装置、设备和存储介质</td>   <td>广东省</td>   <td>CN112861941A</td>   <td>2021-05-28</td>   <td>本申请公开了一种多标签图像分类方法、装置、设备和存储介质,方法包括：响应于图像分类请求,获取待检测图像；对所述待检测图像进行特征提取得到图像特征；获取所述待检测图像对应的预置分类器,所述预置分类器是基于图神经网络对训练图像进行标签嵌入后得到的,其中,所述待检测图像和所述训练图像为同一场景图像；将所述图像特征和所述预置分类器融合,得到所述待检测图像的多标签分类结果。解决了现有的多标签分类方法在复杂场景和多个对象的输入图像上由于忽略对象之间的拓扑结构,导致分类结果准确度较差的技术问题。</td>   <td>1.一种多标签图像分类方法,其特征在于,包括：响应于图像分类请求,获取待检测图像；对所述待检测图像进行特征提取得到图像特征；获取所述待检测图像对应的预置分类器,所述预置分类器是基于图神经网络对训练图像进行标签嵌入后得到的,其中,所述待检测图像和所述训练图像为同一场景图像；将所述图像特征和所述预置分类器融合,得到所述待检测图像的多标签分类结果。</td>   <td>G06K9/62;G06K9/46;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         高成英;                   陈俊霖       </td>   <td>中山大学</td>   <td>一种基于混合模态输入的图像检索方法及装置</td>   <td>广东省</td>   <td>CN112861944A</td>   <td>2021-05-28</td>   <td>本发明公开了一种基于混合模态输入的图像检索方法及装置,所述方法首先将待检索混合模态图像输入至训练好的混合模态检索模型中,混合模态检索模型首先对待检索混合模态图像中各种模态的图像元素的原始特征向量进行提取,然后基于模态信息注意力子模型,将原始特征向量转化为混合模态关系图；继而通过图卷积网络子模型,将混合模态关系图投影到可比较的特征空间中,获得可比较的特征向量；最后根据可比较特征向量计算待检索混合模态图像与各个待比对图像的相似度分数；最终得到对应的目标图像,得到检索结果。通过实施本发明能够方便用户构造检索条件,提高图像检索的准确性。</td>   <td>1.一种基于混合模态输入的图像检索方法,其特征在于,包括：获取待检索混合模态图像；其中,所述待检索混合模态图像包括至少两种不同模态的图像元素；将所述待检索混合模态图像输入至预设的混合模态检索模型中,以使所述混合模态检索模型根据所述待检索混合模态图像从预设的各待比对图像中,选定与所述待检索混合模态图像对应的目标图像,获得检索结果；其中,所述根据所述待检索混合模态图像从预设的各待比对图像中,选定与所述待检索混合模态图像对应的目标图像,获得检索结果具体包括：提取所述待检索混合模态图像中每一图像元素的原始特征向量；将各所述图像元素的原始特征向量输入至模态信息注意子模型中,以使所述模型信息注意力子模型根据各所述图像元素的原始特征向量生成混合模态关系图；通过预设图卷积网络子模型,将所述混合模态关系图投影至特征空间生成每所述图像元素的可比较特征向量；根据各所述图像元素的可比较特征向量计算所述待检索混合模态图像与所述各待比对图像的相似度分数,继而根据所述相似度分数从所述各待比对图像中选定与所述待检索混合模态图像对应的目标图像,获得检索结果。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李尹健;                   卢宇彤       </td>   <td>中山大学</td>   <td>一种面向神经网络异步训练的学习率调整方法</td>   <td>广东省</td>   <td>CN112861991A</td>   <td>2021-05-28</td>   <td>本发明公开了一种面向神经网络异步训练的学习率调整方法,所述方法包括以下步骤：初始化参数；将神经网络的参数发送给所有空闲的计算节点；直到已经接收了c个计算结果；对于c个接收的计算梯度,分别调整它们的学习率；使用步骤S4中得到的学习率和接收的c个梯度,对网络进行一步梯度下降的更新；判断网络精度是否满足要求。若已达到要求,则完成训练,将回应2发送给所有计算节点,退出；否则回到步骤B,并将回应1发送给所有本轮计算完成的节点,进行下一轮循环的训练。本发明的有益效果在于,延迟梯度的学习率不再会随着数目的增多而线性上升,同时在计算时考虑到了当前接收的其它梯度的延迟情况和样本批量大小,将整体的学习率调整得更加平衡、更加科学。</td>   <td>1.一种面向神经网络异步训练的学习率调整方法,其特征在于,所述方法包括以下步骤：S1初始化参数；S2将神经网络的参数发送给所有空闲的计算节点：对在上一次循环中计算完成并已经提交了计算结果的所有节点,参数服务器将更新后的参数分别发给它们,让它们开始下一轮的计算；在此步骤后整体进入下一轮计算,当前轮次t-(glob)＝t-(glob)+1,将所有接收了最新网络的节点的轮次更新等于总体轮次t-i＝t-(glob)；S3等待任一节点计算完毕；接收其计算结果,反复执行此操作,直到已经接收了c个计算结果；S4对于c个接收的计算梯度,分别调整它们的学习率；S5使用步骤S4中得到的学习率和接收的c个梯度,对网络进行一步梯度下降的更新；S6判断网络精度是否满足要求。若已达到要求,则完成训练,将回应2发送给所有计算节点,退出；否则回到步骤B,并将回应1发送给所有本轮计算完成的节点,进行下一轮循环的训练。</td>   <td>G06K9/62;G06F17/16;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         洪楷铎;                   郑伟诗       </td>   <td>中山大学</td>   <td>基于模型无关元学习的无监督少样本图像分类方法、系统及存储介质</td>   <td>广东省</td>   <td>CN112861995A</td>   <td>2021-05-28</td>   <td>本发明公开了一种基于模型无关元学习的无监督少样本图像分类方法、系统及存储介质,包括下述步骤：生成训练数据,得到元训练集和元测试集；构造卷积神经网络模型,在卷积神经网络模型中引入快权重和慢权重,所述快权重在内部循环中进行迭代,所述慢权重在外部循环进行优化求解；利用元训练集和元测试集对优化后的卷积神经网络模型进行训练,得到优化后的神经网络模型；引入无监督相关损失,提高卷积神经网络模型分类效果；将待分类的图像输入到训练好的卷积神经网络,得到分类结果。本发明结合数据采样、数据增强、和无监督图像分类中的方法,提升模型无关原学习方法的训练速度,解决少样本学习无监督样本生成和计算效率的问题。</td>   <td>1.基于模型无关元学习的无监督少样本图像分类方法,其特征在于,包括下述步骤：生成训练数据,得到元训练集和元测试集；构造卷积神经网络模型,在卷积神经网络模型中引入快权重和慢权重,所述快权重在内部循环中进行迭代,所述慢权重在外部循环进行优化求解,所述卷积神经网络包括三部分,具体为：第一部分为神经网络的前面几层卷积层,用来提升较为通用的特征,称为卷积层前部,网络参数记为w-f,将w-f作为慢权重；第二部分为网络除卷积层前部外的卷积层,称为卷积层后部,参数记为w-b,w-b作为快权重和慢权重；第三部分为最后的全连接层参数记为w,w＝w-(bias)+w-(linear),其中w-(bias)是快权重,w-(linear)是慢权重；利用元训练集和元测试集对优化后的卷积神经网络模型进行训练,得到优化后的神经网络模型；引入无监督相关损失,提高卷积神经网络模型分类效果；将待分类的图像输入到训练好的卷积神经网络,得到分类结果。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨猛;                   钟琴       </td>   <td>中山大学</td>   <td>一种基于主动半监督字典学习的图像分类方法</td>   <td>广东省</td>   <td>CN112861999A</td>   <td>2021-05-28</td>   <td>本发明提供一种基于主动半监督字典学习的图像分类方法,包括以下步骤：S1：获取训练样本集,将训练样本集分类为有标签样本集和无标签样本集；S2：构建初始的半监督字典模型；S3：分别构建有标签样本集和无标签样本集的判别表示；S4：构建图像分类目标函数；S5：结合主动学习对图像分类目标函数进行更新,得到更新好的图像分类目标函数,将更新好的图像分类目标函数用于图像分类。本发明提供一种基于主动半监督字典学习的图像分类方法,通过整合主动学习机制使得图像分类目标函数的更新更加高效并且更适用于现实场景,能够有效地利用大量无标签样本,解决了目前的图像分类方法还无法有效地利用大量无标签样本的问题。</td>   <td>1.一种基于主动半监督字典学习的图像分类方法,其特征在于,包括以下步骤：S1：获取训练样本集,将训练样本集分类为有标签样本集和无标签样本集；S2：根据有标签样本集和无标签样本集构建初始的半监督字典模型；S3：根据半监督字典模型分别构建有标签样本集和无标签样本集的判别表示；S4：根据有标签样本集和无标签样本集的判别表示构建图像分类目标函数；S5：结合主动学习对图像分类目标函数进行更新,得到更新好的图像分类目标函数,将更新好的图像分类目标函数用于图像分类。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨猛;                   黄俊凯       </td>   <td>中山大学</td>   <td>一种基于协同表示的样本不平衡分类方法</td>   <td>广东省</td>   <td>CN112862000A</td>   <td>2021-05-28</td>   <td>本发明提供一种基于协同表示的样本不平衡分类方法,一种基于协同表示的样本不平衡分类方法,包括以下步骤：S1：获取样本不平衡的数据集；S2：在样本不平衡的数据集中选择方差最大的类作为基类,并计算各类别的权重；S3：根据各类别的权重构建基于协同表示带权重的分类器；S4：通过基于协同表示带权重的分类器对样本不平衡的数据集中的数据进行分类。本发明提供一种基于协同表示的样本不平衡分类方法,通过基于协同表示带权重的分类器对样本不平衡的数据集进行数据分类,能够更好地建模样本间的分布信息,更充分地利用数据分布的信息,解决了目前分类器在样本不平衡的分类任务中并未充分利用数据分布的信息的问题。</td>   <td>1.一种基于协同表示的样本不平衡分类方法,其特征在于,包括以下步骤：S1：获取样本不平衡的数据集；S2：在样本不平衡的数据集中选择方差最大的类作为基类,并计算各类别的权重；S3：根据各类别的权重构建基于协同表示带权重的分类器；S4：通过基于协同表示带权重的分类器对样本不平衡的数据集中的数据进行分类。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         田海博;                   梁岫琪       </td>   <td>中山大学</td>   <td>一种隐私保护下的去中心化数据建模方法</td>   <td>广东省</td>   <td>CN112862001A</td>   <td>2021-05-28</td>   <td>本发明涉及一种隐私保护下的去中心化数据建模方法。包括：S1.初始化与本地模型训练：从服务器处获取模型,参与方利用本地数据输入模型进行训练,进而得到本地的模型参数；S2.模型参数加密发送与回传：参与方在本地完成模型参数加密后进行参数发送,服务器将聚合后的模型参数返还给参与方,各参与方用新的模型参数基于全同态加密运算和本地数据,开始新一轮的训练；S3.迭代停止：重复上述步骤直到全局模型参数[W]收敛。本发明解决了当前机器学习领域存在的数据孤岛和隐私泄露等问题；充分发挥各参与方数据的优势,降低数据中心化存储的风险,降低参与方数据隐私泄露的可能性,提升多方机器学习的安全性。</td>   <td>1.一种隐私保护下的去中心化数据建模方法,其特征在于,包括以下步骤：S1.初始化与本地模型训练：从服务器处获取模型,参与方利用本地数据输入模型进行训练,进而得到本地的模型参数；S2.模型参数加密发送与回传：参与方在本地完成模型参数加密后进行参数发送,服务器将聚合后的模型参数返还给参与方,各参与方用新的模型参数基于全同态加密运算和本地数据,开始新一轮的训练；S3.迭代停止：重复上述步骤直到全局模型参数[W]收敛。</td>   <td>G06K9/62;G06F21/60;G06F21/62;G06F21/64</td>  </tr>        <tr>   <td>中国专利</td>   <td>         吴嘉婧;              夏一钧;              刘洁利;                   郑子彬       </td>   <td>中山大学</td>   <td>一种图神经网络信息增强方法、装置及设备</td>   <td>广东省</td>   <td>CN112862003A</td>   <td>2021-05-28</td>   <td>本申请公开了一种图神经网络信息增强方法、装置及设备,方法包括：根据预置节点特征数据和预置节点邻接矩阵识别不同节点参与的网络模体；基于网络模体,构建每个节点对应的初始模体特征向量；将初始模体特征向量输入预置LSTM模型中进行聚合处理,得到不同节点的自身模体特征向量,预置LSTM模型包括注意力机制；根据自身模体特征向量进行邻居节点模体向量的聚合操作,得到每个节点的目标模体特征向量；融合目标模体特征向量和初始节点特征向量,得到增强特征向量。本申请能够解决现有技术忽略了网络节点之间因交互模式不同产生的丰富结构信息,导致图神经网络学习的特征信息缺乏代表性的技术问题。</td>   <td>1.一种图神经网络信息增强方法,其特征在于,包括：根据预置节点特征数据和预置节点邻接矩阵识别不同节点参与的网络模体；基于所述网络模体,构建每个节点对应的初始模体特征向量；将所述初始模体特征向量输入预置LSTM模型中进行聚合处理,得到不同节点的自身模体特征向量,所述预置LSTM模型包括注意力机制；根据所述自身模体特征向量进行邻居节点模体向量的聚合操作,得到每个节点的目标模体特征向量；融合所述目标模体特征向量和初始节点特征向量,得到增强特征向量。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         彭溧阳;              黎健龙;              唐劲驰;                   胡延庆       </td>   <td>中山大学</td>   <td>一种基于神经网络多模型结合的虫口密度预测方法及系统</td>   <td>广东省</td>   <td>CN112862168A</td>   <td>2021-05-28</td>   <td>本发明公开了一种基于神经网络多模型结合的虫口密度预测方法及系统,该方法包括：监测茶小绿叶蝉数据和气候数据并进行数据处理和数据划分,得到训练数据集和验证数据集；基于训练数据集和验证数据集得到训练完成的多个模型；按照预设的权重系数对模型进行集成,得到集成模型；获取一段时间内的茶小绿叶蝉数据和气候数据,经过处理后输入到集成模型,输出茶小绿叶蝉虫口密度的预测结果。该系统包括：数据处理模块、训练模块、集成模块和预测模块。通过使用本发明,能够准确预测茶小绿叶蝉虫口密度,从而预测虫害发生情况。本发明作为一种基于神经网络多模型结合的虫口密度预测方法及系统,可广泛应用于虫害预测领域。</td>   <td>1.一种基于神经网络多模型结合的虫口密度预测方法,其特征在于,包括以下步骤：监测茶小绿叶蝉数据和气候数据并进行数据处理和数据划分,得到训练数据集和验证数据集；基于训练数据集和验证数据集分别对预构建的BP神经网络模型、卷积神经网络模型和长短期记忆模型进行训练和参数调优,得到训练完成的BP神经网络模型、卷积神经网络模型和长短期记忆模型；按照预设的权重系数对训练完成的BP神经网络模型、卷积神经网络模型和长短期记忆模型进行集成,得到集成模型；获取一段时间内的茶小绿叶蝉数据和气候数据,经过处理后输入到集成模型,输出茶小绿叶蝉虫口密度的预测结果。</td>   <td>G06Q10/04;G06K9/62;G06N3/04;G06N3/08;G06Q50/02</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑子彬;              钟志杰;                   陈伟利       </td>   <td>中山大学</td>   <td>一种智能庞氏骗局检测方法、装置、终端及存储介质</td>   <td>广东省</td>   <td>CN112862493A</td>   <td>2021-05-28</td>   <td>本申请公开了一种智能庞氏骗局检测方法、装置、终端及存储介质,本申请通过将庞氏骗局合约的检测以及洗钱模式账户关系的检测结合起来,对两次检测结果进行迭代交叉检测,提升了检测准确度,使最终结果更加可靠,解决了目前的智能庞氏骗局检测技术检测的准确率不稳定的技术问题。</td>   <td>1.一种智能庞氏骗局检测方法,其特征在于,包括：S1、获取待检测的目标智能合约,并提取所述目标智能合约的合约特征,所述合约特征包括：字节码特征和创建者账户特征；S2、将所述合约特征作为智能庞氏骗局合约检测模型的输入变量,以通过所述智能庞氏骗局合约检测模型的运算,获得所述智能庞氏骗局合约检测模型输出的第一检测结果；S3、根据获取到的区块链交易记录以及已知骗局账户,计算各个账户与所述已知骗局账户之间的关联程度值,并根据所述关联程度值、区块链交易记录以及已知骗局账户,结合非法交易分类模型,得到洗钱关系账户集,其中所述关联程度值为根据所述账户与所述已知骗局账户之间的距离值换算得到的；S4、将洗钱嫌疑账户的创建者账户特征与所述合约特征作为所述智能庞氏骗局合约检测模型的输入变量,以通过所述智能庞氏骗局合约检测模型的运算,获得所述智能庞氏骗局合约检测模型输出的第二检测结果,其中,所述洗钱嫌疑账户包括：所述洗钱关系账户集内的洗钱关系账户,和/或,与所述洗钱关系账户相邻的账户；S5、若所述第一检测结果与所述第二检测结果一致,则输出所述目标智能合约的智能庞氏骗局检测结果,若所述第一检测结果与所述第二检测结果不一致,则根据所述第二检测结果更新所述洗钱关系账户集,然后用当前的第二检测结果作为新的第一检测结果,然后返回步骤S4,以便根据更新后洗钱关系账户集获得新的第二检测结果。</td>   <td>G06Q20/40;G06F21/56</td>  </tr>        <tr>   <td>中国专利</td>   <td>         梁爽;              倪江群;                   刘庆亮       </td>   <td>中山大学</td>   <td>一种基于通道空间注意力机制的JPEG图像隐写分析方法</td>   <td>广东省</td>   <td>CN112862655A</td>   <td>2021-05-28</td>   <td>本发明提供一种基于通道空间注意力机制的JPEG图像隐写分析方法,包括以下步骤：对待检测JPEG图像进行解压；构建基于通道空间注意力机制的卷积神经网络并进行训练；将解压后的待检测JPEG图像输入卷积神经网络中进行分类,计算分类概率向量；根据分类概率向量判定待检测JPEG图像是否为载密图像,完成JPEG图像隐写分析。本发明提供一种基于通道空间注意力机制的JPEG图像隐写分析方法,基于通道空间注意力机制,设计了一种JPEG图像隐写分析方法,有效突出对隐写分析有用的残差特征,提高隐写分析信号的信噪比；同时,本方法可接受任意尺寸JPEG图像的输入,可以更多地保留图像的特征信息,有效提升检测的准确率。</td>   <td>1.一种基于通道空间注意力机制的JPEG图像隐写分析方法,其特征在于,包括以下步骤：S1：对待检测JPEG图像进行解压；S2：构建基于通道空间注意力机制的卷积神经网络并进行训练；S3：将解压后的待检测JPEG图像输入卷积神经网络中进行分类,计算分类概率向量；S4：根据分类概率向量判定待检测JPEG图像是否为载密图像,完成JPEG图像隐写分析。</td>   <td>G06T1/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         康显桂;              王立华;                   尹雪梅       </td>   <td>中山大学</td>   <td>一种抗打印扫描拍摄的数字图像水印方法</td>   <td>广东省</td>   <td>CN112862656A</td>   <td>2021-05-28</td>   <td>本发明提出一种抗打印扫描拍摄的数字图像水印方法,解决了如何抵抗几何变换、打印扫描及拍摄对数字图像水印的不利影响,兼顾水印提取与水印图像主观质量的问题,数字图像水印嵌入过程基于原始图像的Zernike矩,通过量化嵌入的方法进行水印嵌入,根据水印嵌入前后的Zernike矩重构图像,对重构图像的差值采用选择性区域截断的方式,叠加到原始图像上生成水印图像,提升了水印图像的主观质量,基于Zernike矩变换,设计了一种匹配的数字图像水印提取算法,其中包含图像自动定位方法,提取水印过程可以抵抗旋转、缩放、噪声、压缩等带来的影响,从而保证了水印信息的有效提取。</td>   <td>1.一种抗打印扫描拍摄的数字图像水印方法,其特征在于,所述方法包括数字图像水印嵌入过程及数字图像水印提取过程,所述数字图像水印嵌入过程首先确定待嵌入水印的原始图像,基于原始图像的Zernike矩,通过量化嵌入的方法进行水印嵌入,根据水印嵌入前后的Zernike矩重构图像,对重构图像的差值采用选择性区域截断的方式,叠加到原始图像上生成水印图像；所述数字图像水印提取过程配合数字图像水印嵌入过程,将数字图像水印嵌入过程生成的水印图像经打印扫描/拍摄后作为待测图像,基于Zernike矩变换,进行水印提取。</td>   <td>G06T1/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;              全小虎;              丁颜玉;              李仕仁;                   谭德志       </td>   <td>广州智慧城市发展研究院;中山大学</td>   <td>一种基于区块链网络下的区块实现方法及系统</td>   <td>广东省</td>   <td>CN109272316B</td>   <td>2021-05-25</td>   <td>本发明实施例公开了一种基于区块链网络下的区块实现方法及系统,区块链系统与应用端建立交易节点和共识节点,所述方法包括如下步骤：所述区块链系统控制着交易节点在非交付共识下与应用端实现交易互动,并控制着共识节点在与应用端存在交付共识时产生区块,并将所产生的区块发送给交易节点,所述交易节点将所述区块存储在本地账本中。在本发明实施例中,将负责交易的节点和负责共识的节点区分开来,交易节点与应用紧紧相关,而共识节点只需获得应用的背书交易,然后独立的运行复杂的共识环节,可以使得共识节点独立采用算力大的终端,减少交易直接相关节点的负担,从而提升整个区块链的效率。</td>   <td>1.一种基于区块链网络下的区块实现方法,其特征在于,区块链系统与应用端建立交易节点和共识节点,所述方法包括如下步骤：所述区块链系统控制着交易节点在非交付共识下与应用端实现交易互动,并控制着共识节点在与应用端存在交付共识时产生区块,并将所产生的区块发送给交易节点,所述交易节点将所述区块存储在本地账本中；所述共识节点采用SM2加密算法进行签名验签的安全性进行加密处理、SM3杂凑算法进行非对称加密；所述采用SM2加密算法进行签名验签的安全性进行加密处理包括：建立连续椭圆曲线方程；任意取椭圆曲线上两点P、Q,作直线交于椭圆曲线的另一点R',过R'做y轴的平行线交于R,定义P+Q＝R；将椭圆曲线离散化到有限素数域Fp,其中p为质数,Fp中有p个元素；对于上述描述的有限素数域Fp,首先用随机数发生器产生一个随机整数；通过计算多倍点P＝d*G得到密钥对(d,P)其中d为私钥,P为公钥；所述区块链系统控制着交易节点在非交付共识下与应用端实现交易互动,并控制着共识节点在与应用端存在交付共识时产生区块,并将所产生的区块发送给交易节点,所述交易节点将所述区块存储在本地账本中包括：所述应用端向交易节点提出调用智能合约的请求,并发送自己产生的交易；交易节点调用智能合约上的接口,在本地账本的基础上对于应用端发送来的交易进行背书,然后将背书后的交易返回给应用端；应用端将背书后的交易发送给共识节点,共识节点将收集到的交易进行共识产生区块,并发送给区块链系统上的交易节点；所述交易节点对所述区块进行验证,并将区块更新到自己的本地账本中。</td>   <td>G06Q20/38</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;                   叫洁宁       </td>   <td>中山大学</td>   <td>基于超分辨图像生成的低分辨率行人重识别系统和方法</td>   <td>广东省</td>   <td>CN109993072B</td>   <td>2021-05-25</td>   <td>本发明公开了一种基于超分辨图像生成的低分辨率行人重识别系统和方法,该系统包括行人属性引导的超分辨图像生成网络模型和行人重识别网络模型,该方法步骤为：选取高、低分辨率图像样本及行人属性向量；训练超分辨率图像生成网络模型；训练行人重识别网络模型；联合训练超分辨图像生成网络模型和行人重识别网络模型；将低分辨率行人图像测试集与对应的行人属性向量输入到联合训练后的超分辨图像生成网络模型和行人重识别网络模型提取行人图像特征；计算行人图像特征的余弦相似度,根据余弦相似度得到不同分辨率的行人图像匹配的结果,本发明实现低分辨率图像细节恢复,同时扩大了网络的容量,提高低分辨率行人重识别的效果。</td>   <td>1.一种基于超分辨图像生成的低分辨率行人重识别方法,其特征在于,包括下述步骤：S1：选取高分辨率行人图像样本h、低分辨率行人图像样本l,选取与行人图像对应的行人属性向量z、与行人图像不对应的行人属性向量S2：训练行人属性引导的超分辨率图像生成网络模型：采用条件循环对抗生成网络构建超分辨率图像生成网络,高分辨率图像h经过第一生成器G-(h→l)生成低分辨率图像低分辨率图像与行人属性向量经过第二生成器G-(l→h)得到重建的高分辨率图像低分辨率图像l与行人属性向量经过第二生成器G-(l→h)生成高分辨率图像经过第一生成器G-(h→l)得到重建的低分辨率行人图像低分辨率图像l和生成的低分辨率图像经过第一判别器D-(h→l)得到判定为真实低分辨率图像的概率分别为：ρ-r＝D-(h→l)(l)和高分辨率图像h和行人属性向量z经过第二判别器D-(l→h)得到概率值：s-r＝D-(l→h)(h,z)；生成的高分辨率图像和行人属性向量z进行通道拼接后,经过第二判别器D-(l→h)得到概率值：高分辨率图像h和与图像不相符的行人属性向量经过第二判别器D-(l→h)得到概率值：第一判别器D-(h→l)的损失函数为：第二判别器D-(l→h)的损失函数为：                  根据对抗生成网络构建判别器损失函数L-D：第一生成器G-(h→l)的损失函数为：第二生成器G-(l→h)的损失函数为：循环对抗生成网络的重构损失函数L-c为：其中,λ-1、λ-2为损失函数的权重值；根据循环对抗生成网络的重构损失函数L-c构建生成器损失函数L-G：                  交替迭代更新超分辨率图像生成网络模型参数,输出超分辨行人图像；S3：训练行人重识别网络模型：采用超分辨行人图像和高分辨率的行人图像输入行人重识别网络,行人重识别网络采用残差卷积神经网络结构提取图像中的行人特征,分类器将超分辨行人图像和高分辨率的行人图像中的真实行人特征区分；S4：联合训练超分辨图像生成网络模型和行人重识别网络模型：行人重识别网络训练中的参数回传到超分辨图像生成网络模型,更新超分辨图像生成网络参数,采用交替迭代更新参数,先更新判别器的参数,同时更新生成器和行人重识别网络的参数；S5：选取低分辨率行人图像测试集、高分辨率行人图像测试集,低分辨率行人图像测试集与对应的行人属性向量z输入到联合训练后的超分辨率图像生成网络,生成高分辨率行人图像,生成的高分辨率图像与高分辨率行人图像测试集分别通过联合训练后的行人重识别模型提取行人图像特征；S6：计算行人图像特征的余弦相似度,余弦相似度越大的行人图像的相似程度越低,根据相似程度得到不同分辨率的行人图像匹配的结果。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              张穗安;                   卢泽丰       </td>   <td>中山大学</td>   <td>一种基于聚类生成伪标签的无监督行人重识别方法及系统</td>   <td>广东省</td>   <td>CN112836675A</td>   <td>2021-05-25</td>   <td>本发明公开了一种基于聚类生成伪标签的无监督行人重识别方法及系统,该方法包括：构建浅层网络A、深层网络B和浅层网络C并对网络进行训练,得到训练完成的浅层网络C；基于训练完成的浅层网络C对待查行人图像和数据库行人图像进行特征提取并计算特征距离,得到相似度列表。该系统包括：训练模块和识别模块。通过使用本发明,能够很好的减少伪标签噪声带来的对无监督行人重识别的影响,提高识别性能。本发明作为一种基于聚类生成伪标签的无监督行人重识别方法及系统,可广泛应用于行人重识别领域。</td>   <td>1.一种基于聚类生成伪标签的无监督行人重识别方法,其特征在于,包括以下步骤：获取带标签的行人图像训练集并对特征提取网络进行初步训练,得到浅层网络A和深层网络B；将行人图像经过数据增强后分别输入到浅层网络A和深层网络B,得到对应的特征向量f-A和f-B；将特征向量f-A和f-B进行拼接融合,得到融合后特征向量f-(cat)；基于融合后特征向量f-(cat)对行人图像数据集的特征向量进行聚类,并为行人图像数据集中的行人图像赋予伪标签；基于伪标签对浅层网络A和深层网络B进行训练并根据浅层网络A的参数对浅层网络C进行更新,得到训练完成的浅层网络C；将待查行人图像和数据库行人图像输入到训练完成的浅层网络C,得到待查行人图像特征和数据库行人图像特征集合；对待查行人图像特征和数据库行人图像特征集合进行特征距离计算,得到相似度列表。</td>   <td>G06K9/00;G06K9/46;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         卢宇彤;                   邓雷       </td>   <td>中山大学</td>   <td>一种优化的随机森林处理不平衡数据集的方法</td>   <td>广东省</td>   <td>CN112836735A</td>   <td>2021-05-25</td>   <td>本发明公开了一种优化的随机森林处理不平衡数据集的方法,所述方法包括数据预处理、随机森林模型的构建和分类预测,其中,所述数据预处理部分将找出少数类样本最近邻的k个多数类样本,组成难区分的区域,将这个区域的样本在原始数据集中进行重标签,且在难区分的区域进行少数类样本的生成,将重标签后的原始数据以及新增样本后的难区分区域作为不同的训练集输出；所述随机森林模型的构建将经过所述数据预处理部分处理的2个数据集作为模型的训练集,得到两个随机森林模型,所述分类预测将分两个阶段进入到所述的两个随机森林模型进行验证,最后获得样本的分类预测结果。本发明达到对少数类预测性能提升的同时,对多数类的预测正确率不会下降严重的目的。</td>   <td>1.一种优化的随机森林处理不平衡数据集的方法,其特征在于,所述方法包括数据预处理、随机森林模型的构建和分类预测,其中,所述数据预处理将找出少数类样本最近邻的k个多数类样本,组成难区分的区域,将这个区域的样本在原始数据集中进行重标签,且在难区分的区域进行少数类样本的生成,将重标签后的原始数据以及新增样本后的难区分区域作为不同的训练集输出；所述随机森林模型的构建将经过所述数据预处理部分处理的2个数据集作为模型的训练集,得到两个随机森林模型；所述分类预测将分两个阶段进入到所述的两个随机森林模型进行验证,最后获得样本的分类预测结果。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         高成英;                   吴星       </td>   <td>中山大学</td>   <td>一种面向目标分类系统的通用目标攻击方法及装置</td>   <td>广东省</td>   <td>CN112836764A</td>   <td>2021-05-25</td>   <td>本发明公开了一种面向目标分类系统的通用目标攻击方法及装置。所述方法,包括步骤：S1、对随机生成的噪音矩阵进行初始化,得到初始噪音矩阵；S2、将所述初始噪音矩阵叠加至自然图像,得到伪装图像,并通过标签误导方法和特征误导方法误导所述目标分类系统,得到误导后的目标分类系统,以获取所述误导后的目标分类系统对所述伪装图像的分类结果；S3、对所述初始噪音矩阵进行更新得到更新噪音矩阵,并将所述初始噪音矩阵更新为所述更新噪音矩阵；S4、迭代执行步骤S2～S3直至满足预设停止条件,获取当前所述初始噪音矩阵和/或当前所述伪装图像。本发明能够实现定向攻击目标分类系统,获取具有最佳攻击效果的噪音矩阵和/或伪装图像。</td>   <td>1.一种面向目标分类系统的通用目标攻击方法,其特征在于,包括步骤：S1、对随机生成的噪音矩阵进行初始化,得到初始噪音矩阵；S2、将所述初始噪音矩阵叠加至自然图像,得到伪装图像,并通过标签误导方法和特征误导方法误导所述目标分类系统,得到误导后的目标分类系统,以获取所述误导后的目标分类系统对所述伪装图像的分类结果；S3、对所述初始噪音矩阵进行更新得到更新噪音矩阵,并将所述初始噪音矩阵更新为所述更新噪音矩阵；S4、迭代执行步骤S2～S3直至满足预设停止条件,获取当前所述初始噪音矩阵和/或当前所述伪装图像。</td>   <td>G06K9/62;G06F17/16;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈旭;              黄广敬;                   周知       </td>   <td>中山大学</td>   <td>基于博弈论的自组织式联邦学习方法</td>   <td>广东省</td>   <td>CN112836828A</td>   <td>2021-05-25</td>   <td>本发明公开了一种基于博弈论的自组织式联邦学习方法。在网络中,移动用户设备或边缘设备可以通过使用联邦学习,以一种知识共享并且保护自身数据隐私的方式,共同构建一个性能优异人工智能模型。通过在中心服务器上部署方法协调自组织式联邦学习的进行,从而实现一个稳定,相对公平,高效的由自组织式的联邦学习。本发明首先对用户对模型的偏好和训练成本进行刻画,对用户进行行为分析,然后使用博弈论的方法计算纳什均衡点,使得每个用户在合作中达到策略上的共识。在达到共识的基础上引入阈值机制,保证一定的公平性。最后提出快速寻找近似最优的阈值的方法。</td>   <td>1.一种基于博弈论的自组织式联邦学习方法,其特征在于,包括以下步骤：S1.用户基本信息收集,首先由网络中一群用户组成一个利益社群,在社群里的用户拥有相同的模型任务需求,并且向中心服务器发起联邦学习请求,每个用户将自身的基本信息,包括对模型的偏好和训练成本信息发给中心服务器,中心服务器作为中立权威的协调者,协调各个用户策略以保证联邦学习的进行；S2.中心服务器进行利益协调,由中心服务器根据用户信息,设定一个阈值,阈值要求每个用户至少提供最低阈值的训练数据量参与模型训练,否则不允许用户参与到自组织式联邦学习中来；中心服务器根据阈值为每个用户计算策略,使得每个用户接受该均衡策略,并且不再作任何改变；即,为每个用户计算一个策略,使得每个用户无法从单方面改变自身的策略来提升自身收益,从而达到纳什均衡；S3.中心服务器快速寻找近似最优阈值,寻找最优阈值使得用户经过中心服务器利益协调步骤后,集群用户总收益最大；S4.联邦学习模型训练,每个用户接收来自中心服务器的策略建议,确定是否接受该策略建议,一旦所有用户达成策略共识并且接受该建议,模型训练开始,所有用户依据该策略建议来进行联邦学习模型建模。</td>   <td>G06N20/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林琪璇;              李愚;              邵嘉桢;              覃荣琛;                   徐政基       </td>   <td>中山大学</td>   <td>一种提高偏振红外热成像仪图像质量的方法及系统</td>   <td>广东省</td>   <td>CN112837312A</td>   <td>2021-05-25</td>   <td>本发明公开了一种提高偏振红外热成像仪图像质量的方法及系统,该方法包括：基于商品级红外热成像仪和偏振片拍摄不同偏振方向的图像；对不同偏振方向的图像进行融合,得到融合图像；基于图像质量评价函数评价融合前后的图像并定量判断图像质量的提升结果。该系统包括：拍摄模块、融合模块和评价模块。通过使用本发明,利用偏振成像技术,通过图像融合对红外热成像仪的图像进行修正,从而在不大幅度提升成本的情况下提高图像质量。本发明作为一种提高偏振红外热成像仪图像质量的方法及系统,可广泛应用于图像质量提升技术领域。</td>   <td>1.一种提高偏振红外热成像仪图像质量的方法,其特征在于,包括以下步骤：基于商品级红外热成像仪和偏振片拍摄不同偏振方向的图像；对不同偏振方向的图像进行融合,得到融合图像；基于图像质量评价函数评价融合前后的图像并定量判断图像质量的提升结果。</td>   <td>G06T7/00;G06T5/30;G06T5/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈树超;              陈洪波;              刘立志;              黎浩江;              徐绍凯;              傅嘉文;                   朱志华       </td>   <td>桂林电子科技大学;中山大学</td>   <td>一种胸部CT图像中的食管癌分割方法</td>   <td>广西壮族自治区</td>   <td>CN108596884B</td>   <td>2021-05-18</td>   <td>本发明公开了一种胸部CT图像中的食管癌分割方法,首先选取多组含有食管癌的CT图像,将含有食管癌的CT图像作为训练样本；对选取的CT图像进行预处理,获取食管癌特征,并进行特征描述后,得到的图像作为训练数据；建立基于全卷积神经网络的食管癌语义分割模型,将描述后的食管癌特征作为全卷积神经网络的特征输入作为学习样本进行训练,得到食管癌分割网络模型；食管癌的三维重建,对得到的食管癌分割网络模型所得到的食管癌分割结果进行三维重建和分析,得到三维空间下的食管癌影像组学参数；将得到的食管癌影像组学参数进行可视化显示。该方法模型规模小,速度快,准确度高。</td>   <td>1.一种胸部CT图像中的食管癌分割方法,其特征在于,具体包括如下步骤：1)选取多组含有食管癌的CT图像,将含有食管癌的CT图像作为训练样本,并作为步骤3)的数据范围；2)对步骤1)选取的CT图像进行预处理,获取食管癌特征,并进行特征描述后,得到的图像作为步骤3)的训练数据；3)建立基于全卷积神经网络的食管癌语义分割模型,将步骤2)描述后的食管癌特征作为全卷积神经网络的特征输入,作为学习样本进行训练,得到食管癌分割网络模型；4)食管癌的三维重建,对步骤3)得到的食管癌分割网络模型所得到的食管癌分割结果进行三维重建和分析,得到三维空间下的食管癌影像组学参数,参数为步骤5)做准备；5)将步骤4)得到的食管癌影像组学参数进行可视化显示。</td>   <td>G06T7/00;G06T7/11;G06T17/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈洪波;              任帅;              林润;              徐绍凯;                   黄勇慧       </td>   <td>桂林电子科技大学;中山大学</td>   <td>一种基于动态MRI信息融合的肝癌定量分析方法</td>   <td>广西壮族自治区</td>   <td>CN109461139B</td>   <td>2021-05-18</td>   <td>本发明公开了一种基于动态MRI信息融合的肝癌定量分析方法,该方法结合机器学习方法,利用XGboost模型算法,进行自学习,自优化,构建活性肝癌细胞识别最优模型,优化了活性肝癌区域的检测手段,为医生对肝癌的TACE治疗疗效进行评估提供更为精准的临床信息,从而为肝癌的精准治疗方案的制定提供技术支持和可靠依据。</td>   <td>1.一种基于动态MRI信息融合的肝癌定量分析方法,其特征在于,具体包括如下步骤：1)对采集到的肝癌患者的DCMRI图像,采用概率图谱与形状模型相结合的方法,同时引入适当的肝脏图像的特征,构造目标函数,实现对肝脏DCMRI图像的准确而且快速的分割,从DCMRI图像中分割得到肝脏图像区域；2)提取肝脏图像区域内各体素在DCMRI图像中的信号变化特征信息,作为活性肿瘤的识别特征；3)利用XGBoost模型算法构建活性肝癌区域识别模型,将步骤2)中提取的各体素的DCMRI特征信息输入该识别模型中进行识别；4)用训练集数据来对步骤3)构建的模型进行训练,并采用测试集数据对模型进行测试,获得模型的最优参数；5)利用训练好的识别模型对TACE治疗肝癌的疗效进行评估,得到肝脏活性肿瘤区域,并进行定量分析和三维显示,为制定肝癌进一步治疗方案以及预后预测提供精确可靠的信息。</td>   <td>G06T7/00;G06T7/11;G06T7/33;G16H30/20;G16H50/20;G16H50/30</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谢舜道;                   谭洪舟       </td>   <td>中山大学</td>   <td>一种模糊可识别二维码及其生成、识别方法</td>   <td>广东省</td>   <td>CN112819120A</td>   <td>2021-05-18</td>   <td>本发明提供一种模糊可识别二维码,包括定位图案、格式信息和数据；所述定位图案由模糊不变形状组成；所述格式信息存储于由模糊不变形状组成的定位图案的模糊不变特征中；所述定位图案间设置有数据带,所述数据存储于所述数据带上。本发明还提供一种模糊可识别二维码的生成、识别方法,其生成的模糊可识别二维码可以在模糊图像中直接被识别,不需要去模糊等预处理操作,简化识别流程,加快在模糊图像中的识别速度且在模糊图像中,利用模糊不变图形可以被快速识别的特点,可以快速定位到该二维码；同时,由于格式信息的存储使用了模糊不变特征,模糊并不能破坏这些特征,因此即使在严重模糊的图像中,仍然可以正确识别该二维码。</td>   <td>1.一种模糊可识别二维码,其特征在于,包括定位图案、格式信息和数据；其中：所述定位图案由模糊不变形状组成；所述格式信息存储于由模糊不变形状组成的定位图案的模糊不变特征中；所述定位图案间设置有数据带,所述数据存储于所述数据带上。</td>   <td>G06K19/06;G06K7/14</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张益强;              袁均良;              骆伟祺;                   叶玮材       </td>   <td>中山大学</td>   <td>一种面向微服务的金融回测容错系统及方法</td>   <td>广东省</td>   <td>CN112819640A</td>   <td>2021-05-18</td>   <td>本发明提出一种面向微服务的金融回测容错系统及方法,涉及金融量化回测的技术领域,解决了当前金融回测出现错误时,仅以宏观的容器状态数据为判定标准,无法正确定位出错位置的问题,数据获取模块获取金融市场行情数据,数据库模块存储数据,回测模块运行回测任务,金融回测任务运行失败时容错模块采用宏观的容器状态信息和回测模块返回的错误代码相结合的办法,根据金融回测系统的数据链路来回推定位出错的位置,并输出错误恢复方法,并通过日志告警模块将容错模块不能恢复的金融回测任务运行错误信息录入日志,向管理人员发出告警通知,避免了多次无意义的重试,保证了金融回测容错系统的正确性。</td>   <td>1.一种面向微服务的金融回测容错系统,其特征在于,所述金融回测容错系统用于运行量化策略的金融回测任务,系统包括：数据获取模块,用于从金融数据提供方获取金融市场行情数据,并写入数据库模块中；数据库模块,对金融市场行情数据进行持久化存储；回测模块,从数据库模块中获取金融市场行情数据,运行金融回测任务,并在金融回测任务运行失败时,根据不同的错误现象返回不同的错误代码；容错模块,包括恢复判断单元及日志告警模块,用于实时监控采集数据获取模块、数据库模块及回测模块的容器状态信息,在金融回测任务运行失败时,根据数据获取模块、数据库模块及回测模块的容器状态信息,及回测模块返回的错误代码,分层次定义分类金融回测任务中出现的错误,形成错误处理列表,定位出错的位置,并输出错误恢复方法,通过恢复判断单元判断金融回测任务运行错误是否恢复,通过日志告警模块记录不能恢复的金融回测任务运行错误信息,并向管理人员发出金融回测任务运行失败告警通知日志告警模块,用于记录容错模块不能恢复的金融回测任务运行错误信息,并向管理人员发出任金融回测任务运行失败告警通知。</td>   <td>G06Q40/06;G06Q40/04;G06F16/25</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈翔;              安小洁;              叶东山;                   龚杰       </td>   <td>中山大学</td>   <td>一种面向家具板材的圆孔检测方法、系统及装置</td>   <td>广东省</td>   <td>CN112819823A</td>   <td>2021-05-18</td>   <td>本发明公开了一种面向家具板材的圆孔检测方法、系统及装置,该方法包括：对数据进行预处理；对预构建的RCF模型进行训练,得到边缘检测模型；对待测样本进行边缘粗提取,得到边缘概率图；选取像素点集；根据预设规则拟合计算圆心坐标、圆孔半径和方差并输出检测结果。该系统包括：预处理模块、边缘检测模型训练模块、边缘提取模块、像素点集选取模块和结果输出模块。该装置包括存储器以及用于执行上述面向家具板材的圆孔检测方法的处理器。通过使用本发明,能够在采样条件有限的情况下对包含复杂纹理的板材表面圆孔轮廓进行精准识别。本发明作为一种面向家具板材的圆孔检测方法、系统及装置,可广泛应用于计算机视觉边缘检测。</td>   <td>1.一种面向家具板材的圆孔检测方法,其特征在于,包括以下步骤：构建数据集并对数据集中的数据进行预处理,得到预处理后的数据；基于预处理后的数据对预构建的RCF模型进行训练,得到边缘检测模型；获取待测样本并基于边缘检测模型对待测样本进行边缘粗提取,得到边缘概率图；根据边缘概率图选取像素点集；将最小二乘法与Ransac算法结合并用于所选取的像素点集的拟合,得到圆心坐标、圆孔半径信息。</td>   <td>G06T7/00;G06T7/13;G06T5/40;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;              许剑锋;              王德明;              吴劲;              丁颜玉;                   段志奎       </td>   <td>广州智慧城市发展研究院;中山大学</td>   <td>一种低功耗检卡控制系统及方法</td>   <td>广东省</td>   <td>CN112800795A</td>   <td>2021-05-14</td>   <td>本发明提供一种低功耗检卡控制系统及方法,包括主机控制模块和从机控制模块；从机控制模块用于在电子标签在场时,发送主机唤醒指令,以及在接收到从机唤醒指令后,从低功耗状态切换至正常状态；主机控制模块用于在处于正常状态时,发送从机唤醒指令,以及在接收到主机唤醒指令后,从低功耗状态切换至正常状态,本发明通过配置低功耗检卡机制,以超低的功耗完成对外部电子标签是否在场的检测,实现RFID读写器从低功耗状态至正常状态的切换,主机控制模块和从机控制均可在低功耗状态下运行,整个RFID读写器功耗更低,获取电子标签信息时更稳定。</td>   <td>1.一种低功耗检卡控制系统,其特征在于,包括：主机控制模块和从机控制模块；所述从机控制模块用于在电子标签在场时,向所述主机控制模块发送主机唤醒指令,以及在从机控制模块接收到从机唤醒指令后,从机控制模块从低功耗状态切换至正常状态；所述主机控制模块用于在处于正常状态时,向从机控制模块发送所述从机唤醒指令,以及在主机控制模块接收到所述主机唤醒指令后,主机控制模块从低功耗状态切换至正常状态；所述从机控制模块包括：从机控制单元、从机门控时钟单元、参数基准单元和射频检测单元,所述从机门控时钟单元、所述参数基准单元和所述射频检测单元均与所述从机控制单元电性连接；参数基准单元用于提供天线载波的参考电平阈值；射频检测单元用于以检卡周期对电子标签进行检测,获取天线载波信号,根据所述参考电平阈值,得到天线载波幅度的变化值,并根据所述天线载波幅度的变化值,判断是否有电子标签在场,以及当电子标签在场时,向从机控制单元发送在场指令；从机控制单元用于根据所述在场指令,向主机控制模块中的主机控制单元发送主机唤醒指令,以及在从机控制单元接收从机唤醒指令后,令从机控制模块从低功耗状态切换至正常状态。</td>   <td>G06K7/10;G07C9/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         余建兴;              王世祺;                   印鉴       </td>   <td>中山大学</td>   <td>一种基于多模态知识推理的公交主动安全预警方法</td>   <td>广东省</td>   <td>CN112800920A</td>   <td>2021-05-14</td>   <td>本发明提供一种基于多模态知识推理的公交主动安全预警方法,该方法首先对公交车前置摄像头拍摄的视频提取关键帧；然后基于关键帧构建场景图谱,用于描述公交车所处场景细粒度信息；场景图谱是一种图形式的数据结构,其节点对应图像中的实体对象,描述了这些对象的相关属性,譬如“红灯状态的红绿灯”、“奔跑的行人”、“邻车”、“交通标志牌”等；边则描述了对象间的关系,譬如实体对象“小孩”和“足球”间存在的关系“踢”；以图谱池化的方式,抽取场景图谱的层级化信息,并最终输出一个表征场景图谱整体语义的分布式编码表示；最后,将场景图谱分布式编码表示与传感器模态数据融合,并基于融合结果判断是否需要向公交驾驶员发出安全预警。</td>   <td>1.一种基于多模态知识推理的公交主动安全预警方法,其特征在于,包括以下步骤：S1：采集公交前置摄像头拍摄的视觉模态数据和传感器模态结构化数据,并进行预处理：S2：对步骤S1中的预处理后的视觉模态数据进行关键帧提取；S3：对步骤S2提取出的关键帧进行细粒度场景表示；S4：将步骤S3与预处理后的传感器模态结构化数据进行多模态知识融合；S5：利用步骤S4得到的数据衡量当前行车安全性。</td>   <td>G06K9/00;G06K9/62;G06N5/04;B60Q9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑子彬;              薛更盛;                   陈志广       </td>   <td>中山大学</td>   <td>一种数字货币交易所的比特币地址挖掘方法及装置</td>   <td>广东省</td>   <td>CN112801784A</td>   <td>2021-05-14</td>   <td>本发明公开了一种数字货币交易所的比特币地址挖掘方法及装置,其中,方法包括：获取比特币区块链上的交易信息,根据交易信息获取多个初始地址；基于交易信息,构建每个初始地址的比特币转移网络；基于预设启发式规则,对比特币转移网络中的地址进行聚类,得到每个初始地址的地址集合；训练地址分类模型,通过地址分类模型对地址集合中的比特币地址进行分类,得到地址集合中的比特币地址的类别。解决了现有的比特币交易所地址识别和分类的方法依赖资源多,需要进行人工数据标注,特征提取的过程复杂,无法大范围的使用的技术问题。</td>   <td>1.一种数字货币交易所的比特币地址挖掘方法,其特征在于,包括：获取比特币区块链上的交易信息,根据所述交易信息获取多个初始地址；所述初始地址为拥有比特币最多的地址；基于所述交易信息,构建每个所述初始地址的比特币转移网络；基于预设启发式规则,对所述比特币转移网络中的地址进行聚类,得到每个所述初始地址的地址集合；其中,所述地址集合中的地址归属于同一个数字货币交易所；训练地址分类模型,通过所述地址分类模型对所述地址集合中的比特币地址进行分类,得到所述地址集合中的比特币地址的类别。</td>   <td>G06Q40/04;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张志勇;              黎厚枫;              丘昌镇;              王鲁平;                   王亮       </td>   <td>中山大学</td>   <td>一种单目标跟踪方法和装置</td>   <td>广东省</td>   <td>CN112802060A</td>   <td>2021-05-14</td>   <td>本申请公开了一种单目标跟踪方法和装置,将获取的可见光模板图像、可见光搜索图像、红外光模板图像和红外光搜索图像输入至目标跟踪模型进行特征提取；并通过目标跟踪模型对可见光模板特征向量和可见光模板特征向量进行模板特征融合得到融合模板特征,对可见光搜索特征向量和红外光搜索特征向量进行搜索特征融合得到融合搜索特征,然后对融合模板特征和融合搜索特征进行互相关计算得到融合响应图,并根据融合响应图获取目标的位置,直至可见光视频或红外光视频结束,得到目标的跟踪结果。本申请解决了现有的目标跟踪方法基于可见光图像进行目标跟踪,容易受到恶劣的照明、雾气和恶劣天气等恶劣条件的影响,导致目标跟踪结果准确性较低的技术问题。</td>   <td>1.一种单目标跟踪方法,其特征在于,包括：获取可见光视频和红外光视频的第n帧图像,得到可见光搜索图像和红外光搜索图像,其中,n&gt;1,所述可见光视频和所述红外光视频的第一帧图像用于获取可见光模板图像和红外光模板图像；将所述可见光模板图像、所述可见光搜索图像、所述红外光模板图像和所述红外光搜索图像输入至目标跟踪模型；通过所述目标跟踪模型对所述可见光模板图像、所述可见光搜索图像、所述红外光模板图像和所述红外光搜索图像进行特征提取,分别得到可见光模板特征向量、可见光搜索特征向量、红外光模板特征向量和红外光搜索特征向量；通过所述目标跟踪模型对所述可见光模板特征向量和所述可见光模板特征向量进行模板特征融合得到融合模板特征,以及对所述可见光搜索特征向量和所述红外光搜索特征向量进行搜索特征融合得到融合搜索特征；通过所述目标跟踪模型对所述融合模板特征和所述融合搜索特征进行互相关计算得到融合响应图,并根据所述融合响应图获取目标的位置；设n＝n+1,并返回所述获取可见光视频和红外光视频的第n帧图像,得到可见光搜索图像和红外光搜索图像的步骤,直至所述可见光视频或所述红外光视频结束,得到所述目标的跟踪结果。</td>   <td>G06T7/246;G06T7/11</td>  </tr>        <tr>   <td>中国专利</td>   <td>         许志明;              鲁鹏程;              刘少江;              倪伟传;                   万智萍       </td>   <td>中山大学新华学院</td>   <td>基于改进差分阈值和位移匹配模型的移动车辆跟踪方法</td>   <td>广东省</td>   <td>CN108520528B</td>   <td>2021-05-11</td>   <td>本发明公开了基于改进差分阈值和位移匹配模型的移动车辆跟踪方法,包括以下步骤：获取至少两个图像帧,以相邻的两个图像帧中前一图像帧为匹配模板,后一图像帧为检测模板；对匹配模板上的运动区域进行检测,获取参考目标并进行标识；对检测模板上的运动区域进行检测,并与匹配模板上的参考目标进行比对得到检测目标在检测模板的位置；采用位移匹配模型确定参考目标和检测目标是否为同一目标；当检测目标与参考目标为同一目标时进行标识,否则不标识；重复上述步骤,对目标进行标识跟踪。本发明只检测并跟踪移动车辆；可以准确实时地跟踪多个移动车辆目标,在智能交通监控领域具有较好的应用前景。</td>   <td>1.基于改进差分阈值和位移匹配模型的移动车辆跟踪方法,其特征在于,包括以下步骤：获取至少两个图像帧,以相邻的两个图像帧中前一图像帧为匹配模板,后一图像帧为检测模板；对匹配模板上的运动区域进行检测,获取参考目标并进行标识；对检测模板上的运动区域进行检测,并与匹配模板上的参考目标进行比对得到检测目标在检测模板的位置；采用位移匹配模型确定参考目标和检测目标是否为同一目标；当检测目标与参考目标为同一目标时进行标识,否则不标识；重复上述步骤,对目标进行标识跟踪；所述对匹配模板上的运动区域进行检测,获取参考目标并进行标识,具体包括：对匹配模板上的目标车辆构成的像素进行分簇,计算每个簇中心点位置及每个簇中各像素点到簇中心点的距离均值；对每个簇进行标识；所述对检测模板上的运动区域进行检测,并与匹配模板上的参考目标进行比对得到检测目标在检测模板的位置；具体包括：对检测模板上的目标车辆构成的像素进行分簇,计算每个簇中心点位置及每个簇中各像素点到簇中心点的距离均值；对得到检测模板上的簇的距离均值与匹配模板上的距离均值进行比对,将检测模板上的簇的距离均值与匹配模板上的距离均值相符的进行预标识；所述采用位移匹配模型确定参考目标和检测目标是否为同一目标；具体为：对检测目标和参考目标的位移进行判断；当参考目标与检测目标的位移在设定阈值内时,则检测目标与参考目标属于同一目标,检测目标进行标识；当检测目标与参考目标的位移不在设定阈值内时,则检测目标与参考目标不是同一目标；对检测目标和参考目标的位移进行判断；当参考目标与检测目标的位移在设定阈值内时,则检测目标与参考目标属于同一目标,检测目标进行标识；具体为：对检测模板上的预标识的检测目标与匹配模板上的标识相同的参考目标之间的位移进行判断,当该位移在设定阈值内,则确定检测目标与参考目标为同一目标,则对检测目标进行最终标识。</td>   <td>G06T7/246</td>  </tr>        <tr>   <td>中国专利</td>   <td>         孔繁校;                   陈龙       </td>   <td>中山大学</td>   <td>一种基于激光雷达与扇状空间分割的路沿检测系统及其方法</td>   <td>广东省</td>   <td>CN110781827B</td>   <td>2021-05-11</td>   <td>本发明涉及一种基于激光雷达与扇状空间分割的路沿检测系统及其方法。方法包括：1.激光雷达扫描车辆周围环境,获取反射点云信息并转换到本地构建的三维坐标系中；2.对点云数据进行预处理,把每一帧点云中的地面数据分离并提取出来；3.根据激光雷达与点云的数据特征,将坐标系中的空间分割为扇状的结构体,根据地面信息和扇状结构体,识别道路延伸方向；4.使用并行的路沿检索算法,提取点云中的路沿候选点；5.对路沿候选点进行聚类,根据扇状空间特征,排除干扰点集合；6.对最后确定的路沿点做B样条曲线拟合,得到路沿检测结果。本发明适应性强,能适应各种形状的道路,能减少障碍物的影响,精度和还原度高,可靠性强,误差率低。</td>   <td>1.一种基于激光雷达与扇状空间分割的路沿检测系统,其特征在于,包括：点云采集模块,用于通过32/64线激光雷达扫描车辆周围环境,采集周围环境的点云数据并进行处理,将带有空间坐标、反射亮度和雷达环数的点云数据转换给本地坐标系中,将每一帧数据输出给地面分离模块；地面分离模块：用于从一帧点云数据中提取出当前点云集的道路路面,所述道路路面指的是点云空间中所有物体最贴近地面的点所组成的曲面,地面点云集合输出给路沿检测模块；扇状空间分割模块：用于根据激光雷达反射点云的特性,将三维坐标内的空间分成不同的扇状区域,扇状区域的特性由点云的数据特性所决定,能匹配点云的分布特征,根据地面检测结果,将扇状结构输出给道路延伸识别模块和路沿检测模块；道路延伸识别模块：用于接收地面点云集合和扇状空间结构,结合两者特征,检测出车辆的可行驶区域,以此判断自动驾驶场景中道路的延伸方向,并将结果输出给路沿检测模块；路沿检测模块：用于接收地面点云集合和扇状空间结构,根据道路的延伸方向将点云数据分类,通过方位角排序、点法线差聚类、点坐标数值滤波的方法,并行地从点云数据中提取各雷达扫描线检测得到的路沿,根据各扫描线检测得到的路沿特征点,进行基于欧几里得聚类方法的处理,得到多个路沿特征点集合,输出给路沿点筛选模块；路沿点筛选模块：用于接收聚类点集合,根据扇状空间结构,排除候选路沿特征点中的干扰点,得到可靠性高的路沿点结果,输出给路沿拟合模块；路沿拟合模块：用于接收最终的路沿点,根据路沿点连接关系,用基于B样条曲线拟合的算法求出相应的路沿,结合道路延伸方向,得到每一帧的路沿检测结果。</td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;                   陈培佳       </td>   <td>中山大学</td>   <td>基于置信度自适应和差分增强的视频显著物体检测方法</td>   <td>广东省</td>   <td>CN112784745A</td>   <td>2021-05-11</td>   <td>本发明公开了一种基于置信度自适应和差分增强的视频显著物体检测方法,所述方法输入一对原图和光流图,编码器分别提取不同层级的空间特征和时间特征；提取到的同一层级的空间特征和时间特征被送入置信度自适应模块中进行重新校正,使得有用信息被传递,噪声信息被抑制；然后,差分信息增强模块利用差分信息对重新校正后的空间特征和时间特征进行互补增强并得到融合特征；在不同层级的融合特征经过解码器层层上采样,最终得到显著物体图。提出的差分信息增强模块通过提取差分信息增强了空间信息和时间信息完整表示显著物体的能力,有利于模型完整地分割出显著物体。</td>   <td>1.基于置信度自适应和差分增强的视频显著物体检测方法,其特征在于,所述方法包括输入一对原图和光流图,编码器分别提取不同层级的空间特征和时间特征；提取到的同一层级的空间特征和时间特征被送入置信度自适应模块中进行重新校正,使得有用信息被传递,噪声信息被抑制；然后,差分信息增强模块利用差分信息对重新校正后的空间特征和时间特征进行互补增强并得到融合特征；在不同层级的融合特征经过解码器层层上采样,最终得到显著物体图。</td>   <td>G06K9/00;G06K9/40;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈定安;              肖航;              郎嵬;              陈婷婷;                   李名豪       </td>   <td>中山大学</td>   <td>一种基于LiDAR点云和二维图像的古建筑可视化配色方法</td>   <td>广东省</td>   <td>CN112785724A</td>   <td>2021-05-11</td>   <td>本发明公开了一种基于LiDAR点云和二维图像的古建筑可视化配色方法,包括：获取LiDAR的3D点云数据并提取建筑物三维点云数据；将建筑物的三维点云数据拆分成具有不同属性的类平面结构；采集对应的正射照片,经过预处理得到平面点云数据；完成平面拟合工作,获得平面的法向量；将平面点云数据与建筑物的三维点云数据做法向量重叠计算点云质心重叠；基于k近邻分类算法进行色彩交换处理,完成可视化配色工作。通过使用本发明,使得无序的点云数据可以客观地被赋予符合实际属性的RGB色彩信息。本发明作为一种基于LiDAR点云和二维图像的古建筑可视化配色方法及系统,可广泛应用于地理信息科学技术领域。</td>   <td>1.一种基于LiDAR点云和二维图像的古建筑可视化配色方法,其特征在于,包括以下步骤：S1、获取LiDAR的3D点云数据并进行半径滤波处理,提取得到建筑物三维点云数据A；S2、将建筑物的三维点云数据A按照几何位置关系拆分成具有不同属性的类平面结构点云B；S3、采集具有多种属性的类平面结构对应的正射照片,经过预处理得到平面点云数据C；S4、基于最小二乘法完成平面点云数据C与类平面结构点云B的平面拟合工作,获得平面的法向量；S5、根据平面的法向量将平面点云数据C与类平面结构点云B做法向量重叠计算并通过点云质心重叠,得到重叠点云数据D；S6、基于k近邻分类算法对重叠点云数据D进行色彩交换处理,完成可视化配色改良工作。</td>   <td>G06T19/20;G06T7/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘宁;                   朱坤鑫       </td>   <td>中山大学</td>   <td>一种基于证据融合理论的室内地标更新方法及装置</td>   <td>广东省</td>   <td>CN112766122A</td>   <td>2021-05-07</td>   <td>本发明公开了一种基于证据融合理论的室内地标更新方法及装置,该方法包括：获取用户上传的地标视频,从地标视频中提取视频序列帧,并将视频序列帧转换成视频分数值；将视频分数值代入用户预设的猜想场景函数式,得到猜想场景概率值；对猜想场景概率值进行一阶融合计算和二阶融合计算,得到二阶猜想场景概率值,采用猜想场景概率值进行二阶数据冲突计算得到二阶数据冲突指标值；当二阶猜想场景概率值满足预设的变化值要求且二阶数据冲突指标值满足预设的冲突指标值时,确定用户达到的待更新的室内地标发生变化；将视频序列帧替换预存的序列帧,完成室内地标更新的操作。本发明可以根据地标视频实现地标的自动更新,从而减少更新所需的时间和成本。</td>   <td>1.一种基于证据融合理论的室内地标更新方法,其特征在于,所述方法包括：获取用户上传的地标视频,从所述地标视频中提取视频序列帧,并将所述视频序列帧转换成视频分数值,其中,所述地标视频为用户达到待更新的室内地标后拍摄的视频；将所述视频分数值代入用户预设的猜想场景函数式,得到猜想场景概率值；对所述猜想场景概率值进行一阶融合计算和二阶融合计算,得到二阶猜想场景概率值,采用所述猜想场景概率值进行二阶数据冲突计算得到二阶数据冲突指标值；当所述二阶猜想场景概率值满足预设的变化值要求且所述二阶数据冲突指标值满足预设的冲突指标值时,确定用户达到的待更新的室内地标发生变化；将所述视频序列帧替换预存的序列帧,完成室内地标更新的操作。</td>   <td>G06K9/00;G06K9/62;G06F16/29;G06F16/23</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王锦萍;              李军;              谭晓军;              黄力;                   陈霞       </td>   <td>中山大学;惠州市德赛西威汽车电子股份有限公司</td>   <td>基于自适应空间模式的深度胶囊网络图像分类方法及系统</td>   <td>广东省</td>   <td>CN112766340A</td>   <td>2021-05-07</td>   <td>本发明公开了一种基于自适应空间模式的深度胶囊网络图像分类方法及系统,该方法包括：对训练用图像进行图像预处理,并构建训练集；基于训练集进行适应空间单元深度胶囊网络的训练,得到训练完成的深度胶囊网络；获取待测图像并将待测图像输入训练完成的深度胶囊网络进行图像分类,得到图像分类结果；所述深度胶囊网络包括ASPConvs模块、ASPCaps模块和全连接胶囊层。该系统包括：预处理模块、训练模块和分类模块。通过使用本发明,自适应学习复杂的物结构信息,提升纹理密集区域的图像分类精度。本发明作为一种基于自适应空间模式的深度胶囊网络图像分类方法及系统,可广泛应用于图像分类领域。</td>   <td>1.基于自适应空间模式的深度胶囊网络图像分类方法,其特征在于,包括以下步骤：对训练用图像进行图像预处理,并构建训练集；基于训练集进行适应空间单元深度胶囊网络的训练,得到训练完成的深度胶囊网络；获取待测图像并将待测图像输入训练完成的深度胶囊网络进行图像分类,得到图像分类结果；所述深度胶囊网络包括ASPConvs模块、ASPCaps模块和全连接胶囊层。</td>   <td>G06K9/62;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         卢宇彤;              蓝嘉璐;                   陈志广       </td>   <td>中山大学</td>   <td>一种基于动态权重D-XGBoost模型的预测方法及系统</td>   <td>广东省</td>   <td>CN112766356A</td>   <td>2021-05-07</td>   <td>本发明公开了一种基于动态权重D-XGBoost模型的预测方法及系统,该方法包括：获取数据集并对数据集中的数据进行格式统一,得到统一的数据；对统一的数据进行数据降维、数据清洗和过采样处理,得到预处理后的数据集；基于动态权重策略的D-XGBoost模型对预处理后的数据集进行参数预测。该系统包括：数据预处理模块和参数预测模块。通过使用本发明,避免了少数类样本容易与周围的多数类样本产生重叠难以分类的问题。本发明作为一种基于动态权重D-XGBoost模型的预测方法及系统,可广泛应用于数据预测领域。</td>   <td>1.一种基于动态权重D-XGBoost模型的预测方法,其特征在于,包括以下步骤：获取数据集并对数据集中的数据进行格式统一,得到统一的数据；对统一的数据进行数据降维、数据清洗和过采样处理,得到预处理后的数据集；基于动态权重策略的D-XGBoost模型对预处理后的数据集进行参数预测。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         康显桂;              王立华;              尹雪梅;                   黄梓生       </td>   <td>中山大学</td>   <td>一种可抗屏幕拍摄的图像水印方法</td>   <td>广东省</td>   <td>CN112767227A</td>   <td>2021-05-07</td>   <td>本发明提出一种可抗屏幕拍摄的图像水印方法,解决了现有图像水印方法不利于水印有效提取的问题,所述方法包括水印嵌入过程与水印提取过程,所述水印嵌入过程将水印嵌入宿主图像,水印提取过程基于所述水印嵌入过程,对已嵌入水印且经屏幕拍摄的宿主图像进行透视矫正后,进行水印提取操作,提取宿主图像已嵌入的水印信息,保证了水印提取信息的有效性。</td>   <td>1.一种可抗屏幕拍摄的图像水印方法,其特征在于,所述方法包括水印嵌入过程与水印提取过程,所述水印嵌入过程将水印嵌入宿主图像,水印提取过程基于所述水印嵌入过程,对已嵌入水印且经屏幕拍摄的宿主图像进行透视矫正后,进行水印提取操作,提取宿主图像已嵌入的水印信息。</td>   <td>G06T1/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王国利;              李振昌;                   郭雪梅       </td>   <td>中山大学</td>   <td>一种应用特征解耦的低剂量CT图像修复方法及系统</td>   <td>广东省</td>   <td>CN112767273A</td>   <td>2021-05-07</td>   <td>本发明公开了一种应用特征解耦的低剂量CT图像修复方法及系统,该方法包括：获取原始低剂量CT扫描图像；对原始低剂量CT扫描图像进行特征解耦,得到待处理的特征；将待处理的特征进行修复处理并聚合,得到聚合后特征；对聚合后特征进行解码,恢复出修复后的正常剂量CT扫描图像。该系统包括：图像获取模块、特征解耦模块、修复聚合模块和恢复模块。本发明考虑混合失真之间的相互干扰,对低剂量CT图像进行去噪和修复。本发明作为一种应用特征解耦的低剂量CT图像修复方法及系统,可广泛应用于图像修复领域。</td>   <td>1.一种应用特征解耦的低剂量CT图像修复方法,其特征在于,包括以下步骤：获取原始低剂量CT扫描图像；对原始低剂量CT扫描图像进行特征解耦,得到待处理的特征；将待处理的特征进行修复处理并聚合,得到聚合后特征；对聚合后特征进行解码,恢复出修复后的正常剂量CT扫描图像。</td>   <td>G06T5/00;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;                   苏志荣       </td>   <td>中山大学</td>   <td>一种基于顶视角的行人三维检测跟踪方法及系统</td>   <td>广东省</td>   <td>CN112767442A</td>   <td>2021-05-07</td>   <td>本发明公开了一种基于顶视角的行人三维检测跟踪方法及系统,该方法包括：对双目相机获取到的左右图像进行立体校正和立体匹配处理；空洞填充并转换得到高度图；对高度图进行建模处理得到二维高度前景图；投影并转换为顶视角下的二维平面点云投影图,检测得到行人头部三维坐标点；将行人头部三维坐标点映射回二维图像坐标系中,并定位出每个行人的边界框；结合行人头部三维坐标点和定位出的行人边界框,进行帧间行人匹配跟踪。通过使用本发明,能够有效克服行人间遮挡导致漏检的问题,提高行人检测召回率。本发明作为一种基于顶视角的行人三维检测跟踪方法及系统,可广泛应用于行人检测领域。</td>   <td>1.一种基于顶视角的行人三维检测跟踪方法,其特征在于,包括以下步骤：对双目相机获取到的左右图像进行立体校正和立体匹配处理,得到深度图；对深度图进行空洞填充,并转换得到高度图；基于MOG2背景建模方法对高度图进行建模处理,得到二维高度前景图；将二维高度前景图投影到三维高度前景点云图中并转换为顶视角下的二维平面点云投影图,检测得到行人头部三维坐标点；将行人头部三维坐标点映射回二维图像坐标系中,并结合该头部三维坐标点在二维图像中定位出每个行人的边界框；结合行人头部三维坐标点和定位出的行人边界框,利用行人中心点的帧间距离以及边界框中的颜色直方图特征进行帧间行人匹配跟踪。</td>   <td>G06T7/246;G06T7/13;G06T7/194;G06T7/73;G06T7/80;G06K9/00;G06K9/46</td>  </tr>        <tr>   <td>中国专利</td>   <td>         闫琦;              杨锐;                   黄继武       </td>   <td>深圳大学;中山大学</td>   <td>一种检测并定位语音片段内的平滑处理的方法</td>   <td>广东省</td>   <td>CN110060703B</td>   <td>2021-05-04</td>   <td>本发明公开了一种检测并定位语音片段内的平滑处理的方法,包括以下步骤：S1.选定平滑滤波器；S2.选取原始语音,提取原始语音集,并通过所述滤波器处理成训练语音集；S3.从原始语音和训练语音集提取特征集；S4.将原始语音的特征集和训练语音集的特征集各筛选出样本,采用分类器训练出SVM分类器模型；S5.选取待测语音,将待测语音进行分帧,对每一帧信号都提取待测语音特征集；S6.使用步骤S4的SVM分类器模型对待测语音特征集进行分类,判断信号是否经过平滑处理,如果是,则定位平滑处理所在的位置。本发明的优点在于,本发明提出的方法比现有同类的检测方法明显具有更高的检测率,可以作为判别数字语音是否被平滑处理的一种高成功率的方法。</td>   <td>1.一种检测并定位语音片段内的平滑处理的方法,其特征在于,包括以下步骤：S1.选定平滑滤波器；S2.选取原始语音,提取原始语音集,并通过所述滤波器处理成训练语音集；S3.从所述原始语音和训练语音集提取特征集；包括以下步骤：S3.1.对所述步骤S2的原始语音集和训练语音集中的每一段语音片段均进行差分计算,得到每一段语音片段对应的差分信号；S3.2对所述步骤S3.1的差分信号进行标准差计算,计算结果作为原始语音集和训练语音集中的每一段语音片段的特征集合的第一部分；S3.3对所述步骤S3.1的差分信号进行傅里叶变换,得到差分信号所对应的频域信号；S3.4所述步骤S2的原始语音信号采样率作为Fs,对步骤S3.3的频域信号在Fs/4到Fs/2的频率区间中的频率信号进行标准差计算,计算结果作为原始语音集和训练语音集中的每一段语音片段的特征集合的第二部分；S3.5采用窗口长度为5的中值滤波器,对所述步骤S2的原始语音集和训练语音集中的每一段语音片段进行滤波,并计算每一段语音片段对应的残差；S3.6对所述步骤S3.5的残差进行差分计算,得到差分信号,并对差分信号进行标准差计算,得到标准差值,作为原始语音集和训练语音集中的每一段语音片段的特征集合的第三部分；S4.将所述原始语音的特征集和训练语音集的特征集各筛选出样本,采用分类器训练出SVM分类器模型；S5.选取待测语音,将所述待测语音进行分帧,对每一帧信号都提取待测语音特征集；S6.使用所述步骤S4的SVM分类器模型对待测语音特征集进行分类,判断信号是否经过平滑处理,如果是,则定位平滑处理所在的位置。</td>   <td>G10L25/54;G10L25/51</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郝晋奎;              赵一天;              岳星宇;              李飞;              张秀兰;                   刘江       </td>   <td>中国科学院宁波材料技术与工程研究所慈溪生物医学工程研究所;中国科学院宁波材料技术与工程研究所;中山大学中山眼科中心</td>   <td>基于AS-OCT图像的3D虹膜表面重建和量化方法及分割网络</td>   <td>浙江省</td>   <td>CN112750141A</td>   <td>2021-05-04</td>   <td>本发明公开一种基于AS-OCT图像的3D虹膜表面重建和量化方法及分割网络,使用基于小波变换的U型分割网络获取虹膜,提取精确的虹膜上边缘；利用提取到的虹膜上边缘,采用基于自适应泊松圆盘采样的引导优化算法进行3D虹膜表面的重建；从3D虹膜表面提取与曲率相关的特征,用于辅助闭角型青光眼的筛查和诊断。本发明首次提出了一种有效的用于3D虹膜重建和量化的框架,为评估虹膜形态,研究虹膜和眼科疾病的相关性提供了新的途径。本发明将小波变换引入到分割网络,在减少冗余信息的同时可以保留足够的细节信息,进而获得更高的分割精度。</td>   <td>1.一种基于AS-OCT图像的3D虹膜表面重建和量化方法,其特征在于包括如下步骤：1)使用基于小波变换的U型分割网络获取虹膜,提取精确的虹膜上边缘；2)利用提取到的虹膜上边缘,采用基于自适应泊松圆盘采样的引导优化算法进行3D虹膜表面的重建；3)从3D虹膜表面提取与曲率相关的特征,用于辅助闭角型青光眼的筛查和诊断。</td>   <td>G06T7/12;G06T7/181;G06T7/55</td>  </tr>        <tr>   <td>中国专利</td>   <td>         吴迪;              吴宇杰;              黄志川;                   胡淼       </td>   <td>中山大学</td>   <td>一种基于离散小波变换的领域自适应Wi-Fi手势识别方法</td>   <td>广东省</td>   <td>CN112733609A</td>   <td>2021-04-30</td>   <td>本发明涉及一种基于离散小波变换的领域自适应Wi-Fi手势识别方法,包括以下步骤：S1：采集手势的CSI信号、手势类别和领域类别,并将其作为训练数据；S2：将采集到的CSI信号按照小波变换的BVP特征提取方法提取到对应的特征；S3：使用提取到的特征、手势类别和领域类别同时对深度神经网络进行训练,得到深度神经网络模型；S4：将新采集到的需要分类的CSI信号按照小波变换的BVP特征提取方法提取到对应的特征,再输入到深度神经网络模型,即可获得对应的手势分类。提出新的基于小波变换的BVP特征提取方法,并结合采集到的CSI信号提取出对应的BVP特征,使用深度神经网络将提取到的特征构建模型用于手势识别,以增强模型的跨领域识别能力。</td>   <td>1.一种基于离散小波变换的领域自适应Wi-Fi手势识别方法,其特征在于,包括以下步骤：S1：采集手势的CSI信号、手势类别和领域类别,并将其作为训练数据；S2：将采集到的CSI信号按照小波变换的BVP特征提取方法提取到对应的BVP特征矩阵；S3：使用提取到的BVP特征矩阵、手势类别和领域类别同时对深度神经网络进行训练,得到深度神经网络模型；S4：将新采集到的需要分类的CSI信号按照小波变换的BVP特征提取方法提取到对应的特征,再输入到深度神经网络模型,即可获得对应的手势分类。</td>   <td>G06K9/00;G06N3/04;G06N3/08;H04W24/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周凡;              蒋维娜;                   刘宁       </td>   <td>中山大学</td>   <td>一种基于模糊数相似度的多因素舆情风险评估方法</td>   <td>广东省</td>   <td>CN112734154A</td>   <td>2021-04-30</td>   <td>本发明公开了一种基于模糊数相似度的多因素舆情风险评估方法。首先,使用扩展优劣解距离法得到风险因素的影响程度,同时使用层次分析法得到指标模型结构及各层元素的排列次序；其次,处理不同数据类型的输入得到风险因素事实系数评估值并进行融合；最后,基于模糊数相似度将评估综合值映射到风险等级与模糊数转换系统中,得到舆情风险等级。本发明可以多源融合工作流程中的风险因素,更加全面、综合地对当前舆情风险给出细粒度的评估；对负面话题和正面话题进行不同的统计策略,较传统方法更具合理性,能够较好的反映舆情倾向性,构建的指标模型有利于风险因素的细化,同时风险指标具有更客观的影响程度评价。</td>   <td>1.一种基于模糊数相似度的多因素舆情风险评估方法,其特征在于,所述方法包括：基于历史工作文书和工作流程规则库抽取舆情风险因素,构建风险因素集合,并组建专家组,由专家使用短语集合对该风险因素集合的影响程度进行表述,采集以上数据形成风险因素影响程度数据集；使用层次分析法构建初始舆情风险指标模型的三层结构以及定义各层级节点,该模型由目标层、准则层和指标层三部分组成,其中,目标层确定评估的主体是舆情风险,准则层基于对历史工作文书的鱼骨图分析定义决策者、当事人、案情属性、舆情状态四个标准准则,指标层由风险因素以及对应的影响程度所构成；基于所述风险因素影响程度数据集构建评价矩阵,使用扩展优劣解距离法综合多维评价矩阵计算得到各项风险因素的影响程度；基于所述各项风险因素的影响程度,使用层次分析法确定所述初始舆情风险指标模型中各层元素的排列次序,得到体系结构完整的最终舆情风险指标模型；收集分析当前所处理工作的相关舆情数据,对所述最终舆情风险指标模型中的风险因素进行评估得到模型指标层中每个风险指标的事实系数；融合所述最终舆情风险指标模型指标层中的风险因素影响程度与所述事实系数,得到基于多因素评估的舆情风险综合值；基于模糊数的图形特性,通过计算基于回转半径的模糊数相似度将所述基于多因素评估的舆情风险综合值映射到风险等级与模糊数转换系统中,从而得到当前所处理工作的舆情风险等级；如果所述舆情风险等级或其变化趋势满足触发预警条件,则按照风险预案执行相应的预警操作。</td>   <td>G06Q10/06;G06K9/62;G06F40/216;G06Q50/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郝华颖;              赵一天;              蒋珊珊;              李飞;              张秀兰;                   刘江       </td>   <td>中国科学院宁波材料技术与工程研究所慈溪生物医学工程研究所;中国科学院宁波材料技术与工程研究所;中山大学中山眼科中心</td>   <td>一种基于卷积循环神经网络的AS-OCT图像的房角分类方法</td>   <td>浙江省</td>   <td>CN112712531A</td>   <td>2021-04-27</td>   <td>本发明公开一种基于卷积循环神经网络的AS-OCT图像的房角分类方法,使用全局扫描对齐方法对AS-OCT切片进行对齐操作,解决眼睛不自主运动和眼睛的光轴不适当放置的可能性导致的图像错位；采用深度学习分割算法对虹膜进行分割,从而由虹膜根部确定ACA的区域；基于卷积循环神经网络同时对二维图像和图像序列信息进行建模,提高网络对窄角和粘连的分类性能。本发明能够对开、狭窄和粘连的青光眼进行准确分类,达到世界先进水平。</td>   <td>1.一种基于卷积循环神经网络的AS-OCT图像的房角分类方法,其特征在于包括如下步骤：1)分别采集黑暗和明亮光照条件下的AS-OCT图像,得到两个AS-OCT图像序列；2)使用全局扫描对齐方法分别对两个AS-OCT图像序列进行对齐操作；3)采用深度学习分割算法对虹膜进行分割,由虹膜根部确定ACA区域,分别得到黑暗和明亮光照条件下ACA区域的AS-OCT图像序列；4)构建卷积循环神经网络,分别将黑暗和明亮光照条件下ACA区域的AS-OCT图像序列输入卷积循环神经网络,提取二维图像特征,并对两种光照条件下的二维图像特征序列进行动态分析,最后将两种光照条件下获得的动态特征融合并进行ACA分类。</td>   <td>G06T7/11;G06T7/00;G06N3/08;G06N3/04;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              严志伟;              陈蔓薇;                   李烨       </td>   <td>中山大学</td>   <td>一种基于异构复合主干的目标检测方法及系统</td>   <td>广东省</td>   <td>CN112699914A</td>   <td>2021-04-23</td>   <td>本发明公开了一种基于异构复合主干的目标检测方法及系统,该方法包括：获取训练数据并对训练数据进行预处理,得到预处理数据；基于异构复合主干架构构建目标检测网络；基于预处理数据和预设的训练策略对目标检测网络进行训练,得到训练后的目标检测网络；获取待测数据并输入到训练后的目标检测网络,输出检测结果。该系统包括：预处理模块、网络构建模块、训练模块和检测模块。通过使用本发明,整合两个异构主干网络学习到的互补特征,避免特征冗余,从而增强检测器总体的特征表达与目标检测性能。本发明作为一种基于异构复合主干的目标检测方法及系统,可广泛应用于目标检测网络领域。</td>   <td>1.一种基于异构复合主干的目标检测方法,其特征在于,包括以下步骤：获取训练数据并对训练数据进行预处理,得到预处理数据；基于异构复合主干架构构建目标检测网络；基于预处理数据和预设的训练策略对目标检测网络进行训练,得到训练后的目标检测网络；获取待测数据并输入到训练后的目标检测网络,输出检测结果。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赵铜铁钢;                   陈泽鑫       </td>   <td>中山大学</td>   <td>一种基于地理信息数据的流域单元用水量划分方法</td>   <td>广东省</td>   <td>CN112700137A</td>   <td>2021-04-23</td>   <td>本发明提供一种基于地理信息数据的流域单元用水量划分方法,包括以下步骤：S1：输入地理信息数据；S2：裁剪栅格数据：提取每个区域的Polygon数据,并对土地利用栅格数据和人口密度栅格数据进行掩膜裁剪操作；分别计算行政区内各流域的人口比例和各类土地利用面积比例；S3：输入统计数据：输入行政区的城镇与农村人口统计数据以及各项用水量数据；S4：划分用水量：计算各流域的当前城镇人口和农村人口数量；计算行政区内各流域的各项用水量,从而将用水量按照流域单元划分。本发明提供一种基于地理信息数据的流域单元用水量划分方法,解决了目前测算行政区域在流域内的用水量的主要手段是经验性的方法,还未形成一种通用的方法的问题。</td>   <td>1.一种基于地理信息数据的流域单元用水量划分方法,其特征在于,包括以下步骤：S1：输入地理信息数据；所述地理信息数据包括行政区流域边界数据、土地利用栅格数据和人口密度栅格数据；S2：裁剪栅格数据：提取行政区流域边界数据中每个区域的Polygon数据,并根据每个区域的Polygon数据对土地利用栅格数据和人口密度栅格数据进行掩膜裁剪操作；根据经过掩膜裁剪操作之后的土地利用栅格数据和人口密度栅格数据分别计算行政区内各流域的人口比例和各类土地利用面积比例；S3：输入统计数据：输入行政区的城镇与农村人口统计数据以及各项用水量数据；S4：划分用水量：根据行政区内各流域的人口比例,计算各流域的当前城镇人口和农村人口数量；根据行政区内各流域的各类土地利用面积比例以及当前城镇人口和农村人口数量,计算行政区内各流域的各项用水量,从而将用水量按照流域单元划分。</td>   <td>G06Q10/06;G06Q50/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         万海;              刘子良;              宋晓彤;                   罗炜麟       </td>   <td>中山大学</td>   <td>基于MCS与BDD联合求解故障树顶事件概率的方法</td>   <td>广东省</td>   <td>CN112700161A</td>   <td>2021-04-23</td>   <td>本发明提出了一种基于MCS与BDD联合求解故障树顶事件概率的方法,针对现有技术容易造成内存爆炸以及不准确的技术问题,运用布尔可满足性问题求解器对故障树的布尔表达式进行求解,基于故障树的最小割集(Minimal CutSet,MCS),根据出现在所述最小割集内的底事件的发生概率来衡量该底事件对于故障树顶事件发生概率的影响程度,选取出现发生概率更大的底事件优先加入到二分决策图的构造过程中,以此降低二分决策图(Binary DecisionDiagram,BDD)结构复杂度,从结构复杂性上降低了内存爆炸发生的可能,优化了顶事件发生概率的求解过程,最终能够以较低的硬件运行条件下,实现快速、准确地对风险控制对象进行故障树定量分析。</td>   <td>1.一种基于MCS与BDD联合求解故障树顶事件概率的方法,其特征在于,包括以下步骤：S01,获取风险控制对象的故障树,构造所述故障树的布尔表达式；所述故障树由相互之间以逻辑门连接的事件组成,位于所述故障树最上端的事件为顶事件,位于所述故障树最下端的事件为底事件；S02,运用布尔可满足性问题求解器对所述布尔表达式进行求解,获取所述故障树的最小割集；S03,获取所述底事件的发生概率；建立二分决策图,按照所述发生概率从大到小的顺序,将出现在所述最小割集内的底事件逐一加入所述二分决策图中完成所述二分决策图的构造；S04,对所述二分决策图进行求解获得顶事件概率。</td>   <td>G06Q10/06;G06N3/08;G06N7/00;G06F17/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              吴炆芳;              朱雄泳;              陈荣军;              谢舜道;                   刘付康       </td>   <td>中山大学;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学花都产业科技研究院</td>   <td>基于压缩感知的高动态范围图像去伪影融合方法</td>   <td>广东省</td>   <td>CN107730479B</td>   <td>2021-04-20</td>   <td>本发明公开了一种基于压缩感知的高动态范围图像去伪影融合方法。首先,对输入的多曝光图像序列进行压缩采样；接着,使用重构方法进行重构得到压缩感知后的多曝光图像序列；然后归一化经过压缩感知的图像集,对图像集使用基于PatchMatch和秩最小化算法进行多曝光图像去伪影融合得到目标的高动态(high dynamic range,HDR)图像。本发明利用K-SVD字典学习、压缩感知和去伪影融合的最新研究成果,能够有效降低采样率、存储空间和计算复杂度,得到去除伪影和模糊的HDR图像。</td>   <td>1.一种基于压缩感知的高动态范围图像去伪影融合方法,其特征在于包括有如下步骤：1)对输入多曝光图像序列进行分块压缩采样；2)对压缩采样后的图像块进行LDR图像序列的重构；3)对经过压缩感知后的多曝光图像序列进行高动态范围图像的去伪影融合；所述步骤3)的具体实现过程为：31)定义输入的经过压缩感知后的图像中任意一点处的像素值为B,对灰度值B归一化处理之后该点的灰度值为I：I＝B/255\*MERGEFORMAT(13)32)现假设源图像为S,参考图像R,源图像和参考图像合成的图像为L,PatchMatch算法就是一个以参考图像为模板,配准源图像生成图像L的过程；由于PatchMatch算法是处理一对图像,现假设输入图像为I-1...I-N,其中N＝5,首先令I-3为参考图像R,则I-3和I-4作为其源图像S,然后令I-2和I-4作为参考图像R,I-1和I-5分别作为I-2和I-4对应的源图像S；33)现定义PatchMatch算法合成图像L的二次函数：                  其中τ为灰度映射函数,Ω为图像R和图像S的图像域,i为图像域上的任意一个像素点,n(i)为以i中心p×p的邻域,其中p为邻域的大小,故j是邻域n(i)上的像素点,R(i)是图像R上的第i个像素点,S(i+u(j))是图像S上的第(i+u(j))个像素点,其中u(j)表示从图像L上的像素点j映射到图像S的偏移量,α为一个归一化的因子,其中w-τ和w-u为一对权重函数,w-τ(i)表示图像R中像素点i映射到图像L的比重,w-u(j)表示图像S中像素点j加上偏移量u(j)映射到图像L的比重；34)灰度映射函数τ定义如下：                  其中灰度映射函数的导数τ′≥0,τ(·)∈[0,1],i为图像域Ω上的像素点,故L(i)表示为图像L上的第i个像素点,使用迭代重加权最小二乘法算法求解灰度映射函数,则将灰度映射函数τ的目标函数重写为：                  ω-i(τ)为对每个像素点引入的权重；其中求解目标函数τ过程中,τ和权重因子ω更新为：                                    其中n表示迭代次数,δ为一个很小的正常数；35)权重函数w-τ定义如下：                  当图像R中图像域上的像素点的灰度值小于3/255,或者大于252/255时,像素点将会被clipped,否则,就不clipped；36)权重函数w-u定义如下：                  其中d(·,·)表示输入参数之间的空间距离；υ-1,υ-2为两个归一化参数,分别取对应空间距离的75百分位数；37)对于参数x和y,d(x,y)＝||x-y||～2,而对于表示为：在图像R和图像S的图像域上的任意一个像素点i,取以i为中心,大小为p×p的邻域,得到图像块和然后图像块经过灰度映射函数得到而图像块相对于i平移了u(i)得到最后求两者的空间距离；同理得其中τ～(-1)(·)灰度映射函数的逆函数；38)通过上面定义的函数知,PatchMatch算法实际上就是求解二次函数的过程,输入图像R和图像S,并分别对两幅图像进行向下采样,分别得到图像R和图像S的金字塔图像集,从金字塔顶端向下迭代,求得在对应每层金字塔图像下合成的图像L和灰度映射函数τ,将此结果作为下一次的迭代的初始值,当迭代完成后即得到最终的配准图像L,依照此方法,即得到输入图像I-1...I-N配准后的图像L-1...L-N；39)利用伽马曲线对已经配准好的图像集进行辐射校准,消除潜在的相机的移动而产生的噪音定义伽马(gamma)函数为：gamma＝cr～γ\*MERGEFORMAT(21)其中r为输入图像,c和γ为常数,c取为1,γ取为2.2；这里将经PatchMatch配准过的图像集用gamma曲线进行辐射校准；310)对配准后的图像集使用秩最小化算法得到批量的对齐图像,首先列向量化所有输入的矩阵得到矩阵D,并初始化低秩矩阵A和噪声矩阵E,根据增广拉格朗日乘子法经过内迭代和外迭代得到最佳低秩矩阵A,则噪声矩阵E＝D-A,最后对得到的低秩矩阵和噪声矩阵进行调整m×n大小的图像,即得到输入图像L对应的低秩图像和噪声图像；311)输入对齐后的图像集A,将图像A合成目标HDR图像：                  其中nImg表示为输入图像的数量,x∈{r,g,b},r,g,b为彩色图像的三个通道；A(x)和H(x)分别输入图像和HDR图像的x通道图像,最后融合H(x)即得到HDR图像H。</td>   <td>G06T5/50;G06T7/30</td>  </tr>        <tr>   <td>中国专利</td>   <td>         卢伟;              张芳;                   刘红梅       </td>   <td>中山大学</td>   <td>一种基于图像放大策略的二值图像可逆信息隐藏方法</td>   <td>广东省</td>   <td>CN108335257B</td>   <td>2021-04-20</td>   <td>本发明涉及数字取证、信息安全的领域,更具体地,涉及一种基于图像放大策略的二值图像可逆信息隐藏方法,该方法够实现在信息提取后无损地恢复原始图像。利用十字形模式块来放大原图以达到可逆信息隐藏的目的,通过对32种十字形模式块进行分析并分类,并选择合适的模式块作为参考模式块,以得到视觉质量较好的参考图；在确定参考模式块后,分析原图中以每个像素点为中心的十字形模式块类型,采用设计的放大策略,将每个像素点放大成为一个2×2的块,得到参考图；最后采用一种块内可翻转像素的选择策略,翻转像素点,进行信息的嵌入。发明可用于信息安全特别是二值图像秘密信息传输方面。</td>   <td>1.一种基于图像放大策略的二值图像可逆信息隐藏方法,其特征在于,包括以下步骤：S1.放大图像,输入原始图像X,其大小为W×H,使用图像放大策略将图像放大,得到大小为2W×2H的参考图RX；所述的S1步骤包括：S11.选择参考模式块,选择十字型的模式块TC-(i,j),表示为：TC-(i,j)＝{I-c,I-0,I-1,I-2,I-3}其中,I-c表示的是中心像素点,I-0到I-3表示的分别是I-c周围上右下左四邻域像素点；S12.旋转不变性具有相同的Vn和Vr值,按照具有旋转不变性和相同的中心像素值进行分类,十字形模式块共有5个像素,则共有32种十字形模式块,将32种十字形模式块分为12类别,根据可逆性、嵌入容量、视觉质量选择其中的两个模式块A1和A2,其中,A1的Vn＝2,Vr＝2,I-c＝0；A2的Vn＝2,Vr＝2,I-c＝1；A1和A2嵌入容量大,放大后图像视觉质量好,则有三种策略,分别以A1、A2和A1+A2为参考模式块；Vn表示的是四邻域中与中心像素点值相同的像素点的个数,Vr表示旋转的特性；Vr-(i,j)、Vn-(i,j)分别表示以(i,j)为中心的十字形模式块的Vr、Vn值,计算公式为：                                    其中表示I+1,表示异或操作,“·”表示逻辑与操作；S13.放大图像,为了将图像放大为2H×2W,原图中的每个像素点放大为2×2的块；I-(ck)表示的是2×2块从左上角顺时针转的四个像素点；为了确定块中的像素值,针对每种策略,像素值的确定方法如下：以A1为参考模式块,扫描原图中的每个像素点来放大原图,如果以扫描的每个像素点为中心的十字形模式块类型为参考模式块A1,则I-(ck)由原图中相邻的两个像素点I-k和I-((k+3)mod4)决定,如果该模式块类型不是所选的参考模式块类型A1,则Ick由该点本身决定,计算公式为：                  以A2为参考模式块,如果以像素点为中心的十字形模式块类型为参考模式块A2,则I-(ck)由原图中相邻的两个像素点I-k和I-((k+3)mod4)决定,如果该模式块类型不是所选的参考模式块类型A2,则I-(ck)由该点本身决定,I-(ck)计算公式为：                  以A1+A2为参考模式块,I-(ck)由原图中相邻的两个像素点I-k和I-((k+3)mod4)决定,否则I-(ck)由该点本身决定,I-(ck)计算公式为：                  S2.分块,将参考图RX分为非重叠的2×2的块；S3.选择并置乱,选择所有的非全黑或非全白块组成一个块序列S,使用一个随机序列用做密钥K去置乱块序列S,得到一个新的序列S-1；S4.嵌入秘密信息,对S-1中的每一块使用一种块内可翻转像素的选择策略,选择嵌入的位置,通过翻转可嵌入位置的像素点将秘密信息M的比特嵌入到块中,嵌入完成后得到嵌入秘密信息的块序列S-2；所述的S4步骤包括：S41.以A1为参考模式块,一个2×2块中嵌入2比特,在得到由非全黑或非全白的2×2块组成的块序列后,为了保证可逆性和视觉质量,由值相同的I-k和I-((k+3)mod4)计算得到的I-(ck)将不用作嵌入点,保证像素间的连续性,剩下的两个点则用作嵌入点嵌入秘密信息；S42.以A2为参考模式块,同样地,以单个模式块A2作为参考模式块时,在得到由非全黑或非全白的2×2块组成的块序列后,由值相同的I-k和I-((k+3)mod4)计算得到的I-(ck)将不用作嵌入点,剩下的两个点用作嵌入点嵌入秘密信息；S43.以A1+A2为参考模式块,将中心点像素保存在放大的块中,每个块序列中的每个2×2块均只能嵌入1比特,由值相同的I-k和I-((k+3)mod4)计算得到的I-(ck)将不用作嵌入点,保存中心像素的点也不用作嵌入点,将剩下的一个点用来嵌入秘密信息；S5.反置乱,使用步骤S3中的密钥K对快序列S-2进行反置乱,得到新的序列S-3；S6.生成隐写图Y,扫描参考图RX和块序列S-3,用S-3中的块去替换RX中的非全黑和非全白块,得到最终的隐写图Y,大小为2W×2H。</td>   <td>G06T1/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李昊昕;              郑伟诗;                   胡海峰       </td>   <td>中山大学</td>   <td>第一人称视角动作识别方法、系统及存储介质</td>   <td>广东省</td>   <td>CN112686194A</td>   <td>2021-04-20</td>   <td>本发明公开了一种第一人称视角动作识别方法、系统及存储介质,所述方法包括以下步骤：定位视频中的关键区域；提取关键区域的特征和全局特征；将关键区域的特征划分为用于表示两种交互主体的两个组别；构建显式关系建模的长短时记忆网络；进行动作识别。本发明采用弱监督的关键区域定位的技术方案,能够自动地定位参与到动作中的人或物体,减少了人和物体位置标注的需求；本发明还通过设计不同类型的连接,能在长短时记忆网络的基础上进一步显式建模视频中的不同关系；本发明还通过网络结构自动搜索技术,实现了自动的网络结构设计,根据数据特征自动选择最优的结构,减少了人工设计网络的负担。</td>   <td>1.一种第一人称视角动作识别方法,包括以下步骤：使用动作类别作为监督,定位视频中的关键区域；通过所述关键区域的位置信息提取关键区域的特征；提取整个视频的特征作为全局特征；通过学习将所述关键区域的特征划分为用于表示两种交互主体的两个组别；所述两种交互主体包括摄像头穿戴者的身体部位,以及与摄像头穿戴者交互的人或物体；构建显式关系建模的长短时记忆网络,在基础的长短时记忆网络结构中设计候选连接；所述候选连接包括不同帧之间的时序关系,两个交互主体组别之间的交互关系,以及交互主体和全局特征之间的上下文关系；通过网络结构自动搜索的方法,以数据驱动的方式搜索最优的长短时记忆网络结构,并进行动作识别。</td>   <td>G06K9/00;G06K9/46;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              刘柏宏;                   卢泽丰       </td>   <td>中山大学</td>   <td>一种基于多方案并联关注机制的行人重识别方法及系统</td>   <td>广东省</td>   <td>CN112686200A</td>   <td>2021-04-20</td>   <td>本发明公开了一种基于多方案并联关注机制的行人重识别方法及系统,该方法包括：将待识别的行人图片a和在库图片g分别输入到特征提取网络以及特征增强网络,提取得到对应的原始行人特征向量t-a和t-g；将原始行人特征向量t-a和t-g各自输入到全连接层,得出行人类别c-a和c-g；对行人类别c-a和c-g进行特征比对,求取欧氏距离,并与阈值进行比较,得到判断结果。该系统包括：特征向量提取模块、类别提取模块和特征比对模块。通过使用本发明,能够放大关注机制的特征增强作用,在行人重识别任务中有较好的识别性能。本发明作为一种基于多方案并联关注机制的行人重识别方法及系统,可广泛应用于计算机视觉中的行人图像处理领域。</td>   <td>1.一种基于多方案并联关注机制的行人重识别方法,其特征在于,包括以下步骤：将待识别的行人图片a和在库图片g分别输入到特征提取网络以及特征增强网络,提取得到对应的原始行人特征向量t-a和t-g；将原始行人特征向量t-a和t-g各自输入到全连接层,得出行人类别c-a和c-g；对行人类别c-a和c-g进行特征比对,求取欧氏距离,并与阈值进行比较,得到判断结果。</td>   <td>G06K9/00;G06K9/46;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马锦华;                   陈曦       </td>   <td>中山大学</td>   <td>一种无需训练且无监督的目标协同定位方法、系统及装置</td>   <td>广东省</td>   <td>CN112686256A</td>   <td>2021-04-20</td>   <td>本发明公开了一种无需训练且无监督的目标协同定位方法、系统及装置,该方法包括：获取图像集合；将图像集合输入到预训练的CNN模型并得到特征集；对卷积激活张量进行降维,返回特征向量；根据卷积激活张量和特征向量,生成热图；基于热图生成边界框,完成目标协同定位。该系统包括：数据获取模块,卷积激活张量生成模块、特征向量生成模块、热图生成模块和边界框生成模块。该装置包括存储器以及用于执行上述无需训练且无监督的目标协同定位方法的处理器。通过使用本发明,能够解决计算机视觉中目标协同定位的问题同时提高现有模型的可重用性。本发明作为一种无需训练且无监督的目标协同定位方法、系统及装置,可广泛应用于目标定位领域。</td>   <td>1.一种无需训练且无监督的目标协同定位方法,其特征在于,包括以下步骤：获取数据,得到图像集合；将图像集合输入到预训练的CNN模型并将生成的卷积激活张量进行收集,得到特征集；基于TSNE算法对特征集中的卷积激活张量进行降维,返回特征向量；根据卷积激活张量和特征向量,生成热图；基于热图构建二值矩阵并生成边界框,完成目标协同定位。</td>   <td>G06K9/32;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘玉葆;              黄聪;              葛又铭;                   李聪       </td>   <td>中山大学</td>   <td>一种社交网络的嵌入表示方法、装置、设备和存储介质</td>   <td>广东省</td>   <td>CN112686766A</td>   <td>2021-04-20</td>   <td>本申请公开了一种社交网络的嵌入表示方法、装置、设备和存储介质,方法包括：响应于分析请求,获取待分析社交网络；对所述待分析社交网络中的各节点生成对应的随机游走序列；对各所述随机游走序列上的节点进行节点对采集,得到采集节点对；从所有所述采集节点对中选取保留节点对；根据所述保留节点对对应的网络训练参数,得到所述待分析社交网路的嵌入表示结果。解决了现有对社交网络的嵌入表示只考虑了网络结构信息,导致得到的有用信息不够准确技术问题。</td>   <td>1.一种社交网络的嵌入表示方法,其特征在于,包括：响应于分析请求,获取待分析社交网络；对所述待分析社交网络中的各节点生成对应的随机游走序列；对各所述随机游走序列上的节点进行节点对采集,得到采集节点对；从所有所述采集节点对中选取保留节点对；根据所述保留节点对对应的网络训练参数,得到所述待分析社交网路的嵌入表示结果。</td>   <td>G06Q50/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         齐志新;                   张慧       </td>   <td>中山大学</td>   <td>一种基于极化雷达遥感影像的城市内涝区域检测方法</td>   <td>广东省</td>   <td>CN112270675B</td>   <td>2021-04-16</td>   <td>本发明提供一种基于极化雷达遥感影像的城市内涝区域检测方法,包括以下步骤：S1：获取极化雷达遥感影像,并对极化雷达遥感影像进行预处理,提取其后向散射系数和干涉相干系数；S2：得到有意义的地物斑块；S3：提取城市范围；S4：构建城市洪水指数,根据城市洪水指数检测出城市内涝区域。本发明提供一种基于极化雷达遥感影像的城市内涝区域检测方法,根据城市受淹前和城市受淹中的后向散射系数和干涉相干系数的变化特征,构建城市洪水指数并通过城市洪水指数检测出城市内涝区域,实现对城市内涝区域的准确、及时提取,解决了由于洪水的发生常常伴随着多云多雨的天气,光学遥感无法获取有效的数据,导致难以及时获取洪水淹没信息的问题。</td>   <td>1.一种基于极化雷达遥感影像的城市内涝区域检测方法,其特征在于,包括以下步骤：S1：获取城市受淹前和城市受淹中的极化雷达遥感影像,并对极化雷达遥感影像进行预处理,提取其后向散射系数和干涉相干系数；S2：通过对受淹前和受淹中的全部极化雷达遥感影像的后向散射系数和干涉相干系数进行联合分割,得到有意义的地物斑块；S3：基于城市受淹前的极化雷达遥感影像的后向散射系数和干涉相干系数提取城市区域分布范围；S4：基于城市受淹前和城市受淹中的极化雷达遥感影像的后向散射系数和干涉相干系数的变化特征,构建城市洪水指数,根据城市洪水指数检测出城市内涝区域；在步骤S4中,假设图像t1,图像t2和图像t3分别是三景重复轨道的极化雷达遥感影像,其中,图像t1和图像t2是在城市受淹前获取的,图像t3是在城市受淹中获取的,则通过以下公式计算城市洪水指数:                  其中,UFI表示城市洪水指数,UFI值越大代表该区域为城市内涝区域的可能性越大；σ-(t2)(VV)表示图像t2的VV极化后向散射系数；σ-(t3)(VV)表示图像t3的VV极化后向散射系数；ρ-(t1t2)(VV)表示图像t1和图像t2之间的干涉相干系数；ρ-(t2t3)(VV)表示图像t2和图像t3之间的干涉相干系数。</td>   <td>G06T7/00;G06T7/11;G06T5/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         汪玉蓉;              黄晓霞;                   陈国林       </td>   <td>中山大学</td>   <td>环境反向散射通信信号处理方法</td>   <td>广东省</td>   <td>CN112668352A</td>   <td>2021-04-16</td>   <td>本发明公开了一种环境反向散射通信信号处理方法,其包括获取混合信号,获取混合信号在IQ域上对应的多个数据点,使用基于高斯混合模型的最大期望聚类算法对各数据点进行聚类,确定目标点簇,以及以目标点簇中各数据点对应的通信信号的可能状态,作为对混合信号的解码结果进行返回等步骤。本发明能够在多个电子标签发送的通信信号产生信号碰撞的情况下,以较高的准确率获得解码结果,从而恢复出各电子标签发送的通信信号,为在环境反向散射通信中实现并行通信提供支持,可以提高环境反向散射通信的数据吞吐量,本发明是在阅读器侧的数据处理方法,无需对电子标签的电路结构或者控制算法进行改造,因此改造成本低本发明广泛应用于通信技术领域。</td>   <td>1.一种环境反向散射通信信号处理方法,其特征在于,包括：获取混合信号；所述混合信号包括多个电子标签各自发出的通信信号,所述通信信号由所述电子标签通过反射或不反射环境信号形成；获取所述混合信号在IQ域上对应的多个数据点；其中,一个所述数据点对应一个所述通信信号的一个可能状态；使用基于高斯混合模型的最大期望聚类算法对各所述数据点进行聚类,确定目标点簇；所述目标点簇包括多个所述数据点；以所述目标点簇中各所述数据点对应的所述通信信号的可能状态,作为对所述混合信号的解码结果进行返回。</td>   <td>G06K7/10;G06K9/62;G06F17/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张冬雨;              成奕彬;                   林倞       </td>   <td>中山大学</td>   <td>一种自监督学习与骨骼信息的行为识别方法</td>   <td>广东省</td>   <td>CN112668492A</td>   <td>2021-04-16</td>   <td>本发明公开了一种自监督学习与骨骼信息的行为识别方法,涉及计算机视觉技术领域。方法包括以下步骤：S1、构建可配置的深度模型；S2、在网络预训练阶段,根据预设的光流预测任务获取预训练样本；其中,预训练样本包括骨骼视频以及机器自动生成的光流预测任务的标签；利用预训练样本对变换网络进行训练,获取变换器网络的初始参数θ′；S3、在网络微调阶段,根据初始参数θ′对变换器网络进行初始化,结合初始化后的变换器网络与进行随机初始化的微调分类网络,构建微调深度模型；S4、将待识别的骨骼视频输入训练完成的微调深度模型中,由微调分类网络输出分类预测结果。本发明在保证较高精度的前提下,实现了效果、鲁棒性和泛化性更好的人体行为识别。</td>   <td>1.一种基于自监督学习与骨骼信息的行为识别方法,其特征在于,包括以下步骤：S1、构建可配置的深度模型,所述深度模型包括变换器网络、预训练分类网络和微调分类网络；其中,变换器网络和预训练分类网络作用于网络预训练阶段,变换器网络和微调分类网络作用于网络微调阶段；S2、在网络预训练阶段,根据预设的光流预测任务获取预训练样本；其中,所述预训练样本包括骨骼视频以及机器自动生成的光流预测任务的标签；利用所述预训练样本对变换网络进行训练,获取所述变换器网络的初始参数θ′；S3、在网络微调阶段,根据所述初始参数θ′对变换器网络进行初始化,结合初始化后的变换器网络与进行随机初始化的微调分类网络,构建微调深度模型；S4、将待识别的骨骼视频输入训练完成的微调深度模型中,由微调分类网络输出分类预测结果。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谢晓华;              林民钊;                   赖剑煌       </td>   <td>中山大学</td>   <td>基于垂直俯视角的行人标注、检测和性别识别方法</td>   <td>广东省</td>   <td>CN112668508A</td>   <td>2021-04-16</td>   <td>本发明公开了一种基于垂直俯视角的行人标注、检测和性别识别方法,包括：获取视频并基于视频得到视频帧的垂直俯视角图像；基于视频标注方法对垂直俯视角图像进行标注,得到行人旋转全身框；基于行人旋转全身框构建行人轨迹并对行人轨迹进行性别属性标注,得到带标签的行人旋转全身框；基于行人旋转全身框对预构建的行人检测网络进行训练,得到行人检测模型；基于带标签的行人旋转全身框对预构建的行人性别识别网络进行训练,得到行人性别识别模型。本发明基于垂直俯视角的情况下不会涉及行人的身份特征,能很好地保护行人隐私。本发明作为一种基于垂直俯视角的行人标注、检测和性别识别方法,可广泛应用于行人检测领域。</td>   <td>1.基于垂直俯视角的行人标注、检测和性别识别方法,其特征在于,包括以下步骤：获取视频并基于视频得到视频帧的垂直俯视角图像；基于视频标注方法对垂直俯视角图像进行标注,得到行人旋转全身框；基于行人旋转全身框构建行人轨迹并对行人轨迹进行性别属性标注,得到带标签的行人旋转全身框；基于行人旋转全身框对预构建的行人检测网络进行训练,得到行人检测模型；基于带标签的行人旋转全身框对预构建的行人性别识别网络进行训练,得到行人性别识别模型。</td>   <td>G06K9/00;G06K9/32;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苏勤亮;                   高月       </td>   <td>中山大学</td>   <td>基于生成对抗网络不确定性注意力增强分布外图像检测法</td>   <td>广东省</td>   <td>CN112668655A</td>   <td>2021-04-16</td>   <td>本发明提供一种基于生成对抗网络不确定性注意力增强分布外图像检测法,该方法引入了生成对抗网络,使得分类器的分类结果更加可靠,并且考虑了判别器的不确定性,并且利用不确定性对特征进行加权,使分类器更加关注分布内样本不同于分布外样本的区域,忽略掉两种数据容易混淆的区域,极大地提升了softmax最大分类概率值在分布外样本检测任务中的有效性,以及其在两类数据中的区分性。对于分布外样本,利用不确定性计算出的特征注意力图会让分类器关注更加错误的区域,导致更低的自信分数,进而提升检测效果；本方法不会使用判别器作为分布外样本的检测手段,对于边缘样本不容易产生误判,可以得到更好的检测效果。</td>   <td>1.一种基于生成对抗网络不确定性注意力增强分布外图像检测法,其特征在于,包括以下步骤：S1：图像输入特征提取器获得特征,并将特征输入至判别器判别是否为分布内样本,同时将特征输入至判别器的不确定性估计器获取判别结果的不确定性；S2：利用S1得到的判别器不确定性进行注意力图的计算,并用该注意力图对S1获取的特征进行加权；S3：将S1和S2得到的加权后的特征作为分类器的输入得到数据分类的概率值。</td>   <td>G06K9/62;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苏勤亮;                   高月       </td>   <td>中山大学</td>   <td>基于分类器预测不确定性的注意力增强分布外图像检测法</td>   <td>广东省</td>   <td>CN112668657A</td>   <td>2021-04-16</td>   <td>本发明提供一种基于分类器预测不确定性的注意力增强分布外图像检测法,该方法考虑了分类器的不确定性,使得分类结果更加可靠,并且利用不确定性对特征进行加权,使分类器更加关注样本对于分类有利的区域,忽略掉分布内样本固有噪声带来的影响,极大地提升了softmax最大分类概率值在分布外样本检测任务中的有效性,以及其在两类数据中的区分性。对于分布外样本,利用不确定性计算出的特征注意力图会让分类器关注更加错误的区域,导致更低的自信分数,进而提升检测效果；本方法不会过度局限于训练数据,对于边缘样本不容易产生误判,可以得到更好的检测效果。</td>   <td>1.一种基于分类器预测不确定性的注意力增强分布外图像检测法,其特征在于,包括以下步骤：S1：图像重构特征提取以及降维处理；S2：利用S1得到的低维数据进行多分类概率计算,提取有效的类别概率特征；S3：将S1和S2得到的数据作为一分类器的输入得到数据异常的概率值。</td>   <td>G06K9/62;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         梅媛;              肖丹阳;                   吴维刚       </td>   <td>中山大学</td>   <td>一种高效通信且保护隐私的个性化联邦学习方法</td>   <td>广东省</td>   <td>CN112668726A</td>   <td>2021-04-16</td>   <td>本发明提供一种高效通信且保护隐私的个性化联邦学习方法,包括以下步骤：S1：从中央服务器拉取当前全局模型W-t到所有客户端中,初始化各个客户端的本地模型S2：执行E轮本地训练,得到新的本地模型S3：将的模型参数发送到中央服务器；S4：在中央服务器中对接收到的模型参数聚合,得到聚合结果W-(t+1)；S5：根据W-(t+1)将所有客户端的本地模型更新为S6：判断是否完成预定迭代次数；若是,则完成个性化联邦学习；若否,则令t＝t+1,并返回步骤S2进行下一轮个性化联邦学习。本发明提供一种高效通信且保护隐私的个性化联邦学习方法,解决了现有的个性化联邦学习方法没有实现个性化客户端本地模型与全局模型的平衡的问题。</td>   <td>1.一种高效通信且保护隐私的个性化联邦学习方法,其特征在于,包括以下步骤：S1：从中央服务器拉取当前全局模型W-t到所有客户端中,初始化各个客户端的本地模型其中,i为客户端序号,t为当前个性化联邦学习的轮数；S2：在客户端i中执行E轮本地训练,得到新的本地模型S3：基于对分层参数组合进行可变频率更新的方式,将的模型参数发送到中央服务器；S4：在中央服务器中对接收到的模型参数聚合,得到聚合结果W-(t+1)；S5：基于对分层参数组合进行可变频率更新的方式,根据W-(t+1)将所有客户端的本地模型更新为S6：判断是否完成预定迭代次数；若是,则完成个性化联邦学习；若否,则令t＝t+1,并返回步骤S2进行下一轮个性化联邦学习。</td>   <td>G06N20/20;G06N3/08;H04L29/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘玉葆;                   黄楚茵       </td>   <td>中山大学</td>   <td>一种长短期交通预测方法</td>   <td>广东省</td>   <td>CN112668797A</td>   <td>2021-04-16</td>   <td>本申请公开了一种长短期交通预测方法,获取构建的交通路网图中的节点的第一历史交通数据,通过预置交通预测模型中的第一卷积层对第一历史交通数据进行卷积处理；通过模型中的第一个迭代RNN算子对卷积处理后的第一历史交通数据进行交通预测,输出第一个时间步的交通预测结果,将第一个时间步的交通预测结果输入到下一个迭代RNN算子进行交通预测,直至第T-p个迭代RNN算子输出第T-p个时间步的交通预测结果,通过模型中的拼接模块将T-p个时间步的交通预测结果拼接后输入到第二卷积层进行卷积处理,输出最终的交通预测结果。本申请解决了现有的交通预测方法存在预测误差累计较高,以及不能同时兼顾长短期预测精度的技术问题。</td>   <td>1.一种长短期交通预测方法,其特征在于,包括：将交通路网构建为图结构,得到交通路网图；获取所述交通路网图中的节点的第一历史交通数据；将所述第一历史交通数据输入到包含第一卷积层、T-p个迭代RNN算子、拼接模块和第二卷积层的预置交通预测模型,使得所述第一卷积层对所述第一历史交通数据进行卷积处理,第一个迭代RNN算子对卷积处理后的所述第一历史交通数据进行交通预测,输出第一个时间步的交通预测结果,将所述第一个时间步的交通预测结果输入到下一个迭代RNN算子进行交通预测,直至第T-p个迭代RNN算子输出第T-p个时间步的交通预测结果,所述拼接模块对T-p个时间步的交通预测结果进行拼接,所述第二卷积层对拼接后的交通预测结果进行卷积处理,输出最终的交通预测结果。</td>   <td>G06Q10/04;G06F17/16;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         任传贤;                   许耿鑫       </td>   <td>中山大学</td>   <td>一种基于共享解码器和残差塔式结构的眼底图像血管分割方法</td>   <td>广东省</td>   <td>CN112669285A</td>   <td>2021-04-16</td>   <td>本发明公开了一种基于共享解码器和残差塔式结构的眼底图像血管分割方法,所述方法包括以下步骤：通过数据输入模块得到训练数据集图像块、测试数据集图像块；通过残差塔式模块,得到残差塔式序列；通过编码模块得到多等级语义特征；通过共享解码模块,得到多等级概率图；将多尺度标签、残差塔式序列、共享解码器得到的概率图构造成模型总损失,并利用PyTorch进行梯度优化,训练编码模块、共享解码模块中的参数；将测试数据集图像块依次输入到训练后的编码模块和共享解码模块以得到概率图,对所得概率图并进行拼接、二值化处理得到最终的分割结果。本发明解决了血管口径分布不均、眼底图像对比度较弱问题。</td>   <td>1.一种基于共享解码器和残差塔式结构的眼底图像血管分割方法,其特征在于,所述方法利用处理模块实现,所述处理模块包括：数据输入模块、残差塔式模块、编码模块、共享解码模块、损失模块、数据输出模块,所述方法包括以下步骤：S1：数据输入模块接收带标签的训练数据集和待分割的测试数据集,并分别进行切片预处理得到训练数据集图像块、测试数据集图像块；S2：将训练数据集图像块的标签输入残差塔式模块,并将其进行多尺度下采样,构造多尺度标签,将多尺度标签上采样到与训练数据集图像块相同的分辨率,对上采样后的多尺度标签利用异或运算生成相邻尺度标签的残差,对残差下采样输出残差塔式序列；S3：将训练数据集图像块输入至编码模块,所述编码模块利用交替串联的L个双重卷积层和(L-1)个下采样对训练数据集图像块处理,得到多等级语义特征；S4：将多等级语义特征输入至共享解码模块进行共享解码,输出(L-1)个概率图；S5：将多尺度标签、残差塔式序列、共享解码器得到的概率图构造成模型总损失,并利用PyTorch进行梯度优化,训练编码模块、共享解码模块中的参数；S6：将测试数据集图像块利用训练后的编码模块、共享解码模块进行处理得到测试数据集图像块的概率图,数据输出模块对测试数据集图像块的概率图进行拼接,拼接后的概率图进行二值化处理得到最终的分割结果。</td>   <td>G06T7/00;G06T7/10;G06T3/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈焕杰;              王国利;              郭雪梅;                   谢泳伦       </td>   <td>中山大学</td>   <td>一种面向云端部署的多目标轨迹跟踪方法及系统</td>   <td>广东省</td>   <td>CN112669345A</td>   <td>2021-04-16</td>   <td>本发明公开了一种面向云端部署的多目标轨迹跟踪方法及系统,该方法包括：对预构建的目标检测模型进行FP16推断优化加速,得到优化后的目标检测模型；获取摄像头的实时图像数据并基于优化后的目标检测模型对实时图像数据进行检测,得到人员框和轨迹框；根据当前人员框中的人员信息,选择匹配方式与上一帧存在的轨迹信息进行匹配,得到匹配结果；根据匹配结果对人员信息和轨迹状态进行更新。该系统包括：优化模块、检测模块、匹配模块和更新模块。本发明能够提高多目标跟踪的性能。本发明作为一种面向云端部署的多目标轨迹跟踪方法及系统,可广泛应用于多目标跟踪技术领域。</td>   <td>1.一种面向云端部署的多目标轨迹跟踪方法,其特征在于,包括以下步骤：对预构建的目标检测模型进行FP16推断优化加速,得到优化后的目标检测模型；获取摄像头的实时图像数据并基于优化后的目标检测模型对实时图像数据进行检测,得到人员框和轨迹信息；根据当前人员框中的人员信息,选择匹配方式与上一帧存在的轨迹信息进行匹配,得到匹配结果；根据匹配结果对人员信息和轨迹状态进行更新。</td>   <td>G06T7/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周榆明;                   成慧       </td>   <td>中山大学</td>   <td>一种适用于多平台协同感知的地图融合方法</td>   <td>广东省</td>   <td>CN112669358A</td>   <td>2021-04-16</td>   <td>本发明公开了一种适用于多平台协同感知的地图融合方法,方法包括：获取里程信息和局部地图信息；根据所述里程信息对所述局部地图信息进行预处理,得到子地图特征点；根据所述子地图特征点进行点云配准,得到子地图间的匹配关系；根据所述里程信息、局部地图信息和子地图间的匹配关系构建全局位姿图,优化得到融合地图。本发明实施例降低了配准难度且扩展了应用场景,可广泛应用于点云处理技术领域。</td>   <td>1.一种适用于多平台协同感知的地图融合方法,其特征在于,包括：获取里程信息和局部地图信息；根据所述里程信息对所述局部地图信息进行预处理,得到子地图特征点；根据所述子地图特征点进行点云配准,得到子地图间的匹配关系；根据所述里程信息、局部地图信息和子地图间的匹配关系构建全局位姿图,优化得到融合地图。</td>   <td>G06T7/33;G06T7/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘芳;              郑伟诗;              邝嘉健;              关杰鸿;                   张青       </td>   <td>中山大学</td>   <td>基于三维重建技术的虚拟数据集开发方法、系统及存储介质</td>   <td>广东省</td>   <td>CN112669448A</td>   <td>2021-04-16</td>   <td>本发明公开了一种基于三维重建技术的虚拟数据集开发方法、系统及存储介质,所述方法包括以下步骤：批量建模人物模型；重建三维真实场景模型；构建虚拟数据集。本发明主要目的在于克服真实数据集的隐私问题以及采集周期长,成本高等问题,提供一种基于三维重建技术的虚拟数据集开发方法、系统及存储介质,通过开发一种结合人物建模技术以及三维场景重建技术的虚拟场景仿真平台,实现重建三维真实场景模型,批量建模人物模型和批量生成大量视角、空间位置、姿态、性别、肤色、年龄、着装风格各异的行人图像。</td>   <td>1.一种基于三维重建技术的虚拟数据集开发方法,其特征在于,包括以下步骤：批量建模人物模型,所述人物模型包括肤色、身高、体重、性别及着装；重建三维真实场景模型,利用无人机拍摄采集真实场景多视角全覆盖RGB图像,从多张多视角2D图像中推算3D信息,实现三维真实场景模型的重建；构建虚拟数据集,基于人物模型和三维真实场景模型进行虚拟场景仿真平台仿真,在场景中的不同位置标定摄像机,采集不同摄像机角度,同一行人不同角度、不同动作的图像,得到虚拟数据集。</td>   <td>G06T17/00;G06T13/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         孔方圆;              丁圣勇;                   朝红阳       </td>   <td>中山大学</td>   <td>一种通过神经网络实现人脸属性转换的方法</td>   <td>广东省</td>   <td>CN107665339B</td>   <td>2021-04-13</td>   <td>本发明提供一种通过神经网络实现人脸属性转换的方法,该方法通过训练生成网络G-Net,其中,生成网络G-Net负责生成图像,即输入一个随机向量获得一个视觉上真实的人脸图像；训练属性判别网络E-Net,其中,属性判别网络E-Net负责判别属性,即判断当前图片是否具有限定的属性；在生成网络G-Net和属性判别网络E-Net完成训练后,把生成网络G-Net和属性判别网络E-Net串联在一起,即G-Net的输出为E-Net的输入,进行人脸属性转换操作；该方法可以快速生成效果自然的图片,解决生成结果可能是不自然人脸或者不是人脸的问题,不需要手工二次修改。</td>   <td>1.一种通过神经网络实现人脸属性转换的方法,其特征在于,包括以下步骤：S1：训练生成网络G-Net,其中,生成网络G-Net负责生成图像,即输入一个随机向量获得一个视觉上真实的人脸图像；S2：训练属性判别网络E-Net,其中,属性判别网络E-Net是一个二分类网络,负责判别属性,即判断当前图片是否具有限定的属性；S3：在生成网络G-Net和属性判别网络E-Net完成训练后,把生成网络G-Net和属性判别网络E-Net串联在一起,即G-Net的输出为E-Net的输入,进行人脸属性转换操作；所述步骤S3中进行人脸属性转换操作的过程是：S31：z-0表示G-Net的输入,I表示G-Net的输出,O表示原始图像,首先随机产生z-0,得到I,然后定义损失函数为固定好学习到的网络参数,反向传播求O在G-Net输入空间的表示z-1,即当输入为z-1时G-Net的输出图像为O；S32：将串接的G-Net和E-Net作为一个整体网络,固定好学习到的网络参数,使用z-1作为输入,得到对于属性的判断,E-Net输出为1表示拥有该属性,为0表示不拥有该属性,反向传播寻找z-2达到属性转换的效果,即从原来不拥有该属性变为拥有该属性,反向传播的loss函数为或者从原来拥有该属性变为不拥有该属性,反向传播的loss函数为S33：将z-2作为G-Net的输入,得到的输出即为属性转换结果图。</td>   <td>G06K9/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         万海;              张漫榕;              刘亚男;              黄佳莉;              曾娟;                   范科峰       </td>   <td>中山大学</td>   <td>一种面向社交平台表情包的多模态情感分析方法</td>   <td>广东省</td>   <td>CN112651448A</td>   <td>2021-04-13</td>   <td>本发明提供一种面向社交平台表情包的多模态情感分析方法,包括以下步骤：S1：使用爬虫工具从社交平台爬取表情包图片,并对表情包图片进行情感标注后,进行预处理；S2：获取步骤S1爬取到的表情包图片的语义信息,得到每张表情包图片对应的文本信息特征向量表示；S3：获取步骤S1爬取到的表情包图片的视觉特征,得到每张表情包图片对应的视觉特征向量表示；S4：所述文本信息特征向量表示和视觉特征向量表示经多模态融合,得到多模态融合特征向量表示；S5：所述多模态融合特征向量表示经过分类器得到情感识别结果,选取置信度最高的情感识别结果作为预测的情感,本发明引入了图片的文本语义特征,能够更好地捕获表情包中的隐含语义信息。</td>   <td>1.一种面向社交平台表情包的多模态情感分析方法,其特征在于,包括以下步骤：S1：使用爬虫工具从社交平台爬取表情包图片,并对表情包图片进行情感标注后,进行预处理；S2：获取步骤S1爬取到的表情包图片的语义信息,得到每张表情包图片对应的文本信息特征向量表示；S3：获取步骤S1爬取到的表情包图片的视觉特征,得到每张表情包图片对应的视觉特征向量表示；S4：所述文本信息特征向量表示和视觉特征向量表示经多模态融合,得到多模态融合特征向量表示；S5：所述多模态融合特征向量表示经过分类器得到情感识别结果,选取置信度最高的情感识别结果作为预测的情感。</td>   <td>G06K9/62;G06K9/32;G06F16/951;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;              王弘远;                   陈刚       </td>   <td>中山大学</td>   <td>一种并行化类脑仿真的加速方法</td>   <td>广东省</td>   <td>CN112651502A</td>   <td>2021-04-13</td>   <td>本发明涉及神经网络仿真技术领域,具体涉及一种并行化类脑仿真的加速方法,包括以下步骤：S1、在每个时间片中筛选出所有处于激活状态的神经元；S2、将所有筛选出来的神经元的所有输出突触拆分；S3、拆分后分配给若干个线程来完成脉冲发放任务。本发明的并行化类脑仿真的加速方法,通过将待发放脉冲的神经元先筛选出来,再将发放任务均匀分配到各个线程的方式,在一定程度上均衡了每个线程的工作量,有效地提高了仿真效率。</td>   <td>1.一种并行化类脑仿真的加速方法,其特征在于,包括以下步骤：S1、在每个时间片中筛选出所有处于激活状态的神经元；S2、将所有筛选出来的神经元的所有输出突触拆分；S3、拆分后分配给若干个线程来完成脉冲发放任务。</td>   <td>G06N3/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;              王弘远;                   陈刚       </td>   <td>中山大学</td>   <td>一种用于优化并行类脑仿真的图划分方法</td>   <td>广东省</td>   <td>CN112651503A</td>   <td>2021-04-13</td>   <td>本发明涉及神经网络仿真技术领域,具体涉及一种用于优化并行类脑仿真的图划分方法,包括以下步骤：S1、将图划分为若干个分部,然后循环执行以下步骤；S2、当网络未收敛时,按照原则选取整个网络中具有最大正收益数的收益(x,n,p,q)；S3、将子族群p整体移动至q分部,分别找出具有最多和最少节点数的分部a和b；S4、计算平衡数c；S5、如果平衡数c大于0,则从分部a至b平衡性移动c个节点；S6、如果当前划分结果的总割边数小于历史划分结果的总割边数,则执行以下步骤：更新历史划分结果,按照扰动数执行扰动,扰动数*＝衰减系数；否则,退出总循环。本发明的用于优化并行类脑仿真的图划分方法,消耗的时间短,效率高。</td>   <td>1.一种用于优化并行类脑仿真的图划分方法,其特征在于,包括以下步骤：S1、将图划分为若干个分部,然后循环执行以下步骤,当网络未收敛时,执行步骤S2,否则执行步骤S6；S2、当网络未收敛时,按照原则选取整个网络中具有最大正收益数的收益(x,n,p,q)；S3、将子族群p整体移动至q分部,分别找出具有最多和最少节点数的分部a和b；S4、计算平衡数c；S5、如果平衡数c大于0,则从分部a至b平衡性移动c个节点；S6、如果当前划分结果的总割边数小于历史划分结果的总割边数,则执行以下步骤：更新历史划分结果,按照扰动数执行扰动,扰动数＝扰动数*衰减系数；否则,退出总循环。</td>   <td>G06N3/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;              王弘远;                   陈刚       </td>   <td>中山大学</td>   <td>一种基于并行化的类脑仿真编译的加速方法</td>   <td>广东省</td>   <td>CN112651504A</td>   <td>2021-04-13</td>   <td>本发明涉及神经网络仿真技术领域,具体涉及一种基于并行化的类脑仿真编译的加速方法,包括以下步骤：S1、构建神经网络时,创建若干个族群,每个族群包含上百万个神经元；S2、按照神经元族群并行构建神经元数组；S3、按照族群之间的连接并行构建突触数组以及神经元到突触数组的映射关系。本发明的基于并行化的类脑仿真编译的加速方法,通过并行算法加速仿真框架的速度,大大减少了用户的等待时间。</td>   <td>1.一种基于并行化的类脑仿真编译的加速方法,其特征在于,包括以下步骤：S1、构建神经网络时,创建若干个族群,每个族群包含上百万个神经元；S2、按照神经元族群并行构建神经元数组；S3、按照族群之间的连接并行构建突触数组以及神经元到突触数组的映射关系。</td>   <td>G06N3/10;G06F8/41</td>  </tr>        <tr>   <td>中国专利</td>   <td>         徐韩悦;                   陈龙       </td>   <td>中山大学</td>   <td>基于点线特征和深度滤波器的轻量级单目视觉定位方法</td>   <td>广东省</td>   <td>CN110807809B</td>   <td>2021-04-09</td>   <td>本发明涉及一种基于点线特征和深度滤波器的轻量级单目视觉定位方法。包括：采集标定图像信息对相机进行标定,得到相机的畸变参数和内参矩阵；采集图像数据,进行畸变校正,得到校正后的图像数据；检测校正后图像的特征点；选取关键帧初始化系统；提取图像线段特征和特征点,跟踪每一帧得到图像的位姿；三维特征加入时使用CNN初始化深度滤波器；对种子进行迭代更新直到收敛。本发明适用于计算性能受限的设备,可用于纹理比较缺乏的场景,不计算、匹配描述子,具有更高的运行速度；加入了线段特征,能够得到更多的特征点；使用了CNN估计的深度值初始化深度滤波器,大大降低迭代的次数。</td>   <td>1.一种基于点线特征和深度滤波器的轻量级单目视觉定位方法,其特征在于,包括以下步骤：S1.使用待标定单目相机采集含有标定板信息的图像数据,使用标定得到相机模型的内参和畸变参数；S2.将载有单目相机的设备放置至目标环境中,记录传感器数据,对采集到的每一帧图片进行畸变校正,并计算ORB特征和LSD线段特征；S3.跟踪步骤S2得到的特征点,确定第一个关键帧和第二个关键帧,计算单应矩阵和基础矩阵,并确定最佳的参数,作为单目初始化全局的尺度；S4.将采集线段特征得到的线段进行等距采样得到新的点,对所有ORB特征点和LSD线段特征上的点,确定特征块,使用直接法对所有特征块的位姿参数迭代,通过最小化特征块灰度差值,得到相机的初始位姿；利用得到的初始位姿和当前帧具有共视关系的关键帧,计算仿射变换并最小化光度误差,来对特征块的位置进行优化；最后利用得到的位姿和特征位置计算重投影误差,通过高斯牛顿法优化位姿和三维点的位置；S5.计算当前帧与所有关联关键帧平移量,如果平移足够大则确定当前帧是关键帧；S6.对于关键帧,使用CNN计算特征点的深度值初始化深度滤波器；对于普通帧,融合观测数据和预测数据,得到新的关于深度估计的高斯分布。</td>   <td>G06T7/73;G06T7/80;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         朱亚琛;              陈龙;                   刘聪       </td>   <td>中山大学</td>   <td>一种基于图描述子的激光SLAM回环检测系统及方法</td>   <td>广东省</td>   <td>CN110910389B</td>   <td>2021-04-09</td>   <td>本发明涉及一种基于图描述子的激光SLAM回环检测系统及方法。包括语义分割模块,全图描述子提取模块,全图描述子匹配模块,顶点描述子提取模块,顶点描述子匹配模块和几何一致性验证模块。本发明利用了从点云数据中提取的语义信息形成了全图描述子和顶点描述子两种图描述子来进行点云帧和语义物体的表征。相比于传统算法从像素级别提取描述子,本发明可以避免繁重的法向量计算任务和避免视角大幅变化带来的问题,可以更加快速和鲁棒地检测回环。本发明先通过全图描述子粗略地筛选出潜在的回环候选帧,接着用顶点描述子更加精细地匹配查询帧与回环候选帧的细节信息,是一个由粗到精的查找过程,为方法的实时性提供了理论保障。</td>   <td>1.一种基于图描述子的激光SLAM回环检测系统,其特征在于,包括：语义分割模块：用于感知外部环境,提取无序点云数据中的语义物体信息,得到物体的预测置信度和其质心三维空间坐标并输出给全图描述子提取模块和顶点描述子提取模块；全图描述子提取模块：用于以点云数据中物体的质心为顶点,顶点与顶点之间的欧几里得距离为边构成一个完全图,将图中所有边按长度存放到一维计数向量中,该向量即全图描述子,后将全图描述子输出到全图描述子匹配模块；全图描述子匹配模块：用于利用KD树这一数据结构加速查找查询帧与历史所有帧欧氏距离最近的n个帧,并把这n个帧都记为回环候选帧,后把回环候选帧输出给顶点描述子提取模块；顶点描述子提取模块：用于对查询帧和所有回环候选帧中的每一个顶点,将与该顶点相连的边按长度存放到一维计数向量中,该向量即为对应顶点的顶点描述子,后输出到顶点描述子匹配模块；顶点描述子匹配模块：利用欧氏距离度量查询帧与回环候选帧中有相似预测置信度的语义物体的顶点描述子,得出查询帧与每一回环候选帧中顶点的一一对应关系；并利用基于RANSAC的几何一致性验证方法得出查询帧与所有回环候选帧的位姿变换关系和匹配误差,并依据这个匹配误差判断是否存在回环。</td>   <td>G06T7/10;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王若梅;              周明杰;                   周凡       </td>   <td>中山大学</td>   <td>一种基于多特征融合和随机森林模型的新闻故事分割方法</td>   <td>广东省</td>   <td>CN112633241A</td>   <td>2021-04-09</td>   <td>本发明公开了一种基于多特征融合和随机森林模型的新闻故事分割方法。首先以新闻视频作为输入源,进行视觉特征提取和听觉特征提取,包括新闻主题字幕特征提取、直播间特征提取、镜头切换特征提取和静音区特征提取；其次对新闻视频进行语音识别,获得语音识别结果,确定具体候选边界点；接着将语音识别结果作为输入,进行语义特征提取,包括概要匹配特征提取、语义相似度特征提取和文本深度特征提取；再次手动标注新闻视频特征对随机森林模型进行训练,将提取的视频特征值和具体候选边界输入训练好的模型进行目标视频的二分类任务,归类结果为新闻故事单元边界和非边界；最后以归类结果对目标视频进行分割,获得最终的新闻视频故事单元。</td>   <td>1.一种基于多特征融合和随机森林模型的新闻故事分割方法,其特征在于,所述方法包括：以新闻视频作为输入源,进行视觉特征提取包括新闻主题字幕特征提取、直播间切换特征提取和镜头切换特征提取,进行听觉特征提取包括静音区特征提取；以新闻主题字幕帧的时间节点作为输入源,确定候选边界范围,以所述新闻视频作为输入源进行语音识别,获得语音识别结果,确定具体候选边界点；以所述语音识别结果作为输入,进行语义特征提取,包括概要匹配特征提取、语义相似度特征提取和文本深度特征提取；使用手动标注出新闻故事单元边界点和边界点处特征后的视频作为训练集,对随机森林模型进行训练,将所述新闻视频已提取的视频特征值和所述具体候选边界点输入训练好的随机森林模型进行二分类任务,归类结果为新闻故事单元边界和非边界两类；以所述归类结果对目标视频进行分割,获得最终结果即新闻视频的故事单元。</td>   <td>G06K9/00;G06K9/32;G06K9/62;G06F40/289</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈刚;              何涛;              张余;              凌晔华;                   黄凯       </td>   <td>中山大学</td>   <td>一种基于神经网络的环视立体视觉匹配系统、方法及介质</td>   <td>广东省</td>   <td>CN112633324A</td>   <td>2021-04-09</td>   <td>本发明公开了一种基于神经网络的环视立体视觉匹配系统、方法及存储介质,该系统包括特征提取模块、立体匹配模块、处理模块和拼接模块；本发明通过多个方向的环视深度信息,可以很好的用于无人系统自动避障、自主导航决策,提升了无人机、无人车等无人系统的自由度,拓展了无人系统的应用场景,同时,整个系统采用硬件友好设计,可以轻松的部署在硬件系统中,提升了系统的处理速度。此外,使用神经网络进行特征提取,具有高精度的优点,所述系统可以应用于高实时性和高精度要求的应用场景中。另一方面,本发明实施例所述系统相比于激光雷达、结构光、TOF等技术而言,成本低廉,易于大规模推广。本发明可广泛应用于图像处理技术领域。</td>   <td>1.一种基于神经网络的环视立体视觉匹配系统,其特征在于,包括：特征提取模块,用于利用二值神经网络对多幅原始图像进行特征提取,得到多幅第一图像；立体匹配模块,用于对多幅所述第一图像的像素点进行立体匹配,得到多幅第二图像；处理模块,用于对多幅所述第二图像进行一致性检测处理和中值滤波处理得到多幅深度图；拼接模块,用于对多幅所述深度图进行拼接,得到环视深度图。</td>   <td>G06K9/62;G06K9/46;G06K9/38;G06T3/40;G06T5/00;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周素红;              周淑丽;              郑重;                   卢俊文       </td>   <td>中山大学</td>   <td>一种基于疫情传播风险的疫苗分配方法、系统及装置</td>   <td>广东省</td>   <td>CN112633681A</td>   <td>2021-04-09</td>   <td>本发明公开了一种基于疫情传播风险的疫苗分配方法、系统及装置,该方法包括：获取用户移动轨迹并对用户移动轨迹进行标准化处理,得到用户轨迹数据；根据预设参数和用户轨迹数据确定疫情流行病参数；构建基于Agent和SEIR的疫情传播模型,并模拟自然传播过程,得到疫情风险地图；根据疫情风险地图确定疫苗投放策略,并模拟疫苗投放后的疫情结果。该系统包括：用户轨迹模块,参数模块、传播模型模块、风险地图模块和策略模块。该装置包括存储器以及用于执行上述基于疫情传播风险的疫苗分配方法的处理器。通过使用本发明,可提高疫苗分配效率。本发明作为一种基于疫情传播风险的疫苗分配方法、系统及装置,可广泛应用于资源分配领域。</td>   <td>1.一种基于疫情传播风险的疫苗分配方法,其特征在于,包括以下步骤：获取用户移动轨迹并对用户移动轨迹进行标准化处理,得到用户轨迹数据；根据预设参数和用户轨迹数据确定疫情流行病参数；根据用户轨迹数据和疫情流行病参数构建基于Agent和SEIR的疫情传播模型,并模拟自然传播过程,得到疫情风险地图；根据疫情风险地图确定疫苗投放策略,并基于Agent和SEIR的疫情传播模型模拟疫苗投放后的疫情结果。</td>   <td>G06Q10/06;G06F16/29;G16H50/80</td>  </tr>        <tr>   <td>中国专利</td>   <td>         康显桂;              廖异;                   阳建华       </td>   <td>中山大学</td>   <td>基于生成对抗网络的端到端JPEG域图像隐写方法</td>   <td>广东省</td>   <td>CN112634117A</td>   <td>2021-04-09</td>   <td>本发明提供一种基于生成对抗网络的端到端JPEG域图像隐写方法。该方法由三部分完成：编码器,解码器以及判别器。生成对抗网络的生成网络部分由编码器与解码器联合构成,对抗部分由判别器完成。编码器负责秘密信息的嵌入,解码器负责秘密信息的提取,判别器对载体图像和载密图像进行区分,将该分类误差作为损失函数来进行对抗训练,不断提升生成对抗网络的性能。另外加入干扰层用于模拟实际传输信道中会受到的常见干扰。本发明提出的针对JPEG域图像的隐写方法通过修改DCT系数的方式对JPEG图像进行秘密信息的嵌入与提取,具有较强的实用性；训练过程中添加干扰层,提高了算法在实际应用场景下的鲁棒性；通过与判别器进行对抗训练,增强了隐写算法的安全性。</td>   <td>1.一种基于生成对抗网络的端到端JPEG域图像隐写方法,其特征在于,秘密信息的嵌入与提取均由生成对抗网络来完成,所述的生成对抗网络包括编码器、解码器和判别器,另外加入干扰层用于模拟实际传输信道中会受到的常见干扰；所述生成对抗网络的训练包括以下步骤：S1：将秘密信息和载体图像在JPEG域上的DCT系数矩阵输入到编码器中,由编码器输出载密图像对应的DCT系数矩阵；S2：将载密图像的DCT系数矩阵输入到IDCT变换模块中,得到空域载密图像；S3：将空域载密图像输入干扰层,得到加入干扰后的噪声载密图像的DCT系数；S4：将上述生成的噪声载密图像的DCT系数输入到解码器中,得出解密信息；S5：将S1中的载体图像的DCT系数矩阵输入到IDCT变换模块中,得到空域载体图像；S6：将S5中的空域载体图像和S2得到的空域载密图像输入到判别器中,判别器对空域载体图像和空域载密图像进行二分类,将该分类误差作为损失函数,并把该损失函数反向传播从而进行生成对抗网络的更新；S7：重复步骤S1-S6,直至得到训练后的生成对抗网络。</td>   <td>G06T1/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>              韦舒心       </td>   <td>中山大学</td>   <td>基于递进学习策略的图像修复方法、系统、介质及设备</td>   <td>广东省</td>   <td>CN112634157A</td>   <td>2021-04-09</td>   <td>本发明公开了一种基于递进学习策略的图像修复方法、系统、存储介质及终端设备,包括：将待修复图像输入粗生成模型,以得到粗生成结果；提取所述粗生成结果的特征图,以得到前景特征图；将所述前景特征图与背景特征图进行匹配,并将得到的与所述前景特征图匹配度最高的背景样本块对所述前景特征图进行重构。本发明能够在对图像中大面积破损区域进行生成时,保持图像语义完整并生成较为清晰的细节,对结构性强的图片有较好的修复效果,从而提升了用户图像修复体验。</td>   <td>1.一种基于递进学习策略的图像修复方法,其特征在于,所述方法包括：将待修复图像输入粗生成模型,以得到粗生成结果；提取所述粗生成结果的特征图,以得到前景特征图；将所述前景特征图与背景特征图进行匹配,并将得到的与所述前景特征图匹配度最高的背景样本块对所述前景特征图进行重构。</td>   <td>G06T5/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈刚;              张余;                   黄凯       </td>   <td>中山大学</td>   <td>一种实时的双目相机矫正方法、装置及存储介质</td>   <td>广东省</td>   <td>CN112634372A</td>   <td>2021-04-09</td>   <td>本发明公开了一种实时的双目相机矫正方法、装置及存储介质,该方法将矫正过程部署到FPGA上进行,包括：通过双目相机获取第一图像中多个像素点坐标,所述第一图像为矫正前的图像；将多个所述像素点坐标分别存储在所述FPGA上的多个随机存储器中；利用所述FPGA上的多个随机存储器,并根据多个所述像素点坐标,利用双线性插值法进行插值,得到第二图像,所述第二图像为矫正后的图像。本发明能够解决现有矫正方法存在的高延迟、高功耗、高资源等问题,通过将双目相机矫正过程部署到FPGA硬件设备上,可以实时高效的对相机进行矫正处理。为将整个立体匹配过程部署到硬件上做了很好的铺垫；同时,本发明能够实现基于FPGA的低功耗、低资源的同时,实时高效地进行矫正处理。本发明可广泛应用于机器视觉领域。</td>   <td>1.一种实时的双目相机矫正方法,将矫正过程部署到FPGA上进行,其特征在于,包括：通过双目相机获取第一图像中多个像素点坐标,所述第一图像为矫正前的图像；将多个所述像素点坐标分别存储在所述FPGA上的多个随机存储器中；利用所述FPGA上的多个随机存储器,并根据多个所述像素点坐标,利用双线性插值法进行插值,得到第二图像,所述第二图像为矫正后的图像。</td>   <td>G06T7/80;G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谢晓华;              黄俊嘉;                   赖剑煌       </td>   <td>中山大学</td>   <td>一种基于边缘增强的图像去模糊方法</td>   <td>广东省</td>   <td>CN112634153A</td>   <td>2021-04-09</td>   <td>本发明公开了一种基于边缘增强的图像去模糊方法,该方法包括：获取运动模糊图像并基于边缘提取算法对运动模糊图像进行处理,得到运动模糊图像边缘；将运动模糊图像和运动模糊图像边缘输入到预设的去模糊神经网络进行特征提取和特征融合,得到去模糊图像。本发明在保证去模糊质量的情况下,减少模型推导时间和模型参数数量。本发明作为一种基于边缘增强的图像去模糊方法,可广泛应用于图像处理领域。</td>   <td>1.一种基于边缘增强的图像去模糊方法,其特征在于,包括以下步骤：获取运动模糊图像并基于边缘提取算法对运动模糊图像进行处理,得到运动模糊图像边缘；将运动模糊图像和运动模糊图像边缘输入到预设的去模糊神经网络进行特征提取和特征融合,得到去模糊图像。</td>   <td>G06T5/00;G06T5/50;G06T7/13;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              谢志勇;              陈荣军;              谢舜道;              林远鑫;              朱雄泳;                   曾衍瀚       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种离焦QR码图像盲复原方法</td>   <td>广东省</td>   <td>CN108765305B</td>   <td>2021-04-06</td>   <td>本发明公开了一种离焦QR码图像盲复原方法,先对QR码图像进行灰度化处理以及边缘检测处理后得到边缘图像,然后对边缘图像进行处理得到边缘直线以及导数值变化率最大的点,根据边缘直线和导数值变化率最大的点可以计算得到离焦半径,进而可以得到点扩散函数,最后根据点扩散函数对模糊的QR码图像进行复原,在尽可能恢复图像质量的同时,减少计算量,并且减少复原的时间。</td>   <td>1.一种离焦QR码图像盲复原方法,其特征在于：包括以下步骤：A、对输入的QR码图像进行灰度化处理；B、对灰度化处理后的QR码图像进行截取,得到边缘图像；C、对边缘图像进行边缘检测,得到边缘矩阵；D、对边缘矩阵进行逐列扫描,获取边缘直线的位置；E、对边缘图像进行求导,并通过计算得到导数值变化率最大的点；F、计算边缘直线和导数值变化率最大的点之间的距离,得到估计的离焦半径；G、根据离焦半径计算得到点扩散函数,根据点扩散函数对散焦模糊的QR码图像进行复原；所述步骤E对边缘图像进行求导的具体步骤为：对边缘图像进行二次求导,其中第一次求导的公式为：dx(i,j)＝I(i+1,j)-I(i,j)；dy(i,j)＝I(i,j+1)-I(i,j)；其中I为边缘图像,I(i,j)为边缘图像I中位置为(i,j)处的值,在求得一阶导数后,根据以下公式对一阶导数进行再次求导：G(x,y)＝dx(i,j)-dy(i,j)；所述步骤E通过计算得到导数值变化率最大的点,根据边缘图像I的二阶导数以及边缘直线的法线方向得到导数值变化率最大的点。</td>   <td>G06T5/00;G06T7/13;G06K19/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              吴卓亮;              欧阳柳;                   谢晓华       </td>   <td>中山大学</td>   <td>一种多特征融合的人脸图像光照识别方法及系统</td>   <td>广东省</td>   <td>CN109583375B</td>   <td>2021-04-06</td>   <td>本发明公开了一种多特征融合的人脸图像光照识别方法,包括步骤：对每一张不同光照情形下的人脸图像,提取图像的协方差矩阵,计算图像的区域差别特征,二者融合作为图像的统计特征；基于神经网络方法提取人脸图像的深度特征；将所述统计特征和深度特征进行融合,得到融合后特征；对融合后特征进行分类,以实现人脸图像的光照识别。本发明还公开了一种多特征融合的人脸图像光照识别系统。本发明能够实现对不同光照条件下人脸图像的区分,且准确率高、识别速度快、未来技术更新容易。</td>   <td>1.一种多特征融合的人脸图像光照识别方法,其特征在于,包括步骤：对每一张不同光照情形下的人脸图像,提取图像的协方差矩阵,计算图像的区域差别特征,二者融合作为图像的统计特征；基于神经网络方法提取人脸图像的深度特征；将所述统计特征和深度特征进行融合,得到融合后特征；对融合后特征进行分类,以实现人脸图像的光照识别；提取图像的协方差矩阵,步骤是：对于一张输入的人脸图像,计算图像的协方差矩阵,得到大小为n×n的矩阵,然后将矩阵拉直成为n×n一维长度的特征；对于n×n一维长度的特征,利用预先训练好的auto-encoder模型,将数据进行降维,得到降维后的特征；计算图像的区域差别特征,步骤是：对于一张输入的人脸照片,求出左边和右边的平均亮度差值作为图像的左右偏差,求出上面和下面的平均亮度差值作为图像的上下偏差,求出整张图的平均亮度值,将这3个值作为人脸图像提取出的区域差别特征；将降维后的特征与区域差别特征串联,得到统计特征。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              张培熙;                   谢晓华       </td>   <td>中山大学</td>   <td>一种具有光照鲁棒性的行人再识别方法</td>   <td>广东省</td>   <td>CN109919073B</td>   <td>2021-04-06</td>   <td>本发明公开了一种具有光照鲁棒性的行人再识别方法,其包括以下步骤：获取具有光照差异的行人再识别数据集,从中选取训练样本,将训练样本划分为正常光照图片和昏暗光照图片；初始化一个深度卷积神经网络netA,用正常光照图片对其进行训练；初始化一个与netA结构完全相同的深度卷积神经网络netB,用昏暗光照图片对其进行训练；合并两种光照的数据集,同时对netA和netB进行协同训练,使两个网络均收敛；测试netA和netB各自的性能,选择性能较高的网络作为最终模型。本发明在不增加最终模型参数的情况,通过双网络的协同学习,增强行人再识别网络对昏暗图片的识别能力,提高模型对于光照的鲁棒性。</td>   <td>1.一种具有光照鲁棒性的行人再识别方法,其特征在于,包括步骤：步骤S1：获取具有光照差异的行人再识别数据集,从中选取训练样本,将训练样本划分为正常光照图片和昏暗光照图片；步骤S2：初始化一个深度卷积神经网络netA,用正常光照图片对其进行训练；初始化一个与netA结构完全相同的深度卷积神经网络netB,用昏暗光照图片对其进行训练；步骤S3：合并两种光照的数据集,同时对netA和netB进行协同训练,使两个网络均收敛；步骤S4：测试netA和netB各自的性能,选择性能较高的网络作为最终模型；所述步骤S3中,对netA和netB进行协同训练,步骤是：步骤S31：获取训练后得到的两个深度网络netA和netB；步骤S32：合并训练集里面所有图片,包括昏暗光照图片以及正常光照图片,将其作为netA和netB的训练数据,采用交叉熵损失和加权KL散度损失对两个网络进行训练,直至netA和netB的损失均收敛,完成训练。</td>   <td>G06K9/00;G06K9/20;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陶文宇;              戴志强;              朱祥维;              李芳;              陈伟翔;                   陈彦莛       </td>   <td>中山大学</td>   <td>一种室外环境视觉惯性SLAM方法及装置</td>   <td>广东省</td>   <td>CN112613372A</td>   <td>2021-04-06</td>   <td>本发明公开了一种室外环境视觉惯性SLAM方法及装置,方法包括：通过提取原始图像的图像梯度信息得到梯度图像,对梯度图像进行阈值分割处理得到待优化图像；根据待优化图像计算得到待整合边界数据；采用多项式拟合算法对待整合边界数据进行整合处理,得到最终边界数据；根据最终边界数据对原始图像进行分割,得到天空区域图像,将惯性导航数据以及非天空区域图像进行SLAM初始化得到SLAM框架,根据SLAM框架进行SLAM构图定位。本发明实施例通过采用多项式拟合算法对边界线进行修正,能够有效降低运算量和运算时间,能够快速、准确地得到天空区域分割的效果,从而能够有效提高定位和轨迹建图的准确性。</td>   <td>1.一种室外环境视觉惯性SLAM方法,其特征在于,包括：通过提取原始图像的图像梯度信息得到梯度图像,对所述梯度图像进行阈值分割处理得到待优化图像；根据所述待优化图像的参数定义天空边界函数,根据梯度优化能量函数对所述天空边界函数进行计算得到待整合边界数据；采用多项式拟合算法对所述待整合边界数据进行整合处理,得到最终边界数据；根据所述最终边界数据对所述原始图像进行分割,得到天空区域图像,将惯性导航数据以及所述非天空区域图像进行SLAM初始化得到SLAM框架,根据所述SLAM框架进行SLAM构图定位。</td>   <td>G06K9/00;G06K9/34;G06K9/46;G06K9/62;G01C21/16</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;                   罗京       </td>   <td>中山大学</td>   <td>基于弱监督学习的用户定制化目标检测方法、系统和存储介质</td>   <td>广东省</td>   <td>CN112613548A</td>   <td>2021-04-06</td>   <td>本发明公开了一种基于弱监督学习的用户定制化目标检测方法、系统及存储介质,方法包括下述步骤：构建目标检测架构,包括客户端和服务端；在客户端上传训练所需的图像数据和图像类别标注数据,服务端根据标注类别数构建WSDDN-PCL弱监督目标检测模型；服务端使用用户上传的图像和标注数据训练弱监督目标检测模型,训练好的模型保存在服务端；在客户端上传需要检测的图像数据,服务端加载训练好的目标检测模型,并对用户上传的图像数据进行检测,将检测结果存储在服务端；用户从服务端下载检测结果,完成目标检测任务。本发明的方法可以定制化地从线上图库中爬取数据并训练目标检测模型,并将复杂的计算过程放到服务器进行,同时满足易用性和快速性的要求。</td>   <td>1.基于弱监督学习的用户定制化目标检测方法,其特征在于,包括下述步骤：构建目标检测架构,所述目标检测架构包括客户端和服务端,所述客户端采用PyQT设计,用于与服务端交互、采集网络数据及过滤不良数据；所述服务端使用tornado搭建,用于接收用户上传数据、创建目标检测模型、训练模型、存储模型、存储训练数据和检测结果,所述服务器数据库使用MySQL管理,用于图像数据、标注数据和模型的存储,所述目标检测模型使用Pytorch搭建；在客户端上传训练所需的图像数据和图像类别标注数据,服务端根据标注类别数构建WSDDN-PCL弱监督目标检测模型；服务端使用用户上传的图像和标注数据训练弱监督目标检测模型,训练好的模型保存在服务端；在客户端上传需要检测的图像数据,服务端加载训练好的目标检测模型,并对用户上传的图像数据进行检测,将检测结果存储在服务端；用户从服务端下载检测结果,完成目标检测任务。</td>   <td>G06K9/62;G06K9/46;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;                   陈柏高       </td>   <td>中山大学</td>   <td>基于双模型交互式半监督学习的离线行人跟踪方法、系统及存储介质</td>   <td>广东省</td>   <td>CN112614150A</td>   <td>2021-04-06</td>   <td>本发明公开了一种基于双模型交互式半监督学习的离线行人跟踪方法、系统及存储介质,所述方法包括：选用两个基于神经网络的行人跟踪模型,使用有标签训练数据对其进行有监督训练；伪标签预测并进行离线插值优化；采用交互式半监督方法进行学习；最终预测和输出。本发明采用离线插值优化方法,利用了完整的视频信息,对行人轨迹断开的部分进行了插值修正,使得行人轨迹断开情况较少,受行人遮挡的影响较少。本发明还提出了一种利用双模型针对测试数据进行半监督自学习的方法,使得模型能逐步熟悉测试数据,在多轮迭代之后性能获得较大提升,面对模型未见过的场景,也能保持良好的表现。</td>   <td>1.一种基于双模型交互式半监督学习的离线行人跟踪方法,其特征在于,包括下述步骤：选用两个基于神经网络的行人跟踪模型,使用有标签训练数据对其进行有监督训练,直至模型能基本拟合训练数据,得到在训练数据集上预测表现良好初始模型权重F-(T1)和F-(T2)；令迭代模型F-(S1)＝F-(T1),F-(S2)＝F-(T2)；使用所述迭代模型F-(S1)和F-(S2)对无标签测试数据直接进行预测,同时使用离线插值优化方法对结果进行优化,得到所述迭代模型F-(S1)和F-(S2)的输出结果,即伪标签1和伪标签2；所述离线插值优化方法用于拼接断开的行人轨迹并使中间帧丢失的行人轨迹边界框得到恢复；采用交互式半监督方法进行学习：使用所述迭代模型F-(S1)输出的伪标签1作为无标签测试数据的训练标签,得到伪标记数据1,将所述伪标记数据1和有标签训练数据混合到一起,作为所述迭代模型F-(S2)的训练数据,对其进行再次训练；同理,反过来将所述迭代模型F-(S2)输出的伪标签2与无标签测试数据结合得到伪标记数据2,将所述伪标记数据2与有标签训练数据混合作为迭代模型F-(S1)的训练数据并对其训练；对所述F-(S1)和F-(S2)进行循环迭代训练；使用所述迭代后的模型F-(S1)和F-(S2)对无标签测试数据进行预测,即得最终的输出结果。</td>   <td>G06T7/20;G06K9/62;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              程凤雯;              张伟;                   刘泽华       </td>   <td>中山大学</td>   <td>一种基于部分解耦条件下通道分配的动作识别方法及系统</td>   <td>广东省</td>   <td>CN112597856A</td>   <td>2021-04-02</td>   <td>本发明公开了一种基于部分解耦条件下通道分配的动作识别方法及系统,该方法包括：获取视频信息并对视频信息进行处理,抽取视频帧图像；将视频帧图像输入到预设的卷积神经网络进行动作识别,得到识别结果；所述预设的卷积神经网络包括残差层、两个R(2+1)块、Decoupled-3D模块、池化层和全连接层。该系统包括：视频帧抽取模块和识别模块。本发明通过Decoupled-3D模块分配空间和时间上的通道维度信息来平衡模型的表达能力。本发明作为一种基于部分解耦条件下通道分配的动作识别方法及系统,可广泛应用于模型改进领域。</td>   <td>1.一种基于部分解耦条件下通道分配的动作识别方法,其特征在于,包括以下步骤：获取视频信息并对视频信息进行处理,抽取视频帧图像；将视频帧图像输入到预设的卷积神经网络进行动作识别,得到识别结果；所述预设的卷积神经网络包括残差层、两个R(2+1)块、Decoupled-3D模块、池化层和全连接层。</td>   <td>G06K9/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;                   卢少豪       </td>   <td>中山大学</td>   <td>基于二阶段聚类的无监督车辆重识别方法、系统及存储介质</td>   <td>广东省</td>   <td>CN112597871A</td>   <td>2021-04-02</td>   <td>本发明公开了一种基于二阶段聚类的无监督车辆重识别方法、系统及存储介质。所述方法包括以下步骤：数据收集；预处理；相同车型下的局部聚类；进行簇合并并得到识别结果。本发明提出的二阶段聚类方法充分利用了车辆的特性,从网络上爬取大量的带粗粒度标签的数据,利用这些数据训练出来的分类器可以先将目标数据集的车辆分为N个部分,然后在每个部分单独进行聚类得到簇。第二阶段聚类则将上述得到簇再做一次相同的聚类,用于合并一些相似的簇,由于一开始的分类器并不可靠,合并相似的簇使得之前分错的簇得以合并,使得最终的伪类标更可信,时效性更高,从而达到更好的训练效果。实验结果证明该方法能在目前一些开源数据集中取得最优的水平。</td>   <td>1.一种基于二阶段聚类的无监督车辆重识别方法,其特征在于,包括以下步骤：数据收集,收集大量带车型标注的图片；预处理,用目标检测算法检测所述带车型标注的图片的车辆位置并做裁剪；相同车型下的局部聚类,利用裁剪后的图片训练车型分类器,所述车型分类器用于将目标数据分为若干个簇,然后基于密度的聚类算法在每个相同车型中聚类,当两张图片的距离小于距离阈值时,则认为这两张图片属于同一个簇,否则,不属于同一个簇；进行簇合并,将所述相同车型下的局部聚类阶段得到的簇定义为簇中心,计算簇内所有样本的特征的均值并作为所述簇中心的特征,如果两个簇的簇中心小于设定阈值,则利用聚类算法将两个簇合并；两次层次聚类后,得到多个簇,每个簇里面的样本属于同一辆车。</td>   <td>G06K9/00;G06K9/46;G06K9/62;G06N3/02</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苏勤亮;                   欧宏宇       </td>   <td>中山大学</td>   <td>一种基于多通道图卷积的文本分类方法</td>   <td>广东省</td>   <td>CN112598044A</td>   <td>2021-04-02</td>   <td>本发明提供一种基于多通道图卷积的文本分类方法,该方法从不同角度对文本节点和单词节点建立多个图,可以对单词节点间的同质性进行更全面的建模。对每个图都在一个对应的通道内进行卷积,并且在每一层图卷积网络中,同一节点在不同通道中的特征信息可以相互交流,并用门机制控制信息交流的通过量,提高特征提取的效果。</td>   <td>1.一种基于多通道图卷积的文本分类方法,其特征在于,包括以下步骤：S1：从单词相似性和相关性角度构造文本之间的关系图；S2：利用S1得到的多个关系图进行多通道图卷积；S3：在S1的多通道图卷积过程中让同一节点在不同通道间交流信息,并用门机制控制节点信息在交流时通过量。</td>   <td>G06K9/62;G06F40/216;G06F40/284;G06F40/242;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陶文宇;              戴志强;              朱祥维;              李芳;              陈伟翔;                   陈彦莛       </td>   <td>中山大学</td>   <td>一种天空区域分割方法及装置</td>   <td>广东省</td>   <td>CN112598679A</td>   <td>2021-04-02</td>   <td>本发明公开了一种天空区域分割方法及装置,方法包括：通过提取原始图像的图像梯度信息得到梯度图像,对梯度图像进行阈值分割处理得到待优化图像；基于阈值阶数根据梯度优化能量函数计算待优化图像的能量函数值和边界数据,在能量函数值为最大能量函数值时,将边界数据作为待整合边界数据；采用多项式拟合算法对待整合边界数据进行整合处理,得到待修正边界数据；根据待修正边界数据获得分割后的天空区域,计算天空区域的真实像素均值,根据真实像素均值对待修正边界数据进行修正得到最终边界数据,根据最终边界数据得到天空区域的分割结果。本发明实施例采用多项拟合算法对待整合边界数据进行整合处理,能够有效提高天空区域的分割效果。</td>   <td>1.一种天空区域分割方法,其特征在于,包括：通过提取原始图像的图像梯度信息得到梯度图像,对所述梯度图像进行阈值分割处理得到待优化图像；基于阈值阶数根据梯度优化能量函数计算所述待优化图像的能量函数值和边界数据,在所述能量函数值为最大能量函数值时,将所述边界数据作为待整合边界数据；采用多项式拟合算法对所述待整合边界数据进行整合处理,得到待修正边界数据；根据所述待修正边界数据获得分割后的天空区域,计算所述天空区域的真实像素均值,根据所述真实像素均值对所述待修正边界数据进行修正得到最终边界数据,根据所述最终边界数据得到天空区域的分割结果。</td>   <td>G06T7/11;G06T7/136</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈刚;              仲崇豪;                   孟海涛       </td>   <td>中山大学</td>   <td>一种激光雷达立体相机融合的深度估计方法、装置及其介质</td>   <td>广东省</td>   <td>CN112233163B</td>   <td>2021-03-30</td>   <td>本发明公开了一种激光雷达立体相机融合的深度估计方法、装置及其介质,该方法包括：获取立体相机的当前帧左图像和当前帧右图像；获取雷达左图像和雷达右图像；将当前帧左图像与雷达左图像融合得到第一左图像；将当前帧右图像与雷达右图像融合得到第一右图像；将第一左图像输入二值神经网络中进行特征提取,并聚合得到第一特征左图像；将第一右图像输入二值神经网络中进行特征提取,并聚合得到第一特征右图像；获取第一特征左图像和第一特征右图像之间的初始匹配代价；基于交叉的雷达信任聚合和半全局立体匹配算法优化所述初始匹配代价并提取视差图；根据所述视差图进行深度估计。本发明能够获得准确可靠的深度预测,广泛应用于图像处理技术领域。</td>   <td>1.一种激光雷达立体相机融合的深度估计方法,其特征在于,包括：获取立体相机的当前帧左图像和当前帧右图像；获取雷达左图像和雷达右图像,所述雷达左图像与所述当前帧左图像对应同一物体同一部位的图像,所述雷达右图像与所述当前帧右图像对应同一物体同一部位的图像；将所述当前帧左图像与所述雷达左图像融合得到第一左图像；将所述当前帧右图像与所述雷达右图像融合得到第一右图像；将所述第一左图像输入二值神经网络中进行特征提取,并聚合得到第一特征左图像；将所述第一右图像输入二值神经网络中进行特征提取,并聚合得到第一特征右图像；获取所述第一特征左图像和所述第一特征右图像之间的初始匹配代价；基于交叉的雷达信任聚合和半全局立体匹配算法优化所述初始匹配代价并提取视差图；根据所述视差图进行深度估计；所述基于交叉的雷达信任聚合和半全局立体匹配算法优化所述初始匹配代价这一步骤,包括：在所述雷达左图像中确定第一目标点,并通过所述第一目标点勾画十字图形,所述第一目标点为所述雷达左图像中的任意一个有效点,所述有效点为点值大于零的点；通过第一公式,获取第一距离,所述第一距离为竖直方向或水平方向距离第二目标点最长的距离,所述第二目标点为所述当前帧左图像中与所述第一目标点对应的点；所述第一公式为：；式中,表示所述第一距离,表示所能达到的最大数值,表示所述第二目标点的坐标,表示所述第二目标点竖直方向或水平方向上的点的坐标,          表示指示函数,用于表示坐标和坐标之间的像素强度的差值是否小于一个阈值；其中,所述通过第二公式计算得到,所述第二公式为：；式中,表示坐标的像素强度,表示坐标的像素强度；表示坐标的像素强度与坐标的像素强度的绝对差值,表示像素强度差的阈值；根据所述第一距离,通过第三公式对所述初始匹配代价进行优化,所述第三公式为：；式中,表示点坐标,表示雷达左图像中坐标的视差,表示所述第一目标点的坐标,坐标与坐标相对应,数值完全相同,表示点坐标与坐标在竖直方向上或水平方向上的距离,表示指示函数,用于表示坐标和点坐标之间的像素强度的差值是否小于一个阈值；表示优化后点坐标在视差处的匹配代价,表示利用加权汉明距离方法获取的点坐标在视差处的初始匹配代价；根据优化后的匹配代价,通过半全局立体匹配算法提取视差图。</td>   <td>G06T7/521;G06T7/593;G06T5/00;G06T5/50;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         顾家宝;                   杨然       </td>   <td>中山大学</td>   <td>一种基于无人小车的三维重建系统及其方法</td>   <td>广东省</td>   <td>CN112580487A</td>   <td>2021-03-30</td>   <td>本发明提供一种基于无人小车的三维重建系统,其中包括树莓派、PC端、双目相机模块、RGB相机模块、控制舵机和IO扩展控制板。双目相机模块通过USB与树莓派通信连接,RGB相机模块通过USB与树莓派通信连接,树莓派与PC端通过WIFI双向通信,树莓派通过GPIO与IO扩展控制板通信连接,IO扩展控制板通过PWM控制与其连接的车轮驱动电机和控制舵机的移动,RGB相机模块位于控制舵机上。本发明还提供一种基于无人小车的三维重建方法。本发明能够自动对三维模型缺失位置进行分析,根据缺失位置重新规划无人小车行驶路径,根据行驶路径再次进行拍摄,完成三维重建。</td>   <td>1.一种基于无人小车的三维重建系统,其特征在于：包括树莓派、PC端、双目相机模块、RGB相机模块、控制舵机和IO扩展控制板,其中,树莓派：采集双目相机模块、RGB相机模块和PC端的数据信息,并下发移动指令给IO扩展控制板；控制舵机：RGB相机模块位于控制舵机上,控制舵机与IO扩展控制板连接,根据IO扩展控制板的移动指令完成移动IO扩展控制板：根据树莓派下发的移动指令控制与其连接的小车的驱动电机和控制舵机完成移动；双目相机模块：包括IMU和双目摄像头,两者跟随小车移动获取位姿信息；RGB相机模块：包括RGB摄像头获取图像的色彩信息输入树莓派；PC端：具有SfM系统,根据树莓派上传的数据完成三维重建,并对树莓派下发移动路径信息。</td>   <td>G06K9/00;G06T17/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谢曦;              徐嘉荣;              黎洪波;              胡宁;              陈惠琄;              李柏鸣;              钟成锦;              李仁杰;              何根;                   杭天       </td>   <td>中山大学</td>   <td>一种鼠类行为分析方法及装置</td>   <td>广东省</td>   <td>CN112580552A</td>   <td>2021-03-30</td>   <td>本申请实施例公开了一种鼠类行为分析方法和装置,其中方法包括：采集用于训练的训练视频片段；获取视频中活动区域的边界框,将边界框和鼠类的信息作为鼠类目标检测的神经网络的的监督信息,并对人工卷积神经网络进行训练；采用训练后的人工卷积神经网络对鼠类视频片段进行目标检测,得到预设大小的感兴趣区域；根据连续两帧中感兴趣区域中心坐标的位移和时间差,得到实时运动总里程,从而获得鼠类的运动状态。本申请所提供鼠类行为分析方法可以有效对小鼠的行为信息进行量化和分析,为动物行为研究提供可靠的研究信息,解决了采取佩戴时传感器的方式来对实验动物进行追踪的办法存在的鲁棒性低,无线传感器传输距离短,使用成本高的问题。</td>   <td>1.一种鼠类行为分析方法,其特征在于,包括步骤：S1,采集用于训练的训练视频片段；S2,获取所述训练视频片段视频中鼠类的活动区域的边界框,并将所述边界框和鼠类的信息作为鼠类目标检测的第一人工卷积神经网络的第一监督信息,并对所述第一人工卷积神经网络进行训练；S3,采集用于分析的鼠类视频片段；S4,采用训练后的第一人工卷积神经网络对所述鼠类视频片段进行目标检测；S5,在检测到鼠类的视频帧中标记出预设大小的边界框和所述鼠类的信息,并将所述边界框内的区域定义为感兴趣区域；S6,获取所述鼠类视频片段的连续两帧中所述感兴趣区域的中心坐标的位移和时间差,得到鼠类的瞬时速度,对所述瞬时速度进行积分,得到鼠类的实时运动总里程；S7,根据所述实时运动总里程获得鼠类的运动状态。</td>   <td>G06K9/00;G06K9/32;G06K9/46;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈龙;              叶梓豪;                   刘坤华       </td>   <td>中山大学</td>   <td>一种基于互相关的多传感器数据时间对齐系统及其方法</td>   <td>广东省</td>   <td>CN112580683A</td>   <td>2021-03-30</td>   <td>本发明提供一种基于互相关的多传感器数据时间对齐系统,其中,数据处理模块是对相机传感器和激光雷达传感器的数据进行格式转换,将数据处理模块处理后的数据分别输入视觉帧处理模块、点云帧处理模块,得出图像的三轴角度差序列和点云帧的三轴角度差序列,序列输入模块是能够直接输出三轴角度差序列的传感器,序列对齐模块对各个三轴角度差序列进行对齐处理得出时域对齐的多传感器数据组。本发明还提供一种基于互相关的多传感器数据时间对齐方法。本发明不需要时间戳信息作为参考数据,可以完全基于原始数据本身所包含的信息来进行时间对齐,应用场景更广,可适用于各类的数据采集场景,泛用性更强,准确度更高。</td>   <td>1.一种基于互相关的多传感器数据时间对齐系统,其特征在于：包括数据处理模块、序列输入模块、视觉帧处理模块、点云帧处理模块和序列对齐模块；数据处理模块：对相机传感器和激光雷达传感器的数据进行格式转换分别输入视觉帧处理模块和点云帧处理模块；序列输入模块：能够直接采集三轴角度差序列输入序列对齐模块；视觉帧处理模块：对数据处理模块输入的模块进行计算处理得出图像的三轴角度差序列输入序列对齐模块；点云帧处理模块：对数据处理模块输入的模块进行计算处理得出点云帧的三轴角度差序列输入序列对齐模块；序列对齐模块：将输入的各个三轴角度差序列进行对齐处理得出时域对齐的多传感器数据组。</td>   <td>G06K9/62;G06K9/46;G06T7/246;G06T7/269;G01C21/00;G01C21/16;G01S17/58;G01S17/86;G01S17/88</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;              马健荣;                   张青       </td>   <td>中山大学</td>   <td>基于双向光照估计与融合修复的图像曝光校正方法、系统及存储介质</td>   <td>广东省</td>   <td>CN112581392A</td>   <td>2021-03-30</td>   <td>本发明公开了一种基于双向光照估计与融合修复的图像曝光校正方法、系统及存储介质,所述方法包括：将原始输入图像I及其反向图Inv输入双向光照估计网络,得到前向光照图S和反向光照图Sinv；联合I、Inv、S和Sinv得到欠曝光校正结果UC和过曝光校正结果OC；融合网络将所述I、UC和OC中曝光质量良好的区域融合在一起,输出最终图像。本发明还利用神经网络从海量数据中进行训练学习,并利用损失函数进行约束,可得到高质量的图像输出。本发明运用双向光照估计网络和融合网络的技术方案,只需一张待曝光校正图像作为输入,能够完成任意曝光程度图像的校正任务。</td>   <td>1.一种基于双向光照估计与融合修复的图像曝光校正方法,其特征在于,包括以下步骤：将待曝光校正的原始输入图像I进行反向处理得到反向图Inv,其中Inv＝1–I；将原始输入图像I及反向图Inv输入双向光照估计网络中；所述双向光照估计网络包括前向光照估计网络和反向光照估计网络,其中,原始输入图像I送入前向光照估计网络中,输出为前向光照图S；反向图Inv送入反向光照估计网络中,输出为反向光照图Sinv；联合所述原始输入图像I、反向图Inv、前向光照图S和反向光照图Sinv,得到欠曝光校正结果UC和过曝光校正结果OC；所述欠曝校正结果UC是通过调高图像的亮度,将图像中较暗的区域进行亮度增强得到的,其中UC＝I/S；所述过曝校正结果OC是通过调低图像的亮度,将图像中较亮的区域进行亮度减弱得到的,其中OC＝1-Inv/Sinv；融合网络通过输出权重图的方式来将所述原始输入图像I、欠曝光校正结果UC和过曝光校正结果OC中曝光质量良好的区域融合在一起,得到曝光良好的结果图像。</td>   <td>G06T5/00;G06T5/50;G06T7/90</td>  </tr>        <tr>   <td>中国专利</td>   <td>         熊会元;              刘建勋;              刘羽;                   李同同       </td>   <td>中广核工程有限公司;中山大学</td>   <td>一种放射性废物包识别定位方法与装置</td>   <td>广东省</td>   <td>CN112581519A</td>   <td>2021-03-30</td>   <td>本发明公开一种放射性废物包识别定位方法与装置,方法包括：制作废物包点云模板及屏蔽容器点云模板,通过利用环境约束特征向量的两阶段匹配方法识别出屏蔽容器的位姿及废物包的位姿,以此进行识别定位,根据识别定位的结果对废物包进行装载或卸载。本发明实现对屏蔽容器姿态与内部状态识别与精确定位,实现了屏蔽外箱内货包的精确识别与定位,解决了对环境光照、遮挡等敏感的传统技术无法解决的箱内货包识别定位问题。</td>   <td>1.一种放射性废物包识别定位方法,其特征在于,制作废物包点云模板及屏蔽容器点云模板,通过两阶段匹配算法识别出屏蔽容器的位姿及废物包的位姿,以此进行识别定位,根据识别定位的结果对废物包进行装载或卸载,具体步骤如下：S1、通过激光扫描传感器采集场景点云,提取标定好的车道有效区域点云,进行降噪滤波去除杂乱随机离群点,并提取地面点云；S2、去除地面点云,并对场景内物体点云进行聚类,获得各个物体点云,并通过筛选获取聚类的车辆点云及屏蔽容器点云；S3、应用屏蔽容器点云模板对获取的屏蔽容器点云进行两阶段位姿匹配,识别得到屏蔽容器的位姿,如位姿正常,则根据位姿指导装卸装置卸载屏蔽容器的顶盖,如位姿异常,则重新识别屏蔽容器的位姿；S4、根据识别得到的屏蔽容器位姿,对屏蔽容器内点云进行分析,设定点云垂向密度最小阈值和最大阈值,检测屏蔽容器内是否装载废物包及屏蔽容器内部状态是否正常；S41、如点云垂向密度大于最大阈值,则屏蔽容器内装载有废物包,当屏蔽容器内装载有废物包时,要进行卸载,卸载过程如下：S411、提取屏蔽容器内的废物包点云并提取边界；S412、应用废物包点云模板对提取的废物包点云进行两阶段位姿匹配,获得废物包的位姿并传输给卸载系统,卸载系统发送操作指令给装卸装置抓取废物包进行卸载；S413、在卸载过程中实时监测废物包的位姿并反馈给卸载系统；S42、如点云垂向密度小于最小阈值,则屏蔽容器内为空,当屏蔽容器内为空时,要进行装载,装载过程如下：S421、直接将屏蔽容器的内部空间位姿传输给装载系统,装载系统发送操作指令给装卸装置抓取废物包进行装载；S422、在装载完成后检测废物包的位姿并反馈给装载系统；S43、其他状态则表明容器内存在异物,当屏蔽容器内存在异物时,则上报中控系统,在中控系统进行检查排除异常之后重新识别屏蔽容器的位姿。</td>   <td>G06T7/521;G06T7/73;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘冶;              刘宇琛;              彭楠;              陈宇恒;              杨泽锋;                   印鉴       </td>   <td>广州赫炎大数据科技有限公司;中山大学</td>   <td>一种基于手机游戏商店的用户流失预测方法及系统</td>   <td>广东省</td>   <td>CN107609708B</td>   <td>2021-03-26</td>   <td>本发明提供一种基于手机游戏商店的用户流失预测方法及系统,包括以下步骤：从服务器日志中采集用户的基本信息、行为信息和游戏信息,并分为训练集用户和预测集用户；对训练集用户建立流失用户标签,并对原始数据进行预处理；对训练集用户和预测集用户的基本信息、行为信息和游戏信息进行特征提取、选择及规范化；根据训练集用户的特征和流失用户标签,训练梯度提升决策树算法得到用户流失预测模型；根据预测集用户的特征,通过用户流失预测模型识别出手机游戏商店的流失用户。本发明基于手机游戏商店的业务场景,能够快速准确识别潜在流失用户,为手机游戏商店及时召回流失用户提供决策支持。</td>   <td>1.一种基于手机游戏商店的用户流失预测方法,其特征在于,包括以下步骤：S1：从服务器日志中采集训练集用户和预测集用户的基本信息、行为信息和游戏信息,对训练集用户建立流失用户标签,并对原始数据进行预处理；具体包括步骤：S11：根据需要进行用户流失预测的时间段,从服务器日志中采集对应时间段的训练集用户和预测集用户的基本信息、行为信息和游戏信息,并对训练集用户建立流失用户标签；其中,所述流失用户的定义为：前n天上线的用户中,在后m天内未达到活跃条件的用户记为流失用户,其中活跃条件为用户的已发生事件总数大于j并且活跃时间大于k天；其中,n、m、j和k为可调的参数；S12：对训练集用户和预测集用户的基本信息、行为信息和游戏信息进行数据清洗,包括剔除异常用户和用户的无效事件；其中,将账号数大于设定阈值的设备中的所有账户定义为异常用户；将相同账号下,与上一个事件相同并且时间间隔小于设定阈值的事件定义为无效事件；S2：对训练集用户和预测集用户的基本信息、行为信息和游戏信息进行特征提取、选择及规范化；具体包括步骤：S21：基于训练集用户和预测集用户的基本信息和游戏信息,提取基本特征和游戏特征；S22：基于训练集用户的行为信息,提取行为特征；S23：根据训练集用户的流失用户标签,对训练集用户的行为特征进行特征选择,获取关键行为特征；S24：基于训练集用户的关键行为特征和预测集用户的行为信息,提取预测集用户的关键行为特征；S25：规范化训练集用户和预测集用户的基本特征、游戏特征和关键行为特征；S3：根据训练集用户的特征和流失用户标签,训练梯度提升决策树算法得到用户流失预测模型；其中,所述训练梯度提升决策树算法的方法包括：通过设定考核指标,并采用K折交叉验证法获取最优用户流失预测模型；所述考核指标包括精确率和召回率；所述精确率指预测为流失用户中实际为流失用户的概率,所述召回率指实际为流失用户中预测为流失用户的概率；相对于所述精确率,赋予所述召回率更大的权重；S4：根据预测集用户的特征,通过用户流失预测模型识别出手机游戏商店的流失用户。</td>   <td>G06Q10/04;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         余阳;              陈秀;              吴晓鹏;                   陈家文       </td>   <td>广州天源信息科技股份有限公司;中山大学</td>   <td>一种结合社会网络和位置的线下商户推荐方法</td>   <td>广东省</td>   <td>CN108154425B</td>   <td>2021-03-26</td>   <td>本发明涉及一种结合社会网络和位置的线下商户推荐方法,用于电信电子商务公共服务综合支撑平台,包括以下步骤：数据预处理,对电信数据进行处理,得到所需数据表；根据用户通话详单,构建社会关系网络；采用CNM社区发现算法对社会网络进行层次聚类；基于用户位置,根据距离阈值筛选商户,得到候选商户列表；分析用户上网日志信息,构建用户商户二维偏好矩阵；采用基于用户的融合社会关系的协同过滤算法进行推荐。本发明利用用户通话信息构建社会关系网络,并对社会网络进行挖掘,找到联系紧密的用户团体,再结合用户位置,利用基于用户的融合社会关系的协同过滤算法进行推荐,降低计算复杂度,提高推荐的准确性。</td>   <td>1.一种结合社会网络和位置的线下商户推荐方法,其特征在于,包括以下步骤：步骤S1、数据预处理,对信令数据和用户商户数据进行处理,得到用户信息表、用户轨迹表、用户上网日志表、用户通话详单和商户信息表；步骤S2、根据用户通话详单,量化用户之间的社会关系,并构建社会关系网络；步骤S3、采用CNM社区发现算法对所构建的社会关系网络进行层次聚类,发掘社会关系网络中联系紧密的用户团体；步骤S4、基于用户位置,根据距离阈值筛选商户,得到候选商户列表；步骤S5、基于S3中聚类的结果和S4中的候选商户列表,分析用户上网日志信息,构建用户商户二维偏好矩阵；步骤S6、对于步骤S5中构建的用户商户二维偏好矩阵,采用基于用户的融合用户社会关系的协同过滤算法进行推荐；步骤S4的过程如下：在用户轨迹表中,得到用户位置在时间点t所在位置(x-t,y-t),其中x-t代表经度,y-t代表纬度；设定距离阈值D,R为地球半径,计算出距离用户位置(x-t,y-t)不超过D的经纬度范围；通过经纬度范围筛选出一部分符合条件的候选商户,然后计算商户位置(x-c,y-c)与用户位置(x-t,y-t)的距离d；然后去掉距离d大于D的商户,得到最终的候选商户列表；步骤S6采用基于用户的融合用户社会关系的协同过滤算法进行推荐,包括以下步骤：步骤S6.1、对于用户商户二维偏好矩阵,基于用户对商户的偏好向量计算用户之间的相似度并排序,取相似度前K个用户；步骤S6.2、对于相似度前K个用户,根据构建的社会关系网络,计算用户i与相似度前K个用户之间的关系紧密度；步骤S6.3、预测用户i对候选商户的偏好,选取偏好值最大的商户推荐给用户；所述关系紧密度定义为：c(i,j)＝t(i,j)*log(m(i,j))*factor其中,t(i,j)为用户i,j通话次数,m(i,j)为用户i,j通话时长；当用户i,j存在双向通信时factor系数为1,不存在双向通信时factor系数为0.5；预测用户i对候选商户的偏好,计算公式为：                  其中,c(i,j)为用户i,j的关系紧密度,sim(i,j)为用户i,j之间的相似度,为用户j对候选商户的偏好向量；步骤S5所述构建用户商户二维偏好矩阵的步骤如下：步骤S5.1、在用户上网日志表中,取用户最近T时间段的上网日志记录,提取用户搜索字段中的关键词,得到该用户的关键词列表；步骤S5.2、对关键词列表,将每个单词转化为k维的词向量,然后对词向量加权平均,得到该用户的关键字词向量；步骤S5.3、对候选商户列表中的商户,取店铺描述字段,按照步骤S5.1和步骤S5.2中的方法,计算得到商户的关键词向量；步骤S5.4、对每个用户i和商户j依次计算偏好值P-(ij),得到用户商户二维偏好矩阵,其中偏好值P-(ij)定义为用户关键词向量与商户关键词向量的cosine相似度。</td>   <td>G06Q30/06;G06Q50/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;              陈航;                   包笛       </td>   <td>中山大学</td>   <td>基于CNN低层语义特征密度图的人群密度估计方法</td>   <td>广东省</td>   <td>CN109492615B</td>   <td>2021-03-26</td>   <td>本发明属于人群分析技术领域,为基于CNN低层语义特征密度图的人群密度估计方法,包括步骤：数据的预处理,根据原图像的行人位置生成密度图；对原图像和密度图进行切片；对原图像进行MCNN多分支特征提取,对各分支特征进行卷积、池化操作后,通过MCNN特征图融合器对各分支特征进行连接得到MCNN连接特征图,并对其进行卷积操作得到初始的MCNN密度图；对原图像进行卷积得到具有低层语义特征图；将低层语义特征图与MCNN多分支特征提取后各分支生成的特征图在通道数这一维度进行连接,得到连接特征图；用若干层卷积层对连接特征图进行解码,生成最终的密度图；对最终密度图的每个像素相加求和,得到图片中的人数。MAE、MSE较低,准确率和稳定性都较高。</td>   <td>1.基于CNN低层语义特征密度图的人群密度估计方法,其特征在于,包括以下步骤：S1、数据的预处理,根据原图像的行人位置生成密度图；S2、对原图像和步骤S1中生成的密度图进行切片；S3、基于MCNN计算出初始的MCNN密度图：对原图像进行MCNN多分支特征提取,对各分支特征进行卷积、池化操作后,通过MCNN特征图融合器对各分支特征进行连接,得到MCNN连接特征图,对MCNN连接特征图进行卷积操作,得到初始的MCNN密度图；S4、对原图像进行卷积,得到具有低层语义特征图；S5、将低层语义特征图与MCNN多分支特征提取后各分支生成的特征图在通道数这一维度进行连接,完成特征的编码,得到连接特征图；S6、用若干层卷积层对连接特征图进行解码,生成最终的密度图；对得到的最终密度图的每个像素相加求和,得到图片中的人数；步骤S1中,一幅有N个人头的带有标签的人头图像表示为：                  其中,x-i表示人头在图像中的像素位置,δ(x-x-i)表示图像中人头位置的冲击函数,N为图像中的人头总数；如果x位置有人头,则δ(x)为1,否则为0；H(x)为数据预处理之前的行人位置；数据预处理之后的密度图F(x)为：                                    其中,表示高斯核,σ-i表示高斯核的标准差；d-i表示距离x-i人头最近m个人头与该人头的平均距离；β为常数,取0.3；步骤S2进行切片时,对原图像进行长宽均为相同比例的随机切片；所述比例设有三种,分别为原图1/2、1/3和1/4,每种比例切出9张子图像；步骤S3中,初始MCNN密度图和密度图真实值之间使用平方差损失函数得到L-(origin),即L-(origin)＝(output-(MCNN)-target)～2,其中,output-(MCNN)表示MCNN模型的输出,target表示MCNN密度图真实值；步骤S4中,低层语义特征图包含着边缘特征低层语义的信息,密度图修正网络AmendNet模型根据低层语义的信息对步骤S3所生成的初始MCNN密度图进行一次修正；步骤S6中,最终密度图和密度图真实值使用平方差损失函数s得到L-(final),即L-(final)＝(output-(final)-target)～2,其中,output-(final)表示最终密度图修正网络AmendNet模型的输出。</td>   <td>G06K9/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         朱俊勇;              李锴莹;              赖剑煌;                   谢晓华       </td>   <td>中山大学</td>   <td>基于循环生成对抗网络的彩色人脸图像光照域归一化的方法</td>   <td>广东省</td>   <td>CN109815893B</td>   <td>2021-03-26</td>   <td>本发明公开了一种基于循环生成对抗网络的彩色人脸图像光照域归一化的方法,包括以下步骤：S1:建立用于彩色人脸图像光照归一化的循环生成对抗网络模型；S2:建立模型的损失函数；S3:进行模型的训练,并在测试集上测试。本发明是对多种光照下的彩色人脸图像进行到指定目标光照域的转换,输入不均匀光照的彩色人脸图片,使用循环生成对抗网络作为模型架构,以目标均匀光照域为目标,实现人脸图像多光照的归一化,归一化的图像不仅可以较好的保持原有人脸的脸部属性特征,还可以很好实现跨数据集迁移。</td>   <td>1.基于循环生成对抗网络的彩色人脸图像光照域归一化的方法,其特征在于,包括以下步骤：S1：建立用于彩色人脸图像光照归一化的循环生成对抗网络模型,该循环生成对抗网络模型包括生成网络和鉴别网络,所述生成网络通过构建生成器生成转换成指定光照域的人脸图片和人脸特征重构图片,所述鉴别网络通过构建鉴别器来鉴别光照域；具体为：S11：构建生成网络,构建两个生成器G-A和G-B；其中,G-A生成器以多光照域集合A中的人脸图片A-(real)作为输入,生成与目标光照域B具有相同光照特征的图片A-(fake),以G-B生成器生成的图片B-(fake)作为输入,生成与B-(real)具有相同光照特征与人脸特征的重构图片A-(rec)；G-B生成器以G-A生成器生成的图片A-(fake)作为输入,生成与A-(real)具有相同光照特征和人脸特征的重构图片B-(rec),并且以光照域B之中的图像B-(real)作为输入生成与光照域集合A光照特征相同的图片B-(fake)；S12：构建鉴别网络,构建两个鉴别器D-A和D-B,D-A鉴别器用以鉴别人脸光照域集合A中图片A-(real)与生成器G-B生成的图片B-(fake)的光照域类别；D-B鉴别器用以鉴别人脸光照域B中图片B-(real)与生成器G-A生成图片A-(fake)的光照域间类别,鉴别器用以鉴别光照域时候,均采用多尺度特征图来进行鉴别；所述生成器G-A和生成器G-B具有相同的网络结构,其网络共有四个卷积层、六个残差模块和两个转置卷积层,其中第一个卷积层为输入层,两个用作下采样的卷积层与两个用做上采样的转置卷积层中间嵌入六个残差模块,保证输出图像与输入图像的大小一致；S2：建立模型的损失函数,使生成对抗网络训练稳定,并且使得生成器在学习目标光照域光照信息时能够比较好保留输入图像的脸部特征；具体为：S21：对于判别器D-A,其损失函数是对多光照域集合A中的人脸图片A-(real)和将光照域B之中的图像B-(real)输入G-B生成器生成的图像B-(fake)的光照域类别的鉴别损失,使用了多尺度特征图进行鉴别,当鉴别为真实时输出接近1,鉴别为假时输出接近0；S22：对于判别器D-B,其损失函数是对目标光照域集合B中的人脸图片B-(real)和将集合A之中的图像A-(real)输入G-A生成器生成的图像A-(fake)的光照域类别的鉴别损失,使用了多尺度特征图进行鉴别,当鉴别为真实时输出接近1,鉴别为假时输出接近0；S23：对于判别器G-A和G-B的损失函数,均结合了WGAN梯度惩罚策略,判别器G-A和G-B以最小化损失函数为优化目标；S24：生成器的重构损失L-(rec)构建,结合三种距离衡量方法计算重构图像与原始图像之间的距离误差,分别为L-1范数误差,MS-SSIM误差,PSNR误差；S25：生成器G-A的整体损失函数其结合了鉴别器D-B损失函数的相反数和对集合A之中的图像A-(real)与G-B生成器生成的重构图片B-(rec)的重构误差；                  S26：生成器G-B的整体损失函数其结合了鉴别器D-A损失函数的相反数和对集合B之中的图像B-(real)与G-A生成器生成的重构图片A-(rec)的重构误差；                  α-1为重构误差权重参数；S26：生成器总的损失函数为：以最小化该损失函数同时优化更新生成器G-A和G-B的参数；S3：进行模型的训练,将不同光照类别的图像分成不均匀光照域和目标均匀光照域,进行在生成对抗网络中循环训练,并在测试集上测试,并查看生成的人脸图像效果图。</td>   <td>G06K9/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林浩添;              吴天祎;              黄凯;              王婷;                   夏俊       </td>   <td>中山大学中山眼科中心;中山大学</td>   <td>一种白内障手术步骤的识别方法及装置</td>   <td>广东省</td>   <td>CN112560602A</td>   <td>2021-03-26</td>   <td>本发明公开了一种白内障手术步骤的识别方法及装置,该方法包括：获取待识别白内障手术的视频数据；将视频数据转换为连续帧；根据预设的视频流深度学习模型,对连续帧进行特征提取,获取单帧的特征以及连续帧的时序特征；其中,视频流深度学习模型是根据具有预设标签的白内障手术视频训练而生成的；根据视频流深度学习模型,将获取的时序特征进行分类处理,输出分类概率结果；将分类概率结果与预设标签进行匹配,输出各标签对应的手术步骤；其中,手术步骤包括撕囊、超声乳化和打粘弹剂。采用本发明方案,能实现准确识别白内障手术视频的各手术步骤,并提高分类的准确率。</td>   <td>1.一种白内障手术步骤的识别方法,其特征在于,包括：获取待识别白内障手术的视频数据；将所述视频数据转换为连续帧；根据预设的视频流深度学习模型,对所述连续帧进行特征提取,获取单帧的特征以及所述连续帧的时序特征；其中,所述视频流深度学习模型是根据具有预设标签的白内障手术视频训练而生成的；根据所述视频流深度学习模型,将获取的时序特征进行分类处理,输出分类概率结果；将所述分类概率结果与所述预设标签进行匹配,输出各标签对应的手术步骤；其中,所述手术步骤包括撕囊、超声乳化和打粘弹剂。</td>   <td>G06K9/00;G06N3/04;G06N3/08;G16H50/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周雪雯;              辛秦川;                   戴永久       </td>   <td>中山大学</td>   <td>基于深度学习的植物关键物候期时间点预测方法及系统</td>   <td>广东省</td>   <td>CN112560633A</td>   <td>2021-03-26</td>   <td>本发明公开了一种基于深度学习的植物关键物候期时间点预测方法及系统,其方法包括：对采集到的相关气象数据进行归一化处理,并将处理后所形成的若干个特征变量划分至训练集与验证集中；搭建一维卷积神经网络回归模型,并将所述训练集中包含的所有特征变量时间序列导入网络结构进行训练,输出训练模型；以均方误差作为评估指标,将所述验证集中包含的所有特征变量导入所述训练模型进行验证,存储最优模型；将其他区域的气象数据导入所述最优模型进行预测,获取区域内的不同植被物候天数。本发明实施例可覆盖全球尺度下不同生长季节的植被物候,且保证输出结果的高准确率。</td>   <td>1.一种基于深度学习的植物关键物候期时间点预测方法,其特征在于,所述方法包括：对采集到的相关气象数据进行归一化处理,并将处理后所形成的若干个特征变量划分至训练集与验证集中；搭建一维卷积神经网络回归模型,并将所述训练集中包含的所有特征变量时间序列导入网络结构进行训练,输出训练模型；以均方误差作为评估指标,将所述验证集中包含的所有特征变量导入所述训练模型进行验证,存储最优模型；将其他区域的气象数据导入所述最优模型进行预测,获取区域内的不同植被物候天数。</td>   <td>G06K9/00;G06K9/46;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郭斌彬;              肖丹阳;                   吴维刚       </td>   <td>中山大学</td>   <td>基于混合专家模型的个性化联邦学习方法</td>   <td>广东省</td>   <td>CN112560991A</td>   <td>2021-03-26</td>   <td>本发明为克服大规模无状态的移动联邦环境中私有模型很难实现充分训练的缺陷,提出一种基于混合专家模型的个性化联邦学习方法：所有客户端加入联邦学习共同参与全局模型的训练,得到全局模型参数θ-G；每个客户端分别从服务器下载θ-G,并利用该参数初始化客户端中的特征提取层与个性分类层,利用固定基层方法进行个性化获得个性分类层参数；此时客户端i拥有包括特征提取层参数、全局分类层参数的θ-G和个性分类层参数,利用这三者初始化特征提取层、全局分类层和个性分类层,共同训练门控模型,得到门控模型参数最终客户端获得特征提取层、全局分类层、个性分类层和门控模型的参数,完成个性化联邦学习。</td>   <td>1.基于混合专家模型的个性化联邦学习方法,其特征在于,包括以下步骤：S1：所有客户端加入联邦学习共同参与全局模型的训练,得到全局模型参数θ-G；S2：每个客户端分别从服务器下载全局模型参数θ-G,并利用该参数初始化客户端i中的个性分类层、特征提取层与全局分类层的参数,门控模型进行随机初始化；S3：客户端i进行个性化联邦学习,将本地数据输入特征提取层中,得到激活值,然后将所述激活值分别输入全局分类层、个性分类层和门控模型中,所述个性分类层和门控模型分别根据全局模型参数θ-G进行个性化联邦学习,训练得到个性分类层参数和门控模型参数S4：判断是否达到预设的训练轮数,若是,则得到个性分类层参数和门控模型参数客户端i完成个性化联邦学习；若否,则跳转执行S3步骤。</td>   <td>G06K9/62;G06N3/08;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              郭彤彤;                   李中华       </td>   <td>中山大学</td>   <td>一种基于多尺度特征融合的工业表面缺陷检测方法</td>   <td>广东省</td>   <td>CN112561910A</td>   <td>2021-03-26</td>   <td>本发明公开了一种基于多尺度特征融合的工业表面缺陷检测方法,该方法包括：获取训练样本并对训练样本进行预处理,得到训练样本数据；基于小卷积核构建分割网络并根据训练样本数据对分割网络进行训练,得到训练完成的分割网络；基于小卷积核构建分类网络并根据训练样本数据对分类网络进行训练,得到训练完成的分类网络；基于训练完成的分割网络和训练完成的分类网络对待测样本进行检测,得到检测结果；所述训练完成的分割网络包括低层特征提取模块、高层特征提取模块和多尺度特征融合模块。本发明在保证检测准确率的情况下,减少计算量。本发明作为一种基于多尺度特征融合的工业表面缺陷检测方法,可广泛应用于工业缺陷检测领域。</td>   <td>1.一种基于多尺度特征融合的工业表面缺陷检测方法,其特征在于,包括以下步骤：获取训练样本并对训练样本进行预处理,得到训练样本数据；基于小卷积核构建分割网络并根据训练样本数据对分割网络进行训练,得到训练完成的分割网络；基于小卷积核构建分类网络并根据训练样本数据对分类网络进行训练,得到训练完成的分类网络；基于训练完成的分割网络和训练完成的分类网络对待测样本进行检测,得到检测结果；所述训练完成的分割网络包括低层特征提取模块、高层特征提取模块和多尺度特征融合模块。</td>   <td>G06T7/00;G06T7/11;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         方译权;              文永明;                   成慧       </td>   <td>中山大学</td>   <td>一种物体6D位姿估计方法、装置、设备及介质</td>   <td>广东省</td>   <td>CN112562001A</td>   <td>2021-03-26</td>   <td>本发明公开了一种物体6D位姿估计方法、装置、设备及介质,方法包括：对包含目标物体的RGB图像和深度图进行特征提取,得到颜色特征和几何特征；对目标物体的模型信息进行特征提取,得到模型的颜色特征和几何特征；将每个特征点的颜色特征和几何特征进行关联,得到场景的几何特征和模型的几何特征；根据场景的几何特征和模型的几何特征,确定相关图,并确定注意力响应图；根据颜色特征、几何特征、场景的几何特征和模型的几何特征,构建第一融合特征和第二融合特征；进而构建得到总体特征；根据总体特征,通过位姿估计网络确定目标物体的6D位姿。本发明提高了实时性和鲁棒性,可广泛应用于机器人环境感知技术领域。</td>   <td>1.一种物体6D位姿估计方法,其特征在于,包括：对包含目标物体的RGB图像和深度图进行特征提取,得到不同特征点的颜色特征和几何特征；对所述目标物体的模型信息进行特征提取,得到不同特征点的模型的颜色特征和模型的几何特征；将每个所述特征点的颜色特征和几何特征进行关联,得到场景的几何特征和模型的几何特征；根据所述场景的几何特征和所述模型的几何特征,确定所述目标物体的相关图；根据所述相关图确定注意力响应图；根据所述颜色特征、所述几何特征和所述场景的几何特征,构建第一融合特征；所述第一融合特征包含所述场景中的颜色信息和深度信息；根据所述颜色特征、所述几何特征和所述模型的几何特征,构建第二融合特征；所述第二融合特征包含所述模型中的颜色信息和深度信息；根据所述第一融合特征和所述第二融合特征,构建得到总体特征；根据所述总体特征,通过位姿估计网络确定所述目标物体的6D位姿。</td>   <td>G06T7/73;G06K9/46;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         成慧;              杜径舟;                   吴华栋       </td>   <td>中山大学</td>   <td>一种基于深度图像的人物轮廓快速检测与跟踪方法</td>   <td>广东省</td>   <td>CN107403436B</td>   <td>2021-03-23</td>   <td>本发明涉及深度图像的实时人物轮廓检测与跟踪的技术领域,更具体地,涉及一种基于深度图像的人物轮廓快速检测与跟踪方法。一种基于深度图像的人物轮廓快速检测与跟踪方法,其特征在于,包括人物轮廓快速检测和人物轮廓掩模实时跟踪步骤,所述的人物轮廓快速检测步骤如下：S1.通过人物运动确定背景区域；S2.背景模型更新策略依据背景区域深度图像像素值变化的大小与方向进行更新；S3.通过背景减除得到人物轮廓区域,并依赖其区域再次进行背景更新；所述的人物轮廓掩模实时跟踪步骤如下：S4.基于深度信息对人物轮廓掩模区域进快速分割；S5.对分割后轮廓进行基于轮廓掩模重叠性跟踪；S6.重检测策略。</td>   <td>1.一种基于深度图像的人物轮廓快速检测与跟踪方法,其特征在于,包括人物轮廓快速检测和人物轮廓掩模实时跟踪步骤,所述的人物轮廓快速检测步骤如下：S1.通过人物运动确定背景区域；S2.得到步骤S1背景区域后,依据背景区域深度图像像素值变化的大小与方向进行更新；背景更新方法为：当得到人物运动区域后,对此之外的区域进行背景更新；背景的像素值通常不容易发生变化,时间比较长都为一个较为稳定的值；这里将引入一个背景分数图P-(bg-score)去衡量这种变化,这个分数图对应于背景图P-(bg),指示对应位置背景图像素的分数；分数高则说明在背景图对应位置上的像素值命中率高,分数高使得背景图就不容易发生改变,认为是一个坚固的背景；而分数低的值,对于背景图相对位置的像素点容易被改变,相对没那么坚固；而对于背景模型的像素值的更新依赖于这个分数图,对于某个像素值的更新,使用如下的公式(10)进行更新：                            type∈{bg,move,bg-score},其中MAXScore是分数的最大值,I为像素值,I-i为当前帧P-i的像素值；P-(move)表示运动像素图；S3.通过背景减除得到人物轮廓掩模区域,并依赖其区域再次进行背景更新；所述的人物轮廓掩模实时跟踪步骤如下：S4.基于深度信息对人物轮廓掩模区域进行快速分割；S5.对分割后轮廓进行基于轮廓掩模以及基于时序的连通区域重叠区域面积计算的跟踪；S6.人物被物体或者他人完全遮挡或者几乎完全遮挡,而暂时丢失,但人物并未离开摄像机可视范围,对人物进行重检测。</td>   <td>G06T7/12;G06T7/149;G06T7/194;G06T7/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈鹏飞;              陈彩琳;                   郑子彬       </td>   <td>中山大学</td>   <td>一种基于知识图谱的云原生系统故障分析方法</td>   <td>广东省</td>   <td>CN112540832A</td>   <td>2021-03-23</td>   <td>本申请公开了一种基于知识图谱的云原生系统故障分析方法,包括：获取云原生系统中的原始数据,并基于原始数据构建知识图谱,得到图数据；通过异常检测模型对图数据进行异常检测,得到异常节点；计算异常节点和异常节点对应的副本节点的相似度,并基于相似度进行故障根因定位,其中,异常节点对应的副本节点为与异常节点同类型的节点。本申请解决了现有技术忽略了实体之间的交互关系,只能定位到发生故障的实体,难以快速和准确地定位云原生系统的故障根因的技术问题。</td>   <td>1.一种基于知识图谱的云原生系统故障分析方法,其特征在于,包括：获取云原生系统中的原始数据,并基于所述原始数据构建知识图谱,得到图数据；通过异常检测模型对所述图数据进行异常检测,得到异常节点；计算所述异常节点和所述异常节点对应的副本节点的相似度,并基于所述相似度进行故障根因定位,其中,所述异常节点对应的副本节点为与所述异常节点同类型的节点。</td>   <td>G06F9/455;G06F16/36;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张冬傲;              郑培嘉;                   陈德霖       </td>   <td>中山大学</td>   <td>一种云计算环境下安全计算纳什均衡点的方法</td>   <td>广东省</td>   <td>CN112527699A</td>   <td>2021-03-19</td>   <td>本发明提出一种云计算环境下安全计算纳什均衡点的方法,涉及多媒体信息安全的技术领域,解决了如何更加安全地计算纳什均衡点,得到数据资源最优分配策略的问题,本发明从数据资源分配的问题出发,用户客户端与数据中心为得到最佳融合策略进行两方博弈决策时,双方需要利用收益矩阵来计算纳什均衡点以获得最优分配策略,将同态加密技术应用于数据资源分配,结合博弈论,在云端环境下,基于安全乘法协议和安全除法协议,计算过程中不需要向云服务器暴露过多隐私,直接对密文状态下的数据进行计算,在不泄露隐私数据的情况下完成多方计算并且得到纳什均衡点最优策略,而且基于同态加密技术也可以保证计算精度。</td>   <td>1.一种云计算环境下安全计算纳什均衡点的方法,其特征在于,包括：S1.确定数据资源分配过程中参与博弈决策的用户客户端及数据中心；S2.在本地生成公钥pk和私钥sk,将私钥sk保存在本地,并将公钥pk告知云服务器,私钥sk告知私有服务器；S3.确定用户客户端的收益矩阵A及数据中心的收益矩阵B,利用公钥pk分别对用户客户端的收益矩阵A及数据中心的收益矩阵B进行同态加密,加密后上传至云服务器；S4.云服务器通过安全乘法协议及安全除法协议计算出用户客户端及数据中心的纳什均衡点加密结果；S5.云服务器将纳什均衡点加密结果返回至本地,本地利用私钥对纳什均衡点加密结果进行解密。</td>   <td>G06F12/14;H04L9/00;H04L29/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭锦欣;              孙连鹏;              鄢琳;              邓欢忠;              林健新;                   左达任       </td>   <td>广东爱科环境科技有限公司;中山大学</td>   <td>一种地下排水管道缺陷图像采集分类系统及方法</td>   <td>广东省</td>   <td>CN112528922A</td>   <td>2021-03-19</td>   <td>本发明公开了一种地下排水管道缺陷图像采集分类系统及方法,该系统包括移动机器人以及与移动机器人通信连接的后台服务器,移动机器人上设置有控制器以及与控制器电性连接的照明装置、摄像头、GPS定位模块、无线通信模块,控制器通过无线通信模块与后台服务器通信连接；后台服务器包括分类器模块和图像处理模块,分类器模块用于对排水管道缺陷图像进行分类,图像处理模块用于对分类后的图像进行归一化处理。本发明提供的地下排水管道缺陷图像采集分类系统及方法,能够对排水管道进行定位图像采集,并可以对排水管道检测图像进行详细科学分类并处理为标准图像形成排水管道图像标准样本库,为图像识别提供更可靠的数据支持。</td>   <td>1.一种地下排水管道缺陷图像采集分类系统,其特征在于,包括：移动机器人以及与移动机器人通信连接的后台服务器,所述移动机器人上设置有控制器以及与控制器电性连接的照明装置、摄像头、GPS定位模块、无线通信模块,所述控制器通过所述无线通信模块与所述后台服务器通信连接,所述摄像头用于采集排水管道缺陷图像,所述GPS定位模块用于对移动机器人的位置进行定位；所述后台服务器包括分类器模块和图像处理模块,所述分类器模块用于对排水管道缺陷图像进行分类,所述图像处理模块用于对分类后的图像进行归一化处理。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘玉葆;                   李聪       </td>   <td>中山大学</td>   <td>一种基于图卷积网络的图相似性计算方法及装置</td>   <td>广东省</td>   <td>CN112529057A</td>   <td>2021-03-19</td>   <td>本发明公开了一种基于图卷积网络的图相似性计算方法及装置,能够克服现有的基于GNN的图相似性计算模型不能很好的学习图的层次结构的缺点,通过在合理的时间内将图的扁平和层次表示结合在一起来提高图相似度计算的有效性。具体地,本发明实施例提出了一个完全支持反向传播的基于端到端地神经网络的函数,通过仔细设计函数的每一部分,使得该函数能学习到图的扁平的和层次的信息,最终将一对图映射为相似度评分,以同时克服现有技术耗时长及无法捕捉图的层次结构的缺点。</td>   <td>1.一种基于图卷积网络的图相似性计算方法,其特征在于,包括以下步骤：分别确定第一拓扑图和第二拓扑图的初始节点特征；分别求得第一拓扑图和第二拓扑图的节点的嵌入表示；根据第一拓扑图和第二拓扑图的节点的嵌入表示,分别通过注意力机制和DIFFPOOL方法对节点嵌入进行汇总,分别得到第一拓扑图的扁平化全图嵌入和层次化全图嵌入,得到第二拓扑图的扁平化全图嵌入和层次化全图嵌入；采用神经张量网络NTN分别处理第一拓扑图和第二拓扑图在扁平层面以及层次化层面的图嵌入之间的关系；将扁平化图嵌入相似性得分与层次化图嵌入相似性得分进行拼接,将拼接输入一个含有一个隐藏层的全连接神经网络降维汇总,以得到第一拓扑图和第二拓扑图之间的相似性得分。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         印鉴;              蒙权;                   曹志平       </td>   <td>中山大学;广州通达汽车电气股份有限公司</td>   <td>一种基于异构图嵌入技术的广告受众基础属性预估方法</td>   <td>广东省</td>   <td>CN112529621A</td>   <td>2021-03-19</td>   <td>本发明提供一种基于异构图嵌入技术的广告受众基础属性预估方法,该方法根据所有广告受众的历史点击行为,构建一张广告受众的点击行为记录的异构图,并基于异构图嵌入表示技术为这些广告信息(素材id、广告id、广告主id)生成各自的向量表示,以此来保留广告信息之间的相互联系；搭建一个LSTM的神经网络,来自动根据已知基础属性(年龄和性别)的广告受众的历史点击行为训练一个可以根据广告受众的历史点击行为预测其基础属性的网络模型。针对未知基础属性的广告受众,只需要将其历史点击行为输入该训练好的网络模型,就可以根据其历史点击行为从该模型得到这些受众的基础属性。</td>   <td>1.一种基于异构图嵌入技术的广告受众基础属性预估方法,其特征在于,包括以下步骤：S1：根据所有广告受众的历史点击行为,构建一张广告受众的点击行为记录的异构图,并基于异构图嵌入表示技术为这些广告信息生成各自的向量表示,其中广告信息包括素材id、广告id、广告主id；S2：划分数据集,按照广告受众的基础属性是否已知将原始数据集分为两类,其中已知基础属性的这类数据称为训练集,用于训练预测模型,另一类数据称为测试集,用于预测其未知的基础属性,其中,广告受众的基础属性是年龄和性别；S3：将训练集当中广告受众的历史点击行为作为LSTM预测模型的输入特征,年龄和性别作为对应特征的预测结果标签,对LSTM神经网络进行训练,得到一个根据广告受众的点击行为预测其基础属性的预测模型。S4：将测试集当中广告受众的历史点击行为输入训练好的预测模型,预估得到这些用户的基础属性。</td>   <td>G06Q30/02;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王若梅;              罗政煊;              林淑金;                   周凡       </td>   <td>中山大学</td>   <td>一种基于生成对抗网络的服装编辑和生成方法</td>   <td>广东省</td>   <td>CN112529768A</td>   <td>2021-03-19</td>   <td>本发明公开了一种基于生成对抗网络的服装编辑和生成方法。首先在用户原始图像被输入后,使用R-CNN区域检测卷积神经网络对图像中的服装进行检测识别；使用CPN级联金字塔网络来对服装物体进行轮廓点检测,单独提取无背景的服装图像显示给用户；并根据上述数据,返回给用户进行属性修改选择；将用户请求信息进行结构化处理后获得需要修改属性的语义信息,输入至训练好的带有指定属性的逼真图像能力的生成器生成最终服装图像。本发明为用户提供一种依靠计算机视觉技术端到端的服装编辑生成方案,一方面,解决了用户商品检索前对于服装样式进行更改的需求,另一方面,提高了服装编辑的可操作性和生成效果。</td>   <td>1.一种基于生成对抗网络的服装编辑和生成方法,其特征在于,所述方法包括：对服装图像进行结构化预处理,得到预处理后的服装图像；对所述预处理后的服装图像提取信息,包括标注服装属性、标注服装分割点和服装轮廓点,将所提取的信息进行结构化处理,获得向量格式记录的服装属性信息；将所述预处理后的服装图像、所述服装属性和所述服装分割点,作为Mask R-CNN卷积神经网络的输入,获得特征图,对特征图进行分类和回归训练获得网络模型,用于对所述服装图像进行服装属性分类识别和分割点检测,将所有分割点顺序连接得到轮廓图；使用CPN级联金字塔网络,对所述服装轮廓点进行检测,按不同的服装关键点提取整体服装轮廓,配合所述轮廓图,获得所述服装图像的精度较高的轮廓关键点坐标信息；综合所述服装分割点和所述轮廓关键点坐标信息形成精确掩码轮廓图和属性向量,对所述服装图像取掩码提取服装部分,并将轮廓点高亮显示给用户,提供属性修改功能；利用所述服装图像取掩码提取的服装部分预训练判别器D,判别器的网络沿用Att-GAN属性生成对抗网络的判别器,网络采取五层卷积层,卷积结果连接两个不同的全连接层至两个分支判别器Dimg和Datt用于判别生成图像各种属性的准确性；利用所述服装图像和所述向量格式记录的服装属性信息,构建生成器G,生成器采用U型编码-解码网络模型,编码器对所述服装图像取掩码提取的服装部分,提取特征向量,使用ACUs(Attribute Control Units)属性控制单元,各层ACU连接在编码器和解码器对应层之间,ACUs属性控制单元内,特征向量的每一层特征图与所述向量格式记录的服装属性信息生成属性编辑后的特征图,ACUs通过卷积得到综合后的特征图,传入解码器中,解码器反卷积后生成图像；将所述生成图像输入至所述预训练好的判别器D,按属性得到分类结果,来衡量属性编辑程度,并通过损失函数计算损失值,梯度反向传播更新所述生成器和所述辨别器的卷积参数,迭代其相互对抗提升能力过程,得到具有生成带有指定属性的逼真图像能力的生成器G；输入待处理的服装图像,对该图像取掩码提取服装部分并高亮显示轮廓点,得到可供用户选择和修改的服装属性,之后把修改后的服装属性输入所述具有生成带有指定属性的逼真图像能力的生成器G,迭代地生成用户期望的服装图像。</td>   <td>G06T3/00;G06T11/00;G06N3/04;G06N3/08;G06K9/62;G06K9/34;G06K9/46</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苏远超;              李军;              李朋飞;              王丹;                   杜光辉       </td>   <td>西安科技大学;中山大学</td>   <td>混合像元双线性深层解混方法、系统、应用及存储介质</td>   <td>陕西省</td>   <td>CN112529865A</td>   <td>2021-03-19</td>   <td>本发明公开了一种混合像元双线性深层解混方法、系统、应用及存储介质。本发明首先构建一个深层双线性光谱混合模型；然后建立两个深度自编码器来分层对应深层双线性混合模型,采用多任务处理模式对两个深度自编码器进行无监督联合训练,学习出端元、丰度及表征二阶散射的散射参数。本发明在构建解混方法时考虑了二阶散射作用对光谱混合产生的影响,在光谱混合的建模方面更接近高光谱遥感器成像机理的实际情况,能有效抑制散射效应带来的负面影响,最终得到比传统方法精度更高的端元和丰度。同时,利用散射参数能将散射效应的影响情况进行可视化,能直观地反映出散射效应在高光谱图像中的分布情况,为用户分析散射效应分布的合理性提供便利。</td>   <td>1.一种混合像元双线性深层解混方法,其特征在于,包括：A.对双线性光谱混合模型的线性项和双线性项分别进行分层化表征,构建深层双线性光谱混合模型；B.构建两个分别对应着深层双线性混合模型中线性项和双线性项的深度自编码器,采用多任务学习模式对这两个深度自编码器进行无监督联合训练,直至输入深层双线性混合模型的高光谱图像与它的重构图像之间的重构误差达到收敛时停止训练,最终学习出端元、丰度及表征二阶散射的散射参数。</td>   <td>G06T7/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         欧阳子臻;              杨煜基;                   成慧       </td>   <td>中山大学</td>   <td>一种用于室外无人机的单目稀疏光流算法</td>   <td>广东省</td>   <td>CN112529936A</td>   <td>2021-03-19</td>   <td>本发明涉及目标跟踪技术领域,更具体的是涉及一种用于室外无人机的单目稀疏光流算法,包括以下步骤：S1：提取跟踪相邻两帧灰度图像的特征点,记为原始样本点集合；S2：剔除原始样本点集合中的无效特征点,得到抽样样本点集合,并通过抽样样本点集合得到最优模型H-(best)；S3：根据最优模型H-(best),得到无人机在图像坐标系下的速度；S4：将图像坐标系下的速度转换为摄像机坐标系下的速度。本方案中通过对原始样本点集合中的无效特征点进行剔除,即可以消除无人机飞行过程中其自身影子产生的特征点影响,使得无人机在室外飞行时速度估算更加精准。</td>   <td>1.一种用于室外无人机的单目稀疏光流算法,其特征在于,包括以下步骤：S1：提取跟踪相邻两帧灰度图像的特征点,记为原始样本点集合；S2：剔除原始样本点集合中的无效特征点,得到抽样样本点集合,并通过抽样样本点集合得到最优模型H-(best)；S3：根据最优模型H-(best),得到无人机在图像坐标系下的速度；S4：将图像坐标系下的速度转换为摄像机坐标系下的速度。</td>   <td>G06T7/246;G06T7/269;G06T7/277</td>  </tr>        <tr>   <td>中国专利</td>   <td>         高成英;                   张泽坤       </td>   <td>中山大学</td>   <td>一种纱线级针织衣物自动建模方法及装置</td>   <td>广东省</td>   <td>CN112530018A</td>   <td>2021-03-19</td>   <td>本发明公开了一种纱线级针织衣物自动建模方法和装置,所述方法首先获取待建模衣物的三角网格模型,然后根据三角网格模型生成针织图模型；对针织图模型进行网格重建生成针脚网格模型,然后对针脚网格模型进行螺旋化操作,将针脚网格模型自动转换为螺旋式结构,最后进行网格追踪生成待建模衣物的纱线曲线,构成出纱线级针织衣物模型。通过实施本发明实施例,能够自动化生成纱线级针织衣物模型,无需用户手动进行操作设计,极大的提高了纱线级针织衣物模型的构建效率,节约了人工成本。</td>   <td>1.一种纱线级针织衣物自动建模方法,其特征在于,包括：获取待建模衣物的三角网格模型,并根据所述三角网格模型生成所述待建模衣物的针织图模型；其中,所述针织图模型包括若干针织行,所述针织行包括针织短行或针织环行；将所述针织图模型中,每一所述针织短行起始端的短行节点以及末端的短行节点与相邻的前一针织行的对应节点连接,生成基础针脚网格模型,继而对所述基础针脚网格模型进行网格细分,生成所述待建模衣物的针脚网格模型；其中,所述针脚网格模型包括若干针织面片行；所述针织面片行：包括环行面片行或短行面片行；根据待建模衣物管状部件的合并位置将所述针脚网格模型划分为若干针脚网格子模型,将每一所述针脚网格子模型进行螺旋化操作,生成若干螺旋式网格子模型；其中,所述螺旋化操作具体包括：将针脚网格子模型中任意一列网格上移预设单位,生成初步螺旋化针脚网格子模型；将所述初步螺旋化针脚网格子模型中未被移位路径穿过的短行面片行,作为待处理面片行,将所述待处理面片行末端的转向网格单元或起始端的转向网格单元移动预设单位,生成所述螺旋式网格子模型；对每一所述螺旋式网格子模型进行网格追踪,生成每一所述螺旋式网格子模型的纱线曲线,根据所有纱线曲线生成所述待建模衣物的纱线级衣物模型。</td>   <td>G06T17/20;G06F30/10;G06F30/20;G06F113/12</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈龙;              刘坤华;                   叶梓豪       </td>   <td>中山大学</td>   <td>一种基于GAN的带雾图像语义分割方法</td>   <td>广东省</td>   <td>CN112508025A</td>   <td>2021-03-16</td>   <td>本发明提供一种基于GAN的带雾图像语义分割方法,包括以下步骤：S1.建立边界生成器和边界判别器；S2.建立语义分割生成器和语义分割判别器；S3.对边界生成器和边界判别器进行训练；S4.对语义分割生成器和语义分割判别器进行训练；S5.将带雾图像输入完成训练的边界生成器中获得边界图像；S6.带雾图像与边界图像拼接后输入完成训练的语义分割生成器中获得语义分割图像。本发明能够直接对带雾图像进行语义分割,准确率高。</td>   <td>1.一种基于GAN的带雾图像语义分割方法,其特征在于：包括以下步骤：S1.建立边界生成器和边界判别器；S2.建立语义分割生成器和语义分割判别器；S3.对边界生成器和边界判别器进行训练；S4.对语义分割生成器和语义分割判别器进行训练；S5.将带雾图像输入完成训练的边界生成器中获得边界图像；S6.带雾图像与边界图像拼接后输入完成训练的语义分割生成器中获得语义分割图像。</td>   <td>G06K9/34;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;                   李洁铃       </td>   <td>中山大学</td>   <td>一种三维点云数据的去噪方法、装置及存储介质</td>   <td>广东省</td>   <td>CN112508803A</td>   <td>2021-03-16</td>   <td>本发明公开了一种三维点云数据的去噪方法、装置及存储介质,该方法对激光雷达采集的三维点云数据进行处理,包括读取所述三维点云数据；对所述三维点云数据进行预处理,得到原始点云数据；融合粒子滤波法和动态半径滤波法,获取约束条件,所述约束条件包括点云追踪次数、点云相似度和点云FPFH特征差距；根据所述约束条件,对所述原始点云数据进行去噪处理；本发明采用融合粒子滤波法和动态半径滤波法来进行点云去噪处理,能够保证在去除噪声点的同时,不破坏其它点云的环境；去噪效果更佳,更完全,能够实现在不破坏非噪声点云的环境特征的同时,提高去除噪声点的准确率。本发明可广泛应用于数据处理领域。</td>   <td>1.一种三维点云数据的去噪方法,对激光雷达采集的三维点云数据进行处理,其特征在于,包括：读取所述三维点云数据；对所述三维点云数据进行预处理,得到原始点云数据；融合粒子滤波法和动态半径滤波法,获取约束条件,所述约束条件包括点云追踪次数、点云相似度和点云FPFH特征差距；根据所述约束条件,对所述原始点云数据进行去噪处理。</td>   <td>G06T5/00;G06T17/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周健;              王莹;              夏霆坚;              周冬豪;                   郑志鹏       </td>   <td>中山大学肿瘤防治中心(中山大学附属肿瘤医院、中山大学肿瘤研究所)</td>   <td>非齐次三维食管癌能谱CT弱监督自动标注方法与系统</td>   <td>广东省</td>   <td>CN112488996A</td>   <td>2021-03-12</td>   <td>本发明公开了一种非齐次三维食管癌能谱CT弱监督自动标注方法与系统,非齐次三维食管癌能谱CT弱监督自动标注方法包括使用多结构三维响应滤波器增强非齐次能谱CT图像中的其他区域,使用基于信息权重的特征提取算法提取多个关键断层,使用基于通道注意力和跨层融合的U-Net网络获取轮廓顶点,以及使用Poly-RNN算法确定标注信息,使用区域中心点生长映射,获取自动标注等步骤。本发明可以有效地反映病灶的病理和形态结构的变化情况,与手动标注相比具有更高的速度和更低的出错率,与现有的自动标注技术相比,能够更加有效地对食管癌能谱CT数据样本进行自动标注,具有更高的标注准确度和标注效率。本发明广泛应用于医学图像处理技术领域。</td>   <td>1.一种非齐次三维食管癌能谱CT弱监督自动标注方法,其特征在于,包括：使用基于Hessian矩阵的多结构三维响应滤波器,增强非齐次能谱CT图像中除食管癌区域外的其他区域；使用基于信息权重的特征提取算法,从所述非齐次能谱CT图像提取多个关键断层；使用基于通道注意力和跨层融合的U-Net网络,进行对所述非齐次能谱CT图像中的食管癌区域和其他区域的分割,获取所述食管癌区域的至少一组轮廓顶点；使用Poly-RNN算法,根据所述至少一组轮廓顶点确定所述关键断层的标注信息；使用区域中心点生长映射,根据所述标注信息获取所述非齐次能谱CT图像的全序列的自动标注。</td>   <td>G06T7/00;G06T7/11;G06T7/194;G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张晋斌;                   潘嵘       </td>   <td>中山大学</td>   <td>一种基于CNN-LSTM的汉字拼写错别字改正方法</td>   <td>广东省</td>   <td>CN107992211B</td>   <td>2021-03-12</td>   <td>本发明提供一种基于CNN-LSTM的汉字拼写错别字改正方法,该方法主要是利用了文本的上下文来纠错,即每一个汉字根据其上下文来判断该汉字正确与否,若该汉字是错别字的话则会根据其上下文来纠正错误。并且在模型训练中用到了随机改错的训练方式,提高了改正的准确率。</td>   <td>1.一种基于CNN-LSTM的汉字拼写错别字改正方法,其特征在于,包括以下步骤：S1：对于输入的句子进行编码,并对错别字进行过滤；S2：根据获得的过滤后的信息以及上下文信息解码出当前时间节点对应的正确的汉字；所述步骤S1的具体过程是：S11：对于输入的句子,首先使用预训练好的word2vector汉字词向量把输入的句子初始化成一个矩阵,然后开一个固定宽度的窗口,只对窗口内的信息做编码；S12：编码部分的结构包含两个不同的卷积神经网络卷积核,一个用于检测在窗口内的汉字是不是包含错别字,其宽度和高度与窗口的大小以及词向量的大小一致,并且输出要经过一个非线性变换函数sigmoid函数,另外一个用于编码窗口内的汉字信息,其宽度为窗口的宽度,而高度为1,这两种不同的卷积神经网络卷积核的数目由具体的需求来确定,其中第二种卷积神经网络的作用看作是用于来对窗口中的汉字信息的编码,而第一种卷积神经网络的卷积核作用则是一个门对第二种卷积核所编码的信息的过滤；对于两种不同的卷积神经网络的输出,拉平成两个向量之后对于两个向量进行逐元素的相乘：encode＝B*sigmoid(A)其中A,B表示两个不同的卷积神经网络的输出向量表示,encode表示在第t时刻对应的窗口信息的向量表示；所述步骤S2的具体过程是：根据编码部分获得的过滤后的信息以及上下文信息解码出当前时间节点对应的正确的汉字,分为两个过程：一个是解码部分的输入,另外一个是解码部分的输出：1)解码部分的输入：解码部分的每一个时间节点的输入就是编码部分的输出encode,其具体结构是一个双向LSTM结构,在解码的时候可以利用到上下文信息；2)解码部分的输出：对于双向LSTM的输出,其每个时间节点的输出经过一个softmax层,softmax的输出维度是汉字字典的大小,输出的句子就是改正后的句子,对于检测来说,如果对应位置的汉字与输入的汉字不一样,则认为该位置的汉字是错别字。</td>   <td>G06F3/023</td>  </tr>        <tr>   <td>中国专利</td>   <td>         蔡吉涵;                   胡建芳       </td>   <td>中山大学</td>   <td>用于智能人机交互的行为早期识别方法、系统及存储介质</td>   <td>广东省</td>   <td>CN112464861A</td>   <td>2021-03-09</td>   <td>本发明公开了一种用于智能人机交互的行为早期识别方法、系统及存储介质,方法包括：将视频流转化为图像帧；将图片帧转输入卷积神经网络提取特征,得到动作特征序列；将动作特征序列F输入到行为预测模型进行计算：首先利用多头注意力机制输出与动作特征序列F维度相同的向量,将该向量输入前馈神经网络中计算,将上述过程标记为EL Layer,经过L层的堆叠就形成了Transformer Encoder的编码过程；接着将编码结果输入到分类器中计算得到代表分类结果的概率向量；最后取最晚观测时刻对应的输出向量中概率最大的分类作为预测结果。本发明的方法利用时间维度上距离较远的信息之间的交互信息增强模型的表现力；同时,使用行为早期识别技术提前预测出动作分类可以提高交互系统的用户体验。</td>   <td>1.用于智能人机交互的行为早期识别方法,其特征在于,包括下述步骤：将视频流转化为图像帧；将图片帧转输入卷积神经网络提取特征,得到动作特征序列；将动作特征序列F输入到行为预测模型进行计算,所述行为预测模型包括TransformerEncoder计算过程和分类器的计算过程；所述Transformer Encoder计算过程为：利用多头注意力机制输出与动作特征序列F维度相同的向量,将该向量输入前馈神经网络中计算,将前述计算层记为EL Layer,每一层ELLayer的输出维度和原来输入的动作特征序列F维度大小相同,并将该输出作为下一层ELLayer的输入,经过L层的堆叠就形成了Transformer Encoder的编码过程；所述分类器的计算过程为：将编码结果输入到分类器中计算得到代表分类结果的概率向量,所述概率向量代表某一观测时刻对应观测结果的行为早期识别结果；取最晚一个观测时刻对应的输出向量中概率最大的分类作为预测结果。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>              周凡       </td>   <td>中山大学深圳研究院</td>   <td>一种基于草图的图像匹配方法</td>   <td>广东省</td>   <td>CN111931794B</td>   <td>2021-03-09</td>   <td>本申请适用于多媒体信息检索技术领域,提供了一种基于草图的图像匹配方法、装置及计算机可读存储介质,所述图像匹配方法包括：对样本图像进行特征边缘提取,得到样本图像的第一方向梯度直方图特征；其中,样本图像为基于原始图像进行高斯模糊得到；第一方向梯度直方图特征为样本图像的图像边缘像素集中部分像素点的方向梯度直方图特征；图像边缘像素集为基于样本图像进行边缘提取得到；基于第一方向梯度直方图特征从预设图像库中确定出目标图像集。上述图像匹配方法在对样本图像进行特征边缘提取时,可以得到样本图像更多的边缘细节,使得样本图像的第一方向梯度直方图特征能够充分的表示样本图像的边缘特征信息,进而提高了图像匹配的准确率。</td>   <td>1.一种基于草图的图像匹配方法,其特征在于,包括：对样本图像进行特征边缘提取,得到所述样本图像的第一方向梯度直方图特征；其中,所述样本图像为基于原始图像进行高斯模糊得到；基于所述第一方向梯度直方图特征从预设图像库中确定出目标图像集；其中,所述对样本图像进行特征边缘提取,得到所述样本图像的第一方向梯度直方图特征,包括：基于预设边缘检测算子对所述样本图像进行边缘提取,得到所述样本图像的图像边缘像素集；基于所述图像边缘像素集对所述样本图像进行特征提取,得到所述样本图像对应的第一方向梯度直方图特征；其中,所述基于所述图像边缘像素集对所述样本图像进行特征提取,得到所述样本图像对应的第一方向梯度直方图特征,包括：基于预设要求确定所述图像边缘像素集中的目标像素点集；确定所述目标像素点集的重心；对所述目标像素点集和所述重心进行特征提取,得到所述目标像素点集和所述重心对应的第一方向梯度直方图特征。</td>   <td>G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>              周凡       </td>   <td>中山大学深圳研究院</td>   <td>视觉跟踪方法、视频监控方法及终端设备</td>   <td>广东省</td>   <td>CN112036381B</td>   <td>2021-03-09</td>   <td>本申请适用于人工智能技术领域,提供了视觉跟踪方法、视频监控方法及终端设备,利用目标滤波器对当前视频帧图像的第一图像特征进行滤波,得到响应输出矩阵,以当前视频帧图像作为训练样本训练初始滤波器,以使滤波器充分提取图像特征,从而使视觉跟踪模型在多种视频图像中均能够高效实现目标跟踪,提高视觉跟踪模型的鲁棒性。以及根据响应输出矩阵,确定跟踪目标在当前视频帧图像中的实际位置,使得跟踪效果更好。</td>   <td>1.一种视觉跟踪方法,其特征在于,包括：利用目标滤波器对当前视频帧图像的第一图像特征进行滤波,得到响应输出矩阵,所述目标滤波器利用所述当前视频帧图像的上一视频帧图像经过初等变换后得到的多个训练样本进行训练得到；根据所述响应输出矩阵,确定跟踪目标在当前视频帧图像中的实际位置；还包括：根据所述跟踪目标在所述当前视频帧图像中的实际位置和预设尺寸,提取所述当前视频帧图像的第三目标区域,并将所述第三目标区域作为测试样本；对所述第三目标区域进行特征提取,得到第三图像特征；基于目标分类器,对所述第三图像特征进行分类,得到分类结果,其中,所述目标分类器是利用跟踪过程中收集到的视频帧图像对预设分类器进行训练得到；若所述分类结果为测试样本不是正样本,则判定所述跟踪目标出现遮挡。</td>   <td>G06K9/00;G06K9/46</td>  </tr>        <tr>   <td>中国专利</td>   <td>         凌青;              钟淑鑫;              蒙伟光;              文秋实;              曾宪欣;                   冯业荣       </td>   <td>中山大学;中国气象局广州热带海洋气象研究所(广东省气象科学研究所)</td>   <td>基于注意力机制的时空神经网络雷达回波外推预报方法</td>   <td>广东省</td>   <td>CN112446419A</td>   <td>2021-03-05</td>   <td>本发明为基于注意力机制的时空神经网络雷达回波外推预报方法,包括：对雷达回波图像数据去除部分噪声,并选择出有效数据段,对数据段归一化和拆分后,划分为训练序列样本集和测试序列样本集；构建及训练Att-ConvLSTM网络,对雷达回波序列样本的图像根据预设的切片因子进行切片,调整图像的维度,然后输入到基于注意力机制的时空预测神经网络,通过多层网络的前向传播,利用反向传播更新网络权重；利用训练好的Att-ConvLSTM网络以及测试序列样本集进行预测,得到最终的外推图像序列。本发明克服了现有技术对空间信息提取不足、预报时效短的缺点,实现了准确度更高的雷达回波外推预测。</td>   <td>1.基于注意力机制的时空神经网络雷达回波外推预报方法,其特征在于,包括以下步骤：步骤1、数据预处理,对雷达回波图像数据,去除部分噪声,并选择出有效数据段,然后将数据段转换为归一化的灰度数据；基于归一化的数据集,对数据段进行拆分,然后将拆分的数据集划分为训练序列样本集和测试序列样本集；步骤2、构建及训练Att-ConvLSTM网络,对雷达回波序列样本的图像根据预设的切片因子进行切片,调整图像的维度,然后输入到基于注意力机制的时空预测神经网络,通过多层网络的前向传播,利用反向传播更新网络权重；步骤3、利用训练好的Att-ConvLSTM网络以及测试序列样本集进行预测,得到最终的外推图像序列。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         何明光;                   李治玺       </td>   <td>中山大学中山眼科中心</td>   <td>一种基于迁移学习的糖尿病黄斑水肿自动筛查方法</td>   <td>广东省</td>   <td>CN112446860A</td>   <td>2021-03-05</td>   <td>本发明提供一种基于迁移学习的糖尿病黄斑水肿自动筛查方法,通过光学相干断层扫描图像以及眼底彩照对是否患有糖尿病黄斑水肿进行标记,使用训练好的深度残差神经人工智能模型提取患者的眼部特征并根据眼部特征进行有无糖尿病黄斑水肿的分类,随后使用眼底彩照数据集训练通过迁移学习技术对深度残差神经人工智能模型极性调整,得到最终基于眼底彩照的糖尿病黄斑水肿的智能筛查模型。通过基于眼底彩照的糖尿病黄斑水肿的智能筛查模型的对光学相干断层扫描图像以及眼底彩照对是否患有糖尿病黄斑水肿的患者进行识别分类,可提高糖尿病黄斑水肿高危人群筛查的准确率和工作效率,也减少病人诊治疾病的花费,从而使更多的病人得到及时的诊疗。</td>   <td>1.一种基于迁移学习的糖尿病黄斑水肿自动筛查方法,其特征在于：包括自动筛查方法如下步骤：S1：获取光学相干断层的扫描影像及眼底彩照；S2：对光学相干断层扫描影像和眼底彩照进行是否患有糖尿病黄斑水肿的标记,获取光学相干断层扫描训练集和眼底彩照训练集；S3：基于光学相干断层扫描影像,构建深度残差网络初始模型；S4：光学相关断层扫描训练集对深度残差网络初始模型进行训练,获取深度残差神经人工智能模型；S5：利用深度残差神经人工智能模型对光学相干断层的扫描影像进行整体特征的识别和提取,获取待识别患者的眼部特征；S6：利用深度残差神经人工智能模型对待识别的眼底特征进行验证和识别并根据识别结果对光学相干断层扫描影像进行有无糖尿病黄斑水肿的分类,获得有无糖尿病黄斑水肿的光学相干断层扫描影像分类结果；S7：使用眼底彩照训练集通过迁移学习对深度残差神经人工智能模型的全连接层调整,获取基于眼底彩照的糖尿病黄斑水肿的智能筛查模型。</td>   <td>G06T7/00;A61B3/10;A61B3/12;A61B3/14;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苏卓;              汤成熙;              周凡;                   林格       </td>   <td>中山大学</td>   <td>一种基于域自适应学习的多复杂场景目标检测方法</td>   <td>广东省</td>   <td>CN112434586A</td>   <td>2021-03-02</td>   <td>本发明公开了一种基于域自适应学习的多复杂场景目标检测方法。首先收集得到通用目标检测图像集、降质场景图像集；之后利用通用目标检测图像集预训练目标检测网络YOLOv3,然后在此基础上嵌入域自适应模块,再利用通用目标检测图像集、降质场景图像集对其重新进行训练,得到最终的多复杂场景目标检测网络；输入待检测目标的图像即可计算出图像中物体的类别以及位置。本发明能够针对多种不同的降质场景进行目标检测,适用性广；能够在确保检测精度的前提下,实时对图像中的目标作出检测；采用了自适应学习的方法,降低了通用图像与多种不同降质场景图像的域间差异,使得目标检测能够同时在多种场景的图像上表现良好。</td>   <td>1.一种基于域自适应学习的多复杂场景目标检测方法,其特征在于,所述方法包括：收集通用目标检测图像数据,以及多种降质场景下的图像数据,并对数据进行预处理,得到通用目标检测图像集、降质场景图像集；利用所述通用目标检测图像集预训练目标检测网络YOLOv3；在所述预训练完成的目标检测网络YOLOv3的基础上嵌入域自适应模块,并利用所述通用目标检测图像集、所述降质场景图像集对嵌入了域自适应模块的目标检测网络重新进行训练,训练完成后,再把该域自适应模块进行拆除,得到最终的多复杂场景目标检测网络；输入待检测目标的图像,通过所述多复杂场景目标检测网络计算得出图像中特定物体的类别以及位置信息。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;              谢宇光;              单广威;                   印鉴       </td>   <td>中山大学</td>   <td>基于眼部运动细微特征的早期疲劳检测方法及系统</td>   <td>广东省</td>   <td>CN112434611A</td>   <td>2021-03-02</td>   <td>本发明涉及计算机视觉及视频分析技术领域,为基于眼部运动细微特征的早期疲劳检测方法及系统,其方法包括：检测出视频帧序列的人脸位置图像、人脸特征点位置；识别出帧序列中的眨眼帧序列以及眼球运动帧序列,将视频帧序列划分为眨眼与眼球运动交替的帧序列；获取关于每次眨眼的眨眼信息特征、眼球运动信息特征,并融合、组合为眼部运动细微特征序列；训练基于时序的神经网络模型得到疲劳程度检测模型；将需要预测的视频帧序列进行处理,获得人员的眼部运动细微特征序列,输入到疲劳程度检测模型中,判断出当前人员的疲劳程度。本发明可有效检测视频序列中人员的疲劳程度,检测结果包括：清醒、早期疲劳、疲劳,实现早期疲劳的检测。</td>   <td>1.基于眼部运动细微特征的早期疲劳检测方法,其特征在于,包括以下步骤：S1、按序从视频帧序列中读取出每帧图像信息,然后使用人脸检测算法检测出每一帧的人脸位置图像,再通过人脸位置图像及人脸特征点检测算法,检测出对应该帧的人脸特征点位置；S2、获得视频帧序列的若干帧人脸特征点位置后,通过眨眼检测算法,识别出这些帧序列中的眨眼帧序列以及眼球运动帧序列,两次眨眼帧序列之间的帧序列即为眼球运动帧序列,从而将视频帧序列划分为眨眼与眼球运动交替的帧序列；S3、对得到的每段眨眼帧序列,应用眨眼特征提取算法,获取关于每次眨眼的眨眼信息特征；S4、对得到的每段眼球运动帧序列,应用眼球运动信息提取算法,获取每次眨眼间的眼球运动信息特征；S5、将眨眼信息特征与眼球运动信息特征融合为眼部运动细微特征,并以时间序列的方式组合作为眼部运动细微特征序列；S6、对视频训练数据集,通过上述步骤S1-S5提取每个视频的眼部运动细微特征序列,存入到基于时序的神经网络模型中进行训练学习,得到疲劳程度检测模型；S7、对需要预测的视频,将相应的视频帧序列通过步骤S1-S5进行处理,获得视频帧序列中人员的眼部运动细微特征序列；然后将该视频帧序列输入到训练好的疲劳程度检测模型中,疲劳程度检测模型判断出人员的眼部运动模式属于哪种疲劳程度的运动模式,从而判断出当前人员的疲劳程度。</td>   <td>G06K9/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘星成;              赵莹莹;                   刘异橦       </td>   <td>中山大学</td>   <td>基于局部均值距离约束表示的数据分类方法</td>   <td>广东省</td>   <td>CN112434728A</td>   <td>2021-03-02</td>   <td>本发明提供一种基于局部均值距离约束表示的数据分类方法,包括步骤如下：获取训练样本集合,所述训练样本集合包括若干个类别,将训练样本集合按照类别分为若干个训练样本子集,计算未知样本到每个训练样本子集中的训练样本的距离；从每个类别中找到距离未知样本最近的k个训练样本,组成一个邻居样本子集；采用基于协同表示的方法,对未知样本进行重新表示；在基于协同表示的目标函数中加入局部均值向量进行约束,得到新的目标函数；对得到的新的目标函数进行求导,求解表示系数；将表示系数作为每个邻居样本的权重,计算每个类别邻居样本的权重和；将未知样本分类为系数和最大的类别,得到未知样本的类别；判断未知样本集合内的未知样本是否全部分类完毕,如果没有分类完毕,则对下一个未知样本,重复上述分类过程。</td>   <td>1.一种基于局部均值距离约束表示的数据分类方法,其特征在于：所述的方法包括步骤如下：S1：获取训练样本集合,所述训练样本集合包括若干个类别,将训练样本集合按照类别分为若干个训练样本子集,计算未知样本到每个训练样本子集中的训练样本的距离；S2：从每个类别中找到距离未知样本最近的k个训练样本,组成一个邻居样本子集；S3：采用基于协同表示的目标函数,实现邻居样本子集对未知样本的重新表示；S4：在基于协同表示的目标函数中加入局部均值向量进行约束,得到新的目标函数,新的目标函数能使不同类别在表示未知样本时相互竞争；S5：对步骤S4得到的新的目标函数进行求导,求解表示系数；S6：将表示系数作为每个邻居样本的权重,计算每个类别邻居样本的权重和；S7：将未知样本分类为系数和最大的类别,得到未知样本的类别；S8：判断未知样本集合中的未知样本是否全部分类完毕,若全部分类完毕,则结束分类；若还有未知样本没有完成分类,则回到步骤S2进行执行,实现对下一个未知样本的分类。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         田海博;                   罗裴然       </td>   <td>中山大学</td>   <td>一种基于账户模型的高效数字资产交易方法</td>   <td>广东省</td>   <td>CN112419059A</td>   <td>2021-02-26</td>   <td>本发明涉及区块链金融技术领域,更具体地,涉及一种基于账户模型的高效数字资产交易方法。针对现有的AFCoin技术框架,引入区块链森林和背书节点选择方法,融入收据处理过程,给出该框架的完整实现步骤,并将其应用在数字资产交易的领域,进而获得了一种基于账户模型的高效数字资产交易方法。在AFCoin的语义框架下,区块链森林是央行维护的多个区块链结构,不同的商业银行具有不同的区块链,但是由央行统一发布；背书节点选择方法是确保商业银行无法选择特定的负责背书的商业银行,防止商业银行间作弊问题的出现。当把央行作为数字资产的发行方,商业银行看作数字资产的承销方,用户看成数字资产的持有人之后,就可以获得一个高效的数字资产交易方法。</td>   <td>1.一种基于账户模型的高效数字资产交易方法,其特征在于,包括资产发行方CB、资产承销方CMB_1,…,CMB_(nb)以及用户DU_1,…,DU_(nu),其中nb和nu是任意自然数,且随着区块链系统的运行,nb和nu的数值可以动态变化；资产承销方CMB_1,…,CMB_(nb)分别拥有公私钥对(bpk_1,bsk_1),…,(bpk_(nb),bsk_(nb))。用户DU_1,…,DU_(nu)可以在不同的资产承销方注册真实身份,以满足金融系统的监管需求,同时用户分别拥有公私钥对(upk_1,usk_1),…,(upk_(nu),usk_(nu)),用户的公钥进一步生成用户的账户地址；用户的数字资产交易包括以下步骤：S1.创建与提交交易：用户DU_i获取用户DU_j的账户地址,设置数字资产的值v生成未签名的交易Tx',并通过用户DU_i的私钥usk_i生成交易签名Sig_(Tx),发送带签名的交易Tx到资产承销方CMB_s,其中i,j∈{1,…,nu},s∈{1,…,nb}；S2.处理交易：资产承销方CMB_s接收来自用户DU_i的交易Tx,验证交易Tx是否满足交易条件,处理满足交易条件的交易Tx,并输出交易收据Recpt；S3.交易上链：资产承销方CMB_s将交易Tx_1,…,Tx_n的哈希值及交易收据Recpt_1,…,Recpt_n的哈希值打包,形成未签名未背书区块Blk',通过CMB_s的私钥bsk_s生成区块签名Sig_(Blk)并与区块Blk'组装得到未背书的区块Blk”,之后由负责背书的资产承销方CMB_p和CMB_q生成背书签名,并提交携带背书签名的区块Blk、交易Tx_1,…,Tx_n和收据Recpt_1,…,Recpt_n至资产发行方CB,资产发行方CB验证区块Blk的有效性并发布有效的区块至区块链森林中CMB_s的区块链Chain_s上,其中p,q∈{1,…,nb},n是自然数。</td>   <td>G06Q40/04;G06Q20/36;G06Q20/38;G06Q20/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         吴嘉婧;              张如筱;                   郑子彬       </td>   <td>中山大学</td>   <td>基于深度强化学习和联盟链的能量交易方法、装置及设备</td>   <td>广东省</td>   <td>CN112419064A</td>   <td>2021-02-26</td>   <td>本发明涉及一种基于深度强化学习和联盟链的能量交易方法、装置及设备,通过采集能量交易场N个影响买卖双方的状态向量构成第一状态矩阵,在神经网络模型中对状态矩阵进行处理、分析得到动作矩阵、第二状态矩阵和奖励矩阵,还采用第一状态矩阵、动作矩阵、第二状态矩阵和奖励矩阵对神经网络模型进行训练,得到神经网络训练模型,基于神经网络训练模型和联盟链的能量交易方法应用的电动汽车的P2P电量交易中,使得参与交易的电动汽车长期收益最大化,并引入了联盟链,保证电动汽车电量交易的隐私安全,解决了在基于联盟链的P2P电量交易中,如何让买方和卖方得到最大长期效益的技术问题。</td>   <td>1.一种基于深度强化学习和联盟链的能量交易方法,应用于电动汽车电量交易,其特征在于,包括以下步骤：S10.采集能量交易场的交易特征,并将交易特征组成一个状态向量,在t时刻,所述能量交易场中N个状态向量构成第一状态矩阵；其中,所述交易特征包括电动汽车剩余停在能量交易场中的时间、买卖标签、交易能量和交易标价；S20.将所述第一状态矩阵输入深度强化学习的神经网络模型,输出动作矩阵；S30.所述动作矩阵和所述第一状态矩阵经过状态转移函数和奖励函数计算,得到t+1时刻的第二状态矩阵和奖励矩阵；所述第一状态矩阵、所述动作矩阵、所述第二状态矩阵和所述奖励矩阵构成训练矩阵,并将所述训练矩阵存储至所述神经网络模型的回放池中；S40.每隔Δt时刻,从所述神经网络模型的回放池中获取m条所述训练矩阵的数据对所述神经网络模型进行训练,直至所述神经网络模型的损失函数收敛或迭代到最大次数,得到训练后的神经网络训练模型；S50.在所述能量交易场中,将买卖双方需要交易特征构成的状态矩阵输入至所述神经网络训练模型,得到买卖双方交易的能量。</td>   <td>G06Q40/04;G06Q50/06;G06Q20/10;G06Q20/06;G06F16/27;G06F21/64;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李伟忠;              杨欢;              柯尊富;                   陈丽丽       </td>   <td>中山大学</td>   <td>数字病理图像分析方法、系统、设备及存储介质</td>   <td>广东省</td>   <td>CN112419253A</td>   <td>2021-02-26</td>   <td>本发明公开了一种数字病理图像分析方法,包括：获取数字病理图像；从所述数字病理图像中提取感兴趣区域；根据所述感兴趣区域构建自适应的滑动窗口,并通过所述滑动窗口从所述感兴趣区域中截取兴趣块；对所述兴趣块进行过滤处理；将所述兴趣块输入病理模型以进行分析处理。本发明还公开了一种数字病理图像分析系统,一种计算机设备及一种计算机可读存储介质。采用本发明,可对病理图像进行预处理,有效提升人工智能算法的分类准确度,适用性更广、容错性更高、处理难度更低。</td>   <td>1.一种数字病理图像分析方法,其特征在于,包括：获取数字病理图像；从所述数字病理图像中提取感兴趣区域；根据所述感兴趣区域构建自适应的滑动窗口,并通过所述滑动窗口从所述感兴趣区域中截取兴趣块；对所述兴趣块进行过滤处理；将所述兴趣块输入病理模型以进行分析处理。</td>   <td>G06T7/00;G06T7/11</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈刚;                   马发潮       </td>   <td>中山大学</td>   <td>一种深度图的生成方法、装置及存储介质</td>   <td>广东省</td>   <td>CN112419383A</td>   <td>2021-02-26</td>   <td>本发明公开了一种深度图的生成方法、装置及存储介质,其中,该方法主要包括ORB特征点提取和loss函数,可解决在校准后为了保持校准结果而无法进行图像拼接的问题,和可解决由于拼接无关于世界坐标变换,拼接后无法统一校准上下五组图像的问题；本发明使用光学相机传感器,与其他使用激光雷达传感器技术相比而言,本发明的设备成本十分低廉,且通过本发明所述的深度图的生成方法,获得的深度稠密度更高；同时本发明无须通过生成过渡拼接的图像,而是将图像拼接成全景图后直接生成深度图,可以减少计算时间；且本发明只需要校准一组图像,不需要对全部图像进行校准处理,且能够确保校准图像精确性。本发明可广泛应用于图像拼接技术领域。</td>   <td>1.一种深度图的生成方法,其特征在于,包括以下步骤：对多组原始图像进行校正,并进行分组得到第一组图像和第二组图像,所述第一组图像包括至少一组待校准图像,所述第二组图像包括至少一组无需校准的图像；对第一组图像进行校准处理,得到第三组图像；对第一图像集中的所有图像进行ORB特征点提取,获得ORB特征点点集,所述第一图像集包括第二组图像和第三组图像；根据所述ORB特征点点集,通过ransac算法得到初始化旋转矩阵；将所述初始化旋转矩阵通过梯度下降法得到拼接矩阵；将第一图像集中的所有图像通过所述拼接矩阵进行仿射和拼接,得到全景图；使用SAD算法从所述全景图中获得视差,并通过光学原理生成深度图。</td>   <td>G06T7/50;G06T3/40;G06T3/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张乐;                   胡建芳       </td>   <td>中山大学</td>   <td>基于人体骨架序列信息的人物动作视频生成方法、系统及存储介质</td>   <td>广东省</td>   <td>CN112419455A</td>   <td>2021-02-26</td>   <td>本发明公开了一种基于人体骨架序列信息的人物动作视频生成方法、系统及存储介质,所述方法包括以下步骤：利用生成器提取初始纹理特征和初始姿势特征；经转换模块转换成目标纹理特征和目标姿势特征；将目标纹理特征输入到时序模块进行修正并得到最终纹理特征表示；编码器对最终纹理特征表示进行解码得到目标图像；判别器判别生成图像的纹理和姿势并交替更新生成器和判别器。本发明利用时序模型来对人物动作视频的生成进行时序上的建模,通过学习一个视频前后不同帧之间的关联关系来提升图像质量,得到高仿真度的视频。</td>   <td>1.基于人体骨架序列信息的人物动作视频生成方法,其特征在于,利用生成器和判别器进行训练,所述生成器用于生成尽可能逼真的图像,所述判别器用于判别图像的真伪；交替更新所述生成器和判别器达到动态平衡；所述生成器的训练过程为：将第一输入部分和第二输入部分分别经编码器编码为初始纹理特征和初始姿势特征所述第一输入部分为原始图像,所述第二输入部分为初始和目标人体骨架序列热图的级联；将所述初始纹理特征印初始姿势特征输入生成器的纹理转换路径和姿势转换路径中并进行交互引导转换,得到目标纹理特征所述纹理转换路径和姿势转换路径的中间包含了多个转换模块；将多个所述目标纹理特征输入到时序模块中,并经时序模块修正后得到多帧的最终纹理特征表示；将多帧的最终纹理特征表示分别经过解码器进行解码,得到多帧的最终纹理特征对应的目标图像,最终生成动作视频；将生成器生成的多帧图像分别与视频第一帧图像组成图像对,输入到纹理判别器中,计算纹理判别器此时的输出与纹理判别器将某个样本判别为真的输出之间的误差损失,后向传播更新生成器；将生成器生成的多帧图像分别与目标人体骨架序列表示组合,输入到姿势判别器中,计算姿势判别器此时的输出与姿势判别器将某个样本判别为真的输出之间的误差损失,后向传播更新生成器；用生成器生成的多帧图像相应计算出损失函数中除GAN损失外的其他损失项并后向传播更新生成器；所述判别器的训练过程为：将视频的第一帧图像和某一帧的真实图像组成的图像对作为正样本,视频的第一帧图像和生成器生成的某一帧的图像组成的图像对作为负样本,输入到纹理判别器中计算误差损失,后向传播更新纹理判别器；将视频某一帧的真实图像与对应的人体骨架序列热图的串接作为正样本,生成器生成的视频某一帧图像与对应的人体骨架序列热图的串接作为负样本,输入到姿势判别器中计算误差损失,后向传播更新姿势判别器；所述交替更新生成器和判别器达到动态平衡具体为：在一次迭代中,生成器根据输入生成转换后的人物图像,将生成的图像分别和原图像和目标人体骨架序列表示进行组合,分别输入两个判别器计算得到对应的GAN损失项,利用生成的图像继续计算出生成器的所有损失项之后进行反向传播更新生成器参数；接着采用生成的图像分别和原图像和目标人体骨架序列表示组合作为负样本,结合以真实目标图像组合得到的正样本,分别输入到两个判别器中计算损失并反向传播更新判别器参数,提升判别器正确判别图像真伪的能力,在训练过程中交替进行上述步骤更新生成器和判别器,最终达到动态平衡。</td>   <td>G06T13/40;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         何晋豪;              成慧;              周榆明;              黄立祥;                   孔阳       </td>   <td>中山大学</td>   <td>一种地空异构协同地图构建方法</td>   <td>广东省</td>   <td>CN112419501A</td>   <td>2021-02-26</td>   <td>本发明涉及地图构建技术领域,具体地,涉及一种地空异构协同地图构建方法,包括：利用多个移动平台获取环境三维点云数据,并计算多个移动平台的位姿估计结果；根据三维点云数据与位姿估计结果构建局部点云地图,同时优化位姿估计结果,并通过局部点云地图生成场景缩略图；索引场景缩略图得到候选匹配地图对；计算候选匹配地图对之间的刚性变换,并根据进一步筛选匹配对；构建位姿图并优化,融合多个移动平台的位姿图与局部点云地图,得到完整的三维点云地图。本发明通过将候选匹配地图之间的变换关系作为约束对相似的局部点云地图进行联合优化,同时实现了地图数据的融合与优化,使地图构建结果更为精准。</td>   <td>1.一种地空异构协同地图构建方法,其特征在于,包括以下步骤：S1：利用多个移动平台分别获取环境三维点云数据,并计算多个移动平台的位姿估计结果；S2：根据三维点云数据与位姿估计结果构建局部点云地图,同时优化位姿估计结果,并通过局部点云地图生成场景缩略图；S3：提取场景缩略图的全局描述子向量,利用全局描述子向量索引场景缩略图得到候选匹配地图对；S4：计算候选匹配地图对之间的刚性变换,并根据点云匹配度进一步筛选匹配对；S5：利用刚性变换作为约束,构建位姿图并优化,在统一坐标系下融合多个移动平台的位姿图与局部点云地图,得到完整的三维点云地图。</td>   <td>G06T17/05</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;              魏兆基;              陈寒阳;                   郭珊珊       </td>   <td>中山大学</td>   <td>一种基于神经网络的水下视频鱼类识别方法</td>   <td>广东省</td>   <td>CN112418087A</td>   <td>2021-02-26</td>   <td>本发明为基于神经网络的水下视频鱼类识别方法,包括步骤：训练神经网络模型,模型包括依次连接的输入层、第一卷积层、第二卷积层、第三卷积层、最大池化层、全连接层和输出层,第一卷积层对输入层中的每个通道各有一层卷积层对不同通道的信息进行不同的特征提取后进行特征图融合,第二卷积层采用多重卷积的方法对不同尺度的目标提取到不同感受野的尺度,再进行特征图融合、批归一化处理；将水下视频数据中彩色图像的每个通道及其灰度图像作为模型的输入；模型输出多个目标定位框及其置信度,根据置信度进行目标筛除。该方法可以满足实时视频鱼类识别的要求的同时,降低对摄像机拍摄图像的质量要求。</td>   <td>1.一种基于神经网络的水下视频鱼类识别方法,其特征在于,包括以下步骤：(1)、训练神经网络模型；得到的神经网络模型包括依次连接的输入层、第一卷积层、第二卷积层、第三卷积层、最大池化层、全连接层和输出层,输入层针对每个输入图像的每个通道各有一个输入层输出到第一卷积层；第一卷积层针对输入层中的每个通道,各有一层卷积层对不同通道的信息进行不同的特征提取后,进行特征图融合,再输出到第二卷积层；第二卷积层采用多重卷积的方法对不同尺度的目标提取到不同感受野的尺度,然后进行特征图融合、批归一化处理,输出到第三卷积层；第三卷积层采用卷积权重复用的方法,进行下采样和特征提取,最后输出至最大池化层；(2)、将水下视频数据中彩色图像的每个通道及其灰度图像作为神经网络模型的输入数据；(3)、在神经网络模型的输出端输出多个目标定位框及其置信度,根据置信度进行目标筛除。</td>   <td>G06K9/00;G06N3/04;G06N3/08;G06T5/00;G06T7/11;G06T7/194</td>  </tr>        <tr>   <td>中国专利</td>   <td>         吴贺俊;                   祝一帆       </td>   <td>中山大学</td>   <td>一种面向地址场景识别的离线文字识别方法</td>   <td>广东省</td>   <td>CN112418225A</td>   <td>2021-02-26</td>   <td>本发明提供一种面向地址场景识别的离线文字识别方法,该方法对数据集进行预处理：去除数据集标注文本中无法识别的生僻字,以及该标注文本对应的图像,其中,数据集包括图像和图像对应的标注文本；使用ICDAR2017RCTW数据集对连接预选框网络CTPN进行训练；训练卷积循环神经网络CRNN模型；输入预处理后的图像,使用CTPN定位图像中所有文本的位置,并使用矩形框将文本框出,提供矩形的顶点坐标以及宽高；将输出的文本框坐标输入CRNN,对文本框中的文本进行识别,输出预测文本,提高了地址识别的准确率。</td>   <td>1.一种面向地址场景识别的离线文字识别方法,其特征在于,包括以下步骤：S1：对数据集进行预处理：去除数据集标注文本中无法识别的生僻字,以及该标注文本对应的图像,其中,数据集包括图像和图像对应的标注文本；S2：使用ICDAR2017RCTW数据集对连接预选框网络CTPN进行训练；S3：训练卷积循环神经网络CRNN模型；S4：输入预处理后的图像,使用CTPN定位图像中所有文本的位置,并使用矩形框将文本框出,提供矩形的顶点坐标以及宽高；S5：将步骤S4中输出的文本框坐标输入CRNN,对文本框中的文本进行识别,输出预测文本。</td>   <td>G06K9/34;G06K9/32;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄华威;              林康颖;                   郑子彬       </td>   <td>中山大学</td>   <td>一种联邦学习场景下的移动设备算力预测方法和装置</td>   <td>广东省</td>   <td>CN112418342A</td>   <td>2021-02-26</td>   <td>本申请实施例公开了一种联邦学习场景下的移动设备算力预测方法和装置,方法包括收集移动设备上的数据信息；将位置信息进行聚类,得到移动设备的热点位置；通过时间信息记录移动设备在热点位置上的停留时间；计算移动设备不同日期下在热点位置的平均网络状态和平均资源状态；将预处理后的数据作为训练数据,输入到循环神经网络模型进行预测,得到预置时间段后移动设备的网络信息和资源状态信息并输出。基于本申请所提供的方式,联邦学习参数服务器能够预测未来阶段内的设备资源状态,有效地提高联邦学习场景下的有效参与设备的比例和减少联邦学习任务的时间成本。</td>   <td>1.一种联邦学习场景下的移动设备算力预测方法,其特征在于,包括：收集移动设备上的数据信息,所述数据信息包括位置信息、时间信息、日期信息、网络信息和资源状态信息；将所述位置信息进行聚类,得到所述移动设备的热点位置；通过所述时间信息记录所述移动设备在所述热点位置上的停留时间；根据所述日期信息、所述停留时间、所述网络信息和所述资源状态信息,计算所述移动设备不同日期下在所述热点位置的平均网络状态和平均资源状态；将所述位置信息、所述时间信息、所述日期信息、所述平均网络状态和所述平均资源状态进行预处理,作为训练数据；将所述训练数据输入循环神经网络模型进行预测,得到预置时间段后所述移动设备的网络信息和资源状态信息并输出。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郭雨;                   潘嵘       </td>   <td>中山大学</td>   <td>多教师自适应联合知识蒸馏</td>   <td>广东省</td>   <td>CN112418343A</td>   <td>2021-02-26</td>   <td>本发明涉及多教师自适应联合知识蒸馏,对教师模型的中间层特征,选择深度神经网络对这些特征进行融合,并用用加权预测融合各个教师模型的预测结果,得到最终的特征让学生模型学习并构造学生模型的最终损失函数。对于不同的样本,学生模型能够有偏向的学习不同的教师模型的预测结果,将不同教师模型传递的知识有差异的结合,形成更加有效的软标签,引导学生模型的学习,使得学生模型的学习更加有效,令学生模型的最终使用效果更好。</td>   <td>1.多教师自适应联合知识蒸馏,其特征在于,选择深度神经网络教师模型的中间层特征进行融合,并用加权预测融合各个教师模型的预测结果,得到最终特征让学生模型学习并构造学生模型的最终损失函数；损失函数具体为：                                                      其中,y~s是学生模型的logits输出,y是真实标签；是学生模型输出的概率分布和教师模型输出的概率分布；A~s,A~t是学生模型和教师模型提取的特征；α是个超参数,用来控制两种损失的权重；β是控制损失的权重的超参数；T作为温度,用来平滑这两个概率分布。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         涂朝勇;              陈淑敏;                   黎伟标       </td>   <td>中山大学</td>   <td>一种台风灾害评估方法、系统及装置</td>   <td>广东省</td>   <td>CN112418718A</td>   <td>2021-02-26</td>   <td>本发明公开了一种台风灾害评估方法、系统及装置,该方法包括：实时获取洋面台风信息和气象台的台风路径信息,得到台风信息和路径信息；根据台风信息、路径信息和预设规则判断对评估区域是否存在影响,并根据判断结果对评估区域进行实时预评估,得到预评估结果；根据预评估结果对评估区域进行灾情评估并生成评估报告。该系统包括：信息获取模块、预评估模块和评估报告生成模块。该装置包括存储器以及用于执行上述台风灾害评估的处理器。通过使用本发明,较为直观的看出台风灾害所致的破坏总程度。本发明作为一种台风灾害评估方法、系统及装置,可广泛应用于气象监测领域。</td>   <td>1.一种台风灾害评估方法,其特征在于,包括以下步骤：实时获取洋面台风信息和气象台的台风路径信息,得到台风信息和路径信息；根据台风信息、路径信息和预设规则判断对评估区域是否存在影响,并根据判断结果对评估区域进行实时预评估,得到预评估结果；根据预评估结果对评估区域进行灾情评估并生成评估报告。</td>   <td>G06Q10/06;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张吉祺;              林倞;              聂琳;              王广润;                   王广聪       </td>   <td>中山大学</td>   <td>一种基于可微图学习的行人重识别模型的弱监督训练方法</td>   <td>广东省</td>   <td>CN112395997A</td>   <td>2021-02-23</td>   <td>本发明提供一种基于可微图学习的行人重识别模型的弱监督训练方法,该方法首先将行人图片按拍摄时间段分组成袋并分配袋类别标签；然后,捕获每一个袋中所有图片之间的依赖关系,来为该类别的袋中每张图片生成可靠的伪行人类别标签,作为行人重识别模型训练的监督信息；然后,进行行人重识别模型和图模型的一体训练；将图模型损失和重识别损失的线性组合作为总损失函数,利用反向传播算法更新网络所有层的参数。本发明不需要繁重的人工标注成本、几乎不增加计算复杂度也能达到领先的模型性能。</td>   <td>1.一种基于可微图学习的行人重识别模型的弱监督训练方法,其特征在于,包括以下步骤：S1：将行人图片按拍摄时间段分组成袋并分配袋类别标签；S2：捕获每一个袋中所有图片之间的依赖关系,来为该类别的袋中每张图片生成可靠的伪行人类别标签,作为行人重识别模型训练的监督信息；S3：进行行人重识别模型和图模型的一体训练；S4：将图模型损失和重识别损失的线性组合作为总损失函数,利用反向传播算法更新网络所有层的参数。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         吴贺丰;              张小雨;              刘凌波;              林倞;                   王青       </td>   <td>中山大学</td>   <td>一种多模态密集预测的深度信息传输模型的构建方法</td>   <td>广东省</td>   <td>CN112396000A</td>   <td>2021-02-23</td>   <td>本发明提供一种多模态密集预测的深度信息传输模型的构建方法,该方法首先,构建多个子网络是用于RGB图或热图表征学习,再构建一个子网络用于为模式共享；然后,构建一个信息聚集-分布模块IADM,用于完成平移不变的信息提取以及信息聚合传输、信息分布传输。本发明通过学习多模态对齐表示,建立一个包含信息聚合分布模块的多模态密集预测框架,能够充分捕捉不同模态之间的互补信息,很好的完成信息整合。在各种多模态密度预测任务中,该方案显示出了有效性和通用性。</td>   <td>1.一种多模态密集预测的深度信息传输模型的构建方法,其特征在于,包括以下步骤：S1：构建多个子网络是用于RGB图或热图表征学习,再构建一个子网络用于为模式共享；S2：构建一个信息聚集-分布模块IADM,用于完成平移不变的信息提取以及信息聚合传输、信息分布传输。</td>   <td>G06K9/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张舒婕;              马争鸣;                   刘洁       </td>   <td>中山大学</td>   <td>一种基于局部边际最大化的动作识别方法</td>   <td>广东省</td>   <td>CN112396028A</td>   <td>2021-02-23</td>   <td>本发明提出一种基于局部边际最大化的动作识别方法,涉及计算机视觉的技术领域,解决了面对视频动作数据的噪声干扰时,当前通过投影降维的方法忽略原始动作行为数据中判别信息的保持及保护的问题,利用张量对动作视频训练集和待识别动作视频数据集中的每一个动作视频数据进行表征,从而能够充分考虑视频数据的空间信息；通过在低维空间中保持从原始动作视频数据集中所提取的相似性和非相似性系数,使得低维判别性局部的局部边际得以最大化,从而能够更好地保护每个局部所携带的判别信息,然后通过最大化局部边际,提高对判别性局部内的动作数据点的识别准确率。</td>   <td>1.一种基于局部边际最大化的动作识别方法,其特征在于,至少包括：S1.将动作视频数据集划分为动作视频训练集和待识别动作视频数据集；S2.将动作视频训练集和待识别动作视频数据集中的每一个动作视频数据表征为三阶视频序列张量,得到动作视频训练集张量和待识别动作视频数据集张量；S3.基于张量距离公式,将动作视频训练集划分为若干个判别性局部；S4.在高维空间中提取每个判别性局部的相似性系数和非相似性系数；S5.利用多线性投影方法,将动作视频训练集张量、提取到的每个判别性局部的相似性系数和非相似性系数映射到低维空间；S6.建立低维空间最大化局部边际优化函数,基于迭代优化求解优化函数的投影矩阵；S7.利用投影矩阵将动作视频训练集张量和待识别动作视频数据集张量均映射到低维空间,得到低维动作视频训练集张量及低维待识别动作视频数据集张量；S8.将低维动作视频训练集张量作为KNN分类器的训练集,利用KNN分类器对低维待识别动作视频数据集张量进行分类,完成动作识别。</td>   <td>G06K9/00;G06K9/62;G06N20/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;                   张镓伟       </td>   <td>中山大学</td>   <td>一种结合空间变换网络和多尺度特征提取的遮挡行人重识别方法</td>   <td>广东省</td>   <td>CN112396036A</td>   <td>2021-02-23</td>   <td>本发明公开了一种结合空间变换网络和多尺度特征提取的遮挡行人重识别方法,包括下述步骤：用模拟遮挡生成器构建有遮挡的行人图片集；将原始图片与有遮挡的行人图片组成数据集并输入到空间变换网络中进行空间变换纠正；通过卷积神经网络和空间金字塔池化层对纠正后的图进行多尺度特征提取并合并为定长一维特征向量；将定长一维特征向量通过全连接层得到一个包含K个元素的一维特征向量并进行身份分类训练,得到训练好的网络；用训练好的网络提取待查询的行人图像的特征并进行相似度匹配。本发明进行多尺度特征提取,通过结合不同尺度的特征图,使得模型更具鲁棒性；还引入了空间变换网络,可直接嵌入到任意深度网络模型中进行端到端的训练。</td>   <td>1.一种结合空间变换网络和多尺度特征提取的遮挡行人重识别方法,其特征在于,包括下述步骤：利用模拟遮挡生成器在一个行人数据集上构建有遮挡的行人图片；将原始行人图片与新生成的有遮挡的行人图片组成新的数据集,并将新的数据集输入到空间变换网络中进行空间变换纠正；所述空间变换网络用于对新的数据集中的图片进行自动裁剪、平移、缩放,使图片只保留人的部分,得到纠正后的图片；利用卷积神经网络和空间金字塔池化层对所述纠正后的图片进行多尺度特征提取并合成定长一维特征向量；将所述定长一维特征向量通过全连接层得到一个包含K个元素的一维特征向量,再进行行人图像的身份分类训练,得到训练好的网络；利用所述训练好的网络提取待查询的行人图像的特征并进行相似度匹配。</td>   <td>G06K9/00;G06K9/46;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         詹宗沅;              魏朋旭;                   林倞       </td>   <td>中山大学</td>   <td>基于加权最优传输的无监督域自适应视觉目标检测方法、系统及存储介质</td>   <td>广东省</td>   <td>CN112396097A</td>   <td>2021-02-23</td>   <td>本发明公开了一种基于加权最优传输的无监督域自适应视觉目标检测方法、系统及存储介质,方法包括以下步骤：基于最优传输的域间样本对采样方法,基于加权最优传输的候选区域域间特征对齐学习,浅层全局特征对抗对齐学习,深层全局特征对抗对齐学习,上下文特征融合连接。本发明一方面解决了无监督域自适应目标检测中提案候选区域特征如何对齐的问题；另一方面,解决了最优传输算法初始的域间类别分布不均衡问题,根据候选区域类别数量重新分配最优传输算法的初始分布权重,使得两域间同类的候选区域总权重一致,有效的减少了错误类别匹配的发生,保证域差异减小的同时保持类别判别性。</td>   <td>1.基于加权最优传输的无监督域自适应视觉目标检测方法,其特征在于,包括以下步骤：基于最优传输的域间样本对采样方法,采用预训练模型对源域和目标域训练数据图像进行特征编码,全局池化得到源域和目标域数据集的特征编码,构建最优传输模型,通过最优传输算法迭代求解源域和目标域之间的图像样本匹配解,以此匹配解进行训练采样；基于加权最优传输的候选区域域间特征对齐学习,每轮训练迭代中,源域和目标域的图像分别通过检测器的特征提取单元和候选区域提取单元,输出对应图像中的候选区域,池化获得各个候选区域目标特征,根据目标类别信息重新分配源域和目标域的候选区域目标出现的权重,构建最优传输模型,通过最优传输算法迭代求解源域和目标域提案候选区域之间的匹配最优解,对匹配解的域间候选区域构建特征距离最小化目标函数；浅层全局特征对抗对齐学习,采用检测器骨干网络提取浅层全局特征,通过梯度逆转模块和卷积网络结构,输出全局特征各个像素位置的域判别得分；深层全局特征对抗对齐学习,采用检测器骨干网络提取浅层全局特征,通过梯度逆转模块和卷积网络结构后,再经过全连接层输出全局特征域判别得分；上下文特征融合连接,计算域判别器中间特征作为上下文信息,将该上下文特征补充到候选区域的特征中,再对融合后的特征进行分类和回归。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              严志伟;              黄梓轩;              李烨;                   陈绿然       </td>   <td>中山大学</td>   <td>一种基于检测主干与局部特征优化的目标检测方法及系统</td>   <td>广东省</td>   <td>CN112396126A</td>   <td>2021-02-23</td>   <td>本发明公开了一种基于检测主干与局部特征优化的目标检测方法及系统,该方法包括：获取训练数据并对训练数据进行预处理,得到预处理数据；基于长颈主干架构和局部特征优化模块构建目标检测网络；基于预处理数据和预设的训练策略对目标检测网络进行训练,得到训练后的目标检测网络；获取待测数据并输入到训练后的目标检测网络,输出检测结果。该系统包括：预处理模块、网络构建模块、训练模块和检测模块。通过使用本发明,保证检测器在计算力友好的前提下获得满意的性能。本发明作为一种基于检测主干与局部特征优化的目标检测方法及系统,可广泛应用于目标检测网络领域。</td>   <td>1.一种基于检测主干与局部特征优化的目标检测方法,其特征在于,包括以下步骤：获取训练数据并对训练数据进行预处理,得到预处理数据；基于长颈主干架构和局部特征优化模块构建目标检测网络；基于预处理数据和预设的训练策略对目标检测网络进行训练,得到训练后的目标检测网络；获取待测数据并输入到训练后的目标检测网络,输出检测结果。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王青;              叶佳全;              吴贺丰;                   林倞       </td>   <td>中山大学</td>   <td>一种对抗样本检测方法及通用对抗攻击防御系统</td>   <td>广东省</td>   <td>CN112396129A</td>   <td>2021-02-23</td>   <td>本发明公开了一种对抗样本检测方法,所述方法包括：获取训练数据集用于训练深度神经网络模型,获取预测单元A；利用基于训练数据集生成的对抗样本,通过对抗训练方法训练深度神经网络模型,获取预测单元B；将训练数据集和对抗样本均输入至预测单元A、B中进行推理,分别提取相同卷积层输出的特征图并拼接,将拼接图作为分类训练数据集；采用分类训练数据集训练深度神经网络二分类模型,获取对抗样本检测模块；将需检测的输入样本分别输入至预测单元A、B中进行推理,分别提取相同卷积层输出的特征图并进行拼接,然后将拼接图输入至对抗样本检测模块中进行检测,获取检测结果y-(detector)。本发明提升了对抗样本检测准确率,避免损失模型精度的代价。</td>   <td>1.一种对抗样本检测方法,其特征在于,包括以下步骤：S1：获取训练数据集并用于训练深度神经网络模型,获取预测单元A；S2：利用基于训练数据集生成的对抗样本,通过对抗训练方法训练与步骤S1中结构相同的深度神经网络模型,获取预测单元B；S3：将原训练数据集和对抗样本均输入至预测单元A、预测单元B中进行推理,分别提取相同卷积层输出的特征图并进行拼接,将拼接图作为分类训练数据集；S4：采用分类训练数据集训练深度神经网络二分类模型,获取对抗样本检测模块；S5：将需要进行检测的输入样本分别输入至预测单元A、预测单元B中进行推理,分别提取相同卷积层输出的特征图并进行拼接,然后将拼接图输入至对抗样本检测模块中进行检测,获取检测结果y-(detector)。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;              孟浩;                   李博洋       </td>   <td>中山大学</td>   <td>一种高动态范围场景下基于RGB与DVS图像融合的视差图增强方法</td>   <td>广东省</td>   <td>CN112396562A</td>   <td>2021-02-23</td>   <td>本发明属于机器人感知领域,更具体地,涉及一种高动态范围场景下基于RGB与DVS图像融合的视差图增强方法。包括：S1.部署双目RGB相机以及DVS相机,并对双目RGB相机以及DVS相机进行标定；S2.采集场景中双目相机RGB图像以及DVS图像,经过配准之后进行多尺度加权融合；S3.为融合后的图像生成针对计算机视觉的HDR图像；S4.基于步骤S3生成的HDR图像使用改进后的双目立体匹配算法SGM生成视差图。在隧道这类成像动态范围较大的场景中,解决相机出现的欠曝光以及高曝光问题,提高生成图像的质量,同时针对图像边缘区域不连续、不稳定的问题,通过引入其他信息源的方式,尽可能丰富边缘细节信息,提高最终生成的视差图在图像边缘处的准确率。</td>   <td>1.一种高动态范围场景下基于RGB与DVS图像融合的视差图增强方法,其特征在于,包括以下步骤：S1.部署双目RGB相机以及DVS相机,并对双目RGB相机以及DVS相机进行标定；S2.采集场景中双目相机RGB图像以及DVS图像,经过配准之后进行多尺度加权融合；S3.为融合后的图像生成针对计算机视觉的HDR图像；S4.基于步骤S3生成的HDR图像使用改进后的双目立体匹配算法SGM生成视差图。</td>   <td>G06T5/00;G06T5/50;G06T7/13;G06T7/80</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖贤城;              谢晓华;                   赖剑煌       </td>   <td>中山大学</td>   <td>一种基于单阶段多任务协同学习的人像抠图方法及系统</td>   <td>广东省</td>   <td>CN112396598A</td>   <td>2021-02-23</td>   <td>本发明公开了一种基于单阶段多任务协同学习的人像抠图方法及系统,该方法包括：获取数据集并对数据集进行预处理,得到训练人像图、对应的透明度图和对应的三元图；将训练人像图输入到预构建抠图模型,生成训练的三元图和训练的透明度图；损失计算并更新预构建抠图模型的参数,得到训练完成的抠图模型；获取待测图像并输入到训练完成的抠图模型,得到人像前景图。该系统包括：数据预处理模块、训练模块、参数更新模块和预测模块。通过使用本发明,解决现有技术中抠图阶段容易由三元图阶段的错误导致抠图阶段的预测错误。本发明作为一种基于单阶段多任务协同学习的人像抠图方法及系统,可广泛应用于图像抠图领域。</td>   <td>1.一种基于单阶段多任务协同学习的人像抠图方法,其特征在于,包括以下步骤：获取数据集并对数据集进行预处理,得到训练人像图、对应的透明度图和对应的三元图；将训练人像图输入到预构建抠图模型,生成训练的三元图和训练的透明度图；将训练的三元图和训练的透明度图与数据集该训练人像图对应的三元图和对应的透明度图进行损失计算并更新预构建抠图模型的参数,得到训练完成的抠图模型；获取待测图像并输入到训练完成的抠图模型,得到人像前景图。</td>   <td>G06T7/00;G06T7/194;G06T5/30;G06T3/40;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;              龚瑾;              郭英;              何海勇;              郭思璐;              宋日辉;                   梁宏立       </td>   <td>中山大学;中山大学附属第三医院</td>   <td>一种基于内窥镜图像的实时的神经外科手术器械分割方法、设备及存储介质</td>   <td>广东省</td>   <td>CN112396601A</td>   <td>2021-02-23</td>   <td>本发明属于医学图像处理领域和图像分割技术领域,更具体地,涉及一种基于内窥镜图像的实时的神经外科手术器械分割方法。提出了一套针对内窥镜神经外科手术场景的实时器械实例分割方法,能够应用到临床中,起到在术中实时辅助神经外科手术的作用。本发明还提出了一套针对光斑、倒影、模糊等噪声的数据增广方法,丰富样本的同时,提高模型的学习能力和适应性。</td>   <td>1.一种基于内窥镜图像的实时的神经外科手术器械分割方法,其特征在于,包括以下步骤：S1.采集内窥镜手术图像数据,采用人工标注的方式为图像打上标签,标签将前景即器械与背景进行空间上的分割和语义上的分类；构建数据集,设置交叉验证样本,建立器械实例分割数据库,分为训练集和验证集；S2.对数据集进行数据增广,包括翻转、旋转、调整图像强度、添加光斑/高斯噪声、图像混合,从而增加数据集的样本数量,丰富样本；S3.构建网络模型,包括一个特征主干网络,一个特征金字塔网络,一个原型预测分支和一个掩膜系数预测分支；输入是一个二维图像,输出为对该图像的预测结果,包括一组目标检测的边界框、掩膜以及对应类别；S4.用训练数据集作为训练样本,使用反向传播策略对步骤S3中构造的网络模型进行训练,最小化损失函数,得到优化后的网络权重；S5.测试模型,使用验证数据样本对训练好的网络模型进行测试,将验证图像输入网络模型,得到预测结果,与标签对比,判断网络是否具有较好适应性。</td>   <td>G06T7/00;G06T7/136;G06K9/34;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈龙;              陈兆棠;                   张亚琛       </td>   <td>中山大学</td>   <td>一种基于特征点检测与分割的语义地图增量更新方法</td>   <td>广东省</td>   <td>CN112396696A</td>   <td>2021-02-23</td>   <td>本发明提供一种基于特征点检测与分割的语义地图增量更新方法,包括以下步骤：S1.获取先前语义地图和室内外RGB图像,并对RGB图像进行预处理；S2.检测预处理后RGB图像的特征点,将采集的特征点与先前语义地图特征点进行匹配,并计算出相应的姿态变换矩阵；S3.计算预处理后RGB图像的语义分割结果；S4.根据姿态变换矩阵完成坐标变化,对比特征点在坐标变换前和变换后对应的语义标签是否一致,提取增量；S5.将提取的增量信息更新到先前语义地图中；S6.最后修复语义地图更新后出现的空缺区域,获得完整的语义地图。本发明在更新的过程中并不需要对没有改变的实物进行重新计算,节约了计算成本,加快了更新速度。</td>   <td>1.一种基于特征点检测与分割的语义地图增量更新方法,其特征在于：包括以下步骤：S1.获取先前语义地图和室内外RGB图像,并对RGB图像进行预处理；S2.检测预处理后RGB图像的特征点,将采集的特征点与先前语义地图特征点进行匹配,并计算出相应的姿态变换矩阵；S3.计算预处理后RGB图像的语义分割结果；S4.根据步骤S2的姿态变换矩阵完成坐标变化,对比特征点在坐标变换前和变换后对应的语义标签是否一致,从而提取增量；S5.将提取的增量信息更新到先前语义地图中；S6.修复语义地图更新后出现的空缺区域,获得完整的语义地图。</td>   <td>G06T17/05;G06T7/73;G06T7/187;G06K9/34;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;              林楚庭;                   林金城       </td>   <td>中山大学</td>   <td>基于M-CMSE场景识别与E-SFM结合的人群异常行为检测方法</td>   <td>广东省</td>   <td>CN112380936A</td>   <td>2021-02-19</td>   <td>本发明属于人群异常行为检测技术,为基于M-CMSE场景识别与E-SFM结合的人群异常行为检测方法,包括：用基于M-CMSE方法得到场景特征,匹配得出当前场景标签；做前景分割处理得到仅有前景像素点的帧图像；把图像切割成若干个小块,统计每个小块中前景像素点个数,并标记出关键目标块；计算每个关键目标块的速度和方向得到运动特征；根据行人速度、方向,通过E-SFM结合的分析方法,计算出行人的情绪值、社会力,输出帧图像中的异常区域；提取出异常区域中的局部特征,与场景特征、运动特征进行连接构成完整的特征向量I,分类得出异常的分类结果。本发明基于M-CMSE进行场景识别,配合E-SFM结合的异常检测方法,有效提高检测的准确率、稳定性和鲁棒性。</td>   <td>1.基于M-CMSE场景识别与E-SFM结合的人群异常行为检测方法,其特征在于,包括以下步骤：S1、采集视频帧序列前几帧,用基于多尺度空间包络与颜色矩结合的全局特征提取方法,得到场景特征,并匹配运算得出当前场景标签；S2、对输入的视频帧序列做前景分割处理,得到仅有前景像素点的帧图像；S3、输入前景分割处理后的仅有前景像素点的帧图像,设置网格大小,把图像切割成若干个小块,统计每个小块中前景像素点个数,当第i小块的前景像素点超过阈值时标记该块为关键目标块b-i,最终得到每一帧图像的运动目标块集；S4、计算帧图像中每个关键目标块b-i的速度v-i和方向u-i,得到运动特征；S5、根据帧图像中行人的速度、方向,通过情感分析与社会力模型结合分析方法,计算出行人的情绪值,然后计算出行人的社会力,最后输出帧图像中的异常区域；S6、根据帧图像中的异常区域,提取出异常区域中的局部特征,与场景特征、运动特征进行连接构成完整的特征向量I,输入到预先训练好的分类器中进行分类,得出异常的分类结果。</td>   <td>G06K9/00;G06K9/34;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林浩添;                   肖薇       </td>   <td>中山大学中山眼科中心</td>   <td>眼部图像中目标区域的识别方法、装置及电子设备</td>   <td>广东省</td>   <td>CN112381012A</td>   <td>2021-02-19</td>   <td>本发明提供了一种眼部图像中目标区域的识别方法、装置及电子设备,涉及模式识别领域,该方法将获取的待识别的眼部图像输入至预先完成训练的目标区域识别模型；其中,目标区域识别模型包括第一卷积层、第二卷积层以及全连接层；待识别的眼部图像输入至第一卷积层后通过下采样提取得到第一特征结果；通过第二卷积层将第一特征结果中的每一层特征图进行下采样得到第二特征结果；将第二特征结果输入至全连接层,确定待识别的眼部图像中目标区域的识别结果。该方法可对眼部图像中特定的目标区域进行识别,解决了现有技术中缺少利用人眼的巩膜区域、虹膜颜色形状、眼前节及眼底的血管纹路等作为识别手段以确定用户的健康指标数据的技术问题。</td>   <td>1.一种眼部图像中目标区域的识别方法,其特征在于,所述方法包括：获取待识别的眼部数据；其中,所述待识别的眼部数据包括待识别的眼部图像；所述目标区域包括眼前节区域和/或眼底区域；将所述待识别的眼部数据输入至预先完成训练的目标区域识别模型；其中,所述目标区域识别模型包括第一卷积层、第二卷积层以及全连接层；所述待识别的眼部图像输入至所述第一卷积层后,通过下采样提取得到第一特征结果；通过所述第二卷积层将所述第一特征结果中的每一层特征图进行下采样,得到第二特征结果；将所述第二特征结果输入至所述全连接层,确定所述待识别的眼部图像中的目标区域的识别结果；其中,所述识别结果至少包括：眼前节区域和/或眼底区域中是否包含异常区域；所述异常区域用于确定所述眼部图像对应用户的健康指标数据。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06T7/11;G16H50/30</td>  </tr>        <tr>   <td>中国专利</td>   <td>         袁进;              梁姗姗;              钟菁;              钟培勋;                   曾鸿为       </td>   <td>中山大学中山眼科中心;中山大学</td>   <td>一种基于角膜共聚焦图像的菌丝筛查系统以及菌丝筛查方法</td>   <td>广东省</td>   <td>CN112381768A</td>   <td>2021-02-19</td>   <td>本发明提供可了一种基于角膜共聚焦图像的菌丝筛查系统,其特征在于,包括：图像获取模块,用于获取待检测的角膜共聚焦图像；诊断模块,具有菌丝诊断模型,所述诊断模块接收图像获取模块获取的所述待检测的角膜共聚焦图像并将所述待检测的角膜共聚焦图像输入至所述菌丝诊断模型中,所述菌丝诊断模型对所述待检测的角膜共聚焦图像进行图像特征提取,并根据所提取的图像特征判断所述角膜共聚焦图像中是否存在菌丝；菌丝可视化模块,用于在诊断模块诊断出菌丝后从所述待检测的角膜共聚焦图像中提取菌丝区域并生成包含所述菌丝区域的可视化图像。该菌丝筛查系统能够自动地推角膜共聚焦图像进行判断是否存在菌丝并在存在菌丝时使菌丝区域可视化。</td>   <td>1.一种基于角膜共聚焦图像的菌丝筛查系统,其特征在于,包括：图像获取模块,用于获取待检测的角膜共聚焦图像；诊断模块,具有菌丝诊断模型,所述诊断模块接收图像获取模块获取的所述待检测的角膜共聚焦图像并将所述待检测的角膜共聚焦图像输入至所述菌丝诊断模型中,所述菌丝诊断模型对所述待检测的角膜共聚焦图像进行图像特征提取,并根据所提取的图像特征判断所述角膜共聚焦图像中是否存在菌丝；菌丝可视化模块,用于在诊断模块诊断出菌丝后从所述待检测的角膜共聚焦图像中提取菌丝区域并生成包含所述菌丝区域的可视化图像。</td>   <td>G06T7/00;G06T7/11;G06T7/136;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         徐伟嘉;              冯梦思;              刘永红;              卢志想;              黄建彰;                   陈群       </td>   <td>中山大学;广东旭诚科技有限公司</td>   <td>基于多个监测指标的质量控制方法</td>   <td>广东省</td>   <td>CN107480698B</td>   <td>2021-02-12</td>   <td>本发明涉及一种基于多个监测指标的质量控制方法,主要包括：首先通过决策树的基本原理,利用控制变量对预测变量进行分类；然后利用分类结果对测试集进行测算,依据预测结果进行模型调整和优化。输出最终模型结果作为质控标准反向验证多个监测指标的检测样本是否对应决策树中所属类别的区间范围；如果多个监测指标的检测样本对应决策树所属类别的各区间范围内判断为正常监测值反之为异常值。本发明实现了自动化、智能化的可疑数据筛选和判断以及数据质量分析预判等功能,保障了数据的质量,为后期数据使用和环境预报预警提供有力支撑。</td>   <td>1.一种基于多个监测指标的质量控制方法,其特征在于,所述多个监测指标为大气环境监测指标,包括以下步骤：S1.首先对历史数据进行训练集和测试集的划分；S2.利用训练集的数据构建决策树；S3.构建好决策树后利用测试集数据进行验证,验证决策树构建的效果,并根据测试结果不断优化和调整决策树模型；S4.构建最终模型树型结构,输出分类结果中每个类别对应的监测指标取值范围以及每个树枝的预测准确率；S5.选取决策树中预测准确率大于85%的树枝作为质控标准,输出各树枝对应的监测指标取值范围,利用反推过程对多个监测指标的检测样本进行质量控制；如果多个监测指标的检测样本的某个监测指标对应于预测准确率大于85%的树枝对应的监测指标取值范围之外判断为检测样本监测异常,反之为检测样本监测正常；S6.对于预测准确率小于85%的树枝重新进行样本的选择和决策树的训练,给予预测准确率越低的级别以越大的权重进入到训练集中,重复步骤S1-S4,直到预测变量所有类别的预测准确率均在85%以上再进行步骤S5；其中步骤S5所述利用反推过程对多个监测指标的检测样本进行质量控制包括以下步骤：运用历史数据构建决策树,对训练样本进行分类,通过训练样本的分类的结果检验多个监测指标的检测样本是否存在异常现象；针对多个监测指标的检测样本所属的分类类别,观察多个监测指标的检测样本的监测指标是否位于该类别的监测指标取值范围内,如果超出监测指标取值范围判断为检测样本监测异常,反之为检测样本监测正常。</td>   <td>G06K9/62;G06Q10/06;G01N33/00;G01W1/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王立国;              冯瀚生;                   林茂峰       </td>   <td>中山大学</td>   <td>一种波浪能发电装置节间距优化方法及装置</td>   <td>广东省</td>   <td>CN112365026A</td>   <td>2021-02-12</td>   <td>本发明公开了一种波浪能发电装置节间距优化方法及装置,方法包括：设置目标区域的海洋环境参数,并根据目标区域的海洋环境参数计算波能谱；根据目标区域的波能谱和波浪能发电装置的几何参数,计算得到波浪能发电装置的水动力系数；根据水动力系数和谱方法计算得到目标区域中波浪能发电装置在物理约束下的总发电功率的时间平均值；采用优化算法,根据波浪能发电装置的总发电功率的时间平均值对波浪能发电装置中浮子的尺寸参数和节间距参数进行优化,得到波浪能发电装置总发电功率最大时,波浪能发电装置中浮子的最优尺寸参数和最优节间距参数。本发明实施例能够有效提高波浪能发电装置的波浪能捕获效率。</td>   <td>1.一种波浪能发电装置节间距优化方法,其特征在于,包括：根据目标区域的海况以及当地地形,设置海洋环境参数,并根据所述海洋环境参数计算波能谱；其中,所述海洋环境参数包括：非规则波情况下的有效波高和周期或者规则波情况下的波高和周期；根据所述波能谱和所述目标区域中的波浪能发电装置的几何参数,计算得到所述波浪能发电装置的水动力系数；根据所述水动力系数和谱方法求解波浪能发电装置的能量转换系统在计及约束条件下的最优控制量,采用所述最优控制量计算得到所述目标区域中所述波浪能发电装置在物理约束下的总发电功率的时间平均值；采用优化算法,根据所述波浪能发电装置的总发电功率的时间平均值对所述波浪能发电装置中浮子的尺寸参数和节间距参数进行优化,得到所述波浪能发电装置总发电功率最大时,所述波浪能发电装置中浮子的最优尺寸参数和最优节间距参数。</td>   <td>G06Q10/04;G06Q50/06;F03B13/16</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨璐宇;                   成慧       </td>   <td>中山大学</td>   <td>一种三维物体检测方法</td>   <td>广东省</td>   <td>CN112365600A</td>   <td>2021-02-12</td>   <td>本发明涉及机器视觉技术领域,更具体的是涉及一种三维物体检测方法,包括以下步骤：S1：获取物体的点云数据,根据点云数据得到物体的俯瞰特征图；S2：构造二维旋转候选框,对俯瞰特征图进行二维旋转目标检测,提取得到初始特征图F；S3：通过初始特征图F得到旋转物体的二维初步检测结果P；S4：调整初始特征图F的特征值,得到调整特征图f,使初步检测结果P与特征图f中的像素点对齐；S5：将初步检测结果P作为候选框,对调整特征图f进行检测,得到二维旋转物体精细检测结果；S6：将二维旋转物体精细检测结果恢复到点云尺度,得到三维物体检测结果。通过二维旋转候选框检测三维物体,结果更为精确。</td>   <td>1.一种三维物体检测方法,其特征在于,包括以下步骤：S1：获取三维物体的点云数据,根据点云数据得到三维物体的俯瞰特征图；S2：构造二维旋转候选框,运用二维旋转目标检测网络对俯瞰特征图进行二维旋转目标检测,提取得到初始特征图F；S3：通过初始特征图F得到旋转物体的二维初步检测结果P；S4：调整初始特征图F的特征值,得到调整特征图f,使初步检测结果P与特征图f中的像素点对齐；S5：将初步检测结果P作为候选框,对调整特征图f进行检测,得到二维旋转物体精细检测结果；S6：将二维旋转物体精细检测结果恢复到点云尺度,得到三维物体检测结果。</td>   <td>G06T17/20;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘芳;              林嘉韵;              蔡振华;                   黄志杰       </td>   <td>中山大学</td>   <td>一种针对长寿命容器的自适应最优内存预留估计方法</td>   <td>广东省</td>   <td>CN112328355A</td>   <td>2021-02-05</td>   <td>本发明公开了一种针对长寿命容器的自适应最优内存预留估计方法,所述方法应用于数据中心Spark分布式集群的不同阶段,包括以下步骤：S1：在服务器集群的初始阶段执行MEER+策略,搜集服务器中应用程序运行的历史数据,并利用历史数据估计集群在初始阶段的最优内存预留；S2：在服务器集群的稳定阶段执行DEEP-MEER策略,利用历史数据得到稳定阶段的最优内存预留模型,并利用该模型估计当前阶段的最优内存预留。本发明对数据中心中的Spark分布式集群在不同生命周期阶段采用不同的最优内存预留估计策略,在集群初始阶段通过细化步长逼近最优值,提高了估计的准确度,在集群稳定阶段利用丰富的历史数据建立强化学习模型,从而保证了应用程序性能的稳定。</td>   <td>1.一种针对长寿命容器的自适应最优内存预留估计方法,所述方法应用于数据中心Spark分布式集群的不同阶段,其特征在于,包括以下步骤：S1：在Spark分布式集群的初始阶段执行MEER+策略,搜集服务器中应用程序运行的历史数据,并利用历史数据估计Spark分布式集群在初始阶段的最优内存预留；S2：在Spark分布式集群的稳定阶段执行DEEP-MEER策略,利用已知历史数据得到稳定阶段的最优内存预留模型,并利用该模型估计当前阶段的最优内存预留。</td>   <td>G06F9/455</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李仁杰;                   朝红阳       </td>   <td>中山大学</td>   <td>一种基于查找表加速点云分割的方法</td>   <td>广东省</td>   <td>CN112330680A</td>   <td>2021-02-05</td>   <td>本发明属于计算机视觉领域下的3D点云分割领域,更具体地,涉及一种基于查找表加速点云分割的方法,开拓性的将查找表的思想应用到点云分割的问题上来,利用访问查找表代替了神经网络的前向计算,极大的加速了点云分割的过程。本发明创新性的将主成分分析应用于点云分割处理上,让分割网络可以摆脱对空间变换网络模块的依赖,使点云分割网络具有旋转不变性的同时减少了计算量。</td>   <td>1.一种基于查找表加速点云分割的方法,其特征在于,包括以下步骤：S1.点云数据归一化处理,得到尺寸归一化的点云；S2.搭建并训练点云分割网络PointNet,点云分割问题可以看作是对点云中每个点的分类问题；其中,网络的输入为N×3的点云数据,N为点云中包含的点的数量,每个点有三维坐标进行表示；网络输出为N×K,K为点云中每个点的分类标签；训练完毕后保存网络参数；S3.建立特征查找表；S4.对PointNet_Seg_Basic_Cls部分进行微调,进一步提升分割网络的分割准确率；S5.对点云进行快速分割,通过特征查找表,获取点云中每个点的特征并将每个点的特征输入到微调后的PointNet_Seg_Basic_Cls网络中获取该点的分类结果；综合每个点的分类情况,最后得到点云的分割结果。</td>   <td>G06T7/10;G06T7/66;G06T17/20;G06F16/901;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         高峰;              蔡都;              段鑫;              吴小剑;              柯嘉;              王伟;              黄泽平;              朱琪琪;                   钟敏儿       </td>   <td>中山大学附属第六医院</td>   <td>一种基于影像组学的结直肠癌预后预测方法及设备</td>   <td>广东省</td>   <td>CN112329876A</td>   <td>2021-02-05</td>   <td>本申请实施例提供一种基于影像组学的结直肠癌预后预测方法及设备,该方法包括提取患有结直肠癌的患者的医学图像中预设影像组学特征；根据所述预设影像组学特征确定影像组学分数,其中所述影像组学分数用于表示所述患者的生存风险程度；根据所述影像组学分数与患者病理因素得到所述患者的预后预测结果。本申请实施例方法通过获取结直肠癌患者的医学图像中的影像组学特征,从而确定影像组学分数,然后根据影像组学分数与患者病理因素得到患者的预后预测结果,能够廉价、无创地为结直肠癌患者进行术前的有效干预,以及术后的预测。</td>   <td>1.一种基于影像组学的结直肠癌预后预测方法,其特征在于,所述方法包括：提取患有结直肠癌的患者的医学图像中预设影像组学特征；根据所述预设影像组学特征确定影像组学分数,其中所述影像组学分数用于表示所述患者的生存风险程度；根据所述影像组学分数与患者病理因素得到所述患者的预后预测结果。</td>   <td>G06K9/62;G06K9/32;G16H50/30;G16H50/70</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李熙莹;              江倩殷;              邓育新;              李国鸣;                   欧炎丹       </td>   <td>中山大学</td>   <td>一种基于融合特征的视频车辆重识别方法与系统</td>   <td>广东省</td>   <td>CN107622229B</td>   <td>2021-02-02</td>   <td>本发明公开了一种基于融合特征的视频车辆重识别方法与系统,方法包括：确定目标车辆与匹配范围；根据目标车辆与匹配范围采用基于颜色直方图的相似度计算方法,计算目标车辆图像与待匹配车辆图像的颜色特征相似度；根据目标车辆与匹配范围采用基于局部线性约束编码和加权空间金字塔的方向梯度直方图特征相似度计算方法,计算目标车辆图像与待匹配车辆图像的编码方向梯度直方图特征相似度；将计算的颜色特征相似度和编码方向梯度直方图特征相似度进行加权融合。本发明通过将计算的颜色特征相似度和编码方向梯度直方图特征相似度进行加权融合来得到车辆重识别的相似度结果,准确率更高,鲁棒性更强,通用性更高。本发明可广泛应用于图像处理领域。</td>   <td>1.一种基于融合特征的视频车辆重识别方法,其特征在于：包括以下步骤：确定目标车辆与匹配范围；根据目标车辆与匹配范围采用基于颜色直方图的相似度计算方法,计算目标车辆图像与待匹配车辆图像的颜色特征相似度；根据目标车辆与匹配范围采用基于局部线性约束编码和加权空间金字塔的方向梯度直方图特征相似度计算方法,计算目标车辆图像与待匹配车辆图像的编码方向梯度直方图特征相似度,所述加权空间金字塔的权重能根据车辆纹理特征进行调整；将计算的颜色特征相似度和编码方向梯度直方图特征相似度进行加权融合,得到车辆重识别的相似度结果,所述加权融合时的权重能根据环境条件进行调整,所述环境条件包括光照或角度；所述根据目标车辆与匹配范围采用基于局部线性约束编码和加权空间金字塔的方向梯度直方图特征相似度计算方法,计算目标车辆图像与待匹配车辆图像的编码方向梯度直方图特征相似度,包括：分别提取目标车辆图像与待匹配车辆图像的方向梯度直方图特征；采用局部线性约束编码和加权空间金字塔对分别提取的方向梯度直方图特征进行编码,得到目标车辆图像与待匹配车辆图像的编码方向梯度直方图特征；根据得到的编码方向梯度直方图特征进行相似度计算,得到目标车辆图像与待匹配车辆图像的相关系数,所述目标车辆图像与待匹配车辆图像的相关系数ρ_H(I_A,I_B)表达式为：                  其中,I_A为目标车辆图像,I_B为待匹配车辆图像,Cov(I_A,I_B)为目标车辆图像与待匹配车辆图像的协方差,D(I_A)为目标车辆图像的方差,D(I_B)为待匹配车辆图像的方差；所述将计算的颜色特征相似度和编码方向梯度直方图特征相似度进行加权融合,得到车辆重识别的相似度结果,包括：根据计算的编码方向梯度直方图特征相似度计算目标车辆图像与待匹配车辆图像的相关距离,所述目标车辆图像与待匹配车辆图像的相关距离d_H(I_A,I_B)计算公式为：d_H(I_A,I_B)＝1-ρ_H(I_A,I_B)；将计算的颜色特征相似度和计算的相关距离进行加权融合,得到融合后的相似度,并将融合后的相似度作为车辆重识别的相似度结果进行输出,所述加权融合的公式为：d(I_A,I_B)＝λd_c(I_A,I_B)+(1-λ)d_H(I_A,I_B),其中,d(I_A,I_B)为融合后的相似度,d_c(I_A,I_B)为目标车辆图像与待匹配车辆图像的颜色特征相似度,λ为特征融合的权重。</td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈亮;              夏显茁;              刘阳;                   郑子彬       </td>   <td>中山大学</td>   <td>一种基于因子分解机的鲁棒训练方法及系统</td>   <td>广东省</td>   <td>CN112308132A</td>   <td>2021-02-02</td>   <td>本发明提供了一种基于因子分解机的鲁棒训练方法及系统,其中方法包括：获取用户操作平台中用户和目标对象之间的交互数据,并得到用户操作的原始样本和因子分解机的模型参数；根据所述原始样本、模型参数及预设的扰动范围,计算第二分类结果与第一分类结果的差值的最小值,根据所述最小值和第一分类结果得到因子分解机输出的扰动样本的下界；其中,扰动样本为原始样本中添加了扰动向量后的样本；将扰动样本的下界作为增强样本继续训练因子分解机直至得到训练好的因子分解机。本发明提供的基于因子分解机的鲁棒训练方法及系统,能定量地描述因子分解机抵御离散扰动的能力,在对分类准确率稍有影响的情况下,能提升模型抵御样本离散扰动的能力。</td>   <td>1.一种基于因子分解机的鲁棒训练方法,其特征在于,包括：获取用户操作平台中用户和目标对象之间的交互数据,根据所述交互数据得到用户操作的原始样本和因子分解机的模型参数；根据所述原始样本、模型参数及预设的扰动范围,计算第二分类结果与第一分类结果的差值的最小值,根据所述最小值和第一分类结果得到因子分解机输出的扰动样本的下界；其中,所述扰动样本为原始样本中添加了扰动向量后的样本,所述第一分类结果为原始样本经因子分解机输出的分类结果,所述第二分类结果为扰动样本经因子分解机输出的分类结果,所述扰动范围为原始样本中允许添加的扰动向量的数量；将所述扰动样本的下界作为增强样本继续训练因子分解机直至得到训练好的因子分解机。</td>   <td>G06K9/62;G06N20/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张冬傲;              郑培嘉;                   程梓岩       </td>   <td>中山大学</td>   <td>一种加密JPEG图像的可逆信息隐藏方法</td>   <td>广东省</td>   <td>CN112308756A</td>   <td>2021-02-02</td>   <td>本发明提出一种加密JPEG图像的可逆信息隐藏方法,涉及多媒体信息安全的技术领域,解决了当前对加密JPEG图像可逆信息隐藏的研究未兼顾隐私安全和信息嵌入前后文件大小维持度的问题,首先根据JPEG图像的ZRV值对,分离出待嵌入额外信息的块,从块中再随机挑选出m×n个块,并将挑选出的m×n个块进行置乱,更改了原JPEG的结构,然后进行格式兼容加密,充分利用其头部的APPn字段文件数据部分,降低了边信息泄露的风险,而且在额外信息嵌入后,进行信息提取和图像恢复操作,一方面保证了图片拥有者的信息安全,另一方面控制了文件大小的增长,使文件大小不受嵌入的数据大小的影响。</td>   <td>1.一种加密JPEG图像的可逆信息隐藏方法,其特征在于,包括：S1.图像拥有者根据JPEG图像的ZRV值对(ZR,VLI),分离出待嵌入额外信息的块BM和块BO,从块BM和块BO中分别随机挑选出m×n个块,并将挑选出的m×n个块进行置乱；S2.将m×n个块中每一个块ZRV值对的VLI部分加密,作为JPEG图像数据部分；S3.将除m×n个块之外剩余块的二进制流Bstream加密,并将加密后的二进制流Bstream及最后一位块BM的位置LM加入JPEG图像头部的APPn字段,与步骤S2所得的JPEG数据部分组成加密JPEG图像；S4.信息嵌入者在加密JPEG图像的JPEG数据部分进行额外信息的嵌入,得到嵌入数据的加密JPEG图像；S5.信息提取者获得步骤S4所述嵌入数据的加密JPEG图像,进行信息提取后将加密JPEG图像还原至额外信息嵌入前的状态；S6.将还原至额外信息嵌入前状态的加密JPEG图像进行解密,恢复JPEG图像。</td>   <td>G06T1/00;G06F21/60</td>  </tr>        <tr>   <td>中国专利</td>   <td>         尹阁麟;              张青;              郑伟诗;                   骆伟祺       </td>   <td>中山大学</td>   <td>一种基于多尺度生成对抗网络的伪装图像生成方法</td>   <td>广东省</td>   <td>CN112288622A</td>   <td>2021-01-29</td>   <td>本发明公开了一种基于多尺度生成对抗网络的伪装图像生成方法,包括以下步骤,构建多尺度生成对抗网络,包括多个尺度,每个尺度包括生成器、风格转换网络以及判别器；传入模型初始图像,进行预处理；生成器生成虚假图像,并与缩放到同等大小的真实图像一起输入到风格转换网络和判别器,进行判别训练；将当前尺度生成图像的放大结果经图像修改后输入到上一层尺度；重复执行判别训练、生成图像输入到上一层尺度步骤的操作,直至最顶端尺度输出最终的伪装图像。本发明通过构建多尺度对抗生成网络对单张图像进行训练,引入了风格转换网络对图像的风格进行定向判别与生成,实现利用少量数据进行伪装图像的快速生成与较好的伪装效果。</td>   <td>1.一种基于多尺度生成对抗网络的伪装图像生成方法,其特征在于,包括以下步骤：构建多尺度生成对抗网络模型,并嵌入风格转换网络,所述多尺度生成对抗网络模型包括多个尺度,每个尺度包括生成器、风格转换网络以及判别器；利用尺度生成对抗网络模型进行训练和应用,具体为：多尺度生成对抗网络模型的训练阶段,包括下述步骤：将随机噪声图作为最小尺度生成器的输入；判别训练,生成器生成虚假图像,并与缩放到同等大小的真实背景图像一起输入到风格转换网络和判别器,进行判别训练；输入到上一层尺度,将当前尺度生成器的生成图像的放大结果经图像输入到上一层尺度；重复执行判别训练、输入到上一层尺度步骤的操作,直至最顶端尺度执行完相应操作,完成多尺度生成对抗网络模型对真实背景图像的拟合与训练；多尺度生成对抗网络模型的应用阶段：选择除了最小尺度以外的任一尺度作为初始尺度；传入模型初始图像,将粘贴了待隐藏目标的背景图和原背景图同时放缩到初始尺度大小,并进行混合叠加的预处理,作为该尺度输入；当前尺度图像生成,生成器生成结果图像,与缩放到同等大小的粘贴了待隐藏目标的背景图进行混合操作；输入到上一层尺度,将当前尺度生成器的生成图像的放大结果输入到上一层尺度；重复当前尺度图像生成、输入到上一层尺度步骤的操作,直至最顶端尺度执行完相应操作,输出最终的伪装图像。</td>   <td>G06T3/00;G06T11/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈德霖;              张冬傲;                   郑培嘉       </td>   <td>中山大学</td>   <td>一种基于数据打包技术的加密域图像分割优化方法</td>   <td>广东省</td>   <td>CN112288757A</td>   <td>2021-01-29</td>   <td>本发明提出一种基于数据打包技术的加密域图像分割优化方法,涉及加密域图像分割优化的技术领域,解决了当前图像分割方法在分割过程中存在空间资源占用大,计算复杂度高的问题,首先隐私服务提供商生成公钥和私钥发送至客户端,并将私钥发送给图像分割执行服务器,客户端对图像进行加密并进行数据打包,基于数据打包技术实现了计算复杂度及空间资源占用度的降低,提高图像分割的速度,图像分割执行服务器与隐私服务提供商之间通过多方安全计算进行交互,获取加密分割图像,图像分割执行服务器将加密分割图像发送给客户端解密,加密的图像仅能由图像拥有者进行解密,保障了安全隐私。</td>   <td>1.一种基于数据打包技术的加密域图像分割优化方法,其特征在于,包括：S1.隐私服务提供商生成公钥pk和私钥sk,将公钥pk和私钥sk均发送至客户端,并将私钥sk发送给图像分割执行服务器；S2.客户端利用公钥pk对图像进行加密并进行数据打包,压缩加密图像尺寸,并将加密打包后的图像发送至图像分割执行服务器；S3.图像分割执行服务器与隐私服务提供商之间通过多方安全计算及乱码电路技术进行交互,获取加密分割图像；S4.图像分割执行服务器将加密分割图像发送给客户端,客户端解密后得到最终边缘图像。</td>   <td>G06T7/12;G06T7/136;G06T1/00;G06F21/60</td>  </tr>        <tr>   <td>中国专利</td>   <td>         金枝;              肖洁;                   庞雨贤       </td>   <td>中山大学</td>   <td>一种针对图像复原卷积神经网络的压缩方法及装置</td>   <td>广东省</td>   <td>CN112288829A</td>   <td>2021-01-29</td>   <td>本发明公开了一种针对图像复原卷积神经网络的压缩方法及装置,该方法包括：获取待压缩的图像复原卷积神经网络并以非正方形卷积核的侧窗核代替正方形卷积核,得到侧窗核替换后的卷积神经网络；将侧窗核替换后的卷积神经网络的卷积层分解为通道方向空间卷积和线性投影卷积,得到卷积层分解后的卷积神经网络；基于对称扩张卷积和注意力机制对卷积层分解后的卷积神经网络进行性能补偿,得到压缩后的图像复原卷积神经网络。该装置包括存储器以及用于执行上述针对图像复原卷积神经网络的压缩方法的处理器,本发明作为一种针对图像复原卷积神经网络的压缩方法及装置,可广泛应用于网络压缩领域。</td>   <td>1.一种针对图像复原卷积神经网络的压缩方法,其特征在于,包括以下步骤：获取待压缩的图像复原卷积神经网络并以非正方形卷积核的侧窗核代替正方形卷积核,得到侧窗核替换后的卷积神经网络；将侧窗核替换后的卷积神经网络的卷积层分解为通道方向空间卷积和线性投影卷积,得到卷积层分解后的卷积神经网络；基于对称扩张卷积和注意力机制对卷积层分解后的卷积神经网络进行性能补偿,得到压缩后的图像复原卷积神经网络。</td>   <td>G06T9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈武辉;              徐慧颖;                   郑子彬       </td>   <td>中山大学</td>   <td>一种基于强化学习的边缘计算服务激励方法</td>   <td>广东省</td>   <td>CN112288478A</td>   <td>2021-01-29</td>   <td>本发明提供了一种基于强化学习的边缘计算服务激励方法,包括：确定边缘计算服务的提供商和物联网设备的效用函数；建立一个具有两阶段、多领导者多跟随者的博弈问题模型,确定提供商和物联网设备的子博弈问题；构建多智能体马尔可夫决策过程模型,根据基于强化学习的最优定价算法确定每个提供商的最优定价；其中,在最优定价算法中每个物联网设备计算计算服务需求并发送给提供商；经最大次数的博弈后,得到提供商的最优定价和物联网设备的最优计算服务需求。本发明根据多个提供商同时为多个物联网设备提供边缘计算服务的实际情况,综合考虑了隐私保护和多个提供商之间的竞争,为多个提供商和多个物联网设备提供了一个不泄露隐私的激励机制。</td>   <td>1.一种基于强化学习的边缘计算服务激励方法,其特征在于,包括：确定边缘计算服务的提供商和物联网设备的效用函数；建立一个具有两阶段、多领导者多跟随者的博弈问题模型,确定所述提供商和物联网设备的子博弈问题；其中,所述领导者为提供商,所述跟随者为物联网设备；构建多智能体马尔可夫决策过程模型,根据基于强化学习的最优定价算法确定每个提供商的最优定价；其中,在所述最优定价算法中每个物联网设备计算各自的最优计算服务需求并发送给提供商；经最大次数的博弈后,确定提供商的最优定价和物联网设备的最优计算服务需求,得到提供商和物联网设备的效用函数的最大值。</td>   <td>G06Q30/02;G16Y10/45</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄俊艺;                   任传贤       </td>   <td>中山大学</td>   <td>一种基于正样本平衡约束的深度行人再标识方法</td>   <td>广东省</td>   <td>CN107330355B</td>   <td>2021-01-26</td>   <td>本发明提供一种基于正样本平衡约束的深度行人再标识方法,该方法使用的残差网网络结构简洁并且得到广泛应用,足够深的网络结构增强了特征表达能力,并且不需要对网络结构进行特别设计；发现用残差网分类器进行图像特征提取,行人再标识的准确率便可以高于大部分的精心设计的方法；相比于二元组损失和三元组损失的方法,提升结构损失不需要特意生成有效的样本便可以达到类似的效果,并且利用整体的分布信息,学习到的梯度方向更加稳健有效；在提升结构损失的基础上,增加了正样本平衡约束,不仅可以控制正样本对的距离,并且可以平衡正样本对距离和负样本对距离的梯度,使得算法更容易训练以及提升算法性能。</td>   <td>1.一种基于正样本平衡约束的深度行人再标识方法,其特征在于,包括以下步骤：S1：输入数据训练数据集其中,N是样本数量,d是图像像素,c是训练集中不同行人的数量,x_i是d维的列向量,y_i＝[y_(i1),y_(i2),y_(i3),...,y_(ic)]~T是c维的列向量,其中的元素等于1或0,并且X＝[x_1,x_2,x_3,...,x_N],X是d行N列的矩阵；S2：使用softmax分类模型对网络进行预训练；S3：使用基于正样本平衡约束的提升结构损失对网络进行训练；S4：对测试样本图像进行特征提取；S5：利用得到的特征对测试样本进行最近邻KNN分类进而得到再标识结果；所述步骤S2的具体过程是：设置网络的训练学习率η和epoch最大次数T,并且使用softmax分类模型对网络参数W进行预训练,训练的方法为反向传播算法,具体的步骤如下所示：首先初始化网络参数W；若当前epoch次数少于T,则生成迷你批数据然后把,输入到网络进行向前传播计算,计算得到该次迭代的损失函数值然后根据进行向后传播计算,计算梯度最后进行网络参数更新网络参数根据该规则进行不断更新,直到epoch次数等于T；所述步骤S3的具体过程是：把深度网络的损失函数从交叉熵换为提升结构损失,对于训练数据集为定义代表训练样本中正样本对的集合,而则代表负样本对的集合,正样本对距离为D_(i,j),而负样本对距离为D_(i,k)和D_(i,l)是控制负样本对距离的常数参数；把深度网络的损失函数从交叉熵换为提升结构损失,其具体公式如下：                  其中：D_(i,j)＝||Ψ(x_i)-Ψ(x_j)||_2其中增加两个常数参数β和λ,前者控制正样本对距离,后者平衡梯度：                                    再设置网络的训练学习率η和epoch最大次数T,以及三个常数参数α、β、λ,使用反向传播算法对深度网络进行训练,最终得到优化好的网络参数W。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>              齐志新       </td>   <td>中山大学</td>   <td>一种基于多源光学遥感图像的违法建设用地开发自动检测方法</td>   <td>广东省</td>   <td>CN112270291A</td>   <td>2021-01-26</td>   <td>本发明公开了一种基于多源光学遥感图像的违法建设用地开发自动检测方法,包括：分别获取待检测区域两个不同时间的光学遥感影像；计算两个不同时间的光学遥感影像间的土地平整强度,得到土地平整强度图像；利用最大期望算法对土地平整强度图像进行聚类分析,提取待检测区域的建设用地开发对应区域；将提取的建设用地开发对应区域与待检测区域的土地规划进行对比,土地规划严禁开发区域内的建设用地开发即为违法建设用地开发。本发明能够对违法建设用地开发进行短周期、全自动监测,计算简单、易于理解,执行效率高,且无需任何训练样本,能够及时发现并预防违法建设用地导致的各种社会及环境问题,并且显著降低监测成本、提高监测效率。</td>   <td>1.一种基于多源光学遥感图像的违法建设用地开发自动检测方法,其特征在于,包括以下步骤：S1：分别获取待检测区域两个不同时间的光学遥感影像；S2：计算两个不同时间的光学遥感影像间的土地平整强度,得到土地平整强度图像；S3：利用最大期望(Expectation Maximum,EM)算法对土地平整强度图像进行聚类分析,提取待检测区域的建设用地开发对应区域；S4：将提取的建设用地开发对应区域与待检测区域的土地规划进行对比,土地规划范围外的建设用地开发区域即为违法建设用地开发区域。</td>   <td>G06K9/00;G06K9/32;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         卢宇彤;              彭一;                   陈志广       </td>   <td>中山大学</td>   <td>一种基于并行剪枝优化的决策树生成方法及装置</td>   <td>广东省</td>   <td>CN112270352A</td>   <td>2021-01-26</td>   <td>本发明公开了一种基于并行剪枝优化的决策树生成方法及装置,该方法包括：获取训练集并根据训练集和C4.5算法生成决策树,得到待剪枝决策树；基于最小错误剪枝算法对待剪枝决策树进行初步剪枝,得到预处理决策树；根据训练集和并行的k-折交叉验证方法选择最优置信度；根据最优置信度再评估预处理决策树的结点,并根据评估结果对预处理决策树再剪枝,得到剪枝完成的决策树。该装置包括存储器以及用于执行上述基于并行剪枝优化的决策树生成方法的处理器。通过使用本发明,能够克服当前最小错误剪枝算法剪枝不足的缺点。本发明作为一种基于并行剪枝优化的决策树生成方法及装置,可广泛应用于数据挖掘领域中的决策树算法领域。</td>   <td>1.一种基于并行剪枝优化的决策树生成方法,其特征在于,包括以下步骤：获取训练集并根据训练集和C4.5算法生成决策树,得到待剪枝决策树；基于最小错误剪枝算法对待剪枝决策树进行初步剪枝,得到预处理决策树；根据训练集和并行的k-折交叉验证方法选择最优置信度；根据最优置信度再评估预处理决策树的结点,并根据评估结果对预处理决策树再剪枝,得到剪枝完成的决策树。</td>   <td>G06K9/62;G06N3/04;G06N3/08;G06N20/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苏卓;              王冠;                   周凡       </td>   <td>中山大学</td>   <td>一种基于人体身形特征的服装推荐方法</td>   <td>广东省</td>   <td>CN112270354A</td>   <td>2021-01-26</td>   <td>本发明公开了一种基于人体身形特征的服装推荐方法。定义身形特征类型标签,对服装数据集和人体数据集进行标签化处理；利用标签化服装数据集对服装身形特征提取模型进行训练,输出服装身形特征；利用标签化人体数据集对人体身形特征提取模型进行训练,输出人体身形特征；并利用服装身形特征和人体身形特征对服装人体身形匹配网络模型进行训练,输出服装与人体身形匹配概率；最后按照匹配概率降序排列推荐给用户。本发明可以提取到更为丰富的用户身形和服装的隐语义信息,而不是单纯的将用户或者服装进行分类,将匹配规则简化为在隐语义空间内的“距离计算”,从而实现了为用户推荐距离最近的服装商品,提高了推荐的准确度。</td>   <td>1.一种基于人体身形特征的服装推荐方法,其特征在于,所述方法包括：预先定义身形特征类型标签,之后搜集服装数据集和人体数据集,并利用身形特征类型标签对两个数据集进行标签化处理,得到标签化服装数据集和标签化人体数据集；采用深度神经网络建立服装身形特征提取模型,并利用所述标签化服装数据集对该模型进行训练,训练好的模型输出结果为服装身形特征；采用深度神经网络建立人体身形特征提取模型,并利用所述标签化人体数据集对该模型进行训练,训练好的模型输出结果为人体身形特征；采用多层感知神经网络建立服装人体身形匹配网络模型,并利用所述服装身形特征和所述人体身形特征进行训练,训练好的模型输出结果为服装与人体身形匹配概率；用户输入自己的个人图片和候选服装图片,利用所述服装人体身形匹配网络模型计算出候选服装与该用户身形的匹配概率,并设置一个匹配阈值,当匹配概率大于匹配阈值时,符合条件的候选服装按照匹配概率降序排列推荐给用户。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         阮菊俊;              姚梓淳;              秦保家;                   林觅       </td>   <td>中山大学</td>   <td>一种基于计算机视觉的废玻璃人工智能分选方法</td>   <td>广东省</td>   <td>CN112270378A</td>   <td>2021-01-26</td>   <td>本发明属于固体废弃物资源化技术领域,具体涉及一种基于计算机视觉的废玻璃人工智能分选方法。该方法利用计算机视觉人工智能的方式对废玻璃图像信息进行采集分析,根据废玻璃的形状、尺寸,高效地对废玻璃进行识别分类,节省了大量的劳动力,分选得到的废玻璃具有更好的针对性和适用性,显著提高了废玻璃资源化的效率。</td>   <td>1.一种基于计算机视觉的废玻璃人工智能分选方法,其特征在于,将废玻璃分散后,进行图像信息采集,利用计算机视觉智能分析系统模拟人工识别分类,得到形状、尺寸符合要求的废玻璃；其中,所述智能分析系统利用计算机卷积神经网络进行识别分类。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         齐志新;                   张慧       </td>   <td>中山大学</td>   <td>一种基于极化雷达遥感影像的城市内涝区域检测方法</td>   <td>广东省</td>   <td>CN112270675A</td>   <td>2021-01-26</td>   <td>本发明提供一种基于极化雷达遥感影像的城市内涝区域检测方法,包括以下步骤：S1：获取极化雷达遥感影像,并对极化雷达遥感影像进行预处理,提取其后向散射系数和干涉相干系数；S2：得到有意义的地物斑块；S3：提取城市范围；S4：构建城市洪水指数,根据城市洪水指数检测出城市内涝区域。本发明提供一种基于极化雷达遥感影像的城市内涝区域检测方法,根据城市受淹前和城市受淹中的后向散射系数和干涉相干系数的变化特征,构建城市洪水指数并通过城市洪水指数检测出城市内涝区域,实现对城市内涝区域的准确、及时提取,解决了由于洪水的发生常常伴随着多云多雨的天气,光学遥感无法获取有效的数据,导致难以及时获取洪水淹没信息的问题。</td>   <td>1.一种基于极化雷达遥感影像的城市内涝区域检测方法,其特征在于,包括以下步骤：S1：获取城市受淹前和城市受淹中的极化雷达遥感影像,并对极化雷达遥感影像进行预处理,提取其后向散射系数和干涉相干系数；S2：通过对后向散射系数和干涉相干系数进行联合分割,得到有意义的地物斑块；S3：基于城市受淹前的极化雷达遥感影像的后向散射系数和干涉相干系数提取城市区域分布范围；S4：基于城市受淹前和城市受淹中的极化雷达遥感影像的后向散射系数和干涉相干系数的变化特征,构建城市洪水指数,根据城市洪水指数检测出城市内涝区域。</td>   <td>G06T7/00;G06T7/11;G06T5/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张宁;              龙乐思;                   李俊炀       </td>   <td>中山大学</td>   <td>基于虚拟现实技术的古籍阅读方法、系统及其构建方法</td>   <td>广东省</td>   <td>CN112270768A</td>   <td>2021-01-26</td>   <td>本发明公开了一种基于虚拟现实技术的古籍阅读方法、系统及其构建方法,涉及虚拟现实技术领域。其中,方法包括：获取查阅指令,查找对应指令下的古籍内容；展示所述古籍内容；其中,所述古籍内容包括古籍原稿和书稿内容；读取所述古籍内容里文字信息,展示与所述文字信息匹配的标注内容；其中,标注内容分为知识元标注和故事线标注。本发明提出的基于虚拟现实技术的古籍阅读方法能够实现多感官阅读,提升读者的阅读体验、调动读者的阅读兴趣、帮助读者更好的理解书籍中的内容。</td>   <td>1.一种基于虚拟现实技术的古籍阅读方法,其特征在于,包括：获取查阅指令,查找对应指令下的古籍内容；展示所述古籍内容；其中,所述古籍内容包括古籍原稿和书稿内容；读取所述古籍内容里文字信息,展示与所述文字信息匹配的标注内容；其中,所述标注内容包括知识元标注和故事线标注。</td>   <td>G06T19/00;G06T17/20;G06F3/01</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘德昭;              邓欢;              梁雁秋;              黑子清;              李响;              陈素芳;                   李晓芸       </td>   <td>中山大学附属第三医院</td>   <td>一种安全高效的智能麻醉科药物配送收费方法及系统</td>   <td>广东省</td>   <td>CN112258109A</td>   <td>2021-01-22</td>   <td>本发明公开了一种安全高效的智能麻醉科药物配送收费方法及系统。方法包括：输入第一数据信息并进行预验证；对第一数据信息进行查找验证；在预先设置的选择调查表内选择所需要的麻醉药物名称、数量及用法并将该信息打包成第二数据信息；按照预先设置的验证模型对第二数据信息进行验证并生成一药物使用风险概率,若在设定阈值范围内则验证通过并返回带有防伪标签的流水号；根据流水号得出驱动信号；根据驱动信号控制输送机构将药物自动送出,将扣费信息发送至医院收费系统中进行费用扣除,在扣费完成后反馈扣费成功信息至打印模块；在成功将药物自动送出的同时第一数据信息和第二数据信息同步到麻醉记录系统中。本发明有利于麻醉药物的管理和收费。</td>   <td>1.一种安全高效的智能麻醉科药物配送收费方法,其特征在于,该方法包括以下步骤：通过输入模块输入第一数据信息并对第一数据信息进行预验证,在预验证通过后将该第一数据信息发送至第一验证模块,所述第一数据信息至少包括麻醉科医生的工号、密码、病人姓名和住院号；第一验证模块在验证数据库中对第一数据信息进行查找验证,若验证通过则返回验证通过信息,若验证不通过则返回验证错误信息,并将验证错误信息以信息反馈表的形式存储在反馈数据库中；通过选择模块在预先设置的选择调查表内选择所需要的麻醉药物名称、数量及用法并将该信息打包成第二数据信息,并将第二数据信息发送至第二验证模块；第二验证模块按照预先设置的验证模型对第二数据信息进行验证,生成一药物使用风险概率,若该药物使用风险概率在设定阈值范围内则验证通过并返回带有防伪标签的流水号,否则验证不通过提示存在错误；将带有防伪标签的流水号输入至预先设置的驱动模型中进行比对以得出驱动信号；通过驱动信号控制输送机构将对应数量的药物自动送出,同时通过扣费模块将扣费信息发送至医院收费系统中进行费用扣除,在扣费完成后反馈扣费成功信息至打印模块,所述扣费信息包括药物扣费金额和病人住院号；在输送机构成功将药物自动送出的同时第一数据信息和第二数据信息同步到麻醉记录系统中。</td>   <td>G06Q10/08;G16H20/13</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑子彬;                   崔嘉辉       </td>   <td>中山大学</td>   <td>基于区块链的传感器网络共享方法</td>   <td>广东省</td>   <td>CN108053239B</td>   <td>2021-01-19</td>   <td>本发明涉及基于区块链的传感器网络共享方法,采用去中心化合约执行方法,自动执行智能合约,给予需求方相应的权限,避免了中心化的把控机构以及单点故障,而且执行过程中没有一个结点能伪造运算结果,或是控制运算进程,具有不可篡改的特性。需求方和提供方基于传感器网络共享的交易账本采用去中心化账本维护方法进行维护,可以免除提供方以及需求方对账本的维护成本以及对账导致的重复存储问题。采用信誉评价机制,为其他供求用户提供参考。提供方经过传感器网络抽象层发现需求方提出的需求内容,使需求方更容易得到其所需,提供方更容易理解其所提供的能否适合需求者所需。每个中间过程的有序触发均需要签名,保证多方均对交易进行了确认。</td>   <td>1.基于区块链的传感器网络共享方法,其特征在于：包括以下步骤：S1、撮合传感器网络的需求方和提供方；S2、部署智能合约；S3、共享传感器网络；S4、信誉评价；所述步骤S1撮合传感器网络的需求方和提供方的具体步骤如下：S11、需求方按照自己的需求利用Solidity编程语言编写智能合约；S12、需求方对合约利用SHA3算法进行哈希,并对哈希进行签名,然后把结果在网络中进行广播；S13、提供方发现需求方提出的需求,并发现自己拥有满足需求的传感器网络,因而编写自己的共享抽象文件；S14、提供方对共享抽象文件利用SHA3算法进行哈希,并对哈希进行签名,然后把结果在网络中进行广播；所述步骤S13中,提供方经过传感器网络抽象层发现需求方提出的需求内容；所述步骤S3共享传感器网络的具体步骤如下：S31、需求方安装智能合约的约定对合约发起触发交易；S32、采用去中心化合约执行方法,自动执行智能合约,给予需求方相应的权限；S33、需求方基于所得权限进行操作；所述需求方和提供方基于传感器网络共享的交易账本采用去中心化账本维护方法进行维护。</td>   <td>G06Q30/02;G06Q30/06;G06Q20/38;H04L29/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈梓阳;                   郑培嘉       </td>   <td>中山大学</td>   <td>一种云计算环境下的加密图像水印嵌入方法</td>   <td>广东省</td>   <td>CN112233007A</td>   <td>2021-01-15</td>   <td>本发明提供一种云计算环境下的加密图像水印嵌入方法,包括S1：在本地生成同态加密所需的公钥和私钥,并把私钥保存在本地,将公钥告知云服务器；S2：使用公钥对载体图像进行加密,然后将加密图像上传至云服务器；S3：云服务器使用公钥对水印序列进行同态加密；S4：云服务器对加密图像进行水印嵌入,得到嵌入水印后的密文图像。本发明实现的算法可以在不泄露隐私数据的情况下完成对加密图像的水印嵌入,且嵌入水印后的图像峰值信噪比结果与明文域相比基本一致,同时通过利用密文打包,使大部分运算能达到单指令多数据(SIMD)的并行效果,在密文槽大小为100的情况下能实现平均84的加速比,有效提升密文域内水印嵌入的效率。</td>   <td>1.一种云计算环境下的加密图像水印嵌入方法,其特征在于,包括如下步骤：S1：在本地生成同态加密所需的公钥和私钥,并把私钥保存在本地,将公钥告知云服务器；S2：使用公钥对载体图像进行加密,然后将加密图像上传至云服务器；S3：云服务器使用公钥对水印序列进行同态加密；S4：云服务器对加密图像进行水印嵌入,得到嵌入水印后的密文图像。</td>   <td>G06T1/00;G06F21/60</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈刚;              仲崇豪;                   孟海涛       </td>   <td>中山大学</td>   <td>一种激光雷达立体相机融合的深度估计方法、装置及其介质</td>   <td>广东省</td>   <td>CN112233163A</td>   <td>2021-01-15</td>   <td>本发明公开了一种激光雷达立体相机融合的深度估计方法、装置及其介质,该方法包括：获取立体相机的当前帧左图像和当前帧右图像；获取雷达左图像和雷达右图像；将当前帧左图像与雷达左图像融合得到第一左图像；将当前帧右图像与雷达右图像融合得到第一右图像；将第一左图像输入二值神经网络中进行特征提取,并聚合得到第一特征左图像；将第一右图像输入二值神经网络中进行特征提取,并聚合得到第一特征右图像；获取第一特征左图像和第一特征右图像之间的初始匹配代价；基于交叉的雷达信任聚合和半全局立体匹配算法优化所述初始匹配代价并提取视差图；根据所述视差图进行深度估计。本发明能够获得准确可靠的深度预测,广泛应用于图像处理技术领域。</td>   <td>1.一种激光雷达立体相机融合的深度估计方法,其特征在于,包括：获取立体相机的当前帧左图像和当前帧右图像；获取雷达左图像和雷达右图像,所述雷达左图像与所述当前帧左图像对应同一物体同一部位的图像,所述雷达右图像与所述当前帧右图像对应同一物体同一部位的图像；将所述当前帧左图像与所述雷达左图像融合得到第一左图像；将所述当前帧右图像与所述雷达右图像融合得到第一右图像；将所述第一左图像输入二值神经网络中进行特征提取,并聚合得到第一特征左图像；将所述第一右图像输入二值神经网络中进行特征提取,并聚合得到第一特征右图像；获取所述第一特征左图像和所述第一特征右图像之间的初始匹配代价；基于交叉的雷达信任聚合和半全局立体匹配算法优化所述初始匹配代价并提取视差图；根据所述视差图进行深度估计。</td>   <td>G06T7/521;G06T7/593;G06T5/00;G06T5/50;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄旭;                   杨萌       </td>   <td>中山大学</td>   <td>基于SRTM约束的卫星影像密集匹配方法和系统</td>   <td>广东省</td>   <td>CN112233246A</td>   <td>2021-01-15</td>   <td>本发明公开了基于SRTM约束的卫星影像密集匹配方法和系统,方法包括：根据卫星传感器的成像参数,将卫星立体影像纠正为核线立体像对；根据所述核线立体像对构建金字塔的影像；将SRTM的地形产品投影到金字塔顶层立体影像的左影像中,生成初始视差图；根据初始视差图,在金字塔顶层立体影像中确定同名像点之间的灰度特征相似性；对核线立体影像的匹配代价进行积聚,确定金字塔顶层的密集匹配结果；将密集匹配结果作为新的初始视差图,并将初始视差图传递至下一级金字塔,直至确定金字塔底层的密集匹配结果,获取三维点云。本发明能够提高弱纹理区域密集匹配的可靠性和精度,可广泛应用于遥感技术领域。</td>   <td>1.基于SRTM约束的卫星影像密集匹配方法,其特征在于,包括：根据卫星传感器的成像参数,将卫星立体影像纠正为核线立体像对；根据所述核线立体像对构建金字塔的影像,所述金字塔的顶层分辨率与SRTM分辨率保持一致；将所述SRTM的地形产品投影到金字塔顶层立体影像的左影像中,生成初始视差图；根据所述初始视差图约束密集匹配过程,在所述金字塔顶层立体影像中确定同名像点之间的灰度特征相似性；对所述核线立体影像的匹配代价进行积聚,确定所述金字塔顶层的密集匹配结果；将所述密集匹配结果作为新的初始视差图,并将所述初始视差图传递至下一级金字塔,重复上述步骤,直至确定金字塔底层的密集匹配结果,获取三维点云。</td>   <td>G06T17/20;G06T15/04;G01S13/89;G01C7/02</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘洋旗;              王国利;                   郭雪梅       </td>   <td>中山大学</td>   <td>一种结合深度学习和对抗特征解耦的跨域动作识别方法</td>   <td>广东省</td>   <td>CN112232268A</td>   <td>2021-01-15</td>   <td>本发明公开了一种结合深度学习和对抗特征解耦的跨域动作识别方法,该方法包括：根据原始调频连续波信号得到多域二维热图数据集；基于多域二维热图数据集训练得到特征提取模块；基于特征提取模块提取多域特征；基于多域特征训练得到训练后的特征解耦模块和动作分类器；完成域相关特征和域无关特征的解耦并为域无关特征添加权重,得到带权重的域无关特征；微调特征提取模块,得到跨域动作识别模型。通过使用本发明,能够串行训练网络中的各个模块、无需调整各损失值权重,且能够训练得到可跨域识别的即拿即用的网络模型。本发明作为一种结合深度学习和对抗特征解耦的跨域动作识别方法,可广泛应用于动作识别领域。</td>   <td>1.一种结合深度学习和对抗特征解耦的跨域动作识别方法,其特征在于,包括训练步骤：获取原始调频连续波信号并对该信号进行处理,得到多域二维热图数据集；基于多域二维热图数据集训练深度神经网络,得到特征提取模块；基于特征提取模块提取多域特征；基于多域特征训练特征解耦模块和动作分类器,得到训练后的特征解耦模块和动作分类器；根据训练后的特征解耦模块完成域相关特征和域无关特征的解耦并为域无关特征添加权重,得到带权重的域无关特征；以带权重的域无关特征作为标签微调特征提取模块,结合训练后的动作分类器得到跨域动作识别模型。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08;G06F17/14</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周永章;                   吉俊杰       </td>   <td>中山大学</td>   <td>一种基于集成策略的地质灾害易发性评价方法及系统</td>   <td>广东省</td>   <td>CN112232526A</td>   <td>2021-01-15</td>   <td>本发明公开了一种基于集成策略的地质灾害易发性评价方法及系统,该方法包括：获取地质灾害隐患点位置并绘制地质灾害位置图；根据地质灾害位置图构建Tin模型并转化为数字高程模型,得到栅格和对应的栅格数据；提取栅格数据中的相关特征并对栅格赋予ID号；根据预训练的集成模型对提取出的相关特征进行计算预测,得到预测数据；将预测数据与对应的栅格ID号连接,得到对应区域的灾害易发概率。该系统包括：图模块、模型模块、栅格模块、预测模块和匹配结果模块,通过使用本发明,可以提高区域灾害预测的准确率。本发明作为一种基于集成策略的地质灾害易发性评价方法及系统,可广泛应用于灾害预测领域。</td>   <td>1.一种基于集成策略的地质灾害易发性评价方法,其特征在于,包括以下步骤：获取地质灾害隐患点位置并绘制地质灾害位置图；根据地质灾害位置图构建Tin模型并转化为数字高程模型,得到栅格和对应的栅格数据；提取栅格数据中的相关特征并对栅格赋予ID号；根据预训练的集成模型对提取出的相关特征进行计算预测,得到预测数据；将预测数据与对应的栅格ID号连接,得到对应区域的灾害易发概率。</td>   <td>G06N20/10;G06K9/62;G06Q10/04;G06Q10/06;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         田海博;              林会智;                   罗玉琴       </td>   <td>中山大学</td>   <td>一种结合北斗系统的农产品区块链溯源方法及系统</td>   <td>广东省</td>   <td>CN112232840A</td>   <td>2021-01-15</td>   <td>本发明属于结合北斗卫星系统的区块链应用技术领域,更具体地,涉及一种结合北斗系统的农产品区块链溯源方法及系统。本发明采用结合北斗系统的区块链技术,提供了特色农产品的溯源方法。通过北斗系统得到每一件商品的溯源信息,并在区块链上赋予每一件商品唯一标识,商品流通过程在区块链上全程公开可见不可篡改。商品流通过程中的任一方都可以通过二维码获取商品在区块链上的唯一标识,进而获得商品溯源信息,有效防伪验真。</td>   <td>1.一种结合北斗系统的农产品区块链溯源方法,其特征在于,包括以下步骤：S1.出品方创建并提交商品溯源信息：在任意时刻,商品的持有人利用支持北斗系统的手机创建商品的溯源信息,形成区块链交易,并提交到区块链节点；S2.信息处理与上链：区块链节点收到出品方提交的区块链交易后,对其有效性进行验证,当验证通过后,将其打包提交到区块链上；S3.中转方验证商品溯源信息：当中转方收到来自出品方的商品后,通过手机查看区块链中商品目前的溯源信息,并确认该商品的溯源信息,对其进行评价；S4.中转方更新商品溯源信息：如果商品保真,中转方拍摄商品的最新照片、视频信息,并根据北斗系统的时间、位置信息,增加中转方的时间、位置以及当前商品交易的唯一标识、辅助信息,形成新的区块链交易,提交到区块链节点,由区块链节点根据步骤S2完成上链；S5.消费者验证商品溯源信息：消费者收到商品后,通过手机获取当前商品的图片、视频信息来验证当前商品的真伪,并向区块链系统提交评价。</td>   <td>G06Q30/00;G06Q40/04;G06K17/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         聂志成;              杜云飞;              郭贵鑫;              李江;              卢宇彤;              钟康游;              杜量;              曹鹏;                   赵帅帅       </td>   <td>中山大学</td>   <td>一种高性能分布式容器镜像分发系统及方法</td>   <td>广东省</td>   <td>CN112231052A</td>   <td>2021-01-15</td>   <td>本发明公开了一种高性能分布式容器镜像分发系统及方法,该系统包括：主仓库模块、调度模块和节点处理模块。该方法包括：主仓库模块转发镜像拉取请求；调度模块分析得到待拉取文件清单并发送数据至节点处理模块；节点处理模块获取惊险分层文件并通过IB网络传输文件。通过使用本发明,减少容器镜像服务器的服务压力。本发明作为一种高性能分布式容器镜像分发系统及方法,可广泛应用于容器云计算领域。</td>   <td>1.一种高性能分布式容器镜像分发系统,其特征在于,包括：主仓库模块,用于接收容器镜像拉取请求并将该请求转发至调度模块,还作为镜像仓库存储容器镜像；调度模块,包括分布式键值数据库,用于存储和更新系统的当前状态,根据镜像元数据找到镜像分层文件和空闲存储节点并发送到节点代理模块,所述系统的当前状态包括每个镜像的元数据、镜像分层文件的存储位置和每个节点当前的服务状态；节点处理模块,用于根据调度模块发出的指令进行文件拉取并存储,还为其他节点提供镜像文件服务。</td>   <td>G06F9/455;G06F11/14</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郭雪梅;              张玮嘉;                   谢泳伦       </td>   <td>中山大学</td>   <td>基于条件变分自编码器的人体活动识别系统及方法</td>   <td>广东省</td>   <td>CN110020623B</td>   <td>2021-01-15</td>   <td>本发明公开了一种基于条件变分自编码器的人体活动识别系统及方法,该方法包括获取原始时间序列：通过传感器获取采样样本,多个采样样本构成原始时间序列；构建批数据：通过随机序列起始点的数据增强方式构建批数据,得到构造好的传感器批数据X和对应的活动标签批数据Y；训练条件变分自编码器模型：批数据输入到模型中,通过损失函数和反向传播算法训练模型；预测人体活动：将传感器批数据X作为测试数据,输入到训练好的变分自编码器模型中,批数据输入变分自编码器模型得到最终的预测活动标签。本发明以一个采样样本为单位预测其对应的活动标签,具有实时活动识别的能力,能够对同类样本的相关性进行建模,从而提升识别准确率。</td>   <td>1.一种基于条件变分自编码器的人体活动识别方法,其特征在于,包括下述步骤：S1：获取原始时间序列：通过传感器获取采样样本,多个采样样本构成原始时间序列；S2：构建批数据：通过随机序列起始点的数据增强方式构建批数据,包括传感器批数据X、对应的活动标签批数据Y；S3：训练条件变分自编码器模型：构造好的传感器批数据X和对应的活动标签批数据Y输入到条件变分自编码器模型当中,条件变分自编码器采用神经网络的反向传播框架进行训练,通过设定的损失函数和Adam优化算法训练模型,所述损失函数基于交叉熵函数计算得到；所述训练条件变分自编码器模型的具体步骤如下所述：条件变分自编码器模型包括先验网络,识别网络和生成网络；先验网络包括三个先验网络全连接层,第一个先验网络全连接层将输入的传感器批数据X映射成隐变量H_(prior),另外两个先验网络全连接层将H_(prior)作为输入,分别输出先验网络对应的高斯隐变量的均值和方差向量,经过重新参数化得到先验网络高斯隐变量Z_(prior)；识别网络包括三个识别网络全连接层,第一个识别网络全连接层将传感器批数据X和对应的活动标签批数据Y作为输入,对应的活动标签批数据Y先进行One-hot编码,再和传感器批数据X进行张量拼接和维度变换,输入到第一个识别网络全连接层中,输出隐变量H_(encode),另外两个识别网络全连接层将H_(encode)作为输入,分别输出识别网络的高斯隐变量对应的均值和方差向量,经过重新参数化得到识别网络高斯隐变量Z_(encode)；生成网络包括两个生成网络全连接层,生成网络的输入分别经过两个生成网络全连接层,得到的输出采用softmax函数进行计算,其中Z_(prior)和Z_(encode)分别输入到生成网络中,当Z_(prior)作为生成网络的输入时,生成网络输出活动标签的预测概率Y_(prior),当Z_(encode)作为生成网络的输入时,生成网络输出活动标签的重构概率Y_(encode)；S4：预测人体活动：步骤S2构造的传感器批数据X作为测试数据,输入到步骤S3中训练好的变分自编码器模型中,得到最终的预测活动标签。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>              周凡       </td>   <td>中山大学深圳研究院</td>   <td>一种新闻视频的内容关键词提取方法、终端设备及介质</td>   <td>广东省</td>   <td>CN111814770B</td>   <td>2021-01-15</td>   <td>本申请适用于视频处理技术领域,提供了一种新闻视频的内容关键词提取方法、终端设备及介质,通过对目标新闻视频进行内容提取操作得到新闻文本,并对新闻文本进行分词处理得到对应的词语集合；确定目标新闻视频的新闻标题,以及获取预设的命名实体集合；将新闻文本、词语集合、新闻标题以及命名实体集合输入至训练好的关键词提取模型中进行处理,得到词语集合对应的词语得分值矩阵；词语的总得分值是根据词语在新闻文本中出现的概率、词语与新闻标题的相关度、词语在新闻文本中的分布位置得分值以及词语与命名实体集合的匹配度确定得到的；将词语集合中满足预设条件的目标词语确定为新闻视频的内容关键词,从而提高了提取出的内容关键词的准确率。</td>   <td>1.一种新闻视频的内容关键词提取方法,其特征在于,包括：对目标新闻视频进行内容提取操作,得到用于描述所述目标新闻视频的新闻文本,并对所述新闻文本进行分词处理,得到所述新闻文本对应的词语集合；其中,所述对目标新闻视频进行内容提取操作,得到用于描述所述目标新闻视频的新闻文本,包括：对所述目标新闻视频的视频流进行分帧操作,并对所述分帧操作得到的多个视频帧图像均进行光学字符识别操作,得到每个所述视频帧图像对应的第一文本片段；基于所述多个视频帧图像,对所述目标新闻视频的音频流进行分段操作,得到每个所述视频帧图像对应的音频片段,并对每个所述视频帧图像对应的音频片段进行语音识别操作,得到每个所述视频帧图像对应的第二文本片段；根据每个所述视频帧图像对应的第一文本片段和第二文本片段,确定每个所述视频帧图像对应的目标文本片段,包括：得到每个视频帧图像对应的第一文本片段和第二文本片段后,将每个视频帧图像对应的第一文本片段和第二文本片段进行组合,得到每个视频帧图像对应的准文本片段,并对每个视频帧图像对应的准文本片段中重复的句子进行去重处理,得到每个视频帧图像对应的目标文本片段；其中,第一文本片段和第二文本片段均包括至少一个句子；按照视频帧图像对应的时间节点由早到晚的顺序,将所述多个视频帧图像对应的多个目标文本片段进行拼接,得到所述新闻文本；确定所述目标新闻视频的新闻标题,以及获取预设的命名实体集合,包括：若检测到至少有预设数目个视频帧图像对应的目标文本片段中均包括同一个句子,则终端设备从包括该同一个句子的任一目标文本片段中提取该同一个句子,并将该同一个句子确定为目标新闻视频的新闻标题；将所述新闻文本、所述词语集合、所述新闻标题以及所述命名实体集合输入至训练好的关键词提取模型中进行处理,得到所述词语集合对应的词语得分值矩阵；其中,所述词语得分值矩阵中每个元素的值用于表示所述元素在所述词语集合中对应的词语的总得分值；所述词语的总得分值是根据所述词语在所述新闻文本中出现的概率、所述词语与所述新闻标题的相关度、所述词语在所述新闻文本中的分布位置得分值以及所述词语与所述命名实体集合的匹配度确定得到的；根据所述词语得分值矩阵确定所述词语集合中满足预设条件的目标词语,并将所述目标词语确定为所述新闻视频的内容关键词。</td>   <td>G06K9/00;G06K9/32;G06F40/284;G06F40/295</td>  </tr>        <tr>   <td>中国专利</td>   <td>              周凡       </td>   <td>中山大学深圳研究院</td>   <td>一种学生学习情况识别方法、系统、教学终端及存储介质</td>   <td>广东省</td>   <td>CN111932418B</td>   <td>2021-01-15</td>   <td>本申请适用于教学管理技术领域,提供了一种学生学习情况识别方法、系统、教学终端及存储介质,该方法应用于教学终端,包括：获取学生通过学生终端上传的第一笔迹图片；识别第一笔迹图片中的第一文本,以及识别与第一文本对应的第一笔迹信息；提取第一文本和第一笔迹信息中的第一学习特征与第二学习特征；根据第一学习特征,使用预设的第一学习模型对学生的第一学习情况进行识别；根据教学终端中与第二学习特征属于相同类别的目标特征训练第二学习模型,并根据第二学习模型和第二学习特征对学生的第二学习情况进行识别。采用上述方法中的第一学习模型和第二学习模型对学生的学习情况进行全面分析,使分析结果更加符合学生的真实学习情况。</td>   <td>1.一种学生学习情况识别方法,应用于教学终端,其特征在于,所述方法包括：获取学生通过学生终端上传的第一笔迹图片,所述第一笔迹图片中包含第一文本；识别所述第一笔迹图片中的第一文本,以及识别与所述第一文本对应的第一笔迹信息；提取所述第一文本和所述第一笔迹信息中的学习特征,所述学习特征包括第一学习特征与第二学习特征；根据所述第一学习特征,使用预设的第一学习模型对所述学生的第一学习情况进行识别；统计所述教学终端中与所述第二学习特征属于相同类别的目标特征的特征数量；若所述特征数量大于预设阈值,则使用所述目标特征训练第二学习模型,并根据所述第二学习模型和所述第二学习特征对所述学生的第二学习情况进行识别。</td>   <td>G06Q50/20;G06K9/00;G06F40/279;G06F17/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑培嘉;              彭冬毡;                   骆伟祺       </td>   <td>中山大学</td>   <td>云环境下融合全同态加密和离散小波变换的人脸识别方法</td>   <td>广东省</td>   <td>CN112215158A</td>   <td>2021-01-12</td>   <td>本发明提出一种云环境下融合全同态加密和离散小波变换的人脸识别方法,解决了现有方法无法兼顾隐私安全性、识别率和运行效率的问题,通过对客户端输入的人脸图片及云服务器上人脸数据库的人脸图片均进行预处理,将预处理后的客户端输入的人脸图片进行向量切片后,转化为密文分片,利用packing加密方式加密后传送至云服务器,在全同态加密域下进行离散小波变换,预处理后的云服务器上人脸数据库的人脸图片一维向量在明文状态下进行离散小波变换,并通过初次降维,并得到特征向量；基于特征向量,进一步进行降维处理,最后计算欧式距离,确定人脸识别结果,兼顾隐私安全性、识别率和运行效率。</td>   <td>1.一种云环境下融合全同态加密和离散小波变换的人脸识别方法,其特征在于,至少包括：S1.对客户端输入的人脸图片及云服务器上人脸数据库的人脸图片均进行预处理；S2.将预处理后的客户端输入的人脸图片进行向量切片后,转化为密文分片,加密后传送至云服务器；S3.将加密后传送至云服务器的密文分片在全同态加密域下进行离散小波变换,得到离散小波变换后的系数；S4.将预处理后的云服务器上人脸数据库的人脸图片一维向量在明文状态下进行离散小波变换,并通过初次降维,并得到特征向量；S5.根据特征向量,对离散小波变换后的云服务器上人脸数据库的向量组进行降维；在全同态加密域下,对密文分片离散小波变换后的系数进行降维,得到降维后的客户端输入的人脸图片向量；S6.基于全同态加密的加性同态和乘法同态的性质,通过内积计算人脸向量与初次降维后云服务器上人脸数据库的人脸图片向量组中每一个向量的欧式距离；S7.采用基于整数密文的拉格朗日插值比较算法,进行欧式距离的两两比较,并排序；S8.判断排序后的欧式距离是否在阈值T内,若是,人脸识别成功,返回密文识别结果发送至客户端,客户端解密密文识别结果；否则,人脸识别失败。</td>   <td>G06K9/00;G06K9/62;H04L9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         彭冬毡;              郑培嘉;                   骆伟祺       </td>   <td>中山大学</td>   <td>一种同态加密下基于小波降维的人脸识别方法</td>   <td>广东省</td>   <td>CN112215165A</td>   <td>2021-01-12</td>   <td>本发明提出一种同态加密下基于小波降维的人脸识别方法,设计人脸识别的技术领域,解决了现有基于同态加密的人脸识别方法在人脸识别过程中存在占用空间大、人脸识别时间长及实时性差的问题,本发明通过对客户端输入的人脸图片及人脸数据库的人脸图片均进行预处理,然后对客户端传输的加密人脸图片进行同态加密域下的离散小波变换,对人脸数据库中的人脸图片在明文状态下进行小波变换,然后进行降维处理变换,降低了资源空间占用度以及人脸识别过程消耗的时间,另外基于同态加密的性质,加密的人脸图片仅能由图像拥有者进行解密,保证了云服务器和客户端双方对隐私性的要求,而且基于堆排序比较欧式距离可以减少比较次数,提高人脸识别速度。</td>   <td>1.一种同态加密下基于小波降维的人脸识别方法,其特征在于,至少包括：S1.对客户端输入的人脸图片及人脸数据库的人脸图片均进行预处理；S2.对客户端预处理后的人脸图片进行加密,并发送至云服务器；S3.云服务器对客户端传输的加密人脸图片Y进行同态加密域下的离散小波变换；S4.云服务器对人脸数据库中的图片X_i(i＝1,...,N)在明文状态进行离散小波变换,得到X_i′(i＝1,...,N),并通过降维变换,得到特征向量,N表示人脸数据库中人脸图片总个数；S5.通过特征向量对人脸数据库及客户端中的人脸图片向量组进行处理,得到最终降维后的客户端低维密文人脸图片向量组及人脸数据库中的低维明文人脸图片向量组；S6.根据同态加密的性质,云服务器计算客户端低维密文人脸图片向量组与人脸数据库中低维明文人脸图片向量组中每一个人脸图片向量之间的欧式距离；S7.基于堆排序比较密文状态下的欧式距离；S8.判断欧式距离是否在阈值ε内,若是,人脸识别成功,并返回密文结果发送至客户端进行解密,得出解密结果；否则,人脸识别失败。</td>   <td>G06K9/00;G06K9/62;H04L9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赵铜铁钢;                   陈浩玲       </td>   <td>中山大学</td>   <td>一种水文气象空间数据均值估计的块状自举方法</td>   <td>广东省</td>   <td>CN112215299A</td>   <td>2021-01-12</td>   <td>本发明提供一种水文气象空间数据均值估计的块状自举方法,包括以下步骤：S1：从观测区域获取N个水文气象空间数据点的信息,形成样本集合X＝{x<sub>1</sub>,x<sub>2</sub>,...,x<sub>N</sub>}；S2：在样本集合中对样本进行B次块状自举抽样,得到B组重抽样样本集合；S3：分别对B组重抽样样本集合进行均值估计,得到估计均值序列本发明提供一种水文气象空间数据均值估计的块状自举方法,通过块状自举抽样,保证一定范围内的水文气象空间数据点作为一整块进行抽取,避免了破坏数据本身的相依结构,解决了目前使用自举法处理水文气象变量这类空间相关、非独立同分布样本时会失效的问题。</td>   <td>1.一种水文气象空间数据均值估计的块状自举方法,其特征在于,包括以下步骤：S1：从观测区域获取N个水文气象空间数据点的信息,形成样本集合X＝{x_1,x_2,...,x_N}；其中,样本x_i为第i个水文气象空间数据点的信息,i＝1,2,…,N；S2：在样本集合中对样本进行B次块状自举抽样,得到B组重抽样样本集合；S3：分别对B组重抽样样本集合进行均值估计,得到估计均值序列其中,为第b组重抽样样本集合的估计均值,b＝1,2,…,B。</td>   <td>G06K9/62;G06N20/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈龙;              范蕾;              张朝强;                   黎丹       </td>   <td>中山大学</td>   <td>一种平面监督的图像色彩深度信息协同修复系统</td>   <td>广东省</td>   <td>CN107578389B</td>   <td>2021-01-08</td>   <td>本发明涉及图像的技术领域,更具体地,涉及一种平面监督的图像色彩深度信息协同修复系统。在具体实现过程中,该算法主要通过基于样本的图像修复算法对色彩图像修复的同时为深度修复提供线索,然后对初步修复完善的色彩图像使用超像素分割方法进行分割。同时将斜平面平滑方法运用到深度图修复过程中,使用平面拟合方法得到的平面方程估算视差值。以阻塞面或者铰面为分割边界的分割块以及平面拟合过程中得到的局外点将会在下一次迭代过程中重新计算。修复完善的深度图对色彩图像重新修复给予反馈。最后不断迭代以上步骤优化得到最佳的色彩图像及深度图像的修复结果,从而达到对色彩深度信息的协同修复。</td>   <td>1.一种平面监督的图像色彩深度信息协同修复系统,其特征在于,包括色彩图像修复模块、图像分割模块、深度图修复模块和色彩图像在深度图指导下重新修复模块四大部分；色彩图像修复模块：使用基于样本的图像修复方法从色彩图像中未受损区域得到的色彩信息填充到受损区域中,得到视觉上合理的修复结果；图像分割模块：对于色彩图像修复模块得到的色彩图像使用简单的线性迭代聚类算法进行超像素分割,得到分割区域；深度图修复模块：对于图像分割模块得到的每一个分割区域,使用随机抽样一致算法拟合出一平面方程；使用该平面方程估算出丢失的视差值；同时将分割区域之间的边界进行分类处理；以阻塞面或者铰面为分割边界的分割区域以及平面拟合过程中得到的局外点将会在下一次迭代过程中重新计算；色彩图像在深度图指导下重新修复模块：对于从深度图修复模块中反馈得到的重新需要修复区域,从未受损区域获取色彩信息所依据的相似性需要将平面间的差异性考虑在内。</td>   <td>G06T5/00;G06T7/11;G06T7/136</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;                   李启运       </td>   <td>中山大学</td>   <td>一种实时鲁棒的人脸检测方法</td>   <td>广东省</td>   <td>CN109446922B</td>   <td>2021-01-08</td>   <td>本发明涉及计算机视觉识别技术,具体为实时鲁棒的人脸检测方法,包括步骤：获取要进行人脸检测的目标图像并进行预处理；建立并训练检测模型,检测模型包括多个卷积模块、多个Inception模块、多个带残差的Inception模块及多个检测模块,Inception模块是具有两条支路的通道分离卷积模块,带残差的Inception模块是带残差连接的多支路通道分离卷积模块,检测模块用卷积运算最终输出位置信息和分类信息；将目标图像输入训练好的检测模型中,分别获取指定层级上的卷积结果；对获取的卷积结果进行分类和回归；根据回归和分类结果计算出人脸的位置。该方法构建了简单高效的卷积神经网络,减少了检测过程中冗余操作,在CPU上能达到实时效果。</td>   <td>1.一种实时鲁棒的人脸检测方法,其特征在于,包括以下步骤：S1、获取要进行人脸检测的目标图像并进行预处理；S2、建立并训练检测模型；检测模型包括多个卷积模块、多个Inception模块、多个带残差的Inception模块及多个检测模块,第一卷积模块、第一Inception模块、第二Inception模块、第三Inception模块、第一带残差的Inception模块、第二卷积模块、第二带残差的Inception模块、第三卷积模块、第三带残差的Inception模块依次连接,第一带残差的Inception模块、第二带残差的Inception模块及第三带残差的Inception模块分别与第一检测模块、第二检测模块及第三检测模块连接,最终输出位置信息和分类信息；S3、将目标图像输入训练好的检测模型中,分别获取指定层级上的卷积结果；S4、对获取的卷积结果进行分类和回归；S5、根据回归和分类结果计算出人脸的位置；Inception模块是具有两条支路的通道分离卷积模块；每个Inception模块包括两条并联的支路,其中第一条支路是步长s＝2的瓶颈模块,第二条支路由一个步长s＝1的瓶颈模块和步长s＝2的瓶颈模块串联组成；两条支路的输出按照通道连接,组成整个Inception模块的输出。</td>   <td>G06K9/00;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         徐冰;                   张东       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种中餐食物图像的分割方法</td>   <td>广东省</td>   <td>CN107154044B</td>   <td>2021-01-08</td>   <td>本发明提供的方法通过采集中餐食物图像的纹理图像进行后续处理来实现对图像的分割,分割的过程中无需采集多种图像特征,且应用该方法可以提高中餐食物图像分割的准确率,从而助于中餐食物图像的识别。</td>   <td>1.一种中餐食物图像的分割方法,其特征在于：包括以下步骤：S1.使用纹理增强滤波器对拍摄的中餐食物图像进行16个不同尺度参数下的滤波,得到图像在16个不同尺度参数下的纹理图像；记尺度参数为m,m的取值范围为8～16；S2.对于步骤S1得到的16幅纹理图像分别计算其均值,并利用计算得到的均值作为阈值来对相应的纹理图像进行二值化,获得纹理图像在阈值条件下的前景区域和背景区域；S3.对于每张纹理图像分别求取其前景区域的中心点,以用作放置高斯函数的位置,以前景区域包含的像素点数量的k倍作为标准差,构造对应的高斯掩膜函数,其中k的取值范围为0.3～0.5；将得到的16个高斯掩膜函数乘以相对应的权重参数后相加,得到最终的高斯掩膜；S4.将得到的高斯掩膜与中餐食物图像在纹理增强滤波器尺度参数为[0.5m]时所产生的纹理图像相乘,将其得到的结果记为图G,其中[0.5m]表示对0.5m进行取整操作；采用SLIC方法对图G进行超像素分割,分割之后,得到对图像中每个像素点所属的块的类别,称为标记矩阵L,把L记作图G的标记图；S5.对图G中的每个像素具有相同类别标记的像素区域计算出其均值Gk,将均值Gk与图G的整体均值Gu进行比较,若Gk&gt;Gu,则将具有相同标记的像素区域的各个像素点的像素值设为1,并将具有相同标记的像素区域标记为前景区域,否则将具有相同标记的像素区域的各个像素点的像素值设为0,并将具有相同标记的像素区域标记为背景区域；S6.对步骤S5中得到的前景区域和背景区域进行形态学的开运算和闭运算,以平滑前景区域和背景区域的边缘区域,然后对前景区域和背景区域进行分割。</td>   <td>G06T7/11;G06T7/194</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王甲海;              任文彬;                   印鉴       </td>   <td>中山大学;广东恒电信息科技股份有限公司</td>   <td>求解多目标带时间窗周期性车辆路径问题的智能调度算法</td>   <td>广东省</td>   <td>CN106651044B</td>   <td>2021-01-08</td>   <td>本发明提供一种求解多目标带时间窗周期性车辆路径问题的智能调度算法,该算法运用于多目标带时间窗周期性车辆路径问题的设计和优化中,主要涉及物流运输和智能计算两大领域。发明的方法将优化过程包括：第一,从存档中选择解,对解在不同的目标上进行局部搜索,提高解的质量；第二,使用搜索得到的解更新存档。在多目标局部搜索算法中设计了多种邻域操作,避免陷入局部最优,提高算法的收敛速度。本发明对公开的对称样例和非对称样例进行了测试,证明了发明的方法是真实有效的。</td>   <td>1.一种求解多目标带时间窗周期性车辆路径问题的智能调度算法,其特征在于,包括以下步骤：S1：从存档中选择解,对解在不同的目标上进行局部搜索,提高解的质量；S2：使用搜索得到的解更新存档；所述步骤S1的具体过程如下：S111：初始化存档A,评估目标序号obj设置为1,其中obj＝1,2,···,5；S112：初始化解x；S113：对解x进行评估；S114：采用局部搜索LS_(obj)优化x的第obj个目标,得到优化后的解x’；S115：使用解x’更新存档A得到存档A1,并将存档A1复制给存档A；S116：obj＝obj+1；S117：如果obj小于等于目标数M,则执行步骤S112,否则执行步骤S118；S118：从存档A中随机选择一个解x_1,obj设置为1；S119：采用局部搜索LS_(obj)优化x_1的第obj个目标,得到优化后的解x_1’；S120：使用解x_1’更新存档A得到A2,并将存档A2复制给存档A；S121：obj＝obj+1；S122：如果obj小于等于目标数M,则执行步骤S119,否则执行步骤S123；S123：如果未达到结束条件,则执行步骤S118,否则执行步骤S124；S124：输出存档A,程序结束；所述步骤S113的过程是：采用以下评估函数对解x进行评估：                                    f_3＝max{T_(j,d)|j＝1,L,R_d,d＝1,L,T}                                    其中,路径集合R包括了每天访问客户的路径集合R_d,故R＝{R_1,R_2,…,R_T},其中T表示访问周期,每天的路径集合R_d则包含多条具体的路径r_(j,d),r_(j,d)＝&lt;c(1,j,d),c(2,j,d),…,c(N_j,j,d)&gt;,故R_d＝{r_(1,d),r_(2,d),…,r_(m,d)},m表示该天出现的路径数,c(i,j,d)表示第d天中第j条路径中的第i个客户,N_j表示该路径出现的客户的数量,令c(0,j,d)＝c(N_j+1,j,d)＝0表示所有车辆从仓库出发,并最终返回仓库；Dist_(j,d)表示第d天中第j条路径的行驶距离：                  其中,d_(c(i,j,d)c(i+1,j,d))表示客户c(i,j,d)和客户c(i+1,j,d)之间的行驶距离；T_(j,d)表示第d天中第j条路径的行驶时间：                  其中,t_(c(i,j,d)c(i+1,j,d))表示c(i,j,d)和c(i+1,j,d)之间的行驶时间,s_(c(i,j,d))表示客户c(i,j,d)需要的服务时间,令a_(c(i,j,d))表示车辆到达客户c(i,j,d)的时间,l_(c(i,j,d))表示车辆离开客户c(i,j,d)的时间,a_(c(i,j,d))＝l_(c(i-1,j,d))+t_(c(i-1,j,d)c(i,j,d)),如果车辆到客户的时间a_(c(i,j,d))早于该客户的最早服务时间e(i,j,d),则会产生等待时间：w_(c(i,j,d))＝max(0,e_(c(i,j,d))-a_(c(i,j,d))),则l_(c(i,j,d))＝a_(c(i,j,d))+w_(c(i,j,d))+s_(c(i,j,d))；W_(jd)表示第d天中第j条路径上的所有客户的等待时间：                  Dt_(j,d)表示第d天中第j条路径上的所有客户的延迟时间,如果车辆到客户的时间晚于该客户的最迟服务时间l_(c(i,j,d)),则会产生等待时间t_(c(i,j,d))＝max(0,a_(c(i,j,d))-l_(c(i,j,d))),故                  其中,f_1表示在T天中所调度的所有车辆,为评估目标1即obj＝1,优化目标1,可以减少每辆车的消耗；f_2表示在T天中所有车辆行驶的总距离,为评估目标2即obj＝2,优化目标2,可以减少车辆总的花费；f_3表示在T天所调度的所有车辆中每个驾驶员最长的工作时间,为评估目标3即obj＝3,优化目标3可以保证工作量的平衡；f_4表示在T天中所有车辆的等待时间,为评估目标4即obj＝4,优化目标4,可以提高工作效率,避免浪费工作时间；f_5表示在T天中所有车辆的延迟时间,为评估目标5即obj＝5,优化目标5,可以提高客户的满意程度。</td>   <td>G06Q10/04;G06Q10/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨跃东;              卢宇彤;              陈志广;                   肖侬       </td>   <td>中山大学</td>   <td>基于深度网络快速识别单分子纳米孔测序碱基方法</td>   <td>广东省</td>   <td>CN112183486A</td>   <td>2021-01-05</td>   <td>本发明公开了一种基于深度网络快速识别单分子纳米孔测序碱基方法,所述的方法包括步骤如下：S1：从测序原始数据中提取电信号序列,对电信号序列进行第一预处理,得到信号矩阵；S2：构建深度网络模型,对深度网络模型进行训练,直至损失函数达到设定阈值或迭代次数达到设定步数；S3：将步骤S1中得到的信号矩阵输入编码器中提取高维特征信息,输出特征信息矩阵；S4：将步骤S3中得到的特征信息矩阵通过全连接网络层映射生成对应于碱基字符表的概率矩阵；S5：采用基于束搜索算法的连接时序分类模块作为解码器对步骤S4得到的概率矩阵进行束搜索,得到若干个碱基序列,选择其中得分最高的碱基序列作为输出结果。</td>   <td>1.一种基于深度网络快速识别单分子纳米孔测序碱基方法,其特征在于：所述的方法包括步骤如下：S1：从测序原始数据中提取电信号序列,对电信号序列进行第一预处理,得到信号矩阵；S2：构建深度网络模型,对深度网络模型进行训练,直至损失函数达到设定阈值或迭代次数达到设定步数；其中所述的深度网络模型依次连接有编码器、全连接网络层、连接时序分类解码器；S3：将步骤S1中得到的信号矩阵输入编码器中提取高维特征信息,输出特征信息矩阵；S4：将步骤S3中得到的特征信息矩阵通过全连接网络层映射生成对应于碱基字符表的概率矩阵；S5：采用基于束搜索算法的连接时序分类模块作为解码器对步骤S4得到的概率矩阵进行束搜索,得到若干个碱基序列,选择其中得分最高的碱基序列作为输出结果。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周晓豪;                   朝红阳       </td>   <td>中山大学</td>   <td>一种基于符号距离函数的三维重建方法</td>   <td>广东省</td>   <td>CN112184899A</td>   <td>2021-01-05</td>   <td>本发明涉及一种基于符号距离函数的三维重建方法。包括：S1.采集训练数据；S2.选取关键帧作为训练集；S3.对关键帧的深度图进行数据去噪；S4.对多帧深度图进行采样,主要包括物体表面附近的法向上采样；S5.进行点云归一化处理；S6.数据输入SDF函数的深度学习网络进行训练,学出一个更接近原物体的SDF函数；S7.根据深度神经网络进行重构,得到具有精细几何结构的模型。本发明是基于深度学习网络对多帧的真实场景进行学习,得到隐式的连续的SDF函数,建立起物体形状与网络参数的对应关系,生成归一化的体素矩阵,最终利用Marching Cubes算法重构出高质量精细的网格结构。极大的改善了SDF概念在传统体素方法的分辨率不足,以及存储大量离散体素导致的内存问题。</td>   <td>1.一种基于符号距离函数的三维重建方法,其特征在于,包括以下步骤：S1.采集训练数据；S2.选取关键帧作为训练集；S3.对关键帧的深度图进行数据去噪；S4.对多帧深度图进行采样,包括物体表面附近的法向上采样；S5.进行点云归一化处理；S6.数据输入SDF函数的深度学习网络进行训练,得到一个更接近原物体的SDF函数；S7.根据深度神经网络进行重构,得到准确的几何结构的模型。</td>   <td>G06T17/00;G06T5/00;G06K9/62;G06N3/04;G06N3/08;G06N20/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;              刘妮;              郭叙森;                   程龙       </td>   <td>中山大学</td>   <td>一种针对动态视觉传感器事件流的新型去雨方法及系统</td>   <td>广东省</td>   <td>CN112184572A</td>   <td>2021-01-05</td>   <td>本发明涉及一种针对动态视觉传感器事件流的新型去雨方法。创新地提出基于雨水痕迹在时间方向和水平宽度方向上的稀疏性,将传统的H-W视角转换为W-T视角,通过这一转换将雨水从错综复杂的背景环境中提取出来,变成在W-T平面上近似均匀分布的噪声点,大幅降低了去除雨水的难度；使用中值滤波算法清除雨水痕迹,充分利用了图像中的冗余信息,在去噪的同时能最大程度地保持图像的细节特征。中值滤波的本质是把某点的像素值用该点的一个邻域中各点值的中值代替,让周围的像素值接近的真实值,从而消除孤立的噪声点。W-T空间内雨水表现为椒盐噪声,中值滤波可以简单高效的滤除这种噪声的同时保护了图像的尖锐边缘。</td>   <td>1.一种针对动态视觉传感器事件流的新型去雨方法,其特征在于,基于雨痕分布在时间方向和水平宽度方向上的稀疏性与不连续性,提出在宽度-时间即W-T空间对图像进行去雨操作,对图像的高度进行逐像素操作,即对于每一个高度值,检查其对应的W-T平面；具体包括以下步骤：S1.创建缓冲队列：创建一个存储DVS视频流的、深度为d的先入先出缓存队列Q(h×w×d)；当有新的事件帧/视频帧进入Q时,如果Q中已经存在d帧,则将其中最早进入的帧移出；S2.处理Q中的顶部帧I_1,对其去噪,得到原始帧I_1对应的去雨后的帧S3.基于顶部帧I_1修复其去噪结果S4.输出流中推入结果S5.继续循环处理队列Q：从B中弹出顶部帧I_1,Q中不为空时回到步骤S2,Q中为空时等待输入；其中,DVS输出的事件流本质上是一系列H-W空间内二值图像的组合[I_1,I_2,...,I_t],是一个二维张量,为H-W平面的切片；三维张量h为事件帧/视频帧的高；w为事件帧/视频帧的宽；Q为待去雨的图片的队列空间；为H-W平面的I_i对应的去噪结果；为H-W平面的对应的边缘损失修复结果。</td>   <td>G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张小虎;              甘叔玮;              叶雪辀;              杨夏;                   林彬       </td>   <td>中山大学</td>   <td>一种运动员人体姿态测量方法</td>   <td>广东省</td>   <td>CN112183316A</td>   <td>2021-01-05</td>   <td>本发明公开一种运动员人体姿态测量方法,包括如下步骤：获取待测运动员在运动过程中按时间排序的多方位视图,作为待测图像组序列；基于待测图像组序列得到待测运动员各关节点的二维图像坐标序列；基于待测运动员各关节点的二维图像坐标序列得到待测运动员各关节点的初步三维空间坐标序列；基于待测运动员各关节点的初步三维空间坐标序列构建待测运动员的人体三维骨架模板；基于骨架各关节点之间的运动链关系驱动人体三维骨架模板,并与待测运动员各关节点的初步三维空间坐标序列进行匹配优化,得到待测运动员各关节点的实际三维空间坐标序列。能够有效的对运动员运动过程中的姿态进行测量,具有测量过程简单、成本低、实时性高等特点。</td>   <td>1.一种运动员人体姿态测量方法,其特征在于,包括如下步骤：步骤1,获取待测运动员在运动过程中按时间排序的多方位视图,作为待测图像组序列；步骤2,基于待测图像组序列得到待测运动员各关节点的二维图像坐标序列；步骤3,基于待测运动员各关节点的二维图像坐标序列得到待测运动员各关节点的初步三维空间坐标序列；步骤4,基于待测运动员各关节点的初步三维空间坐标序列构建待测运动员的人体三维骨架模板；步骤5,基于骨架各关节点之间的运动链关系驱动人体三维骨架模板,并与待测运动员各关节点的初步三维空间坐标序列进行匹配优化,得到待测运动员各关节点的实际三维空间坐标序列。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         徐瑞华;              李超峰;              陈海畴;              邓一术;                   经秉中       </td>   <td>中山大学肿瘤防治中心</td>   <td>一种轮廓提取方法及装置、电子设备、存储介质</td>   <td>广东省</td>   <td>CN112183541A</td>   <td>2021-01-05</td>   <td>本申请实施例涉及计算机视觉处理技术领域,公开了一种轮廓提取方法及装置、电子设备、存储介质,能够提高轮廓提取的效率。该方法包括：获取待识别图像。通过训练得到的目标分割模型对待识别图像中的目标对象进行轮廓分析,得到轮廓参数,并基于轮廓参数获得N条分段曲线的分段参数,N为正整数。之后,通过目标分割模型,并结合曲线方程对每条分段曲线的分段参数进行曲线解码,获得N条分段曲线各自对应的轮廓点坐标。最后,对N条分段曲线各自对应的轮廓点坐标进行拼接,生成目标对象的目标轮廓。</td>   <td>1.一种轮廓提取方法,其特征在于,所述方法包括：获取待识别图像；通过训练得到的目标分割模型对所述待识别图像中的目标对象进行轮廓分析,得到轮廓参数,并基于所述轮廓参数获得N条分段曲线的分段参数,N为正整数；通过所述目标分割模型,并结合曲线方程对每条分段曲线的分段参数进行曲线解码,获得所述N条分段曲线各自对应的轮廓点坐标；将所述N条分段曲线各自对应的轮廓点坐标进行拼接,生成所述目标对象的目标轮廓。</td>   <td>G06K9/34;G06K9/46;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         潘亮;              李铭慧;              胡天江;              张新;                   王法魁       </td>   <td>中山大学</td>   <td>一种超大航拍条带图像的拼接生成方法及装置</td>   <td>广东省</td>   <td>CN112163995A</td>   <td>2021-01-01</td>   <td>本发明公开了一种超大航拍条带图像的拼接生成方法及装置。所述方法包括：接收无人机航拍图像和所述无人机航拍图像对应的传感器数据,并根据所述传感器数据对所述无人机航拍图像进行预处理,得到待拼接图像；以不同所述待拼接图像作为参考图像和待配准图像,提取所述参考图像和所述待配准图像中最相似的两块感兴趣区域；对两块所述感兴趣区域进行特征点提取和匹配,得到所述参考图像和所述待配准图像之间的仿射变换矩阵以及相对的像素坐标；根据所述传感器数据将所述像素坐标转换为经纬度坐标,并根据所述经纬度坐标拼接所述参考图像和所述待配准图像,得到航拍条带图像。本发明能够快速拼接无人机航拍图像生成超大分辨率的航拍条带图像。</td>   <td>1.一种超大航拍条带图像的拼接生成方法,其特征在于,包括：接收无人机航拍图像和所述无人机航拍图像对应的传感器数据,并根据所述传感器数据对所述无人机航拍图像进行预处理,得到待拼接图像；以不同所述待拼接图像作为参考图像和待配准图像,提取所述参考图像和所述待配准图像中最相似的两块感兴趣区域；对两块所述感兴趣区域进行特征点提取和匹配,得到所述参考图像和所述待配准图像之间的仿射变换矩阵以及相对的像素坐标；根据所述传感器数据将所述像素坐标转换为经纬度坐标,并根据所述经纬度坐标拼接所述参考图像和所述待配准图像,得到航拍条带图像。</td>   <td>G06T3/40;G06T5/00;G06T7/33;G06K9/62;G06K9/46;G06K9/32</td>  </tr>        <tr>   <td>中国专利</td>   <td>         袁进;              王耿媛;              贠照强;              肖鹏;              段铮昱;              骆仲舟;                   黄远聪       </td>   <td>中山大学中山眼科中心</td>   <td>一种眼微血管血流动力学参数自动分析方法</td>   <td>广东省</td>   <td>CN112164046A</td>   <td>2021-01-01</td>   <td>本发明提供了一种眼微血管血流动力学自动分析方法,包括如下步骤：步骤1：输入包含有眼微血管信息的视频文件中需要进行眼微血管血流动力学参数分析的图像集I_o；步骤2：对图像集合I_o中连续的图像进行图像配准；步骤3：对配准后的图像进行图像分割,得到眼微血管的分割掩码图像集S<sub>r</sub>；步骤4：获取分割掩码图像集S<sub>r</sub>中眼微血管的中心线；步骤5：对所述中心线进行拟合,并提取分割掩码图像中眼微血管的边缘,计算所述眼微血管的平均直径、内径和外径；步骤6：构成时空图像；步骤7：对得到的时空图像进行直方图均衡化处理；步骤8：对时空图像进行直线检测处理,对得到的直线的斜率进行统计,并得出分布最密集的斜率k；步骤9：计算眼微血管的血流速度V<sub>a</sub>；步骤10：计算血流量、管壁切应率。</td>   <td>1.一种眼微血管血流速度的分析方法,其特征在于,包括如下步骤：步骤1：输入包含有眼微血管信息的视频文件中需要进行眼微血管血流动力学参数分析的图像集I_o；步骤2：对图像集合I_o中连续的图像进行图像配准,以第一张图像为固定图像,将剩余图像与第一张图像对齐,得到配准后的图像集合I_r；步骤3：对配准后的图像进行图像分割,得到眼微血管的分割掩码图像集S_r；步骤4：获取分割掩码图像集S_r中眼微血管的中心线；步骤5：对所述中心线进行拟合,并提取分割掩码图像中眼微血管的边缘,根据拟合后的中心线和分割掩码图像中眼微血管的边缘计算所述眼微血管的平均观察直径、内径和外径；步骤6：提取步骤5中第一张分割掩码图像中的拟合的每一条中心线,以中心线的坐标为时空图的列,以图像集合I_r中的图像的张数为行,构成时空图像；步骤7：对得到的时空图像进行直方图均衡化处理；步骤8：对时空图像进行直线检测处理,对得到的直线的斜率进行统计,并得出分布最密集的斜率k；步骤9：计算眼微血管的血流速度V_a,采用以下公式：V_a＝k*S*cos(ant(k)),其中S表示图像像素之间的距离,单位为um,由相机确定分辨率后输入。</td>   <td>G06T7/00;G06T7/13;G06T5/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;              白善荣;                   夏俊       </td>   <td>中山大学</td>   <td>一种白内障手术显微镜下单目深度估计方法</td>   <td>广东省</td>   <td>CN110349197B</td>   <td>2021-01-01</td>   <td>本发明涉及一种白内障手术显微镜下单目深度估计方法,通过显微镜系统拍摄两张图像,通过对比计算获得第一张图像中准确的测试物点,再通过代价函数找到测试物点的匹配点,获得两个点之间的视差,从而计算出测试物点的深度。通过显微镜系统获得测试物点的平面坐标后,通过计算获得测试物点的深度,从而在手术的过程的位置转换为三维坐标,为自动手术提供位置的定位。</td>   <td>1.一种白内障手术显微镜下单目深度估计方法,其特征在于,包括以下步骤：步骤一：使用显微镜摄像系统以a毫米垂直位移拍摄两幅图像；步骤二：对每幅图像进行预处理,转换为具有灰度值的图像,并对第一幅图像进行角点检测,得到若干候选角点,并且得到所述角点的坐标；步骤三：若干所述角点以各自为中心,R_j为半径,求出相对于角点的黑点和白点,并且黑点位于角点白色像素区域和白点位于角点的黑色区域,且黑点的像素灰度&lt;0.5,白点的像素灰度&gt;0.5,则该角点为测试物点P′_i,黑点和白点位置的计算公式如下：                                                                                          其中,X_(blacki)和Y_(blacki)分别为黑点的坐标值,X_(whitei)和Y_(whitei)分别白点的坐标值,X_j和Y_j是以角点为中心,R_j为半径的范围内所有像素点的各自坐标,I为该点的灰度值,range为选取的半径最大值；X_i、Y_i分别为测试物点P′_i的坐标值；步骤四：对第一幅图像P′_i向第二幅图像进行匹配,得到匹配点P″_i,P′_i和P″_i之间的的像素距离；步骤五：测试物点的深度计算公式如下：                  其中,r为测试物点P′_i与显微镜放大中心之间的距离,Δr为P′_i和P″_i之间的的像素距离,称为视差。</td>   <td>G06T7/55</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李冠彬;              曾怡瑞;              李灏峰;                   林倞       </td>   <td>中山大学</td>   <td>一种基于在线迭代生成器的对抗防御方法及系统</td>   <td>广东省</td>   <td>CN110796608B</td>   <td>2021-01-01</td>   <td>本发明公开了一种基于在线迭代生成器的对抗防御方法及系统,该方法包括如下步骤：步骤S1,随机初始化生成器网络F的参数θ,并用0初始化与输入图像相同大小的合成图像；步骤S2,给定可能是对抗样本的输入图像,将其定义为参考图像I<sub>z</sub>,将其输入至生成器网络模块,生成合成图像,并交替迭代更新网络参数和合成图像,最终获得去除对抗噪声且与原输入图像语义相同的合成图像,直到符合停止的条件,本发明可在有效地去除对抗噪声的同时,合成与输入图像具有相同语义图像以代替原有的输入图像。</td>   <td>1.一种基于在线迭代生成器的对抗防御方法,包括如下步骤：步骤S1,随机初始化生成器网络F的参数θ,并用0初始化与输入图像相同大小的合成图像；步骤S2,给定可能是对抗样本的输入图像,将其定义为参考图像I_z,将其输入至包含所述生成器网络F的在线迭代生成器模块,生成合成图像,并交替迭代更新网络参数和合成图像,最终获得去除对抗噪声且与原输入图像语义相同的合成图像；步骤S2进一步包括：步骤S200,利用所述生成器网络F进行内迭代,利用生成器网络F近似能量函数,更新合成图像来最小化能量函数,以产生与所述参考图像I_z语义相同的、去除对抗噪声的新图像；步骤S201,利用所述生成器网络F进行外迭代,沿最大化对数似然的方向训练生成器网络F的参数,更新网络参数以使合成图像逐渐靠近参考图像I_z；步骤S202,多次迭代式地进行步骤S200-S201的训练过程,直到符合停止的条件；于步骤S2中,以步骤S1初始化的合成图像和网络参数作为迭代的初始化值,训练所述对抗防御的在线生成器神经网络；于训练过程收敛后,利用训练好的生成器网络在线生成的合成图像代替原始输入图像输入到目标网络。</td>   <td>G06T5/00;G06K9/62;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李熙莹;              朱肯钢;                   江倩殷       </td>   <td>中山大学</td>   <td>一种图像快速去雾方法、系统、终端及存储介质</td>   <td>广东省</td>   <td>CN108876743B</td>   <td>2020-12-29</td>   <td>本发明公开了一种图像快速去雾方法、系统、终端及存储介质,该方法包括：采用OTSU对原始有雾图像进行天空区域和非天空区域划分后,采用最大值滤波和引导滤波对暗通道图优化,根据优化后的暗通道图来确定第一透射率；采用自适应性参数调整方法对第一透射率调整后得到第二透射率；根据第一透射率和第二透射率,按照大气散射模型分别对非天空区域和天空区域进行去雾复原,得到去雾复原图像；采用CLAHE对去雾复原图像进行色调调整。该系统包括获取模块、划分模块、优化模块、调整模块、去雾模块及调色模块。通过使用本发明,能够有效精细地实现有雾图像的去雾处理,去雾效果优且处理运行效率高。本发明可广泛应用于图像处理领域中。</td>   <td>1.一种图像快速去雾方法,其特征在于,包括以下步骤：获取原始有雾图像；采用OTSU对获取得到原始有雾图像进行天空区域和非天空区域的划分；采用最大值滤波方式和引导滤波方式对暗通道图进行优化处理,从而根据优化处理后的暗通道图来确定得出第一透射率；其中,所述暗通道图指的是与原始有雾图像对应的暗通道图,所述第一透射率是用于对原始有雾图像中的非天空区域进行去雾复原处理的透射率；所述采用最大值滤波方式和引导滤波方式对暗通道图进行优化处理,从而根据优化处理后的暗通道图来确定得出第一透射率这一步骤具体包括以下步骤：对原始有雾图像的灰度图进行边缘检测处理,从而获得对应的二值化图像；其中,所述原始有雾图像的表达式为采用大气散射模型来描述的表达式；根据预设的结构元素的填充形状和填充半径,对二值化图像进行边界填充处理,从而得到边界填充图像；在暗通道图中,求取边界填充图像中每一个边界像素点的相邻区域内的最大像素值后,将求取得到的最大像素值作为边界像素点所对应的暗通道值,以对暗通道图进行修正；对修正后的暗通道图进行引导滤波处理,从而得到引导滤波后的暗通道图；根据引导滤波后的暗通道图,按照大气散射模型来确定得到第一透射率；采用自适应性参数调整方法对第一透射率进行调整处理,从而得到第二透射率；其中,所述第二透射率是用于对原始有雾图像中的天空区域进行去雾复原处理的透射率；其中,所述采用的调整处理公式如下所示：                  式中,t_(sky)(x)表示为第二透射率；x表示为像素点的空间坐标；Y表示为第一透射率所对应的像素点的总个数；y表示为第一透射率所对应的像素点中像素值小于k1的像素点的个数；k1表示为下限值；k2表示为上限值；根据第一透射率和第二透射率,按照大气散射模型分别对原始有雾图像中的非天空区域和天空区域进行去雾复原处理,从而得到原始有雾图像所对应的去雾复原图像；采用CLAHE对去雾复原图像进行色调调整处理,从而得到最终去雾图像。</td>   <td>G06T5/00;G06T7/11;G06T7/12</td>  </tr>        <tr>   <td>中国专利</td>   <td>         衣杨;              赵小蕾;              王玉娟;                   石晓红       </td>   <td>中山大学新华学院</td>   <td>基于插件的发票识别方法</td>   <td>广东省</td>   <td>CN107358232B</td>   <td>2020-12-29</td>   <td>本发明公开了一种基于插件的发票识别方法、及识别与管理系统；其中,识别方法包括：将预先创建的识别插件导入到动态链接库中；其中,识别插件基于适应于对应类型的发票的识别算法创建；获取待识别的发票的图像信息,及其类型；根据该待识别的发票的类型从动态链接库中调取对应的识别插件以对该待识别的发票的图像信息进行识别,进而获取所需的发票信息。本发明的优点是：1、员工能够自行录入发票信息,把发票信息提交到后台,供财务人员处理,从而减少财务人员的负担；2、每当有新类型的发票出现时,可以动态地向系统中添加针对该类型发票的识别算法,使系统能够识别新类型的发票,从工程角度解决多种发票识别问题。</td>   <td>1.一种基于插件的发票识别方法,其特征是,包括：将预先创建的识别插件导入到动态链接库中；其中,所述识别插件基于适应于对应类型的发票的识别算法创建；获取待识别的发票的图像信息,及其类型；根据该待识别的发票的类型从所述动态链接库中调取对应的识别插件以对该待识别的发票的图像信息进行识别,进而获取所需的发票信息；所述识别方法 包括：S21、通过色彩分离将发票中的表格从图像中进行分离,对分离出的表格图像进行二值化,用形态学操作对表格进行修复以获取完整的表格；S22、对步骤S21获取的表格进行Hough变换,求其直线倾斜角度,根据倾斜角度对发票原始彩色图像和表格图像进行旋转；S23、采用投影法对表格中各个角点坐标进行定位,并根据定位结果对表格进行切割,以获取表格中的多个不同的信息区域；投影分为水平投影与垂直投影；水平投影包括：对发票表格上的五条水平直线进行积分,能够得到对应X坐标,即可得到五条水平直线在表格图像中的纵坐标R1、R2、R3、R4和R5；垂直投影包括：把发票表格中第二条与第三条水平直线之间的部分截取出来,然后再进行垂直投影,得到九个明显的峰值,设从左到右垂直直线的横坐标分别为C1、C2、C3、C4、C5、C6、C7、C8和C9,九个峰值的横坐标即为表格中垂直线的横坐标C1、C2、C3、C4、C5、C6、C7、C8和C9；S24、对各个信息区域进行二值化、色彩分离、图像去除噪声处理；S25、采用Tesseract引擎对经过步骤S24处理后的信息区域进行字符识别,以获取所需的发票信息；步骤S22具体包括：获取增值税普通发票彩色分离后带表格的图像,然后二值化的表格图像；对二值化的图像进行边缘检测；获取检测后的图像然后进行Hough变换；θ以一为增量,分别计算出每个点的ρ值大小,然后是M[θ][ ρ]加1；其中,参数空间(θ,ρ)表示表格图像中的直线；θ的取值范围为0-360,ρ的取值范围为0-n,n的大小为增值税普通发票对角线长度；M[360][n]为定义的一个计数器；取出二维数组M[θ][ρ]值最大的,此最大值对应的θ为增值税普通发票与水平方向上的倾斜角；通过Hough变换获得的倾斜角θ,根据角度θ对图像倾斜较正。</td>   <td>G06K9/32;G06K9/34</td>  </tr>        <tr>   <td>中国专利</td>   <td>         高成英;                   汤梦玥       </td>   <td>中山大学</td>   <td>一种基于图像分割的深度图像修复方法</td>   <td>广东省</td>   <td>CN107248143B</td>   <td>2020-12-25</td>   <td>本发明涉及一种基于图像分割的深度图像修复方法,通过分割对应的彩色图像作为指导信息,结合数据拟合方法,进行缺失的深度信息的估算工作。本发明提供的基于图像分割的深度图像修复方法,得到的深度图像修复结果,无论是在图像的边缘区域还是非边缘区域,都与现实中物体的深度信息更为接近。</td>   <td>1.一种基于图像分割的深度图像修复方法,其特征在于：通过分割对应的彩色图像作为指导信息,结合数据拟合方法,进行缺失的深度信息的估算工作；其中,所述分割对应的彩色图像包括以下步骤：步骤1：先用高斯滤波器对输入的彩色图像进行滤波；步骤2：对高斯滤波后的彩色图像进行分割处理；步骤3：对步骤2的结果中的细小分割块进行后期的合并处理,形成分割彩色图像阶段的最终结果；其中,基于分割所得结果,对每个分割块进行相互独立、并行处理的修复工作,在进行每个分割块中的无效像素修复工作前,为当前分割块中的有效像素设置状态值、无效像素设置状态值,计算所有无效像素的排序权值,通过依据排序权值进行排序、更新排序权值,确定由外部至内部的修复次序；每个无效像素的修复步骤如下：步骤1：采用本发明所提出的一种基于空间连续性的采样方法,以待修复的无效状态像素p为起点,向其邻域方向射出若干条线段,收集此邻域方向上的样本,构造像素p的样本总集合SS＝(ss_1,ss_2,...,ss_8),在为每一个方向上收集样本的时候,线段的一个端点是像素p,假设另一个端点是像素q,当且仅当同时满足以下3个条件时,继续以p向q的方向延长线段,否则从线段上删去像素q并结束该方向上的样本收集工作：条件1：线段长度小于最大长度L_(max),L_(max)是常量参数；条件2：像素q不是无效状态；条件3：在从p向q方向的线段上,若像素q的前一个像素不是像素p,像素q与其前一个像素之间的深度值差距不能超过最大差距Dif_(max),Dif_(max)是常量参数；对于每一条线段,以像素p为起点开始依次收集线段经过的所有像素,形成一个样本集合ss,此时样本集合ss中包含待修复像素p,在为拟合修复工作提供样本集合的时候将会剔除样本集合中像素p,样本集合构成待修复像素p的样本总集合；步骤2：对ss_i中的样本分别与待修复像素p进行RGB颜色空间上的欧式距离计算,并进行求方差处理,以方差值表示样本集合ss_i的颜色平滑度W_(color)(i)；样本集合ss_i的数据可信度的权值W_(reliability)(i)等于样本集合ss_i中处于有效状态的像素的总数量；对于待修复像素p的样本总集合中的样本集合,在剔除仅包含待修复像素p的样本集合后,计算剩余样本集合相应的W_(color)、W_(reliability),优先依据W_(color)进行升序排序,其次依据W_(reliability)进行降序排序,选择排序后的第一个样本集合(p,s_1,s_2,...,s_m),将该样本集合中的像素p剔除,作为待修复像素p的最佳样本集合；步骤3：依据待修复像素p的最佳样本集合,采用n阶多项式数据拟合方法,拟合样本的变化规律,估算出该无效像素的深度值。</td>   <td>G06T5/00;G06T7/10;G06T7/90</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苏琬棋;              陈志广;              瞿毅力;              邓楚富;              卢宇彤;              肖侬;                   王莹       </td>   <td>中山大学</td>   <td>一种基于生成对抗网络的多域图像转换方法与系统</td>   <td>广东省</td>   <td>CN110084863B</td>   <td>2020-12-25</td>   <td>本发明公开了一种基于生成对抗网络的多域图像转换方法与系统,本发明的多域图像转换方法包括输入指定的X、Y两个模态的原图x、原图y；在重建训练部分针对原图x、原图y分别进行编、解压,分别得到原图特征、重建图、重建特征,并进行特征和图的模态鉴别对抗学习；循环训练部分基于前文的原图特征交换模态的编码器生成重建图、重建图特征以及循环重建图,并再次进行特征和图的模态鉴别对抗学习,最终将循环重建图输出。本发明采用半监督学习方法,既可以利用已有的标签数据也可以使用无标签数据,本发明能够实现多向的多域图像转换而不限于单向域转换或双向的二域转换,对域的数量没有限制,能解决图像风格迁移和医学图像多模态转换等问题。</td>   <td>1.一种基于生成对抗网络的多域图像转换方法,其特征在于实施步骤包括：1)输入指定的X、Y两个模态的原图x、原图y；2)将原图x进行X模态编码得到第一原图特征code_x,将第一原图特征code_x进行X模态解码得到第一重建图x',将第一重建图x'进行X模态编码得到第一重建特征code_x'；将原图y进行Y模态编码得到第二原图特征code_y,将第二原图特征code_y进行Y模态解码得到第二重建图y',将第二重建图y'进行Y模态编码得到第二重建特征code_y'；3)将第一原图特征code_x、第一重建特征code_x'进行特征鉴别,将第二原图特征code_y、第二重建特征code_y'进行特征鉴别对抗学习；将原图x、第一重建图x'进行X模态鉴别对抗学习,将原图y、第二重建图y'进行Y模态鉴别对抗学习；4)将第一原图特征code_x进行Y模态解码得到第三重建图y”,将第三重建图y”进行Y模态编码得到第三重建图特征code_y”,将第三重建图特征code_y”进行X模态解码得到第一循环重建图x”'；将第二原图特征code_y进行X模态解码得到第四重建图x”,将第四重建图x”进行X模态编码得到第四重建图特征code_x”,将第四重建图特征code_x”进行Y模态解码得到第二循环重建图y”'；5)将原图x、第四重建图x”进行X模态鉴别对抗学习,将原图y、第三重建图y”进行Y模态鉴别对抗学习；将第一原图特征code_x、第三重建图特征code_y”进行特征鉴别对抗学习,将第二原图特征code_y、第四重建图特征code_x”进行特征鉴别对抗学习；6)计算系统网络总体损失；7)对系统网络总体损失求导,开启反向传播每个损失函数反向逐层计算出各层参数的梯度值,然后根据各层参数梯度更新这些参数,完成本轮迭代,且所述第一循环重建图x”'以及第二循环重建图y”'构成本轮迭代的输出结果；其中步骤6)的详细步骤包括：6.1)分别计算X模态鉴别损失loss_(D,x)、Y模态鉴别器损失loss_(D,y)、特征鉴别损失loss_(D,c),并计算网络生成总体损失loss_4；6.2)根据X模态鉴别损失loss_(D,x)、Y模态鉴别器损失loss_(D,y)、特征鉴别损失loss_(D,c)以及网络生成总体损失loss_4计算系统网络总体损失；其中步骤6.2)中计算系统网络总体损失的函数表达式如式(1)所示；loss_(total)＝(loss_4+loss_(D,x)+loss_(D,y)+loss_(D,c))×γ  (1)式(1)中,loss_(total)表示系统网络总体损失,loss_4为网络生成总体损失,loss_(D,x)为X模态鉴别损失,loss_(D,y)为Y模态鉴别损失,loss_(D,c)为特征鉴别损失,γ为网络的学习率。</td>   <td>G06T9/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈定安;              陈明薇;              罗明;                   王先伟       </td>   <td>中山大学</td>   <td>一种基于LiDAR点云的电塔受灾风险评估方法及系统</td>   <td>广东省</td>   <td>CN112132795A</td>   <td>2020-12-25</td>   <td>本发明公开了一种基于LiDAR点云的电塔受灾风险评估方法及系统,该方法包括：获取LiDAR的3D点云数据并提取得到电塔点云；根据数字高程模型获取对应区域的地形控制指数并对地形控制指数进行分级,结合电塔点云得到位于不同灾害级别区域的电塔；获取电塔参数并分别计算不同灾害级别区域的电塔风险指数；根据地形控制指数和电塔风险指数生成电塔受灾风险评估分析图。该系统包括：点云处理模块、地形控制指数模块、电塔风险指数模块和分析模块。通过使用本发明,可使输电塔的相对抗洪灾性能可以客观地被观测、评估和分析。本发明作为一种基于LiDAR点云的电塔受灾风险评估方法及系统,可广泛应用于地理信息科学技术领域。</td>   <td>1.一种基于LiDAR点云的电塔受灾风险评估方法,其特征在于,包括以下步骤：获取LiDAR的3D点云数据并进行点云分割,提取得到电塔点云；根据数字高程模型获取对应区域的地形控制指数并对地形控制指数进行分级,结合电塔点云得到位于不同灾害级别区域的电塔；获取电塔参数并基于最小二乘技术的方形金字塔几何模型拟合方法分别计算不同灾害级别区域的电塔风险指数；根据地形控制指数和电塔风险指数生成电塔受灾风险评估分析图。</td>   <td>G06T7/00;G06K9/62;G06T7/11;G06T7/60</td>  </tr>        <tr>   <td>中国专利</td>   <td>         高智凡;              申莹;                   张贺晔       </td>   <td>中山大学</td>   <td>一种心室图像分割方法、系统、装置及存储介质</td>   <td>广东省</td>   <td>CN112132834A</td>   <td>2020-12-25</td>   <td>本发明公开了一种心室图像分割方法、系统、装置及存储介质,该方法包括：获取心室图像并基于特征金字塔架构对心室图像进行特征提取,得到特征图；引入DAPIS损失函数对特征图进行处理,生成预测图像和对应的概率值；结合语义特征融合网络和预测图像对特征图进行拼接融合,得到分割图像。该系统包括：特征提取模块、预测模块和拼接融合模块。该装置包括存储器以及用于执行上述心室图像分割方法的处理器。本发明作为一种心室图像分割方法、系统、装置及存储介质,可广泛应用于医学图像处理领域。</td>   <td>1.一种心室图像分割方法,其特征在于,包括以下步骤：获取心室图像并基于特征金字塔架构对心室图像进行特征提取,得到特征图；引入DAPIS损失函数对特征图进行处理,生成预测图像和对应的概率值；结合语义特征融合网络和预测图像对特征图进行拼接融合,得到分割图像。</td>   <td>G06T7/10;G06K9/46;G06T5/50;G06T7/143</td>  </tr>        <tr>   <td>中国专利</td>   <td>         高智凡;              刘修健;              张贺晔;                   徐梓峰       </td>   <td>中山大学</td>   <td>一种基于模态学习的血管边界检测方法、系统及装置</td>   <td>广东省</td>   <td>CN112132850A</td>   <td>2020-12-25</td>   <td>本发明公开了一种基于模态学习的血管边界检测方法、系统及装置,该方法包括：获取批量图像并将图像划分得到隐藏模态集和目标模态集；基于卷积自动编码器和双向金字塔网络结构构建模态学习模型；根据隐藏模态集和目标模态集对模态学习模型进行参数优化,得到优化模型；获取待测图像并输入到优化模型识别血管边界。该系统包括：获取模块、构建模块、模态学习模块和识别模块。该装置包括存储器以及用于执行上述基于模态学习的血管边界检测方法的处理器。通过使用本发明,可实现在各种血管环境依然可以进行准确的血管边界检测。本发明作为一种基于模态学习的血管边界检测方法、系统及装置,可广泛应用于血管图像处理领域。</td>   <td>1.一种基于模态学习的血管边界检测方法,其特征在于,包括以下步骤：获取批量图像并将图像划分得到隐藏模态集和目标模态集；基于卷积自动编码器和双向金字塔网络结构构建模态学习模型；根据隐藏模态集和目标模态集对模态学习模型进行参数优化,得到优化模型；获取待测图像并输入到优化模型,识别得到血管边界。</td>   <td>G06T7/13;G06K9/46;G06K9/62;G06N3/04;G06N3/08;G06T7/11</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郭雪梅;              王国利;              谢泳伦;                   陈焕杰       </td>   <td>中山大学</td>   <td>一种结合深度学习和图像处理的烟盒缺陷识别方法</td>   <td>广东省</td>   <td>CN112132196A</td>   <td>2020-12-25</td>   <td>本发明公开了一种结合深度学习和图像处理的烟盒缺陷识别方法,该方法包括：获取原始图像并对原始图像进行处理,得到样本图像和烟盒图像；根据样本图像训练神经网络模型,得到特征提取模型；将烟盒图像输入到特征提取模型,并对特征提取模型中的卷积层的数据进行求导及加权求和,得到热力图；根据热力图和烟盒图像,计算得到烟盒图像中的缺陷长宽。通过使用本发明,能准确判断工业流水线上的烟盒是否存在缺陷并计算出烟盒中的缺陷区域范围和缺陷的具体大小。本发明作为一种结合深度学习和图像处理的烟盒缺陷识别方法,可广泛应用于图像处理领域。</td>   <td>1.一种结合深度学习和图像处理的烟盒缺陷识别方法,其特征在于,包括以下步骤：获取原始图像并对原始图像进行处理,得到样本图像和烟盒图像；根据样本图像训练神经网络模型,得到特征提取模型；将烟盒图像输入到特征提取模型,并对特征提取模型中的卷积层的数据进行求导及加权求和,得到热力图；根据热力图和烟盒图像,计算得到烟盒图像中的缺陷长宽。</td>   <td>G06K9/62;G06K9/46;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张贺晔;              刘修健;              高智凡;                   林慧娴       </td>   <td>中山大学</td>   <td>一种基于血管内超声图像的血流储备分数测量方法及系统</td>   <td>广东省</td>   <td>CN112132203A</td>   <td>2020-12-25</td>   <td>本发明公开了一种基于血管内超声图像的血流储备分数测量方法及系统,该方法包括：获取批量血管内超声图像并将批量血管内超声图像划分为训练集和验证集,得到训练集图像和验证集图像；基于训练集图像和验证集图像对预设模型进行优化,得到机器学习模型；获取待测血管内超声图像并进行图像特征提取,得到特征数据；将特征数据输入到机器学习模型,得到待测血管内超声图像的血流储备分数。该系统包括：划分模块、优化模块、特征模块和输出模块。通过使用本发明,根据IVUS图像即可计算FFR值,并且具有准确率高和计算要求低的特点。本发明作为一种基于血管内超声图像的血流储备分数测量方法及系统,可广泛应用于医学图像处理领域。</td>   <td>1.一种基于血管内超声图像的血流储备分数测量方法,其特征在于,包括以下步骤：获取批量血管内超声图像并将批量血管内超声图像划分为训练集和验证集,得到训练集图像和验证集图像；基于训练集图像和验证集图像对预设模型进行优化,得到机器学习模型；获取待测血管内超声图像并进行图像特征提取,得到特征数据；将特征数据输入到机器学习模型,得到待测血管内超声图像的血流储备分数。</td>   <td>G06K9/62;G06K9/46;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨红杰;              周凡;                   王若梅       </td>   <td>中山大学</td>   <td>一种基于运动特征的三维动态网格简化方法及其系统</td>   <td>广东省</td>   <td>CN107527384B</td>   <td>2020-12-22</td>   <td>本发明实施例公开了一种基于运动特征的三维动态网格简化方法及其系统,其中,该方法包括：获取简化处理所需的相关输入参数；进行三维模型中顶点的动态连通性的计算,得到三维动态模型上的运动特征；进行动态模型中各条边的折叠代价的计算,建立边折叠操作表；根据边折叠操作表选取折叠代价最小的一条边进行折叠操作,重复进行折叠操作,直到达到简化要求。实施本发明实施例,由于考虑了原始模型的运动特征,不仅可以有效地简化三维动态模型,同时也降低了在简化过程中的计算复杂度,有助于提高三维动态网格数据的传输效率,节省数据存储空间,对于三维模型的存储、传输、处理和形状分析等应用,特别是实时绘制具有极为重要的意义。</td>   <td>1.一种基于运动特征的三维动态网格简化方法,其特征在于,所述方法包括：获取简化处理所需的相关输入参数；进行三维模型中顶点的动态连通性的计算,得到三维动态模型上的运动特征；进行动态模型中各条边的折叠代价的计算,建立边折叠操作表；根据边折叠操作表选取折叠代价最小的一条边进行折叠操作,重复进行折叠操作,直到达到简化要求；其中,所述获取简化处理所需的相关输入参数的步骤,包括：获取所载入的三维动态模型上的所有边及其顶点数据；获取三维动态模型中的三角形数据信息,根据所述三角形数据计算所有三角形的法向量；获取所有顶点及其所关联的三角形,根据所述顶点及其所关联的三角形信息计算所有边的二次误差矩阵；获取二面角阈值参数以及边数简化参数；其中,所述进行三维模型中顶点的动态连通性的计算,得到三维动态模型上的运动特征的步骤,包括：根据二面角阈值参数计算三维模型中边以及边所对应的两个顶点的动态连通性：当某条边所对应的二面角在相邻帧之间的变化值大于所述二面角阈值参数时,该边以及边所对应的两个顶点即视为连通；根据顶点的动态连通性,遍历整个三维模型,得到动态连通子图,得到模型中的所有运动特征：将3D模型视为一个图,遍历整个图,找出图中的所有动态连通子图；其中,所述进行动态模型中各条边的折叠代价的计算,建立边折叠操作表的步骤,包括：根据获得的运动特征,对模型上的顶点分配权值并计算边的权值；根据二次误差度量模型,计算各条边进行折叠后新顶点的位置和边的折叠代价；建立边折叠操作表,表中数据为各条边在每一帧中的折叠代价之和乘以对应的边的权值；其中,所述根据边折叠操作表选取折叠代价最小的一条边进行折叠操作,重复进行折叠操作,直到达到简化要求的步骤,包括：选择折叠边,即边折叠操作表中值最小的数据所对应的边；对折叠边的两个顶点进行折叠操作；找出所有帧中折叠边所关联的两个三角形的顶点和边；进行边的更新操作,对更新的边计算折叠代价并更新边折叠操作表；当满足简化要求时,输出简化后的动态三维模型。</td>   <td>G06T17/20;G06T7/246</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘弘;                   张子臻       </td>   <td>中山大学</td>   <td>一种基于深度强化学习的动态路径优化问题求解方法</td>   <td>广东省</td>   <td>CN112116129A</td>   <td>2020-12-22</td>   <td>本发明公开了一种基于深度强化学习的动态路径优化问题求解方法,包括以下步骤：S1：动态路径优化问题定义；S2：构建深度强化学习框架,所述深度强化学习框架包括四个组成部分,分别为状态、智能体、动作和奖励,所述状态包括所有顾客及所有点对之间预计所需要的通行时间,所述智能体在不同状态下进行决策,得到对应的动作,所述动作为下一位访问的顾客,所述奖励为从仓库点出发,访问所有顾客后回到仓库点所需要的时间；S3：利用深度强化学习框架得出优化后的路径。本发明利用了深度强化学习算法,将动态路径优化问题的动态环境嵌入到模型中,使得模型能感知到环境的动态变化,从而使其在极短时间内得到一个较优的解。</td>   <td>1.一种基于深度强化学习的动态路径优化问题求解方法,其特征在于,包括以下步骤：S1：动态路径优化问题定义：在有向完全图G＝(V,E)上,其中V代表点集,包含了1个仓库点、c位需要服务的顾客,用集合C表示,和n-c-1个可能需要服务的顾客的地点；E代表边集,由于动态路径优化问题是一个非对称性问题,即不保证边集E中方向相反的边长度相等,求从仓库点出发,然后访问集合C中所有的顾客恰好一次,最后回到仓库点的最小时间；S2：构建深度强化学习框架,所述深度强化学习框架包括四个组成部分,分别为状态、智能体、动作和奖励,所述状态包括所有顾客及所有点对之间预计所需要的通行时间,所述智能体在不同状态下进行决策,得到对应的动作,所述动作为下一位访问的顾客,所述奖励为从仓库点出发,访问所有顾客后回到仓库点所需要的时间；S3：利用深度强化学习框架得出优化后的路径。</td>   <td>G06Q10/04;G06N20/00;G06Q10/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         付青;              朱坤;                   乔宇德       </td>   <td>中山大学</td>   <td>一种操作票自动生成方法、系统、装置及存储介质</td>   <td>广东省</td>   <td>CN112116308A</td>   <td>2020-12-22</td>   <td>本发明公开了一种操作票自动生成方法、系统、装置及存储介质,其中方法包括以下步骤：获取输入的操作目标任务后,获取多个实现所述操作目标任务的操作路径；根据预设策略和设备管理系统中记录的设备信息从所述多个操作路径中获取最佳路径,根据所述最佳路径生成操作票,以及更新所述设备信息；所述预设策略包括均匀使用策略或风险最低策略的至少之一。本发明在操作票的生成过程中,考虑操作票涉及的所有变电站设备充分的使用情况和运行情况,从而获得更加优化的操作路径,同时将使用情况反馈至设备管理系统,使设备管理系统中记录的信息更加完善,可广泛应用于电力系统自动化领域。</td>   <td>1.一种操作票自动生成方法,其特征在于,包括以下步骤：获取输入的操作目标任务后,获取多个实现所述操作目标任务的操作路径；根据预设策略和设备管理系统中记录的设备信息从所述多个操作路径中获取最佳路径,根据所述最佳路径生成操作票,以及更新所述设备信息；所述预设策略包括均匀使用策略或风险最低策略的至少之一。</td>   <td>G06Q10/10;G06Q10/04;G06Q10/06;G06Q50/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;              李家振;              张永东;                   杜云飞       </td>   <td>中山大学</td>   <td>一种基于CPU的生物分子可视化光线追踪渲染方法</td>   <td>广东省</td>   <td>CN112116693A</td>   <td>2020-12-22</td>   <td>本发明为生物分子可视化光线追踪渲染方法,构建三维场景、生物分子的空间填充表达模型；将模型实例化为可变换实例,将可变换实例绑定加入到三维场景中；通过光线从摄像机出发经过像素点射向三维场景中,若光线与原子没有相交则结束追踪,以背景颜色作为像素颜色；若光线与原子发生相交则计算生成反射光线,并设置多个光源分别从多个不同方向射向模型以继续追踪反射光线,若反射光线与多个光源中的光线相交则加入光照贡献值；最后采用渲染方程,根据原子自身颜色和光照度贡献值计算像素的颜色值。本发明使分子结构获得高质量渲染结果的同时实现实时交互帧率,可用于超级计算机上进行生物分子可视化工作。</td>   <td>1.一种基于CPU的生物分子可视化光线追踪渲染方法,其特征在于：构建三维场景,并根据原子信息构建生物分子的空间填充表达模型；将空间填充表达模型实例化为可变换实例,将可变换实例绑定加入到所构建的三维场景中；通过光线从摄像机出发经过像素点射向构建好的三维场景中,若光线与三维场景中的原子没有相交,则结束追踪,以背景颜色作为像素颜色；若光线与三维场景中的原子发生相交,则计算生成反射光线,并设置多个光源分别从多个不同方向射向生物分子的空间填充表达模型以继续追踪反射光线,若反射光线与所设置的多个光源中的光线相交,则加入光照贡献值,否则不加入光照贡献值；最后采用渲染方程,根据原子自身颜色和光照度贡献值计算像素的颜色值。</td>   <td>G06T15/06;G06T1/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苟超;                   卓莹       </td>   <td>中山大学</td>   <td>一种用于人脸关键点检测的主动形状模型参数化方法</td>   <td>广东省</td>   <td>CN112115845A</td>   <td>2020-12-22</td>   <td>本发明提供一种用于人脸关键点检测的主动形状模型参数化方法,该方法使用主动形状模型参数对人脸关键点位置进行编码,使得可在新的空间中预测人脸关键点的位置,对人脸关键点位置的求解从而变得紧凑；以级联回归方法、并利用局部特征来更新主动形状模型参数,不仅提高了运算效率和检测精度,还增强了模型的稳健性。本发明具备检测速度快、匹配精度高、稳健性强等优点,同时有效解决了主动形状模型等参数化方法存在的运算效率低下问题和级联回归等非参数化方法存在的过拟合问题,具备具有较高的实际应用价值。</td>   <td>1.一种用于人脸关键点检测的主动形状模型参数化方法,其特征在于,包括以下步骤：S1：利用人脸关键点数据集的样本数据初始化主动形状模型参数；S2：使用级联回归方法来更新主动形状模型参数,直至迭代次数达到最大取值；S3：对步骤S2中主动形状模型参数的最终结果进行解码,得到对应的人脸关键点位置。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王文宇;              赖韩江;                   潘炎       </td>   <td>中山大学</td>   <td>一种基于数据保护的图像增量学习方法</td>   <td>广东省</td>   <td>CN112115967A</td>   <td>2020-12-22</td>   <td>本发明提供一种基于数据保护的图像增量学习方法,该方法以深度卷积神经网络ResNet为基础,充分利用外部海量图像的信息,对其进行采样并加入训练过程,来缓解新旧样本不均衡所带来的偏差和灾难性遗忘,外部数据即采即用,训练后直接丢弃,不占用存储空间。同时加入针对于各个任务阶段的输出,提取关于任务的特征,提高模型的性能表现。本发明所提出的增量学习方法突破了传统方法的限制,能够灵活广泛地适应多种实际场景的需求,在计算机视觉领域具有重要的研究和应用价值。</td>   <td>1.一种基于数据保护的图像增量学习方法,其特征在于,包括以下步骤：S1：构造以ResNet网络为原型的图像特征提取器,然后添加全连接的任务预测层和图像分类层作为整体的网络架构；S2：为每个阶段的增量数据训练单独的图像分类模型,其中图像分类损失使用交叉熵函数,任务预测损失使用均方误差函数,使用SGD优化器训练网络；S3：对于不存储任何先前类别数据的场景,为避免灾难性遗忘和模型对于当前类别的预测偏向,使用先前模型对大量可用的外部数据进行采样；S4：使用采样数据以及当前类别的训练数据,对新旧两个模型进行融合,使用知识蒸馏引入KL相对熵函数,训练可识别当前所有类别的模型；S5：对于每一增量阶段,重复S2至S4步骤,评估模型时采用任务预测层和图像分类层输出结合的方式预测最终分类。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周凡;              苏卓;              林谋广;                   陈小燕       </td>   <td>中山大学</td>   <td>一种融合注意力机制的多帧视频超分辨率方法</td>   <td>广东省</td>   <td>CN111260560B</td>   <td>2020-12-22</td>   <td>本发明公开了一种融合注意力机制的多帧视频超分辨率方法,包括：采集视频数据并采用视频增强技术对视频数据进行训练以生成训练集及测试集；连接变形卷积特征对齐模块及特征重建模块以构成多帧超分辨率网络,采用训练集对多帧超分辨率网络进行训练；将3D卷积特征对齐模块加入多帧超分辨率网络中,采用训练集对多帧超分辨率网络进行训练；将特征融合模块加入多帧超分辨率网络中,采用训练集对多帧超分辨率网络进行训练；采用训练集对多帧超分辨率网络进行微调以生成多帧超分辨率模型；采用测试集对多帧超分辨率模型进行测试。本发明可通过对大数据的分析有效提高超分辨率效果。</td>   <td>1.一种融合注意力机制的多帧视频超分辨率方法,其特征在于,包括：S1,采集视频数据,并采用视频增强技术对所述视频数据进行训练以生成训练集及测试集；S2,构建变形卷积特征对齐模块及特征重建模块,并连接所述变形卷积特征对齐模块及特征重建模块以构成多帧超分辨率网络,采用所述训练集对所述多帧超分辨率网络进行训练；所述步骤S2包括：将EDVR模型前端的特征对齐模块作为变形卷积特征对齐模块,所述变形卷积特征模块包括多尺度特征提取单元、特征对齐单元及时序/空间融合单元；构建特征重建模块,所述特征重建模块包括多个加入空间及通道注意力机制的残差块；将所述变形卷积特征对齐模块与特征重建模块连接以构成多帧超分辨率网络；采用L1损失函数及所述训练集对所述多帧超分辨率网络进行训练；对所述变形卷积特征对齐模块的时序/空间融合单元进行微调；S3,构建3D卷积特征对齐模块,并将所述3D卷积特征对齐模块加入所述多帧超分辨率网络中,采用所述训练集对所述多帧超分辨率网络进行训练；所述步骤S3包括：构建3D卷积特征对齐模块,所述3D卷积特征对齐模块包括三个3D残差块,第一个3D残差块与第三个3D残差块之间通过相加进行短路连接,每个3D残差块均包括一个激活函数层及三个卷积层；将所述3D卷积特征对齐模块加入所述多帧超分辨率网络的头部,并将所述3D卷积特征对齐模块与特征重建模块连接；采用L1损失函数及所述训练集对所述3D卷积特征对齐模块进行训练；S4,构建特征融合模块,并将所述特征融合模块加入所述多帧超分辨率网络中,采用所述训练集对所述多帧超分辨率网络进行训练；所述步骤S4包括：构建特征融合模块,所述特征融合模块包括卷积层；将3D卷积特征对齐模块及变形卷积特征对齐模块输出的特征输入到所述特征融合模块进行训练；对所述特征融合模块及特征重建模块进行微调；S5,采用所述训练集对所述多帧超分辨率网络进行微调以生成多帧超分辨率模型；S6,采用所述测试集对所述多帧超分辨率模型进行测试。</td>   <td>G06T3/40;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李洽;                   彭振桓       </td>   <td>中山大学</td>   <td>基于多尺度密集混合注意力神经网络的图像去雨方法</td>   <td>广东省</td>   <td>CN112102176A</td>   <td>2020-12-18</td>   <td>本发明公开了一种基于多尺度密集混合注意力神经网络的图像去雨方法,包括以下步骤：构建多尺度密集混合注意力神经网络,包括顺序连接的进入模块、多个密集注意力模块、退出模块、全连接操作单元以及负残差还原操作单元,两两相邻的密集注意力模块之间设置一转接层；将含雨图像输入多尺度密集混合注意力神经网络,进行初始特征提取；密集注意力模块对初始特征处理；对密集注意力模块输出的特征经转接层变为和初始特征相同维度大小的特征；全连接操作与负残差还原操作,最后输出无雨清晰图像。基于本发明,可以充分地去除含雨图像中的雨纹,并且较好的保留图像中的背景信息。</td>   <td>1.基于多尺度密集混合注意力神经网络的图像去雨方法,其特征在于,包括以下步骤：构建多尺度密集混合注意力神经网络,所述多尺度密集混合注意力神经网络包括顺序连接的进入模块、多个密集注意力模块、退出模块,全连接操作单元以及负残差还原操作单元,两两相邻的密集注意力模块之间设置一转接层；所述密集注意力模块包括多个组合模块,每个组合模块包括密集层、选择核单元以及空间注意力块,并按照密集层、选择核单元和空间注意力块的顺序对输入特征依次进行处理,处理结果输入到下一个组合模块；将含雨图像输入多尺度密集混合注意力神经网络,进行初始特征提取；密集注意力模块对初始特征进行处理,所述初始特征经过密集层进行卷积、批标准化和ReLU操作之后,由选择核单元进行划分、融合和选择运算处理,划分运算生成通过不同卷积核大小的卷积操作获得的多分支特征,而不同卷积核大小对应不同的感受野大小,融合运算对多分支特征进行组合聚集,为动态的权重选择获得不同感受野组合的全局表示,选择运算根据权重选择动态自适应地组合聚集不同卷积核大小对应的多分支特征；经过选择核单元处理后特征再由空间注意力块获得的两个空间维度上像素值被权重校正的特征进行通道维度上的组合特征,再利用一个卷积核大小为1的卷积层对组合特征进行降维,获得与输入特征具有相同维度大小的输出特征；对密集注意力模块输出的特征经转接层变为和初始特征相同维度大小的特征,然后输入到下一个密集注意力模块,并重复执行密集注意力模块处理与改变特征维度大小步骤的操作,直至最后一个密集注意力模块完成处理,输出结果；将多个密集注意力模块的输入特征在通道维度上进行级联的全连接操作并输入到退出模块,退出模块输出结果与含雨图像进行相加的负残差还原操作,最后得到无雨清晰图像。</td>   <td>G06T5/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李洽;                   吴佳琪       </td>   <td>中山大学</td>   <td>基于压缩与激励机制神经网络的图像去模糊方法</td>   <td>广东省</td>   <td>CN112102177A</td>   <td>2020-12-18</td>   <td>本发明公开了一种基于压缩与激励机制神经网络的图像去模糊方法,包括以下步骤：获取训练网络所需数据集,所述数据集为多个图像对,每个图像对由模糊图像及其对应的清晰图像组成；构建压缩与激励去模糊网络,所述网络为多尺度网络,每一个尺度的构造相同,包括编码器-解码器结构以及ConvLSTM层；使用数据集对压缩与激励去模糊网络进行训练；使用训练完成的压缩与激励去模糊网络对模糊图像进行处理。本发明在SRN去模糊网络的基础上对特征处理模块中的残差块进行改进,引入压缩与激励机制,得到了应用于本发明网络的SE残差块,进而构成SEDN去模糊网络,使最终恢复的清晰图像质量上更加优秀。</td>   <td>1.基于压缩与激励机制神经网络的图像去模糊方法,其特征在于,包括以下步骤：获取训练压缩与激励机制神经网络所需数据集,所述数据集为多个图像对,每个图像对由模糊图像及其对应的清晰图像组成；构建压缩与激励去模糊神经网络,所述压缩与激励去模糊神经网络为多尺度网络,每一个尺度的构造相同,包括编码器-解码器结构以及ConvLSTM层；所述编码器-解码器结构包括用于从输入图像中抽取特征并处理的编码器部分、用于将复杂的特征复原为对应的清晰图像的解码器部分以及用于在不同的编码器-解码器层级上组合不同层级获取到的特征,对不同层级的特征加以复用的跳跃链接；所述ConvLSTM层设置在所述编码器部分与解码器部分之间,用于将编码器部分抽取与处理过的复杂特征进一步地在压缩与激励去模糊神经网络各尺度间传播；所述编码器部分包括多个顺序连接的Eblock；所述解码器部分包括多个顺序连接的Dblock；使用获取到的数据集对压缩与激励去模糊神经网络进行训练,根据数据集获取网络的输入图像,网络处理输入图像的正向过程,根据所有网络处理恢复结果,计算本次正向传播结果的损失并利用该损失更新压缩与激励去模糊神经网络中可训练参数的权重,反向更新梯度过程；使用训练完成的压缩与激励去模糊神经网络对模糊图像进行处理。</td>   <td>G06T5/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         粟涛;              周雨迪;                   陈弟虎       </td>   <td>中山大学</td>   <td>一种单阶段遥感图像目标检测算法</td>   <td>广东省</td>   <td>CN112102241A</td>   <td>2020-12-18</td>   <td>本发明公开了一种单阶段遥感图像目标检测算法,以Yolo v3为基准,在Yolo v3的特征提取网络中加入金字塔卷积,将Yolo v3的检测网络替换为路径聚合网络,并改进所述路径聚合网络的上采样方式为转置卷积,最后在所述特征提取网络及所述检测网络之间加入空间金字塔池化作为中间连接。本发明的单阶段遥感图像目标检测算法与Yolo v3相比,检测速度基本没有影响,有效提高了检测精度。</td>   <td>1.一种单阶段遥感图像目标检测算法,其特征在于：以Yolo v3为基准,在Yolo v3的特征提取网络中加入金字塔卷积,将Yolo v3的检测网络替换为路径聚合网络(PAN网络),并改进所述路径聚合网络的上采样方式为转置卷积,最后在所述特征提取网络及所述检测网络之间加入空间金字塔池化作为中间连接。</td>   <td>G06T7/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄方军;                   万晨       </td>   <td>中山大学</td>   <td>基于PID控制器的对抗性攻击与防御方法及系统</td>   <td>广东省</td>   <td>CN112085050A</td>   <td>2020-12-15</td>   <td>本发明基于PID控制器的对抗性攻击与防御方法及系统,其方法包括步骤：S1、输入训练数据集和机器学习模型f；S2、根据输入的训练数据集训练机器学习模型f；S3、判断损失函数J是否收敛,若损失函数J不收敛,则采用基于PID控制器的对抗性攻击生成对抗样本x<sup>adv</sup>和原始数据x作为训练数据集对机器学习模型f进行训练,直至损失函数J收敛,得到训练好的机器学习模型f,若损失函数J收敛,则直接输出结果。本发明通过对抗攻击生成对抗样本的过程,能够在相同的扰动约束限制下,实现更高的攻击成功率,可用于评估机器学习模型的性能以及对抗防御方法的有效性；使用对抗攻击所产生的对抗样本对机器学习模型进行对抗训练可作为一种防御方法,以提升模型的鲁棒性。</td>   <td>1.基于PID控制器的对抗性攻击与防御方法,其特征在于,包括以下步骤：S1、输入训练数据集和机器学习模型f；S2、根据输入的训练数据集训练机器学习模型f；S3、判断损失函数J是否收敛,如果损失函数J不收敛,则采用基于PID控制器的对抗性攻击生成对抗样本x~(adv)和原始数据x作为训练数据集对机器学习模型f进行训练,直至损失函数J收敛,得到训练好的机器学习模型f,如果损失函数J收敛,则直接输出结果。</td>   <td>G06K9/62;G06N20/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨耀根;                   张东       </td>   <td>中山大学</td>   <td>一种基于双声纹特征向量和序列到序列建模的多对多语音转换方法</td>   <td>广东省</td>   <td>CN112071325A</td>   <td>2020-12-11</td>   <td>本发明涉及语音合成,语音转换领域,更具体地,涉及一种基于双声纹特征向量和序列到序列建模的多对多语音转换方法。本发明用多说话人的语音合成技术生成大量的平行预料,这为模型的训练提供了极大的方便。然后使用序列到序列的神经网络对输入的源说话人特征建模映射到目标说话人特征。为了实现多对多的语音转换,本发明使用说话人验证任务的模型产生表征说话人身份的声纹特征向量。源说话人和目标说话人的声纹特征向量作为辅助信心加入序列到序列的模型中去。经过模型训练测试,本发明能够取得不错的效果。</td>   <td>1.一种基于双声纹特征向量和序列到序列建模的多对多语音转换方法,其特征在于,包括以下步骤：S1.数据增强：采用文字到语音的多说话人语音合成模块生成平行语料；S2.语音信号的特征提取：对于生成的平行语料,进行提取原音频和目标音频的声学特征；S3.对说话人的身份特征进行编码,得到代表说话人身份的声纹特征向量；S4.利用序列到序列的语音转换模型对步骤S2的声学特征和步骤S3的声纹特征向量进行训练,序列到序列的语音转换模型采用的是编码器和解码器的神经网络进行训练的,在训练阶段,语音转换模型的输入是步骤S2提取的源说话人语音声学特征和步骤S3提取的目标说话人身份编码声纹特征向量,输出的是目标说话人的mel谱,最终让语音转换模型学会源说话人声学特征映射到目标说话人的声学特征；在测试阶段,直接输入源说话人的mel谱和任意目标说话人声纹特征向量,语音转换模型自动转换成任意目标说话人的mel谱；S5.采用声码器模块对步骤S3的mel谱进行转换,输出重构语音的时域波形。</td>   <td>G10L21/003;G10L21/007;G10L17/02;G10L15/02;G10L15/16;G10L25/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王剑波;              李伟忠;              王文军;                   张宁锋       </td>   <td>中山大学</td>   <td>胚胎光镜图像中细胞的识别方法及系统、设备及存储介质</td>   <td>广东省</td>   <td>CN112069874A</td>   <td>2020-12-11</td>   <td>本发明公开了一种胚胎光镜图像中细胞的识别方法,包括：对胚胎光镜图片进行预处理；对预处理后的胚胎光镜图片进行标注处理；将经标注处理后的胚胎光镜图片输入事先训练的FasterRCNN识别模型以生成细胞预测结果,所述FasterRCNN识别模型包括特征提取网络、RPN网络、Roi Align网络、分类回归网络及C-NMS网络；根据所述细胞预测结果进行细胞识别。本发明还公开了一种胚胎光镜图像中细胞的识别系统、一种计算机设备及一种计算机可读存储介质。采用本发明,通过对FasterRCNN网络的深度优化,实现了胚胎光镜图片中细胞的精准提取,同时,本发明还构建了全新的C-NMS网络,通过对被检测物体之间的重叠比例及面积比例的检测,灵活地调整检测分数,显著地降低了漏检率。</td>   <td>1.一种胚胎光镜图像中细胞的识别方法,其特征在于,包括：对胚胎光镜图片进行预处理；对预处理后的胚胎光镜图片进行标注处理；将经标注处理后的胚胎光镜图片输入事先训练的FasterRCNN识别模型以生成细胞预测结果,所述FasterRCNN识别模型包括特征提取网络、RPN网络、RoiAlign网络、分类回归网络及C-NMS网络；根据所述细胞预测结果进行细胞识别。</td>   <td>G06K9/00;G06K9/32;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡天江;              李铭慧;              郑勋臣;              潘亮;                   王勇       </td>   <td>中山大学</td>   <td>一种基于DenseHR-Net的无人机自主着陆目标提取方法及装置</td>   <td>广东省</td>   <td>CN112069997A</td>   <td>2020-12-11</td>   <td>本发明公开了一种基于DenseHR-Net的无人机自主着陆目标提取方法及装置,所述方法包括：通过地面摄像机拍摄无人机的RGB三通道图像后进行图像预处理,得到统一标准尺寸的RGB图像；通过搭建DenseHR-Net目标检测网络模型对RGB图像进行无人机的关键区域检测定位,识别图像中的若干个关键区域的最小外接矩形检测框；根据预设的优先级策略,从图像中的若干个关键区域选取其中一个作为关键点坐标区域,提取出关键点坐标区域的最小外接矩形检测框的中心点坐标,作为无人机着陆的关键点坐标。本发明采用深度学习网络DenseHR-Net对无人机各位置进行目标检测,能够有效提取无人机自主着陆目标,提高无人机各位置的定位精度,同时避免出现机头误检或漏检的情况,并增强了检测算法的鲁棒性。</td>   <td>1.一种基于DenseHR-Net的无人机自主着陆目标提取方法,其特征在于,至少包括如下步骤：在无人机着陆过程中,通过地面摄像机拍摄无人机的RGB三通道图像后进行图像预处理,得到统一标准尺寸的RGB图像；通过搭建DenseHR-Net目标检测网络模型对所述RGB图像进行无人机的关键区域检测定位,识别出所述RGB图像中的若干个关键区域的最小外接矩形检测框；其中,所述关键区域包括无人机机头、无人机机翼和无人机机体；根据预设的优先级策略,从所述RGB图像中的若干个关键区域选取其中一个作为关键点坐标区域,提取出所述关键点坐标区域的最小外接矩形检测框的中心点坐标,作为无人机着陆的关键点坐标。</td>   <td>G06K9/00;G06K9/32;G06K9/46;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡天江;              李铭慧;              郑勋臣;              张嘉榕;                   朱波       </td>   <td>中山大学</td>   <td>基于双级联深度网络的无人机多特征点检测方法及装置</td>   <td>广东省</td>   <td>CN112070085A</td>   <td>2020-12-11</td>   <td>本发明公开了一种基于双级联深度网络的无人机多特征点检测方法及装置。所述方法包括：在采集的每一无人机图像中对无人机的多个特征区域进行类别及边界框标注,得到对应的训练图像；将每一所述训练图像输入预先构建的边界框定位网络进行训练,使所述边界框定位网络输出多个特征区域预测框；根据每一所述特征区域预测框提取对应的感兴趣区域,将所有所述感兴趣区域输入预先构建的特征点回归网络进行训练；通过由训练后的所述边界框定位网络和所述特征点回归网络组成的双级联深度网络对待检测图像进行多特征点检测,得到多个特征点坐标。本发明能够实时稳定且准确地检测无人机的多个特征点。</td>   <td>1.一种基于双级联深度网络的无人机多特征点检测方法,其特征在于,包括：在采集的每一无人机图像中对无人机的多个特征区域进行类别及边界框标注,得到对应的训练图像；将每一所述训练图像输入预先构建的边界框定位网络进行训练,使所述边界框定位网络输出多个特征区域预测框；根据每一所述特征区域预测框提取对应的感兴趣区域,将所有所述感兴趣区域输入预先构建的特征点回归网络进行训练；通过由训练后的所述边界框定位网络和所述特征点回归网络组成的双级联深度网络对待检测图像进行多特征点检测,得到多个特征点坐标。</td>   <td>G06K9/32;G06K9/46;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         况丽娟;                   戴宪华       </td>   <td>中山大学</td>   <td>一种引入attention机制的多尺度目标检测方法</td>   <td>广东省</td>   <td>CN112070713A</td>   <td>2020-12-11</td>   <td>本发明涉及一种引入attention机制的多尺度检测技术,涉及图像处理领域,该方法包括采集待测图像,将待测图像导入attention yolo-v3,attention yolo-v3是在yolo-v3基础上进行扩展的,添加一个具有通道注意力机制的SENet,使用现有的检测对目标进行并行预测。将待测图片输入到一个预训练好的神经网络中(darknet-53+FPN)获得三种尺度的feature map；通过聚类得到三种尺度共九种先验框anchor boxs；feature map网格中的每一点会生成三种候选框bbox；计算预测框与真实框的IOU,为真实框分配一个最佳匹配；再将这些候选的bbox进行分类和边框(BBox)回归,最后nms过滤掉一部分候选的bbox。加入注意力机制训练使得其可以获得高准确率,最后还利用了多尺度训练以及多尺度测试提高了检测精度。</td>   <td>1.一种引入attention机制的多尺度目标检测方法,其特征在于,该attention yolo-v3包括yolo-v3模型的骨架卷积神经网络darknet-53、具有通道注意力机制的SENet、特征金字塔网络、分类器,所述方法包括：采集待测图像,将待测图像输入darknet-53网络,该网络大量使用3*3与1*1卷积层依次连接的形式,并且添加了shortcut连接,其网络结构复杂,有53个卷积层。在darknet-53前向传播过程中加入具有通道注意力机制的SENet。步骤是先对H′×W′×C′的特征输入X进行卷积操作,得到待处理的W×H×C的特征图U.接着对得到的U分别进行Fsq和Fscale.Fsq操作：对于每一个通道进行全局平均池化,得到一个1×1×C的特征图z。计算公式为：其中,U_c表示的是U的第c个通道。Fscale操作：将z作为一个全连接神经网络的输入,该神经网络的权重为W。Fex(*,W)表示的是一个全连接层的计算过程。得到不同通道特征图的权重系数s,再通过与U对应通道上的特征图进行相乘,以此来表示不同通道的特征图的重要性程度。具体计算如下式所示：s＝F_(ex)(z,W)X_c＝F_(scale)(u_c,s_c)＝s_c·u_c在darknet-53卷积结果中取最顶层的特征,自顶向下的采用上采样进行,每进行一次up-sample时,输出特征层扩大一倍。而横向连接则是将上采样的结果和自底向上生成的相同大小的feature map进行融合,从而形成FPN特征金字塔网络结构；在融合之后还会再采用3*3的卷积核对每个融合结果进行卷积,目的是用于消除上采样的混叠效应；采用k-means聚类得到三种尺度共9种尺寸先验框,COCO数据集上在最小的13*13特征图上(有最大的感受野)用较大的先验框(116*90),(156*198),(373*326),中等的26*26特征图上(中等感受野)应用中等的先验框(30x61),(62*45),(59*119),较大的52*52特征图上(较小的感受野)应用较小的先验框(10*13),(16*30),(33*23)。</td>   <td>G06T7/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              肖逢枝;              谢舜道;              陈荣军;              朱雄泳;                   曾衍瀚       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种二维码的二次遍历二值化方法、装置和存储介质</td>   <td>广东省</td>   <td>CN109785353B</td>   <td>2020-12-08</td>   <td>本发明公开了一种二维码的二次遍历二值化方法、装置和存储介质。在获取到原始图像后进行预处理获得输入图像和积分图像,结合积分图像对输入图像进行横向扫描遍历,得出粗定位子图和粗定位参数,再根据粗定位参数对粗定位子图遍历时进行二值化,得出二值化结果图。本发明的方法仅执行了两次遍历,无需对二值图进行对此遍历,大大减少了计算量,加快了计算效率,从而实现了模糊二维码的快速识别。</td>   <td>1.一种二维码的二次遍历二值化方法,其特征在于,包括以下步骤：获取原始图像,对原始图像进行预处理,得出输入图像和对应的积分图像；获取预先设定的扫描宽度,根据积分图像和扫描宽度对所述输入图像进行横向扫描遍历,得出粗定位子图和粗定位参数；根据粗定位参数对所述粗定位子图进行遍历并二值化,得出二值化结果图,所述粗定位参数包括像素位宽和局部二值化的窗口尺寸；所述根据粗定位参数对所述粗定位子图进行遍历并二值化具体包括以下步骤：获取遍历所至粗定位子图的当前像素的局部二值化的窗口尺寸；对当前像素进行局部均值和局部偏差值的计算,并根据局部均值和局部偏差值计算出局部二值化阈值；当检测到当前像素的局部二值化阈值小于或等于输入图像中对应的灰度值时,二值化结果设置为1；其中,所述对当前像素进行局部均值和局部偏差值的计算由以下公式完成：                                    其中,m(x,y)为每个像素所在的kω*kω的方形窗口的局部均值,为每个像素所在的kω*kω的方形窗口的局部偏差,I(x,y)为积分图像；其中,所述根据局部均值和局部偏差值计算出局部二值化阈值由以下公式得出：                  其中T(x,y)为局部二值化阈值,k_1、k_2及C为预先设定的偏置系数。</td>   <td>G06T7/136</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林浩添;              李永浩;              冯伟渤;              赵兰琴;                   郭翀       </td>   <td>中山大学中山眼科中心</td>   <td>一种识别高度近视视网膜病变的人工智能系统</td>   <td>广东省</td>   <td>CN112053321A</td>   <td>2020-12-08</td>   <td>本发明涉及医疗图像处理技术领域,尤其涉及一种识别高度近视视网膜病变的人工智能系统,包括：图像获取模块,用于获取待识别患者的光学相干断层扫描图像和眼底参照；第一识别模块,用于将光学相干断层扫描图像输入到高度近视视网膜病变识别模型中,判断所述光学相干断层图像是否出现病变,并得到病变类型结果；第二识别模块,用于将眼底彩照输入到高度近视视网膜病变分期模型中,判断所述眼底彩照的视网膜病变分期并得到病变分期判断结果；报告生成模块,用于根据病变类型结果和病变分期判断结果,生成诊疗建议报告。本发明能够快速、有效地识别患者的光学相干断层扫描图像和眼底彩照来进行高度近视视网膜常见病变的识别。</td>   <td>1.一种识别高度近视视网膜病变的人工智能系统,其特征在于,包括：图像获取模块,用于获取待识别患者的光学相干断层扫描图像和眼底彩照；第一识别模块,用于将光学相干断层扫描图像输入到高度近视视网膜病变识别模型中,识别所述光学相干断层图像是否出现病变,并得到病变类型结果；第二识别模块,用于将眼底彩照输入到高度近视视网膜病变分期模型中,判断所述眼底彩照的视网膜病变分期,并得到病变分期判断结果；报告生成模块,用于根据所述病变类型结果和所述病变分期判断结果,生成诊疗建议报告。</td>   <td>G06T7/00;G06K9/62;G06N3/04;G06N3/08;A61B3/10;A61B3/12;A61B3/14</td>  </tr>        <tr>   <td>中国专利</td>   <td>              周凡       </td>   <td>中山大学深圳研究院</td>   <td>视觉跟踪方法、视频监控方法及终端设备</td>   <td>广东省</td>   <td>CN112036381A</td>   <td>2020-12-04</td>   <td>本申请适用于人工智能技术领域,提供了视觉跟踪方法、视频监控方法及终端设备,利用目标滤波器对当前视频帧图像的第一图像特征进行滤波,得到响应输出矩阵,以当前视频帧图像作为训练样本训练初始滤波器,以使滤波器充分提取图像特征,从而使视觉跟踪模型在多种视频图像中均能够高效实现目标跟踪,提高视觉跟踪模型的鲁棒性。以及根据响应输出矩阵,确定跟踪目标在当前视频帧图像中的实际位置,使得跟踪效果更好。</td>   <td>1.一种视觉跟踪方法,其特征在于,包括：利用目标滤波器对当前视频帧图像的第一图像特征进行滤波,得到响应输出矩阵,所述目标滤波器利用所述当前视频帧图像的上一视频帧图像经过初等变换后得到的多个训练样本进行训练得到；根据所述响应输出矩阵,确定跟踪目标在当前视频帧图像中的实际位置。</td>   <td>G06K9/00;G06K9/46</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赵有婷;              余志;              何兆成;                   李熙莹       </td>   <td>中山大学</td>   <td>一种基于车牌图像属性标定的车牌识别一体机评测方法及装置</td>   <td>广东省</td>   <td>CN112036401A</td>   <td>2020-12-04</td>   <td>为了更好的车牌识别一体机的性能进行评估,本发明公开一种基于车牌图像属性标定的车牌识别一体机评测方法,包括以下步骤：根据场景单元的参数,评测单元进行第一次参数调整；第一次参数调整后的评测单元,根据场景单元的动态的标准车牌视频进行第二次参数调整；第二次参数调整后的评测单元,根据场景单元的动态的车牌视频进行评测,得到评测结果。本发明公开一种基于车牌图像属性标定的车牌识别一体机评测装置,包括场景单元和评测单元。本发明能够有效结合现场测试和图像库测试的优势,进行动态捕抓测试,避免现有测试方式的缺陷。</td>   <td>1.一种基于车牌图像属性标定的车牌识别一体机评测方法,其特征在于,包括以下步骤：根据场景单元的参数,评测单元进行第一次参数调整；第一次参数调整后的评测单元,根据场景单元的动态的标准车牌视频进行第二次参数调整；第二次参数调整后的评测单元,根据场景单元的动态的车牌视频进行评测,得到评测结果。</td>   <td>G06K9/32;G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>              林子衡       </td>   <td>中山大学</td>   <td>股票收益排名预测方法、系统、储存介质及计算机设备</td>   <td>广东省</td>   <td>CN112036620A</td>   <td>2020-12-04</td>   <td>本发明提出了一种股票收益排名预测方法、系统、储存介质及计算机设备,方法部分包括以下步骤：根据预设的股票池和因子池,获取股市的历史数据,对所述历史数据进行预处理得到按预设的交易周期划分的训练数据以及预测数据；根据所述训练数据,获取所述因子池中各因子的收益评价；根据所述收益评价的大小,筛选出预设的前若干个因子,以所述前若干个因子中所述收益评价为正的部分作为选用因子；根据所述选用因子,构建预测模型；通过所述训练数据对所述预测模型进行差分进化迭代；根据所述预测数据,通过所述差分进化迭代后的预测模型获取所述股票池的股票收益预测排名。</td>   <td>1.一种股票收益排名预测方法,其特征在于,包括以下步骤：根据预设的股票池和因子池,获取股市的历史数据,对所述历史数据进行预处理得到按预设的交易周期划分的训练数据以及预测数据；根据所述训练数据,获取所述因子池中各因子的收益评价；根据所述收益评价的大小,筛选出预设的前若干个因子,以所述前若干个因子中所述收益评价为正的部分作为选用因子；根据所述选用因子,构建预测模型；通过所述训练数据对所述预测模型进行差分进化迭代；根据所述预测数据,通过所述差分进化迭代后的预测模型获取所述股票池的股票收益预测排名。</td>   <td>G06Q10/04;G06Q30/02;G06Q40/04;G06Q40/06;G06N20/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         宋婷婷;              刘明林;              骆伟祺;                   郑培嘉       </td>   <td>中山大学</td>   <td>一种图像的空域隐写增强方法及装置</td>   <td>广东省</td>   <td>CN112037113A</td>   <td>2020-12-04</td>   <td>本发明公开一种图像的空域隐写增强方法及装置,通过根据隐写分析网络的损失函数对测试原图进行求导,以获得测试原图对应的梯度图；其中,隐写分析网络根据训练原图集及其经过初始化隐写后得到的载密训练图像集训练而成；然后对测试原图进行初始化隐写,以获得测试原图的初始代价图；以及,根据梯度图、多个预设参数和预设更新强度,对初始代价图的代价进行更新以获得多个目标代价图；最后按照多个目标代价图对测试原图进行隐写,以获得多个候选载密图像,并从多个候选载密图像中确定出一个候选载密图像作为目标载密图像,从而能够提升安全性能。</td>   <td>1.一种图像的空域隐写增强方法,其特征在于,包括以下步骤：S1：根据隐写分析网络的损失函数对测试原图进行求导,以获得所述测试原图对应的梯度图；其中,所述隐写分析网络根据训练原图集及其经过初始化隐写后得到的载密训练图像集训练而成；S2：对所述测试原图进行初始化隐写,以获得所述测试原图的初始代价图；S3：根据所述梯度图、多个预设参数和预设更新强度,对所述初始代价图的代价进行更新以获得多个目标代价图；每一个所述目标代价图对应一个所述预设参数；S4：按照所述多个目标代价图对所述测试原图进行隐写,以获得多个候选载密图像；所述候选载密图像与所述目标代价图一一对应；S5：从所述多个候选载密图像中确定出一个候选载密图像作为目标载密图像。</td>   <td>G06T1/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         阮文俊;                   翁安林       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种超市路径推荐系统及其方法</td>   <td>广东省</td>   <td>CN108416611B</td>   <td>2020-12-04</td>   <td>本发明公开了一种超市路径推荐方法,先获取用户行走路径以及与用户行走路径对应的购买清单,采用谱聚类算法对用户行走路径进行聚类后得到簇划分集,根据簇划分集计算得到所有簇类的商品概率分布及平均簇类的商品概率分布,当用户购物时,判断用户所属的簇类,根据对应簇类的商品概率分布计算出所有的路径以及路径上的商品概率分布,最后通过A*算法计算得到一条最符合用户需求的最短路径,该路径上的商品会引起用户的兴趣,路径距离也相对较短；本发明的一种超市路径推荐系统,包括智能终端和推荐系统模块,推荐系统模块将推荐的路径信息显示在智能终端为用户导航,本发明可以为用户提供最佳的路径,提高用户体验,刺激消费。</td>   <td>1.一种超市路径推荐方法,其特征在于：包括以下步骤：A、获取数据,其中数据包括用户行走路径以及与用户行走路径一一对应的购买清单；B、使用谱聚类算法对用户行走路径进行聚类,并得到簇划分集C、根据簇划分集计算其所有簇类的所有的商品概率分布以及平均簇类的商品概率分布；D、根据用户的当前状态判断其所属的簇类,并计算得到不同路径下所有已抵达的商品概率总和；E、采用A*算法为用户推荐一条路径上商品概率总和最大,并且路径长度相对较小的路径；其中,所述步骤B的具体步骤为：B1、将每一条用户行走路径看做一个数据点,计算数据点与数据点之间的Levenshtein距离；B2、将数据点之间的Levenshtein距离转化成相对距离,并以此生成相似矩阵W,同时构建度矩阵D；B3、根据相似矩阵W以及度矩阵D计算得到拉普拉斯矩阵L,L＝D-W；B4、构建标准化后的拉普拉斯矩阵L_(norm)＝D~(-1/2)LD~(-1/2)；B5、计算D~(-1/2)LD~(-1/2)最小的k_1个特征值各自所对应的特征向量f,将各自对应的特征向量f组成的矩阵按行标准化,最终组成n*k_1维的特征矩阵F；B6、对特征矩阵F中的每一行作为一个k_1维的样本,共n个样本,用K-means聚类方法进行聚类,聚类维数为k_2；B7、得到簇划分集C所述步骤C的具体步骤为：从簇划分集中取得簇类c_k,遍历簇类c_k的所有数据点即所有用户行走路径,从而得到簇类c_k中商品出现频率以及商品出现总次数,簇类c_k的商品概率分布等于各商品出现频率除以总次数；同时计算通过聚类路径簇类下的平均簇类,得到平均簇类的商品概率分布,其中k＝1,2···k_2,k_2为聚类维数。</td>   <td>G06Q30/02;G06Q30/06;G06Q10/04;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赵小蕾;              衣杨;              刘东琳;              张海;              曾青青;              陈怡华;                   刘利洲       </td>   <td>中山大学新华学院</td>   <td>一种无线充电与智能点餐装置</td>   <td>广东省</td>   <td>CN212061240U</td>   <td>2020-12-01</td>   <td>本实用新型公开了一种无线充电与智能点餐装置,包括安装板、显示板、二维码和点餐显示屏,所述安装板的内部设置有装置主体,且装置主体一侧的中间位置活动连接有螺旋杆,所述装置主体的中间位置活动连接有显示板,且装置主体一侧的底端设置有二维码,所述显示板的一侧设置有活动轴,且显示板顶端一侧的中间位置设置有无线充电放置台。本实用新型通过在无线充电放置台的两侧设置的连接板,可以推动连接板在空心槽内部滑动,并使连接板可以挤压弹簧,此时可以将电子设备放置在无线充电放置台上,此时连接板会受到弹簧的弹力带动橡胶块将电子设备夹紧,从而可以对充电的电子设备进行限位,避免了电子设备从无线充电放置台上面充电时掉落。</td>   <td>1.一种无线充电与智能点餐装置,包括安装板(1)、显示板(3)、二维码(4)和点餐显示屏(6),其特征在于：所述安装板(1)的内部设置有装置主体(2),且装置主体(2)一侧的中间位置活动连接有螺旋杆(102),所述装置主体(2)的中间位置活动连接有显示板(3),且装置主体(2)一侧的底端设置有二维码(4),所述显示板(3)顶端的一侧设置有点餐显示屏(6),且显示板(3)的底端活动连接有支杆(302),所述显示板(3)的一侧设置有活动轴(301),且显示板(3)顶端一侧的中间位置设置有无线充电放置台(307),所述装置主体(2)内部的顶端设置有安置槽(201),且装置主体(2)底端的一侧安装有单片机(5)。</td>   <td>G06Q50/12;H02J7/00;H02J50/00;F16M13/02;F16M11/04;F16M11/10;F16M11/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李冠彬;              刘梦梦;              刘凌波;                   林倞       </td>   <td>中山大学</td>   <td>一种领域适配的跨城市交通流量超分辨率重建方法及系统</td>   <td>广东省</td>   <td>CN112017118A</td>   <td>2020-12-01</td>   <td>本发明公开了一种领域适配的跨城市交通流量超分辨率重建方法及系统,所述方法包括：获取数据,并构建数据集；根据源领域城市区域的低分辨率交通流量图以及高分辨率交通流量图,采用监督学习的方式对预设的超分辨率网络进行预训练,获得优化的超分辨率网络；构建一对孪生优化的超分辨率网络；并采用领域分类器进行对抗训练,并同时更新网络参数；获得一对训练后的孪生超分辨率网络；将所述数据集输入到所述一对训练后的孪生超分辨率网络；获得目标领域城市高分辨交通流量图。本发明能够通过领域适配的方法使得两个不同领域(城市)的流量特征分布相近,从而实现对目标领域(城市)的超分辨率重建,降低对城市交通流量数据量的需求。</td>   <td>1.一种领域适配的跨城市交通流量超分辨率重建方法,其特征在于,包括：获取数据,并构建数据集；其中,所述数据包括：源领域城市区域的低分辨率交通流量图、高分辨率交通流量图以及目标领域城市的低分辨率交通流量图；根据所述源领域城市区域的低分辨率交通流量图以及高分辨率交通流量图,采用监督学习的方式对预设的超分辨率网络进行预训练,获得优化的超分辨率网络；构建一对孪生优化的超分辨率网络,包括超分辨率网络1和超分辨率网络2；并采用领域分类器进行对抗训练,并同时更新网络参数；获得一对训练后的孪生超分辨率网络；其中,所述一对训练后的孪生超分辨率同时更新,共享参数和权重；将所述数据集输入到所述一对训练后的孪生超分辨率网络；获得目标领域城市高分辨交通流量图。</td>   <td>G06T3/40;G06K9/62;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赵铜铁钢;                   杨振华       </td>   <td>中山大学</td>   <td>面向数字高程模型的任意断面集水区边界与河网提取方法</td>   <td>广东省</td>   <td>CN112017282A</td>   <td>2020-12-01</td>   <td>本发明提供的一种面向数字高程模型的任意断面集水区边界与河网提取方法,通过考虑断面所在空间网格单元与其他网格单元的流向隶属关系,划分断面上游和空间网格边界,在剔除目标水域覆盖区的基础上,以此为边界提取集水区河网系,能根据认为设定的河流、湖泊、水库目标水域断面,灵活地提取断面上游的集水边界和河网,明确断面水文影响方位；同时借助Python3语言的开发环境,代码结构简单,操作方便,是自动化提取指点断面集水区河网水系,划分集水单元的一种有效方法。</td>   <td>1.面向数字高程模型的任意断面集水区边界与河网提取方法,其特征在于,包括：S1：将栅格DEM文件.GIFf和矢量shape文件.shp为输入数据,利用开源的python语言包库分别读取DEM数据和矢量计算区域矩形边界以及目标水域数据；S2：统一输入数据的坐标系统,对所有输入数据设置成WGS84地理坐标系；S3：读取目标水域数据的矩形边界后设置缓冲区距离,得到目标水域粗略的集水区范围；S4：采用格网化方法对其进行填洼计算、流向计算、累积流量计算过程,设置流量阈值,提取完整的水系和各集水单元边界,初步计算出流域水系栅格,并划分出多个集水单元；S5：通过流域水系栅格、集水单元和目标水域进行空间叠加与融合,将目标水域属性赋给重叠的栅格水系,依据属性字段将目标水域剔除,以生成不包含目标水域的水系和集水单元；S6：利用不含目标水域的栅格河网、集水单元与指定断面上游集水区进行空间裁剪,最终得到准确的指定断面集水单元和河网水系；S7：将指定断面集水单元和河网水系进行制图输出,实现对集水区河网、边界特征提取的处理。</td>   <td>G06T17/05;G06T17/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李冠彬;              吴捷;                   陈宇洋       </td>   <td>中山大学</td>   <td>一种视频异常事件检测方法</td>   <td>广东省</td>   <td>CN112016403A</td>   <td>2020-12-01</td>   <td>本发明公开了一种视频异常事件检测方法,使用了管道-时间双支结构,在不同的粒度上反映了视频中的信息,一个分支可以把学习到的知识分享给另外一个分支,充当额外的监督作用,鼓励另外一个分支从不同的粒度学习异常事件的特征；从而减轻对人力资源依赖,提高检测效率,同时在只有时序标签的数据集上,利用不同粒度的信息,检测出视频中异常事件发生的事件和区域,并且探索区域之间的关系,提升准确率。</td>   <td>1.一种视频异常事件检测方法,其特征在于,包括：获取视频源文件,分别对所述视频源文件进行管道级别实例抽取和视频级别实例抽取,得到管道级别实例和视频级别实例；分别对所述管道级别实例和视频级别实例进行特征提取,得到管道级别特征和视频级别特征；分别将所述管道级别特征和视频级别特征输入至各自对应的关系建模中进行特征处理,得到管道高级特征和视频高级特征；分别将所述管道高级特征和视频高级特征输入至各自对应的全连接神经网络进行异常预测,得到管道预测数值和视频预测数值；根据所述管道预测数值和视频预测数值计算得到异常事件预测分数。</td>   <td>G06K9/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赵铜铁钢;                   陈浩玲       </td>   <td>中山大学</td>   <td>一种面向遥相关模式的空间自相关聚类方法</td>   <td>广东省</td>   <td>CN112016588A</td>   <td>2020-12-01</td>   <td>本发明提供了一种面向遥相关模式的空间自相关聚类方法,其通过考虑每一个空间网格单元与相邻单元遥相关的相关程度,在局部Moran指数定义的基础上,采用相关系数原始数值而不作中心化处理,从而改进局部Moran指数计算公式,得到新的空间自相关局部指标LISAAC,实现显著正或负遥相关集聚范围的探测,同时实现异常值(即显著正值区域中出现不显著或负值网格,显著负值区域中出现不显著或正值网格)的识别。实验结果表明,本发明能根据遥相关系数本身的标准化性质,实现对不同类型遥相关的空间聚类,结果便于对不同季节的遥相关程度进行横向对比。</td>   <td>1.一种面向遥相关模式的空间自相关聚类方法,其特征在于,包括以下步骤：S1：获取研究区域空间网格坐标信息,依据坐标信息计算空间权重矩阵；S2：获取研究区域网格尺度降雨数据及相同时间范围的大尺度气象因子指标,得到降雨-气象指标时间序列；S3：依据所获得的降雨-气象指标时间序列,逐网格计算降雨-气象指标相关系数r；S4：根据相关系数r及空间权重矩阵,计算逐网格的空间自相关局部指标LISAAC；S5：重排步骤S2中的降雨及气象指标时间序列,获得新的降雨-气象指标时间序列,重复执行步骤S3-S4,直到达到预设的迭代次数n；S6：根据步骤S4得到的n组随机LISAAC,构建参考经验分布H；S7：根据观测LISAAC、观测相关系数r以及经验分布H,得到在指定显著性水平下各网格的分类结果。</td>   <td>G06K9/62;G06Q10/06;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李冠彬;                   赵赣龙       </td>   <td>中山大学</td>   <td>一种基于领域自适应的协同训练方法</td>   <td>广东省</td>   <td>CN112016594A</td>   <td>2020-12-01</td>   <td>本发明公开了一种基于领域自适应的协同训练方法,利用其中一方的高置信度输出对另一方进行训练,对两方输出置信度均低的候选区域利用最大化分类器差异方法进行处理；另外,在骨干网络的特征对齐上,利用了RPN的输出,用以计算特征图上每个点的前景概率,对前景概率较大的区域,在特征对齐时给予更大的权重；实现提高模型在无标签领域上的目标检测能力,降低目标检测模型对标注数据的需求,减轻对人力资源的依赖。</td>   <td>1.一种基于领域自适应的协同训练方法,其特征在于,包括：在对Faster RCNN模型进行每一次训练迭代中,获取含有标注的源领域图片和不含标注的目标领域图片；将所述源领域图片输入至Faster RCNN模型进行目标检测并得到由骨干网络输出的第一源领域特征；同时,将所述第一源领域特征通过梯度翻转层传输至领域分类器进行损失计算；将所述目标领域图片输入至Faster RCNN模型进行目标检测并得到由RPN输出的前景和背景概率、由RPC输出的类别概率,以及由骨干网络输出的第一目标领域特征；根据所述由RPN输出的前景和背景概率、由RPC输出的类别概率,以及由骨干网络输出的第一目标领域特征,计算出Faster RCNN模型的损失总值；对Faster RCNN模型的参数进行优化调整,当训练迭代的次数达到预设阈值,或所述损失总值不超过预设损失值时,停止对Faster RCNN模型的训练迭代。</td>   <td>G06K9/62;G06K9/32;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         邱建秀;              蔡霁初;              王大刚;              王振刚;                   陈建耀       </td>   <td>中山大学</td>   <td>一种基于土壤水分的森林火灾预测方法、装置及存储介质</td>   <td>广东省</td>   <td>CN112016744A</td>   <td>2020-12-01</td>   <td>本发明公开了一种基于土壤水分的森林火灾预测方法、装置及存储介质,所述方法包括：获取监测区域的观测数据；其中,所述观测数据包括不同时间尺度的土壤水分数据以及不同时间尺度的气象观测数据；根据预测时间尺度,选取对应的森林火灾预测模型；根据所述预测时间尺度以及所选取的森林火灾预测模型,从所述观测数据中提取对应的土壤水分数据和气象观测数据；将所提取的土壤水分数据和气象观测数据输入所选取的森林火灾预测模型中,以使所述森林火灾预测模型对所述监测区域发生森林火灾的情况进行预测。本发明技术方案能够提高森林火灾预测的准确性。</td>   <td>1.一种基于土壤水分的森林火灾预测方法,其特征在于,包括：获取监测区域的观测数据；其中,所述观测数据包括不同时间尺度的土壤水分数据以及不同时间尺度的气象观测数据；根据预测时间尺度,选取对应的森林火灾预测模型；根据所述预测时间尺度以及所选取的森林火灾预测模型,从所述观测数据中提取对应的土壤水分数据和气象观测数据；将所提取的土壤水分数据和气象观测数据输入所选取的森林火灾预测模型中,以使所述森林火灾预测模型对所述监测区域发生森林火灾的情况进行预测。</td>   <td>G06Q10/04;G06Q10/06;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑子彬;                   蒋源毅       </td>   <td>中山大学</td>   <td>一种基于马尔科夫链的区块链性能分析方法及装置</td>   <td>广东省</td>   <td>CN112001571A</td>   <td>2020-11-27</td>   <td>本发明提供了一种基于马尔科夫链的区块链性能分析方法及装置,其中,方法包括：根据区块链系统的设计参数,基于马尔科夫链建立区块链系统的状态转移模型；对所述状态转移模型进行求解,得到所述区块链系统平稳状态下的稳态解；根据所述稳态解和计算性能指标的公式计算所述区块链系统的性能指标；根据所述性能指标对所述区块链系统进行性能分析与预测。本发明基于离散时间马尔可夫链理论,建立了区块链系统的性能分析和预测模型,不仅能对采集到的数据进行事后分析或实时监测,还可以根据模型和已知数据对区块链系统的性能进行预测；本发明将交易从到达和出块的过程设置为一个整体,模型简洁直观,且运算量较小。</td>   <td>1.一种基于马尔科夫链的区块链性能分析方法,其特征在于,包括：根据区块链系统的设计参数,基于马尔科夫链建立区块链系统的状态转移模型；对所述状态转移模型进行求解,得到所述区块链系统平稳状态下的稳态解；根据所述稳态解和计算性能指标的公式计算所述区块链系统的性能指标；根据所述性能指标对所述区块链系统进行性能分析与预测。</td>   <td>G06Q10/04;G06F16/27;G06Q40/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              杨泽微;              陈嘉奇;                   吴贺丰       </td>   <td>中山大学</td>   <td>一种基于结构化知识蒸馏的人群计数模型及其方法</td>   <td>广东省</td>   <td>CN112001278A</td>   <td>2020-11-27</td>   <td>本发明公开了一种基于结构化知识蒸馏的人群计数模型及其方法,该方法包括：步骤S1,获取人群图像,利用标注的信息生成对应的真实的人群密度图；步骤S2,多次迭代式将不同的人群图像输入重量级的教师网络进行预训练,提取各层特征,并生成估计的人群密度图；步骤S3,将人群图像输入轻量级的学生网络,提取各层特征,并生成估计的人群密度图；步骤S4,计算对应学生网络与教师网络特征的一元知识相似度和成对知识相关系数；步骤S5,通过一元知识相似度和成对知识相关系数以及各人群密度图,计算损失,更新学生网络的参数；步骤S6,利用不同人群图像多次迭代式地进行S1以及S3-S5的训练过程,直到符合停止条件。</td>   <td>1.一种基于结构化知识蒸馏的人群计数模型,包括：预处理单元,用于获取人群图像,对人群图像进行标注,利用标注的信息产生真实的人群密度图；教师网络处理单元,用于多次迭代式地将所述预处理单元的不同人群图像输入使用结构复杂的重量级网络结构的教师网络进行预训练,产生各层特征,生成估计的第一人群密度图；学生网络预处理单元,用于将所述预处理单元的人群图像输入使用精简的轻量级网络的学生网络,产生各层特征,生成估计的第二人群密度图；知识蒸馏单元,用于对所述教师网络处理单元的各层特征进行知识蒸馏,计算所述学生网络处理单元的各层特征与对应的所述教师网络处理单元的各层特征的一元知识相似度和成对知识相关系数；学生网络更新单元,用于使用所述知识蒸馏单元计算的一元知识相似度和成对知识相关系数,以及所述学生网络预处理单元生成的估计的人群密度图与所述预处理单元生成的真实人群密度图、所述教师网络处理单元生成的估计的人群密度图,计算所述学生网络的各项损失,更新所述学生网络的参数；学生网络迭代训练单元,用于多次迭代式地对不同人群图像进行所述预处理单元、学生网络预处理单元、知识蒸馏单元以及学生网络更新单元的训练过程,直到满足设定的停止条件时停止训练。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         卢伟;                   余木建       </td>   <td>中山大学</td>   <td>一种半色调图像隐写方法</td>   <td>广东省</td>   <td>CN112001832A</td>   <td>2020-11-27</td>   <td>本发明公开了一种半色调图像隐写方法,该方法包括：确定模式块大小；得到密度变化矩阵；得到不确定性变化矩阵；得到纹理变化矩阵；结合密度变化矩阵、不确定性矩阵和纹理变化矩阵,得到失真分数矩阵；对原图和失真分数矩阵进行乱序处理并结合秘密信息输入编码器得到加密图像；对加密图像进行加扰处理并结合秘密信息长度输入解码器得到嵌入信息。通过使用本发明,综合图库和单图的统计信息,着重半色调图像的密度特性,提高安全性和视觉不可察性。本发明作为一种半色调图像隐写方法,可广泛应用于信息安全领域。</td>   <td>1.一种半色调图像隐写方法,其特征在于,包括以下步骤：获取原图并确定模式块大小；计算原图各点所在模式块的密度变化并得到密度变化矩阵；统计图库中所有图像不同密度的模式块的纹理分布并得到不确定性变化矩阵；提取原图纹理分布直方图信息,计算各点翻转前后相邻模式块的平均纹理变化并得到纹理变化矩阵；结合密度变化矩阵、不确定性矩阵和纹理变化矩阵,得到失真分数矩阵；对原图和失真分数矩阵进行乱序处理并结合秘密信息输入编码器得到加密图像；对加密图像进行加扰处理并结合秘密信息长度输入解码器得到嵌入信息。</td>   <td>G06T1/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         卢伟;                   许博智       </td>   <td>中山大学</td>   <td>一种基于纹理特征的深度伪造视频检测方法</td>   <td>广东省</td>   <td>CN112001429A</td>   <td>2020-11-27</td>   <td>本发明公开了一种基于纹理特征的深度伪造视频检测方法,该方法包括：将数据集进行视频分帧,得到训练集和测试集；对训练集和测试集中的视频帧图像进行人脸区域捕获并进行预处理,得到视频帧人脸区域图像训练集和视频帧人脸区域图像测试集；对训练集进行人脸子区域划分,提取训练人脸图像特征向量并训练分类模型,得到训练后的分类模型；对测试集进行人脸子区域划分,提取测试视频帧人脸图像特征向量,输入到训练后的分类模型得到分类结果。通过使用本发明,缩短对视频帧图像检测模型的训练时间的同时,保证较高的检测准确率。本发明作为一种基于纹理特征的深度伪造视频检测方法,可广泛应用于视频检测领域。</td>   <td>1.一种基于纹理特征的深度伪造视频检测方法,其特征在于,包括以下步骤：将含有真实人脸视频和深度伪造人脸视频的数据集进行视频分帧,得到视频帧图像并划分为训练集和测试集；对训练集和测试集中的视频帧图像进行人脸区域捕获并进行预处理,得到视频帧人脸区域图像训练集和视频帧人脸区域图像测试集；对视频帧人脸区域图像训练集进行人脸子区域划分,提取训练人脸图像特征向量并输入到SVM训练分类模型,得到训练后的分类模型；对视频帧人脸区域图像测试集进行人脸子区域划分,提取测试视频帧人脸图像特征向量,输入到经过训练后的分类模型得到测试视频帧分类结果。</td>   <td>G06K9/62;G06K9/00;G06K9/40;G06K9/46</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李冠彬;                   张锐斐       </td>   <td>中山大学</td>   <td>一种息肉分割方法、装置及存储介质</td>   <td>广东省</td>   <td>CN111986204A</td>   <td>2020-11-24</td>   <td>本发明公开了一种息肉分割方法、装置及存储介质,所述方法首先提取待检测者的影像数据,然后将其输入至预设的息肉分割模型,得到最终的息肉分割图像；上述息肉分割模型在对待检测影像数据记性识别时,先提取全局特征以及局部特征,然后根据全局特征和局部特征确定息肉的尺寸,紧接着根据息肉的尺寸分别计算全局特征和局部特征的注意力权重,根据注意力权重进行特征融合,生成与息肉尺寸对应的自适应特征,最后根据自适应特征生成最终的息肉分割图像,在整个息肉的自动化分割过程中,基于息肉的尺寸进行自适应分割,从而提高了息肉分割的准确性。</td>   <td>1.一种息肉分割方法,其特征在于,包括：提取待检测者病变部位的影像数据,获得待检测影像数据；将所述待检测影像数据输入至预设的息肉分割模型中,以使所述息肉分割模型对所述待检测影像数据进行识别,生成与所述待检测影像数据对应的息肉分割图像；其中,所述息肉分割模型对所述待检测影像数据的识别,生成与所述待检测影像数据对应的息肉分割图像,具体包括：从所述待检测影像数据中提取全局特征以及局部特征,并根据所述全局特征以及所述局部特征确定所述待检测影像数据所对应的息肉尺寸,继而根据所述息肉尺寸,确定所述全局特征的注意力权重以及所述局部特征的注意力权重；根据所述全局特征、所述全局特征的注意力权重、所述局部特征以及所述局部特征的注意力权重,生成自适应特征；根据所述自适应特征生成与所述待检测影像数据对应的息肉分割图像。</td>   <td>G06T7/10;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周凡;              陈茜茜;              苏卓;              林淑金;                   王若梅       </td>   <td>中山大学</td>   <td>一种基于场景识别的草图图像翻译方法</td>   <td>广东省</td>   <td>CN111967533A</td>   <td>2020-11-20</td>   <td>本发明公开了一种基于场景识别的草图图像翻译方法。用户在所选取的场景图背景上交互式的逐步绘制草图,系统根据场景识别网络识别出的场景图类别对轮廓草图进行语义匹配得到对象类别,再根据对象类别与草图,利用部分草图形状完成网络与完整图像外观生成网络进行轮廓与外观合成,经过交互式的绘制最终生成需要的前景对象图像。本发明可以允许用户不用进行整个场景级别的草图绘制,只需在现有的场景背景图上描绘前景部分的对象草图,系统会自动进行与背景场景图语义匹配的对象完成,使得最终生成的图像质量与清晰度更高。</td>   <td>1.一种基于场景识别的草图图像翻译方法,其特征在于,所述方法包括：构建对象数据集与场景数据集并进行语义匹配,对对象数据集的原始图像进行边缘检测与简化从而得到完整对象边缘图像,之后进行随机遮挡掩模处理以模拟用户的部分草图输入从而得到不完整对象边缘图像,对场景数据集进行场景识别预训练产生OPlace365-Resnet50网络；用户从所述场景数据集中选取场景背景图,然后利用所述OPlace365-Resnet50网络对所选取的场景背景图进行分类识别,得到场景类别；利用所述完整对象边缘图像和所述不完整对象边缘图像作为数据集训练部分草图形状完成网络,利用所述对象数据集的原始图像和所述完整对象边缘图像作为数据集训练完整图像外观生成网络；所述场景类别通过所述语义匹配得到对应的对象类别,之后所述部分草图形状完成网络根据对象类别和用户描绘的部分草图生成完整的草图轮廓,然后所述完整图像外观生成网络再根据对象类别和所生成的完整草图轮廓生成带有颜色和纹理的前景图像；用户根据所述带有颜色和纹理的前景图像交互式的进行下一笔草图描绘,直到生成在所述用户选取的场景背景图上满足用户需求的前景对象图像,即为最终生成图像。</td>   <td>G06K9/62;G06K9/46;G06T7/13;G06T11/00;G06T11/80;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈定安;              夏林元;                   李倩霞       </td>   <td>中山大学</td>   <td>一种可用于激光扫描的管道弯头的精密三维几何模型构建方法</td>   <td>广东省</td>   <td>CN111951401A</td>   <td>2020-11-17</td>   <td>本发明提供的一种可用于激光扫描的管道弯头的精密三维几何模型构建方法,包括：构建管道弯头的三维几何模型；从激光三维扫描或其他三维成像传感器提取三维几何模型观测值；进行模型参数初始化估计；利用最小二乘估计方法计算模型参数；估算管道弯头的在一个三维空间的精确位置、角度和尺寸。本发明提供的一种可用于激光扫描的管道弯头的精密三维几何模型构建方法,通过激光扫描获取点云等观测数据,通过点云的三维重建,建立管道弯头的三维几何模型,估算弯头的在一个三维空间的精确位置、角度和尺寸,以便应用于基于流体力学的流体通过弯头时的动态监测,竣工图勾画,点云配准、管道配件检测等。</td>   <td>1.一种可用于激光扫描的管道弯头的精密三维几何模型构建方法,其特征在于：包括以下步骤：S1：构建管道弯头的三维几何模型；S2：从激光三维扫描或其他三维成像传感器提取三维几何模型的观测值；S3：进行模型参数初始化估计；S4：利用最小二乘估计方法建立模型参数最佳估算模型,计算模型参数的改正数；S5：估算管道弯头在一个三维空间的精确位置、角度和尺寸。</td>   <td>G06T17/20;G06T7/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李健;              罗铁成;              王钢;                   裴艳丽       </td>   <td>中山大学</td>   <td>一种基于AR技术的MOCVD设备维修辅助方法及系统</td>   <td>广东省</td>   <td>CN111950506A</td>   <td>2020-11-17</td>   <td>本发明公开了一种基于AR技术的MOCVD设备维修辅助方法及系统,该方法包括：调用用户端摄像头获取当前视频图像并上传,得到视频信息；通过目标识别算法对视频信息进行图像模板识别,得到检测目标；将检测目标与资料库内的设备匹配,得到设备信息；将设备信息添加至视频信息后进行视频处理,得到AR视频；将AR视频回传至用户端。该系统包括：用户端和服务器端。通过使用本发明,可以极大的解决MOCVD设备修理困难的问题。本发明作为一种基于AR技术的MOCVD设备维修辅助方法及系统,可广泛应用于半导体设备技术领域。</td>   <td>1.一种基于AR技术的MOCVD设备维修辅助方法,其特征在于,包括以下步骤：调用用户端摄像头获取当前视频图像并上传,得到视频信息；通过目标识别算法对视频信息进行图像模板识别,得到检测目标；将检测目标与资料库内的设备匹配,得到设备信息；将设备信息添加至视频信息后进行视频处理,得到AR视频；将AR视频回传至用户端。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06Q10/00;H04N7/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         成慧;              郑卓祺;              何晋豪;                   陈崇雨       </td>   <td>中山大学</td>   <td>一种用于三维重建的快速深度恢复方法</td>   <td>广东省</td>   <td>CN108062769B</td>   <td>2020-11-17</td>   <td>本发明涉及计算机视觉中的立体视觉的技术领域,更具体地,涉及一种用于三维重建的快速深度恢复方法。利用通过运动恢复结构(Structure From Motion,SFM)得到图像中稀疏特征点的深度后,基于这些稀疏的特征点,结合灰度图像,通过多层下采样利用联合双边滤波器对深度进行扩散,从低分辨到高分辨率,由粗到精地、分层地快速恢复出精准的稠密深度图。该方法结果准确且计算量少,可以用于基于特征点法的同步定位与建图系统(Simultaneous Localization and Mapping,SLAM),将系统中计算得到的稀疏特征点恢复成稠密的深度图,以重建出三维的稠密地图。</td>   <td>1.一种用于三维重建的快速深度恢复方法,其特征在于,包括以下步骤：(1)通过SFM方法计算得到基于Harris角点的特征点的深度,由特征点组成的稀疏的深度图,该深度图分辨率为dim_x×dim_y像素；(2)对稀疏的深度图中有深度值的像素进行预处理；在具有深度值的特征点的八连通区域进行深度值扩散；此步骤保证对稀疏深度图进行下采样的时候深度信息不丢失；(3)利用高斯金字塔向下降采样灰度图,记原分辨率dim_x×dim_y的灰度图为I~0,每一轮下采样倍数为2,经过N轮下采样得到N+1张不同分辨率的灰度图：I~0,I~1,I~2...I~N,其中I~N的分辨率为(dim_x/2~N)×(dim_y/2~N)；(4)经过预处理的稀疏深度图利用最邻近方法进行下采样；记原分辨率dim_x×dim_y的稀疏深度图为D~0,稀疏深度图的下采样倍数为2~N,下采样后的稀疏深度图记为D~N,其分辨率与I~N同为(dim_x/2~N)×(dim_y/2~N)；(5)通过联合双边滤波器,以灰度图I~N作为引导,将低分辨下的稀疏深度图D~N中的特征点的深度扩散到无特征点区域,所得深度图为根据标准的联合双边滤波器,滤波后低分辨稀疏深度图中计算方法如下：                  其中,为归一化系数；S为插值窗口所覆盖的图像像素集合；每一个像素p的深度是该像素滤波前在插值窗口中所有深度的加权平均；权重w_(p,q)由空间相似性与灰度相似性计算：                  上式中,函数s(·)为空间相似性权重,r(·)为灰度相似性权重；具体计算方法如下：(a)空间相似性权重s(·)表示的是插值窗口内像素距离之间的相似性,用高斯核函数来表达,计算方法为：其中σ_s为空间相似性权重的标准差；(b)灰度相似性权重r(·)表示的是插值窗口内像素灰度之间的相似性,用高斯核函数来表达,计算方法式为：其中σ_r为灰度相似性权重的标准差；(6)步骤(5)所得到的使用双三次插值方法上采样得到深度图(7)重复步骤(5)与步骤(6),直至得到清晰稠密的深度图(8)最后将深度图上采样至原图分辨率大小,得到恢复后的深度图D。</td>   <td>G06T7/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         袁媛;              刘颖;                   牛通       </td>   <td>中山大学</td>   <td>一种基于街景图片及机器学习的城市内部贫困空间测度方法及系统</td>   <td>广东省</td>   <td>CN111937016A</td>   <td>2020-11-13</td>   <td>为了构建新型的城市贫困评估方法,本发明公开一种基于街景图片及机器学习的城市内部贫困空间测度方法,包括以下步骤：根据人口普查数据构建多重剥夺指数IMD；在地图信息数据库中获取目标区域的街景图像数据；通过图像分割技术,将目标区域的街景图像数据分割为若干块街景图像数据；基于若干块街景图像数据,结合主成分分析法,得到主因子,将主因子定义为街景因子；将多重剥夺指数IMD和街景因子作为机器学习算法的输入变量,得到城市贫困分数。根据城市贫困分数对城市的贫困程度进行评估。本发明还公开了基于上述方法的一种基于街景图片及机器学习的城市内部贫困空间测度系统。本发明不仅推进城市贫困研究精细化,而且丰富城市贫困度量指标的维度。</td>   <td>1.一种基于街景图片及机器学习的城市内部贫困空间测度方法,其特征在于,包括以下步骤：根据人口普查数据构建多重剥夺指数IMD；在地图信息数据库中获取目标区域的街景图像数据；通过图像分割技术,将目标区域的街景图像数据分割为若干块街景图像数据；基于若干块街景图像数据,结合主成分分析法,得到主因子,将主因子定义为街景因子；将多重剥夺指数IMD和街景因子作为机器学习算法的输入变量,得到城市贫困分数。根据城市贫困分数对城市的贫困程度进行评估。</td>   <td>G06Q10/06;G06Q50/26;G06N20/00;G06K9/00;G06K9/34;G06F16/29</td>  </tr>        <tr>   <td>中国专利</td>   <td>              周凡       </td>   <td>中山大学深圳研究院</td>   <td>一种基于草图的图像匹配方法</td>   <td>广东省</td>   <td>CN111931794A</td>   <td>2020-11-13</td>   <td>本申请适用于多媒体信息检索技术领域,提供了一种基于草图的图像匹配方法、装置及计算机可读存储介质,所述图像匹配方法包括：对样本图像进行特征边缘提取,得到样本图像的第一方向梯度直方图特征；其中,样本图像为基于原始图像进行高斯模糊得到；第一方向梯度直方图特征为样本图像的图像边缘像素集中部分像素点的方向梯度直方图特征；图像边缘像素集为基于样本图像进行边缘提取得到；基于第一方向梯度直方图特征从预设图像库中确定出目标图像集。上述图像匹配方法在对样本图像进行特征边缘提取时,可以得到样本图像更多的边缘细节,使得样本图像的第一方向梯度直方图特征能够充分的表示样本图像的边缘特征信息,进而提高了图像匹配的准确率。</td>   <td>1.一种基于草图的图像匹配方法,其特征在于,包括：对样本图像进行特征边缘提取,得到所述样本图像的第一方向梯度直方图特征；其中,所述样本图像为基于原始图像进行高斯模糊得到；所述第一方向梯度直方图特征为所述样本图像的图像边缘像素集中部分像素点的方向梯度直方图特征；所述图像边缘像素集为基于所述样本图像进行边缘提取得到；基于所述第一方向梯度直方图特征从预设图像库中确定出目标图像集。</td>   <td>G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>              周凡       </td>   <td>中山大学深圳研究院</td>   <td>一种学生学习情况识别方法、系统、教学终端及存储介质</td>   <td>广东省</td>   <td>CN111932418A</td>   <td>2020-11-13</td>   <td>本申请适用于教学管理技术领域,提供了一种学生学习情况识别方法、系统、教学终端及存储介质,该方法应用于教学终端,包括：获取学生通过学生终端上传的第一笔迹图片；识别第一笔迹图片中的第一文本,以及识别与第一文本对应的第一笔迹信息；提取第一文本和第一笔迹信息中的第一学习特征与第二学习特征；根据第一学习特征,使用预设的第一学习模型对学生的第一学习情况进行识别；根据教学终端中与第二学习特征属于相同类别的目标特征训练第二学习模型,并根据第二学习模型和第二学习特征对学生的第二学习情况进行识别。采用上述方法中的第一学习模型和第二学习模型对学生的学习情况进行全面分析,使分析结果更加符合学生的真实学习情况。</td>   <td>1.一种学生学习情况识别方法,应用于教学终端,其特征在于,所述方法包括：获取学生通过学生终端上传的第一笔迹图片,所述第一笔迹图片中包含第一文本；识别所述第一笔迹图片中的第一文本,以及识别与所述第一文本对应的第一笔迹信息；提取所述第一文本和所述第一笔迹信息中的学习特征,所述学习特征包括第一学习特征与第二学习特征；根据所述第一学习特征,使用预设的第一学习模型对所述学生的第一学习情况进行识别；统计所述教学终端中与所述第二学习特征属于相同类别的目标特征的特征数量；若所述特征数量大于预设阈值,则使用所述目标特征训练第二学习模型,并根据所述第二学习模型和所述第二学习特征对所述学生的第二学习情况进行识别。</td>   <td>G06Q50/20;G06K9/00;G06F40/279;G06F17/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              程海杰;                   张权       </td>   <td>中山大学</td>   <td>一种高效的跨摄像头行人双向跟踪方法</td>   <td>广东省</td>   <td>CN109359552B</td>   <td>2020-11-13</td>   <td>本发明公开了一种高效的跨摄像头行人双向跟踪方法,用于获取行人在已出现过摄像头中的完整轨迹。包括步骤：(1)从监控视频中获取图像,对图像中的行人进行检测,根据检测结果建立候选行人库；(2)构建行人再识别模型,提取待查行人和候选行人库中所有图像的特征,一一计算待查行人特征与候选行人库中所有图像特征的距离特征,获取待查行人在其他摄像头下的最佳匹配块,并将其作为跟踪的起始位置；(3)进行目标跟踪,跟踪过程中通过正反向处理视频完成对行人的双向跟踪；(4)将不同摄像头下的轨迹进行整合,得到行人最终的轨迹输出。本发明对真实场景下的跨境跟踪具有速度快、精度高的优点,具有很强的工程意义。</td>   <td>1.一种高效的跨摄像头行人双向跟踪方法,其特征在于,包括步骤：(1)从监控视频中获取图像,对图像中的行人进行检测,根据检测结果建立候选行人库；(2)构建行人再识别模型,提取待查行人和候选行人库中所有图像的特征,一一计算待查行人特征与候选行人库中所有图像特征的距离特征,获取待查行人在其他摄像头下的最佳匹配块,并将其作为跟踪的起始位置；(3)进行目标跟踪,跟踪过程中通过正反向处理视频完成对行人的双向跟踪；(4)将不同摄像头下的轨迹进行整合,得到行人最终的轨迹输出；所述步骤(3)中,通过构建目标跟踪模型进行目标跟踪,步骤是：采用在ImageNet数据集上预训练的VGG模型作为提取行人特征的初始模型,然后利用摄像头视频中的行人和背景图片对模型参数进行精细调整,结合手工特征得到行人的完整表观特征和语义特征,基于上述特征训练得到目标跟踪模型；所述步骤(3)中,进行目标跟踪的过程是：对于给定的待跟踪视频帧,首先根据上一帧的跟踪结果确定跟踪区域；接着对跟踪区域提取深度特征和手工特征,并进行融合；然后让融合特征经过一个预训练好的相关滤波器计算特征的响应图,取响应最大点作为目标跟踪模型下一帧的预测位置,反复迭代此过程获得行人在该摄像头下的单向轨迹；所述步骤(3)中,在目标跟踪过程中,根据跟踪的置信分自适应判断行人跟踪停止的位置,步骤是：将相关滤波得到的响应图的峰值作为第i帧跟踪结果的置信分S_i,指定一个跟踪的最大长度L,模型生成置信分序列并在置信分序列的全局最小值处判定为跟踪停止,其具体的停止帧号可描述为</td>   <td>G06K9/00;G06K9/62;G06T7/246</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              李林静;                   谢晓华       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于归一化像素差特征的人脸姿势分类方法</td>   <td>广东省</td>   <td>CN106980825B</td>   <td>2020-11-13</td>   <td>本发明提供一种基于归一化像素差特征的人脸姿势分类方法,该方法读取待测人脸图片,提取人脸检测窗口的NPD特征,并进行人脸检测；对所有检测出的人脸窗口利用基于NPD特征的改进的稀疏表示的分类算法进行人脸姿势分类,该归一化像素差特征的提取方法仅是通过任意两个像素值计算得来,并且具有尺度不变性,克服了姿势分类问题中的遮挡、光照变化,低分辨率和模糊等困难,并且降低了特征提取的时间复杂度和计算复杂度。</td>   <td>1.一种基于归一化像素差特征的人脸姿势分类方法,其特征在于,包括以下步骤：S1：读取一张待测人脸图片；S2：提取人脸检测窗口的归一化像素差特征,并进行人脸检测；S3：对所有检测出的人脸窗口利用基于归一化像素差特征的改进的稀疏表示的分类算法进行人脸姿势分类；所述步骤S2的过程为：对每一个人脸检测窗口,提取它的归一化像素差特征；其中,两个像素x,y间的归一化像素差特征定义为：                  其中x,y≥0代表两个像素的像素强度值；当x,y＝0时f(0,0)＝0；f(x,y)的符号代表了x,y之间的顺序关系,大小代表了x,y之间的相关差异；f(x,y)是反对称的,所以f(x,y)和f(y,x)对于特征表达来说是等价的,大大减小了特征空间；一个s*s大小的图像块,向量化为一个p*1长度的特征向量,其中p＝s*s,则计算该图像块的归一化像素差特征,则共产生p(p-1)/2个归一化像素差特征,即把原始图像块I＝(x_1,x_2,…,x_p)~T映射到了归一化像素差特征空间f＝(f(x_1,x_2),f(x_1,x_3),…,f(x_(p-1),x_p))~T；所述步骤S3的过程为：将归一化像素差特征与基于稀疏表示的分类算法相结合,应用改进的SRC算法进行稀疏表示,将检测出的人脸窗口按照不同的角度范围将人脸姿势进行分类操作：记为一组训练样本的第i个目标类,s_(i,j),j＝1,2,…,n_i是来自第i类的第j个训练样本,记作一个m维的向量,对于来自这一类测试任一样本y_0∈R_m通过来自A_i中的样本的线性组合来表示,即其中是系数向量,若一共有K个目标类,记A＝[A_1,A_2,…A_K]为来自K个类的n个训练样本的集合,其中n＝n_1+n_2+…+n_K,y_0用所有训练样本的线性组合来表示,即y_0＝Aα,在发生遮挡或者损毁的情况下,测试用样本重写为：                  y_0和遮挡误差e_0通过训练样本字典A和遮挡字典A_e,就能得到相应的稀疏表达,在SRC算法中,遮挡字典A_e被设置为正交矩阵；记NPD特征向量为χ,无遮挡的SRC表示为：χ(y_0)＝X(A_1)α_1+X(A_2)α_2+…+X(A_K)α_K＝X(A)α其中,X(A)＝[X(A_1)X(A_2)…X(A_K)],有遮挡时的SRC表示为：                  其中,X(A_e)是基于归一化像素差特征的遮挡字典,α_e是输入的NPD特征χ(y_1)相应的表示系数向量；通过解L1的范数最小化问题或计算残差r_i(y_0)＝||y_0-Aδ_i(α_1)||_2或残差最小的那一类即为最终的分类结果。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         姜楠;              王军;                   杨青       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于Verilog的判题装置、方法及系统</td>   <td>广东省</td>   <td>CN108596799B</td>   <td>2020-11-13</td>   <td>本发明公开了一种基于Verilog的判题装置,包括录入模块、处理模块、判定模块以及输出模块,可以对用户输入的Verilog答案信息进行仿真处理并进行判定,最后通过输出模块将判定结果输出到用户,可以实现对Verilog硬件语言作业的自动判题；本发明采用的一种基于Verilog的判题方法,可以对Verilog答案信息进行自动判题,同时还能对仿真结果中的波形信息进行转换,从而将对波形信息的对比转换成其他形式的信息的对比,方便判题,可以减少教师的工作量；本发明的一种基于Verilog的判题系统,包括浏览器、处理器和寄存器,可以实现对Verilog答案信息的自动判题,并且可以通过浏览器在任意地方上传Verilog答案信息。</td>   <td>1.一种基于Verilog的判题装置,其特征在于：包括用于接收用户的Verilog答案信息的录入模块(1)、用于对用户的Verilog答案信息进行仿真处理的处理模块(2)、预设参数的判定模块(3)以及将判定结果传输到用户的输出模块(4)；所述处理模块(2)包括：用于检测Verilog答案信息的文件类型是否符合格式要求的检测反馈模块(21)、用于对Verilog答案信息进行仿真的仿真模块(22)和用于将仿真结果进行转换的转换模块(23),所述仿真模块(22)与检测反馈模块(21)连接；所述录入模块(1)将接收的Verilog答案信息传输到检测反馈模块(21),当检测反馈模块(21)检测到Verilog答案信息的文件类型不符合格式要求时,将检测结果通过输出模块(4)输出到用户；当检测反馈模块(21)检测到Verilog答案信息的文件类型符合格式要求时,将Verilog答案信息传输到仿真模块(22),经仿真模块(22)进行处理后得到仿真结果,所述仿真结果包括程序执行结果、代码错误信息或可能存在的波形信息；所述转换模块(23)采用以下任意一种技术对波形信息进行转换：(a)采用脚本程序将波形信息依次转换为JSON格式数据和字符串信息；(b)转换模块(23)采用开源软件或HTML5中的Canvas技术将波形信息转换成图片信息；其中,所述JSON格式数据包括波形信息文件的时间精度、变量定义方式、值变化信息；判定模块(3)根据预设参数对程序执行结果、字符串信息或图片信息进行判定处理,再通过输出模块(4)输出到用户；所述代码错误信息通过输出模块(4)输出到用户。</td>   <td>G06Q50/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         贾卫华;              孙琳;              周婷;              张佩芬;              李超峰;              何彩升;              李铁;              杜平;              刘敬耀;                   黄冠祺       </td>   <td>广州惠侨计算机科技有限公司;中山大学肿瘤防治中心</td>   <td>一种生物标本管理方法、系统、设备及介质</td>   <td>广东省</td>   <td>CN111882166A</td>   <td>2020-11-03</td>   <td>本申请公开了一种生物标本管理方法、系统、设备及介质,入库步骤包括获取入库标本盒的数量、类型及标识；获取空闲状态的缓存区位置信息；将入库标本盒移动至对应的缓存区；保存每个标识与缓存区位置信息的对应关系；存储步骤包括：获取待存储的入库标本盒的数量、类型及标识；获取空闲状态的存储区位置信息；将缓存区中的入库标本盒移动至存储区对应的位置；保存移动后标识与存储区位置信息的对应关系；出库步骤包括：获取出库标本盒的数量和类型,或者获取指定的出库标本盒；获取空闲的缓存区位置信息；将出库标本盒移动至的缓存区；更新保存的标本盒的信息。本申请解决了现有标本存储设备信息孤岛问题以及操作步骤复杂且容易出错的技术问题。</td>   <td>1.一种生物标本管理方法,其特征在于,包括：入库步骤包括：获取入库标本盒的数量、类型及所述入库标本盒上的标识；根据所述入库标本盒的数量和类型,获取空闲状态的缓存区位置信息；将所述入库标本盒移动至对应的所述缓存区；保存每个所述标识与所述缓存区位置信息的对应关系；存储步骤包括：获取待存储的所述入库标本盒的数量、类型及所述待存储的所述入库标本盒上的标识；根据所述待存储的所述入库标本盒的数量和类型,获取空闲状态的存储区位置信息；根据所述标识与所述缓存区位置信息的对应关系,将所述缓存区中的所述入库标本盒移动至所述存储区对应的位置；保存移动后所述标识与所述存储区位置信息的对应关系；出库步骤包括：获取出库标本盒的数量和类型,或者获取指定的出库标本盒；获取空闲的所述缓存区位置信息；根据所述标识与所述存储区位置信息的对应关系,将所述出库标本盒移动至所述缓存区；更新保存的所述标本盒的信息。</td>   <td>G06Q10/06;G06Q50/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         衣杨;              吴昱焜;              张念旭;              谢韬;              李仲泓;                   周翼丰       </td>   <td>中山大学</td>   <td>一种乳腺癌图像识别方法、装置和用户终端</td>   <td>广东省</td>   <td>CN108629761B</td>   <td>2020-11-03</td>   <td>本发明提供了一种乳腺癌图像识别方法、装置和用户终端,其中所述方法包括：将预处理病理图像通过阈值分割算法进行分割,得到分割后二值图像；以淋巴细胞为区域生长算法的种子,以所述分割后二值图像的并集作为目标图片,通过所述区域生长算法分割出区域二值图像；根据所述区域二值图像获得与所述区域二值图像对应的区域面积,并通过所述区域面积计算得到浸润癌间质淋巴细胞比例。本发明所提供的方法实现了通过计算机对乳腺癌的病理图像中的癌细胞的识别,提高了对于乳腺癌病理图像诊断的准确性,避免了人工肉眼判断的误差,减少了对于乳腺癌病理图像诊断的时间,提高了诊断效率,从而为医生的诊断工作带来方便。</td>   <td>1.一种乳腺癌图像识别方法,其特征在于,包括：将预处理病理图像通过阈值分割算法进行分割,得到分割后二值图像；以淋巴细胞为区域生长算法的种子,以所述分割后二值图像的并集作为目标图片,通过所述区域生长算法分割出区域二值图像；其中,所述分割后二值图像包括分割后癌细胞区域二值图像和分割后淋巴细胞区域二值图像；所述“以淋巴细胞为区域生长算法的种子,以所述分割后二值图像的并集作为目标图片,通过所述区域生长算法分割出区域二值图像”包括：对所述分割后癌细胞区域二值图像和所述分割后淋巴细胞区域二值图像取并集,生成混合图像；以所述淋巴细胞作为种子,将所述混合图像通过区域生长算法处理,取所述混合图像与所述区域生长算法分割出区域二值图像的交集,得到原位癌二值图像；取所述原位癌二值图像与所述分割后癌细胞区域二值图像通过异或操作得到浸润癌二值图像；以所述原位癌二值图像、所述浸润癌二值图像、和所述区域生长算法分割出区域二值图像作为所述区域二值图像；根据所述区域二值图像获得与所述区域二值图像对应的区域面积,并通过所述区域面积计算得到浸润癌间质淋巴细胞比例。</td>   <td>G06T7/00;G06T7/11;G06T7/155;G06T7/187;G06T7/62;G06T5/30</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李禹源;              张东;                   吴增程       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于卷积神经网络的织带边缘毛疵缺陷检测方法</td>   <td>广东省</td>   <td>CN108364281B</td>   <td>2020-10-30</td>   <td>本发明公开了一种基于卷积神经网络的织带边缘毛疵缺陷检测方法,利用摄像机采集织带图片,对织带的边缘进行提取,分别得到有毛疵缺陷的样本图片和无毛疵缺陷的样本图片；将采集到的样本图片利用具有多尺度并行训练结构的卷积神经网络进行分类检测,该卷积神经网络能够在增加神经网络深度与宽度的同时,去除普通卷积神经网络中的全连接层,并将一般的卷积转化为稀疏连接,再利用密集成分来近似最优的局部稀疏结构来保持神经网络的高计算性能。因此,本发明的毛疵缺陷检测方法,不仅能够对织带边缘毛疵缺陷进行有效的检测,并且能够有效保持或减小卷积神经网络的计算量,从而提高计算性能。</td>   <td>1.一种基于卷积神经网络的织带边缘毛疵缺陷检测方法,其特征在于：包括以下步骤：A、图像采集及预处理,得到样本图片；B、对样本图片进行图像增强处理,得到训练图片；C、构建具有多尺度并行训练结构的卷积神经网络,所述卷积神经网络包括依次连接的第一卷积层、第二卷积层、第三卷积层、第一最大池化层、多尺度并行训练结构、第一平均池化层、dropout层和第一Softmax层,所述多尺度并行训练结构包括依次连接的第一多尺度并行训练模块、第二多尺度并行训练模块、第三多尺度并行训练模块、第四多尺度并行训练模块和第五多尺度并行训练模块,所述第一多尺度并行训练模块、第二多尺度并行训练模块、第三多尺度并行训练模块、第四多尺度并行训练模块和第五多尺度并行训练模块均包括第四卷积层、第五卷积层、第六卷积层、第七卷积层、第八卷积层、第九卷积层、第十卷积层、第二最大池化层和连接层,所述第四卷积层形成第一卷积组,所述第五卷积层和第六卷积层依次连接形成第二卷积组,所述第七卷积层、第八卷积层和第九卷积层依次连接形成第三卷积组,所述第二最大池化层和第十卷积层依次连接形成第四卷积组,所述第一卷积组、第二卷积组、第三卷积组和第四卷积组相互并联并共同输出到所述连接层；D、利用训练图片对卷积神经网络进行训练处理；E、利用经过训练处理的卷积神经网络进行毛疵缺陷检测。</td>   <td>G06T7/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              贾笑卿;                   李烨       </td>   <td>中山大学;广州市鑫广飞信息科技有限公司</td>   <td>一种基于边缘检测和颜色匹配的异常检测方法</td>   <td>广东省</td>   <td>CN109359513B</td>   <td>2020-10-30</td>   <td>本发明提出一种基于边缘检测和颜色匹配的异常检测方法,通过对不同时刻采集的两幅航拍图像进行配准、颜色平衡、边缘检测和颜色匹配,高效且准确地检测其中的异常区域。针对传统基于图像要素比对的异常检测方法的不足,本发明在图像配准步骤引入基于邻域的特征点匹配对筛选以及基于距离与面积准则的关键点选择策略,提高了图像配准的可靠性；在图像比对环节结合边缘检测与颜色匹配,解决了异常检测与颜色平衡相冲突的问题,提高了异常检测的准确率。</td>   <td>1.一种基于边缘检测和颜色匹配的异常检测方法,其特征在于：包括以下步骤：S1.对两幅图像A、B进行SIFT特征点的检测和匹配,获得特征点匹配对；S2.采用Lowe算法对得到的特征点匹配对进行初步的筛选,初步筛选完毕后进行基于邻域的特征点匹配对筛选；S3.从经过筛选的特征点匹配对中选取三对特征点匹配对,并基于三对特征点匹配对求取仿射变换矩阵,对图像A/B进行仿射变换；S4.对两幅图像A、B中的颜色异常进行初步的检测,将两幅图像转换为Lab颜色空间,然后进行Lab颜色空间下的颜色平衡；S5.对经历过颜色平衡的图像A、B分别进行边缘异常检测、颜色异常检测；其中,步骤S4中所述的Lab颜色空间下的颜色平衡的具体过程如下：分别对图像A、B求取无异常块的颜色均值μ<Sub>t</Sub>、μ<Sub>s</Sub>和标准差σ<Sub>t</Sub>、σ<Sub>s</Sub>,并对图像A进行以下处理：          <Image id="icf0001" he="123" wi="616" file="FDA0002522751750000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,t表示图像A,s表示图像B,t<Sup>new</Sup>是颜色平衡后的图像A,μ<Sub>s</Sub>、μ<Sub>t</Sub>分别为图像B和图像A的颜色均值,σ<Sub>s</Sub>、σ<Sub>t</Sub>分别为图像B和图像A的颜色各通道标准差。</td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨雪榕;              张艳;              童鹏飞;              曲承志;              杨起帆;                   陈金涛       </td>   <td>中山大学</td>   <td>一种多传感器多目标协同探测信息融合方法、系统</td>   <td>广东省</td>   <td>CN111860589A</td>   <td>2020-10-30</td>   <td>本发明公开了一种多传感器多目标协同探测信息融合方法,包括：获取雷达测量设备的原始数据；对所述原始数据进行预处理,得到处理数据；根据时间配准算法及空间配准算法进行传感器信息配准,以使得所有传感器获得同一时刻及同一时空的观测信息；根据目标关联算法及机动目标模型进行点迹-航迹关联,以得到目标航迹；根据K-means算法进行航迹-航迹关联,以得到精确航迹；根据融合算法对所述处理数据进行融合处理,以得到追踪目标的精确航迹及实时运动参数。本发明提供的多目标协同探测信息融合方案,具备配置灵活、通用性强和可扩展性好的优点；通过机载数据处理单元进行数据实时处理,降低远程控制中心的任务负载,增加方案的时效性和安全性。</td>   <td>1.一种多传感器多目标协同探测信息融合方法,其特征在于,包括：获取雷达测量设备的原始数据；对所述原始数据进行预处理,得到处理数据；其中,所述预处理包括：根据光学望远镜的测角进行跟踪误差修正,根据时间插值对齐的方位角突变进行方位角跳点修正,根据雷达测量距离进行传播延时修正及时间插值对齐；根据时间配准算法进行传感器信息配准,以使得所有传感器获得同一时刻的观测信息；根据空间配准算法进行传感器信息配准,以使得所有传感器获得同一空间的观测信息；根据目标关联算法及机动目标模型进行点迹-航迹关联,以得到目标航迹；根据K-means算法进行航迹-航迹关联,以得到精确航迹；根据融合算法对所述处理数据进行融合处理,以得到追踪目标的精确航迹及实时运动参数。</td>   <td>G06K9/62;G01S7/35;G01S13/58;G01S13/86</td>  </tr>        <tr>   <td>中国专利</td>   <td>         衣杨;              李强;              梁达安;              赵福利;              林倩青;                   周晓聪       </td>   <td>中山大学</td>   <td>分类模型训练方法、装置、终端设备和可读存储介质</td>   <td>广东省</td>   <td>CN111860671A</td>   <td>2020-10-30</td>   <td>本发明实施例公开了一种分类模型训练方法、装置、终端设备和可读存储介质,该方法包括：对训练数据集进行预处理以获取标准训练数据集；将所述标准训练数据集中的标准训练样本分为预设数目个类别；对各个类别进行均衡处理以使各个类别中的标准训练样本的数目保持一致；将每一类别中的各个标准训练样本标记对应的类别的标签；利用带有标签的标准训练样本训练分类模型。本发明通过对标准训练样本进行分类,自动为各个标准训练样本添加标签,不仅减少样本标记的时间和人力资源,也可以解决人工标记主观性较强,错误率高的问题。</td>   <td>1.一种分类模型训练方法,其特征在于,该方法包括：对训练数据集进行预处理以获取标准训练数据集；将所述标准训练数据集中的标准训练样本分为预设数目个类别；对各个类别进行均衡处理以使各个类别中的标准训练样本的数目保持一致；将每一类别中的各个标准训练样本标记对应的类别的标签；利用带有标签的标准训练样本训练分类模型。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         梁智昊;              潘炎;                   刘冶       </td>   <td>中山大学;火烈鸟网络(广州)股份有限公司</td>   <td>一种基于强化学习的趋势交易方法及系统</td>   <td>广东省</td>   <td>CN111861752A</td>   <td>2020-10-30</td>   <td>本发明涉及一种基于强化学习的趋势交易方法及系统,包括以下步骤：获取一金融品种的历史行情数据,从所述历史行情数据划分出训练集和测试集；构建金融技术指标及特征变量,设立交易动作规则；从所述训练集中计算出所述金融技术指标,以所述训练集的所述金融技术指标为输入、以所述动作规则作为约束,对一强化学习模型进行训练,最后获取一经过训练的模型；用所述经过训练的模型对所述测试集的数据进行交易预测,获取交易的初次决策；设置一过滤层,通过所述过滤层进行二次决策；设置一回测平台,通过所述回测平台进行测试,获取交易的最终执行结果。本发明通过强化学习模型充分利用市场信息对小概率的高利润交易时机进行有效捕捉,以此获得盈利。</td>   <td>1.一种基于强化学习的趋势交易方法,其特征在于,包括以下步骤：选择一金融品种,获取所选品种的历史行情数据,所述历史行情数据主要包括t时刻的最高价hight<Sub>t</Sub>、最低价low<Sub>t</Sub>、开盘价open<Sub>t</Sub>、收盘价close<Sub>t</Sub>和成交量volume<Sub>t</Sub>,对所述历史行情数据进行数据清洗及数据处理,从所述历史行情数据划分出训练子集和测试子集；构建金融技术指标及特征变量,设立交易动作规则；从所述训练子集中计算出所述金融技术指标,以所述训练子集的所述金融技术指标为输入、以所述动作规则作为约束,对一强化学习模型进行训练,最后获取一经过训练的强化学习模型；从所述测试子集中计算出所述金融技术指标,以所述测试子集的所述金融技术指标为输入,采用所述经过训练的强化学习模型对所述测试子集进行预测,获取交易的初次决策；设置一过滤层,通过所述过滤层进行二次决策,获取交易的二次决策；设置一回测平台,通过所述回测平台进行测试,获取交易的最终执行结果。</td>   <td>G06Q40/04;G06Q40/06;G06N20/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         叶东山;              陈翔;              许坤桓;              安小洁;                   邱继云       </td>   <td>中山大学</td>   <td>一种花纹板材圆孔尺寸检测方法、系统及装置</td>   <td>广东省</td>   <td>CN111861997A</td>   <td>2020-10-30</td>   <td>本发明公开了一种花纹板材圆孔尺寸检测方法、系统及装置,该方法包括：获取输入图像并对输入图像进行滤波去噪处理,得到去噪图像；基于霍夫圆检测方法对去噪图像进行检测,得到第一区域；提取第一区域的曲线段并生成多个候选圆；对多个候选圆进行筛选后通过快速最小二乘法细化,得到最终候选圆；对最终候选圆进行完整性分析,确认得到最终圆。该系统包括：去噪模块、检测模块、曲线段模块、细化模块和确认模块。该装置包括存储器以及用于执行上述基于违章行为识别的安全施工方法的处理器。通过使用本发明,实现在花纹干扰严重的情况下识别图像中的圆孔。本发明作为一种花纹板材圆孔尺寸检测方法、系统及装置,可广泛应用于图像检测领域。</td>   <td>1.一种花纹板材圆孔尺寸检测方法,其特征在于,包括以下步骤：获取输入图像并对输入图像进行滤波去噪处理,得到去噪图像；基于霍夫圆检测方法对去噪图像进行检测,得到第一区域；提取第一区域的曲线段,筛选曲线段并生成多个候选圆；通过均值漂移聚类方式对多个候选圆进行筛选后通过快速最小二乘法细化,得到最终候选圆；对最终候选圆进行完整性分析,确认得到最终圆。</td>   <td>G06T7/00;G06T7/62;G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         孙颖;              陆遥;              林丽;                   陈海斌       </td>   <td>中山大学</td>   <td>基于深度学习的头颈部淋巴结及引流区自动勾画方法</td>   <td>广东省</td>   <td>CN111862021A</td>   <td>2020-10-30</td>   <td>本发明实施例提供一种基于深度学习的头颈部淋巴结及引流区自动勾画方法,该方法利用人体结构的对称性,将头部区域划分为左右两部分进行训练和预测,从而间接增加了模型训练的数据量,同时减少深度学习模型的规模。本发明使用逐步处理优化的勾画策略,首先采用深度学习模型实现较易分割的淋巴引流区,再采用多任务深度学习模型对淋巴引流区进行优化和淋巴结进行分割,充分利用两者之间的关联性,提高二者的分割准确度。本发明在放射治疗计划工作流程中实施人工智能(Artificial Intelligence,AI)辅助的轮廓勾画方法,能有效提高医疗工作者的工作效率的勾画的一致性,提高头颈部肿瘤放射治疗的精准度。</td>   <td>1.一种基于深度学习的头颈部淋巴结及引流区自动勾画方法,其特征在于,包括以下步骤：步骤(1)：回顾性采集头颈部肿瘤患者的放射治疗前的计划CT影像数据,并对其进行预处理；步骤(2)：提取临床回顾性收集的头颈部肿瘤患者计划CT图像上勾画的淋巴引流区和淋巴结的勾画轮廓,并将轮廓线以内的区域赋值为1,轮廓线以外的区域赋值为0,得到每个危及器官的二进制掩模图像；步骤(3)：从头颈部肿瘤病人计划CT影像识别病人头部区域,得到头部的二进制掩模图像；并计算头部区域沿左右方向的中心线作为左右头部区域左右分割线,对计划CT影像和淋巴引流区及淋巴结的二进制掩模图像进行左右划分；步骤(4)：对划分后的计划CT影像和淋巴引流区及淋巴结的二进制掩模图像进行三维裁剪,基于淋巴引流区的勾画结果提取左右、前后和头脚方向的感兴趣区域；步骤(5)：利用提取的感兴趣区域内的归一化后的计划CT图像和淋巴引流区的二进制掩模图像对深度学习模型A进行五折交叉训练和验证,得到训练好的淋巴引流区分割模型A和所有训练病例的淋巴引流区验证的勾画结果；步骤(6)：将步骤(5)中淋巴引流区的分割结果和感兴趣区域内的计划CT图像作为深度学习模型B的输入,将淋巴引流区和淋巴结的二进制掩模图像作为深度学习模型B的输出,对多任务深度学习模型B进行训练,得到训练好的淋巴结和淋巴引流区分割优化模型B；步骤(7)：采集新的头颈部肿瘤患者的计划CT图像数据,按步骤(1-4)对其预处理,得到左右两侧的感兴趣区域内的计划CT影像,将其分别输入训练好的淋巴引流区分割模型A,得到左右淋巴引流区的初步分割结果；步骤(8)：将步骤(7)中得到的左右两侧的感兴趣区域内的计划CT影像和左右淋巴引流区的初步分割结果输入训练好的淋巴结和淋巴引流区分割优化模型B,得到左右两侧淋巴结和淋巴引流区的分割结果。</td>   <td>G06T7/00;G06T7/11;G16H30/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         孙颖;              陆遥;              林丽;              陈海斌;              何振宇;              李巧巧;              何立儒;              习勉;              邓美玲;                   陈锴       </td>   <td>中山大学</td>   <td>全身多部位放射治疗危及器官自动勾画方法</td>   <td>广东省</td>   <td>CN111862022A</td>   <td>2020-10-30</td>   <td>本发明实施例提供一种全身多部位放射治疗危及器官自动勾画方法,该方法采用基于点云配准方法对不同病人进行刚性配准,将大量病人的危及器官勾画结果进行配准,得到各部位危及器官的分布概率的先验知识图,并通过点云配准方法实现危及器官概率分布在待处理医学影像上的映射,得到先验概率分布,引导卷积神经网络模型的训练。使得卷积神经网络模型可以利用模型难以学习的解剖结构关联信息提高自动勾画的精度,和避免常识性的错误勾画。其中所采用的点云配准技术较传统的基于图像灰度的配准技术,具有速度更快,受噪声影响更小的优势。</td>   <td>1.一种全身多部位放射治疗危及器官自动勾画方法,其特征在于,其基于先验约束深度学习模型,并包括以下步骤：步骤(1)：采集各类肿瘤患者放射治疗前的计划CT图像数据和危及器官轮廓勾画数据,并对其进行预处理；步骤(2)：提取步骤(1)中采集的计划CT图像上危及器官勾画结果,并将轮廓线以内的区域赋值为1,轮廓线以外的区域赋值为0,得到每个危及器官的二进制掩模图像；步骤(3)：对步骤(1)中采集的计划CT图像进行身体分割,将CT值不小于-200的区域置为1,CT值大于-200的区域置为0,得到图像中的空气区域的掩模图像；保留空气区域的三维最大连通区域后,再将处理后的掩模图像中值为1的区域置为0,值为0的区域置为1,得到病人的身体区域的掩模图像,并将掩模图像转化为身体表面点云；步骤(4)：选择一个病人作为参考病人,采用点云刚性配准算法将其余病人的身体表面点云配准至参考病人,得到刚性配准参数；利用刚性配准参数将对应病人的危及器官勾画掩模图像刚性映射至参考病人图像坐标系,将各个危及器官按照病人进行平均,得到各个危及器官的空间分布概率图,其取值范围为0至1的浮点数；步骤(5)：利用步骤(4)中计算得到的刚性配准参数将步骤(4)中生成的危及器官空间分布概率图刚性映射至对应病人的图像坐标系,得到对应病人的危及器官分布的先验知识图；步骤(6)：构建基于金字塔结构的卷积神经网络模型,将步骤(5)中处理后的计划CT图像和对应的各危及器官先验知识图谱作为卷积神经网络模型的输入,各危及器官的掩模图像作为卷积神经网络模型的输出,对卷积神经网络模型进行训练；步骤(7)：获取测试病人的计划CT图像数据,按照步骤(1～5)获取病人的危及器官先验知识图,以验证模式输入步骤(6)中训练好的基于金字塔结构的卷积神经网络模型,输出各危及器官的激励函数预测概率,以最大预测概率取对应通道的标签,得到各危及器官的自动分割掩模图像。</td>   <td>G06T7/00;G06T7/11;G06T7/33;G16H30/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马锦华;              李璐圆;              严晓威;                   陈曦       </td>   <td>中山大学</td>   <td>一种基于双指标度量学习的工具异常放置检测方法及系统</td>   <td>广东省</td>   <td>CN111862036A</td>   <td>2020-10-30</td>   <td>本发明公开了一种基于双指标度量学习的工具异常放置检测方法及系统,该方法包括：获取正确放置工具的工具箱图片并标记,得到标记信息；根据标记信息将工具箱图片中的工具单独裁剪出正确工具图片,并对正确工具图片进行数据扩充,得到训练集和验证集；通过训练集和验证集对预设的基于双指标度量学习的工具异常放置检测模型进行训练和验证并调整参数,得到最终工具异常检测模型；获取当前工具箱图片,根据标记信息对当前工具箱图片裁剪出各个当前工具图片后,输入到最终工具异常检测模型并判断是否放置异常。本发明作为基于双指标度量学习的工具异常放置检测方法及系统,可广泛应用于图像处理领域。</td>   <td>1.一种基于双指标度量学习的工具异常放置检测方法,其特征在于,包括以下步骤：获取正确放置工具的工具箱图片并标记,得到标记信息；根据标记信息将工具箱图片中的工具单独裁剪出正确工具图片,并对正确工具图片进行数据扩充,得到训练集和验证集；通过训练集和验证集对预设的基于双指标度量学习的工具异常放置检测模型进行训练和验证并调整参数,得到最终工具异常检测模型；获取当前工具箱图片,根据标记信息对当前工具箱图片裁剪出各个当前工具图片后,输入到最终工具异常检测模型并判断是否放置异常。</td>   <td>G06T7/00;G06T7/10;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         詹国栋;              李明;              邹小兵;              潘悦然;              蔡昆京;                   程铭       </td>   <td>昆山杜克大学;中山大学附属第三医院</td>   <td>一种基于表型测量早期发现综合征的系统</td>   <td>江苏省</td>   <td>CN111862091A</td>   <td>2020-10-30</td>   <td>本发明提出了一种基于表型测量早期发现综合征的系统,包括：数据采集模块,用于多角度瞬间采集被测试者人体头部的RGBD图像数据；数据拼接模块,用于拼接多角度RGBD图像数据；3D建模模块,用于还原并重建成3D头部曲面模型；标志识别模块,用于识别3D头部曲面模型中人体的标志性特征点；表型计算模块,用于根据计算和/或训练人体表型；综合症模型训练模块,用于训练患有不同综合症的模型；综合症预测模块,用于计算被测量者患有各综合征的概率以及其中可能性最大的综合症；测量报告模块,用于生成测量报告。本发明能精确测量并计算人体3D数据的特征点、曲面距离及其他具有医学意义的数据,并根据所测量的数据预测出被测试者最可能患有的疾病。</td>   <td>1.一种基于表型测量早期发现综合征的系统,其特征在于,包括：数据采集模块,用于多角度瞬间采集被测试者人体头部的RGBD图像数据；其中,所述RGBD图像数据包括颜色信息和(x,y,z)坐标信息；数据拼接模块,用于拼接由数据采集模块所采集的多角度RGBD图像数据；3D建模模块,用于根据拼接好的RGBD图像数据还原并重建成3D头部曲面模型；标志识别模块,用于识别3D头部曲面模型中人体的标志性特征点,所述标志性特征点包括但不限于与眉毛、眼睛、鼻子、嘴巴和脸轮廓相关的特征点；表型计算模块,用于根据所识别的特征点以及相关器官和部位的RGBD图像数据,计算和/或训练人体表型；综合症模型训练模块,用于基于人体表型的相关数据训练患有不同综合症的模型；综合症预测模块,用于计算被测量者患有各综合征的概率以及其中可能性最大的综合症,并写入测量报告模块中；测量报告模块,用于生成测量报告,展示计算所得的表型性状和综合症预测报告。</td>   <td>G06T7/00;G06K9/00;G06T17/20;G16H50/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         成慧;                   黄浩洸       </td>   <td>中山大学</td>   <td>一种基于彩色-深度相机的动态物体参数化建模方法</td>   <td>广东省</td>   <td>CN111862139A</td>   <td>2020-10-30</td>   <td>本发明涉及一种基于彩色-深度相机的动态物体参数化建模方法,包括以下步骤：S1、获得物体的彩色-深度图片序列；S2、获取像素级别保边的前景物体mask；S3、进行动作分析,得到目标物体的完整骨骼模型；S4、使用三维重建算法进行三维重建,得到三维重建模型；S5、使用绑定算法把三维重建模型上的每个面片都刚性绑定到骨骼模型上；S6、为已经绑定骨骼模型的三维重建模型生成关键帧动画。本发明的基于彩色-深度相机的动态物体参数化建模方法通过把重建模型刚性绑定到骨骼模型,实现赋予传感器所扫描物体参数化特性的目的,可高效地分析出视频中物体的关节信息,通过准确的稠密刚性绑定,为物体模型赋予运动学特性。</td>   <td>1.一种基于彩色-深度相机的动态物体参数化建模方法,其特征在于,包括以下步骤：S1：固定彩色-深度相机,扫描相机视野中的任意动态刚性物体,获得物体的彩色-深度图片序列；S2：使用前后景分割算法对彩色-深度图片序列进行前景物体分割,获取像素级别保边的前景物体mask；S3：进行动作分析,得到目标物体的完整骨骼模型；S4：使用三维重建算法进行三维重建,得到经过前景物体分割后的彩色-深度图序列中的动态物体的三维重建模型；S5：使用绑定算法把三维重建模型上的每个面片都刚性绑定到骨骼模型上；S6：为已经绑定骨骼模型的三维重建模型生成关键帧动画,验证三维重建模型的运动学特性。</td>   <td>G06T7/194;G06T7/215;G06T13/40;G06T17/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         吴洋鑫;              高一鸣;              梁小丹;                   林倞       </td>   <td>中山大学</td>   <td>一种基于协同模块级搜索的全景分割网络及方法</td>   <td>广东省</td>   <td>CN111862140A</td>   <td>2020-10-30</td>   <td>本发明公开了一种基于协同模块级搜索的全景分割网络及方法,所述网络包括：前景分支模块,基于模块内搜索空间输出前景掩模；背景分支模块,对骨干网络输出的多层不同尺寸的特征图,基于模块内搜索空间搜索得到语义分割结果作为背景特征输出；模块间搜索空间建立单元,基于注意力机制设计模块间搜索空间,以对前景的RPN特征和背景特征进行融合,并根据特征的信息对前景和背景的通道信息进行动态的调整；训练及路径搜索单元,在训练时通过不放回采样对父网络中不同的子网络进行公平的训练,并于训练后采用路径优先的结构搜索方法对父网络进行搜索,得到最优的子网络。</td>   <td>1.一种基于协同模块级搜索的全景分割网络,包括：前景分支模块,用于基于模块内搜索空间寻求最佳的掩膜分支结构,并用搜索得到的最佳掩膜分支结构替换前景分支模块原本的前景掩模分支中的连续普通卷积层,输出前景掩模；背景分支模块,用于对骨干网络输出的多层不同尺寸的特征图,基于模块内搜索空间搜索出最佳的四层子模块结构替换原本的四层子模块,将具有多尺度的逐层特征通过每层的子模块进行提炼处理,最后进行融合得到语义分割结果作为背景特征输出；模块间搜索空间建立单元,用于基于注意力机制设计模块间搜索空间,以对前景的RPN特征和背景特征进行融合,并根据特征的信息对前景和背景的通道信息进行动态的调整；训练及路径搜索单元,用于在训练时通过不放回采样对父网络中不同的子网络进行公平的训练,并于训练后采用路径优先的结构搜索方法对父网络进行搜索,得到最优的子网络。</td>   <td>G06T7/194;G06T7/11;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张艳;              张鑫;              曲承志;                   苏东       </td>   <td>中山大学</td>   <td>一种点云全局运动优化方法及设备</td>   <td>广东省</td>   <td>CN111862311A</td>   <td>2020-10-30</td>   <td>本发明公开了一种点云全局运动优化方法,根据各视角运动矩阵初值,将其重构为低秩稀疏矩阵再进行矩阵恢复；在已知任意两视角运动情况下,提出通用的约束条件加入迭代过程,有效限制迭代次数,提高算法效率,降低随机噪声带来的影响；提出加入柯西权重项衡量两两视角测量的可靠性,有效降低离群点的影响,提高算法的鲁棒性；适用于已知多视角点云相对运动初值和任意两视角变化的真实值求解精确的全局运动问题,以解决维重构中多视角点云全局优化方法过度依赖配准初值、鲁棒性不高、效率低的缺陷问题,剔除初始运动中的随机噪声和异常值,提高点云全局运动恢复精度,获得更精确的重构模型。</td>   <td>1.一种点云全局运动优化方法,其特征在于,包括：通过配准算法获取任意两个点云之间的相对运动初始值；其中,所述相对运动初始值包括N个视角中任意两视角i,j之间刚性变换的旋转矩阵R<Sub>ij</Sub>和平移向量T<Sub>ij</Sub>；根据所述相对运动初始值建立相对运动的低秩稀疏矩阵；获取的不同视角点云,根据由第i个视角变换到第j个视角的旋转角θ和旋转轴<Image id="icf0001" he="72" wi="63" file="FDA0002587975960000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>生成视角约束条件表达方程；根据不同视角点云之间的关系,建立邻接矩阵和权重矩阵并在每次迭代中通过柯西权函数更新权重矩阵；根据所述视角约束条件表达方程、相对运动的低秩稀疏矩阵和更新后的权重矩阵,建立全局运动最优化问题的数学模型；对所述全局运动最优化问题的数学模型进行凸松弛处理,得到最优化问题表达式；通过拉格朗日乘子法对所述最优化问题表达式进行求解,直到满足所有迭代停止条件后输出求解结果；判断当前是否满足视角约束条件,若不满足则更新相对运动初始值,对权重矩阵进行更新迭代；若满足则根据输出的求解结果计算各视角全局运动矩阵。</td>   <td>G06T17/00;G06T15/00;G06T7/30;G06F17/16;G06F17/11</td>  </tr>        <tr>   <td>中国专利</td>   <td>         任磊;              姬进财;              潘广维;                   杨清书       </td>   <td>中山大学</td>   <td>海洋锋区域的获取方法、装置、计算机设备和存储介质</td>   <td>广东省</td>   <td>CN111860146A</td>   <td>2020-10-30</td>   <td>本申请涉及一种海洋锋区域的获取方法、装置、计算机设备和存储介质。所述方法包括：根据获取的卫星遥感观测资料确定粗估计海洋锋区域、多个无人舰艇到达粗估计海洋锋区域的目标路径和沿所述目标路径到达粗估计海洋锋区域的目标时长；根据接收到的各个无人舰艇到达粗估计海洋锋区域时发送的确认到达信息,确定各个无人舰艇在粗估计海洋锋区域的各个初始位置；根据目标时长内接收到的多组卫星遥感观测资料确定的粗估计海洋锋区域的区域变化信息输出调整指令,以指示各无人舰艇基于各自对应的初始位置进行调整,得到每个无人舰艇的调整后位置；根据接收到的各个调整后位置,确定目标海洋锋区域。采用本方法能够提高海域中海洋锋区域的识别准确度。</td>   <td>1.一种海洋锋区域的获取方法,其特征在于,所述方法包括：获取卫星遥感观测资料,并根据所述卫星遥感观测资料确定粗估计海洋锋区域；其中,所述卫星遥感观测资料包括待识别海域的水色值、温度值、盐度值、叶绿素浓度值以及悬浮泥沙浓度值中的至少一个；根据所述卫星遥感观测资料,确定拟派出的多个无人舰艇到达所述粗估计海洋锋区域的目标路径以及沿所述目标路径到达所述粗估计海洋锋区域的目标时长；其中,所述目标路径用于表征所述多个无人舰艇到达所述粗估计海洋锋区域的最短路径；根据接收到的各个无人舰艇到达所述粗估计海洋锋区域时发送的确认到达信息,确定各个无人舰艇在所述粗估计海洋锋区域的各个初始位置；根据所述目标时长内接收到的多组卫星遥感观测资料,确定所述粗估计海洋锋区域的区域变化信息；根据所述区域变化信息输出调整指令,以指示各所述无人舰艇基于各自对应的初始位置进行调整,得到每个所述无人舰艇的调整后位置；根据接收到的各个所述调整后位置,确定目标海洋锋区域。</td>   <td>G06K9/00;G06K9/46;G06K9/62;G06N20/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>              张鹏       </td>   <td>中山大学</td>   <td>基于多尺度的遥感影像地物分类方法、系统、装置及介质</td>   <td>广东省</td>   <td>CN111860207A</td>   <td>2020-10-30</td>   <td>本发明公开了基于多尺度的遥感影像地物分类方法、系统、装置及介质,方法包括：基于不同的预设尺度,对获取到的遥感影像进行图像分块,得到初始图像块集合；接着基于预设的语义分割模型,对所述初始图像块集合中的各个尺度下的图像块进行分割,得到不同尺度的分类结果；然后将相同尺度下各个图像块对应的分类结果拼接,得到不同尺度下的初始地物分类结果图集合；最后基于投票策略,将所述不同尺度下的初始地物分类结果图集合进行合并,得到目标地物分类结果图。本发明能够消除相邻图像块之间的不连续直线型接缝,具有很强的实用性,可广泛应用于图像处理技术领域。</td>   <td>1.基于多尺度的遥感影像地物分类方法,其特征在于,包括：基于不同的预设尺度,对获取到的遥感影像进行图像分块,得到初始图像块集合；基于预设的语义分割模型,对所述初始图像块集合中的各个尺度下的图像块进行分割,得到不同尺度的分类结果；将相同尺度下各个图像块对应的分类结果拼接,得到不同尺度下的初始地物分类结果图集合；基于投票策略,将所述不同尺度下的初始地物分类结果图集合进行合并,得到目标地物分类结果图。</td>   <td>G06K9/00;G06K9/34;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>              张鹏       </td>   <td>中山大学</td>   <td>基于超像素的遥感影像地物分类方法、系统、装置及介质</td>   <td>广东省</td>   <td>CN111860208A</td>   <td>2020-10-30</td>   <td>本发明公开了基于超像素的遥感影像地物分类方法、系统、装置及介质,方法包括：对获取到的遥感影像进行图像分块,得到第一图像块集合；基于预设的语义分割模型,将所述第一图像块集合进行语义分割,得到语义分割结果；对所述第一图像块集合进行超像素分割,得到超像素分割结果；根据所述超像素分割结果,对所述语义分割结果进行边缘修剪,得到第二图像块集合；将所述第二图像块集合进行图像块拼接,得到地物分类结果图。本发明能够消除相邻图像块之间的不连续直线型接缝,实用性高,可广泛应用于图像处理技术领域。</td>   <td>1.基于超像素的遥感影像地物分类方法,其特征在于,包括：对获取到的遥感影像进行图像分块,得到第一图像块集合；基于预设的语义分割模型,将所述第一图像块集合进行语义分割,得到语义分割结果；对所述第一图像块集合进行超像素分割,得到超像素分割结果；根据所述超像素分割结果,对所述语义分割结果进行边缘修剪,得到第二图像块集合；将所述第二图像块集合进行图像块拼接,得到地物分类结果图。</td>   <td>G06K9/00;G06K9/34;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         衣杨;              赵小蕾;              陈嘉谦;              邱泽敏;              刘东琳;              陈怡华;                   李宁       </td>   <td>中山大学新华学院</td>   <td>目标追踪方法、装置、终端设备和可读存储介质</td>   <td>广东省</td>   <td>CN111815677A</td>   <td>2020-10-23</td>   <td>本发明实施例公开了一种目标追踪方法、装置、终端设备和可读存储介质,该方法包括利用特征金字塔网络从模板帧中提取n个不同尺度的模板特征图；利用特征金字塔网络从检测帧中提取n个不同尺度的检测特征图；利用第i个区域候选子网络根据第i个模板特征图和与所述第i个模板特征图尺度相同的检测特征图在所述检测特征图上确定预设备选数目个备选图像、以及各个备选图像对应的分值和位置；当n个区域候选子网络对n个所述模板特征图和对应的检测特征图处理完成后,根据各个备选图像的分值在所有备选图像中确定分值最高的前m个备选图像的作为跟踪目标,所述跟踪目标对应的位置作为目标位置。本方案实现对小目标的准确跟踪。</td>   <td>1.一种目标追踪方法,其特征在于,该方法包括：利用特征金字塔网络从模板帧中提取n个不同尺度的模板特征图；利用特征金字塔网络从检测帧中提取与n个所述模板特征图的尺度一一对应的n个不同尺度的检测特征图；利用第i个区域候选子网络根据第i个模板特征图和与所述第i个模板特征图尺度相同的检测特征图在所述检测特征图上确定预设备选数目个备选图像、以及各个备选图像对应的分值和位置；当n个区域候选子网络对n个所述模板特征图和对应的检测特征图处理完成后,根据各个备选图像的分值在所有备选图像中确定分值最高的前m个备选图像的作为跟踪目标,所述跟踪目标对应的位置作为目标位置。</td>   <td>G06T7/246;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>              周凡       </td>   <td>中山大学深圳研究院</td>   <td>一种新闻视频的内容关键词提取方法、终端设备及介质</td>   <td>广东省</td>   <td>CN111814770A</td>   <td>2020-10-23</td>   <td>本申请适用于视频处理技术领域,提供了一种新闻视频的内容关键词提取方法、终端设备及介质,通过对目标新闻视频进行内容提取操作得到新闻文本,并对新闻文本进行分词处理得到对应的词语集合；确定目标新闻视频的新闻标题,以及获取预设的命名实体集合；将新闻文本、词语集合、新闻标题以及命名实体集合输入至训练好的关键词提取模型中进行处理,得到词语集合对应的词语得分值矩阵；词语的总得分值是根据词语在新闻文本中出现的概率、词语与新闻标题的相关度、词语在新闻文本中的分布位置得分值以及词语与命名实体集合的匹配度确定得到的；将词语集合中满足预设条件的目标词语确定为新闻视频的内容关键词,从而提高了提取出的内容关键词的准确率。</td>   <td>1.一种新闻视频的内容关键词提取方法,其特征在于,包括：对目标新闻视频进行内容提取操作,得到用于描述所述目标新闻视频的新闻文本,并对所述新闻文本进行分词处理,得到所述新闻文本对应的词语集合；确定所述目标新闻视频的新闻标题,以及获取预设的命名实体集合；将所述新闻文本、所述词语集合、所述新闻标题以及所述命名实体集合输入至训练好的关键词提取模型中进行处理,得到所述词语集合对应的词语得分值矩阵；其中,所述词语得分值矩阵中每个元素的值用于表示所述元素在所述词语集合中对应的词语的总得分值；所述词语的总得分值是根据所述词语在所述新闻文本中出现的概率、所述词语与所述新闻标题的相关度、所述词语在所述新闻文本中的分布位置得分值以及所述词语与所述命名实体集合的匹配度确定得到的；根据所述词语得分值矩阵确定所述词语集合中满足预设条件的目标词语,并将所述目标词语确定为所述新闻视频的内容关键词。</td>   <td>G06K9/00;G06K9/32;G06F40/284;G06F40/295</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              李昊曦;              顾建权;                   胡伟鹏       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于双线性联合CNN的人脸验证方法</td>   <td>广东省</td>   <td>CN106096535B</td>   <td>2020-10-23</td>   <td>本发明公开了一种基于双线性卷积神经网络的人脸验证方法。包括下述步骤：1)使用预先准备的人脸图像进行卷积神经网络(下简称CNN)的训练；2)使用训练集中的人脸图片,进行双线性CNN的微调；3)输入待验证的人脸图片,将两张图片进行切分,提取双线性CNN输出的联合特征。4)得到的向量经过一个自编码网络训练,得到最终的验证结果。本发明基于双线性的CNN的方法,并且通过将原始的双线性神经网络的两个重复输入替换成不同的人脸验证输入图像,提出了一种新的人脸验证描述子,它对光照,遮挡和姿态变化具有鲁棒性,且双线性CNN提取的特征比一般CNN全链接层特征维数更小,减少了参数量,从而使得后续的深度信念网络训练更加容易,提高人脸验证的准确率。</td>   <td>1.一种基于双线性联合CNN的人脸验证方法,其特征在于,包括以下步骤：(1)对输入的人脸图像,分别以多尺度矩形框截取人脸图像的多个部分的图像,作为CNN的输入,利用训练集中的人脸图像对CNN进行预训练,获得初始CNN模型；(2)将两个具有相同参数的初始CNN模型相结合形成新的双线性联合CNN；其卷积一池化层的初始化参数由上述步骤(1)给出,两个CNN模型各自的全链接层则替换为一个联合的三层全链接层,这个三层全链接层的输入是由两个初始CNN最后一层卷积一池化层输出矩阵相乘获得,最后一层的softmax多类分类器结构更换成用于判断是否为同一人脸的二分类器,然后这三层全链接层的参数均初始化为零均值方差为σ的高斯分布随机值；(3)将训练集中的人脸图像进行两两配对,然后分别输入新的双线性联合CNN的两端,根据分类训练结果对整个双线性CNN网络的所有参数进行微调(Fine Tuning)；微调时每次先固定双线性CNN模型其中一边结构的参数,然后对另一边CNN模型的参数使用梯度下降法进行调整,经过多次迭代微调以后,获得用于人脸验证的双线性CNN模型；(4)将采用多尺度多通道多区域的方式所截取的参考人脸图像与检测人脸图像采用三层深度自编码网络进行特征二分类,最终输出人脸验证的识别准确率；所述步骤(3)中,将训练集中的人脸图像进行两两配对,根据训练集中人脸图像是否是同一个人作为分类结果,两张人脸图片根据方向和顺序不同,分别分次输入双线性联合CNN,利用网络分类结果和实际分类结果的差的均方值作为优化准则函数对网络的参数使用梯度下降法进行微调,在梯度反向传播的时候,在两个CNN模型的卷积一池化层输出层,亦即全链接层的输入层,由于矩阵相乘运算对各自矩阵可导,在计算微调参数导数时,每次迭代过程先固定其中一个CNN模型的参数,对另一个CNN模型进行参数微调,然后反过来固定另一个CNN模型的参数,对第一个CNN模型的参数进行微调,经过多次迭代自动调参过程以后,模型总损失函数收敛,从而获得用于人脸验证的双线性CNN模型。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              杨梁;              王腾;              张俊轩;                   王伟轩       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于神经网络及图像关注点的图像描述生成方法</td>   <td>广东省</td>   <td>CN106777125B</td>   <td>2020-10-23</td>   <td>本发明提供一种基于神经网络及图像关注点的图像描述生成方法,该方法采用两层字嵌入结构,而不是原先的一层嵌入结构,这样更有效的学习字表达；图像的特征表达是直接作为m-RNN模型的输入的,这样能充分利用循环层的容量,允许使用小维度的循环层；借助决策软关注机制,本发明将图像显著区域的关注度体现出来,并作为多模态层的一个输入。通过这个方式,有效地利用了目标或场景间的轻重关系,针对性地描绘图像的语义特性。</td>   <td>1.一种基于神经网络及图像关注点的图像描述生成方法,其特征在于,包括以下步骤：S1：构建每一时刻帧t的图像的多模态模型：1)训练集中已标注图像的文本描述信息分成单个字集,用one-hot向量表示对应字,作为模型的文本模块的输入,并经过两个嵌入层投影至一个稠密字表达空间,成为具有语义的字表达向量W<Sub>t</Sub>；2)字表达向量用于循环卷积神经网络RNN某时刻帧t的输入,进行循环卷积神经网络RNN计算,该时刻帧t的循环层激活Rt是由当前时刻帧的字表达向量和之前时刻帧t-1的循环层R<Sub>t-1</Sub>共同决定的；3)已标注图像经过一个卷积神经网络CNN,并提取图像的L个显著特征；4)图像的特征作为LSTM的输入,LSTM中的隐藏层信息采取一种决策‘soft’关注机制可以获得指定区域特征在全局图像的重要程度,其重要程度和其特征通过求期望可以算出包含区域关注信息的上下文向量；5)将以上的字表达向量、循环层信息、图像特征和上下文向量通过转换矩阵投影至同一维度的多模态空间上并直接元素相加,再用元素比例双曲线正切函数激活,最后通过softmax层得到下一字的概率分布；S2：对构建的模型进行训练：整个模型的损失函数是对应图片的文本标注的混乱度,其等价于字集的平均对数似然值,对其使用标准梯度下降算法,通过反向传播算法学习模型参数。</td>   <td>G06K9/62;G06K9/46;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张志勇;              丘昌镇;              荣易成;              王鲁平;                   王亮       </td>   <td>中山大学</td>   <td>基于三色四灯标记识别的无人机姿态估计方法及系统</td>   <td>广东省</td>   <td>CN111784768A</td>   <td>2020-10-16</td>   <td>本发明实施例涉及一种基于三色四灯标记识别的无人机姿态估计方法及系统,通过在目标无人机的机翼和尾翼的尖端设置有四个信号灯作为信号标记,减少了目标无人机姿态变动过程中标志被遮挡的几率；四个信号灯使用三种颜色,提高了无人机飞行姿态估计的鲁棒性,通过对采集的目标无人机的图像进行处理,检测得到图像特征,在识别图像特征与信号灯对应关系后,得到信号灯的二维图像坐标,将信号灯的三维坐标和二维图像坐标输入姿态估计模型中得到目标无人机的位置以及姿态,实现无人机自动实时姿态估计。该方法易挑选避开背景的色调,降低了图像特征的识别难度,解决了现有对无人机飞行姿态估计方法稳定性低的问题。</td>   <td>1.一种基于三色四灯标记识别的无人机姿态估计方法,其特征在于,在目标无人机的机翼和尾翼的尖端设置有四个用于标记的信号灯,四个所述信号灯具有三种颜色,基于三色四灯标记识别的无人机姿态估计方法包括以下步骤：步骤S1.获取采集设备采集具有四个信号灯标志的目标无人机飞行状态的图像,在HSV颜色空间对所述图像进行处理,得到具有高亮度高饱和度的图像特征；步骤S2.根据四个所述信号灯三种颜色的色调从所述图像特征中提取与四个所述信号灯对应的连通域,识别所述连通域与四个所述信号灯一一对应关系,建立四个所述信号灯的二维图像坐标；步骤S3.基于目标无人机建立四个所述信号灯的三维坐标,将所述三维坐标和所述二维图像坐标输入姿态估计模型中,输出目标无人机的位置以及姿态；其中,所述姿态估计模型中设置有采集设备的参数。</td>   <td>G06T7/70;G06T7/187;G06T7/136;G06T7/90</td>  </tr>        <tr>   <td>中国专利</td>   <td>         梁小丹;              谢震宇;              董浩业;                   吴博文       </td>   <td>腾讯科技(深圳)有限公司;中山大学</td>   <td>基于人工智能的虚拟试穿方法、装置、服务器及存储介质</td>   <td>广东省</td>   <td>CN111784845A</td>   <td>2020-10-16</td>   <td>本申请提供了一种基于人工智能的虚拟试穿方法、装置、服务器及存储介质,属于图像处理技术领域。本申请通过获取源衣物图像的至少一个第一关键点和至少一个第二关键点,根据第一关键点在源衣物图像中确定至少两个衣物图像块,实现对衣物图像中不同衣物区域的划分,进而根据第一关键点和第二关键点,分别对至少两个衣物图像块进行变形,可以根据不同衣物区域的变形程度来对衣物进行变形,以使合并得到的变形衣物图像比较符合衣物实际变形情况,再将变形衣物图像与目标人物图像进行融合,即可以得到试穿效果图像,从而可以缩小虚拟试穿效果和实际试穿效果的差距,提高虚拟试穿的效果,进而提高用户体验。</td>   <td>1.一种基于人工智能的虚拟试穿方法,其特征在于,所述方法包括：获取源衣物图像的至少一个第一关键点和至少一个第二关键点,所述至少一个第一关键点用于标识衣物边缘的源位置,所述至少一个第二关键点为基于目标人物图像对衣物进行变形后的所述至少一个第一关键点的目标位置；根据所述至少一个第一关键点,在所述源衣物图像中确定至少两个衣物图像块；根据所述至少一个第一关键点和至少一个第二关键点,分别对所述至少两个衣物图像块进行变形,得到至少两个变形衣物图像块；对所述至少两个变形衣物图像块进行合并,得到变形衣物图像；将所述变形衣物图像与所述目标人物图像融合,得到试穿效果图像。</td>   <td>G06T19/00;G06Q30/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄敏;              郑健;              刘芳;              张腾;                   毛峰       </td>   <td>中山大学</td>   <td>基于区域整体指引可达最大化的道路指路标志布设方法</td>   <td>广东省</td>   <td>CN106709599B</td>   <td>2020-10-16</td>   <td>本发明涉及一种基于区域整体指引可达最大化的道路指路标志布设方法,包括以下步骤：S1.针对区域内的单条道路,构建指引该条道路的指路标志布设方案；S2.将区域内指引每条道路的指路标志布设方案进行叠加,获取区域道路指路标志叠加布设方案；S3.构建人工蜂群算法优化模型,以区域道路整体指引可达最大化为目标,对叠加布设方案进行优化,获取最优的区域道路指路标志布设方案。</td>   <td>1.基于区域整体指引可达最大化的道路指路标志布设方法,其特征在于：包括以下步骤：S1.针对区域内的单条道路,构建指引该条道路的指路标志布设方案；S2.将区域内指引每条道路的指路标志布设方案进行叠加,获取区域道路指路标志叠加布设方案；S3.构建人工蜂群算法优化模型,以区域道路整体指引可达最大化为目标,对叠加布设方案进行优化,获取最优的区域道路指路标志布设方案；所述人工蜂群算法优化模型对叠加布设方案进行精简优化的具体过程如下：S1.初始阶段：初始化多个蜜源,一个蜜源表示一个可行的区域道路指路标志叠加布设方案；S2.计算各个蜜源的适应度函数：<Image id="icf0001" he="103" wi="425" file="FDA0002540935020000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中f表示蜜源的适应度函数；P(R<Sub>i</Sub>)表示蜜源中道路R<Sub>i</Sub>的指引可达率,Q(R<Sub>i</Sub>)表示蜜源中道路R<Sub>i</Sub>的指引系数,适应度函数值越大,蜜源越优,即区域道路指路标志叠加布设方案越优；S4.引领蜂阶段：此阶段内每个引领蜂对其关联的蜜源进行邻域搜索,获取一个邻域蜜源,然后对比邻域蜜源与原蜜源的适应度函数值,若邻域蜜源的适应度函数值高于原蜜源的适应度函数值,则将邻域蜜源更新为其关联的蜜源；S5.跟随蜂阶段：此阶段内每个跟随蜂采用轮盘赌的选择方式选择一个蜜源进行关联,每个跟随蜂对其关联的蜜源进行邻域搜索,获取一个邻域蜜源,然后对比邻域蜜源与原蜜源的适应度函数值,若邻域蜜源的适应度函数值高于原蜜源的适应度函数值,则将邻域蜜源更新为其关联的蜜源；S6.重复执行步骤S4、S5,重复执行的过程中,若某个蜜源连续k次未更新即放弃该蜜源,与该蜜源关联的引领蜂或跟随蜂转变为侦查蜂,并在搜索空间内重新搜索一个新蜜源取代被抛弃的蜜源,然后执行步骤S4或S5；S7.通过步骤S4、S5、S6获取最优的区域道路指路标志布设方案；所述指引可达率P(R<Sub>i</Sub>)表示如下：          <Image id="icf0002" he="135" wi="290" file="FDA0002540935020000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中S<Sub>1</Sub>(R<Sub>i</Sub>)表示区域内所有指引道路R<Sub>i</Sub>的指路标志指引信息中能够构成可达指引路径的指引信息数量；S(R<Sub>i</Sub>)表示区域内指引道路R<Sub>i</Sub>的所有指路标志指引信息数量；所述道路R<Sub>i</Sub>的指引系数Q(R<Sub>i</Sub>)表示如下：          <Image id="icf0003" he="205" wi="377" file="FDA0002540935020000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中G(R<Sub>i</Sub>)表示道路R<Sub>i</Sub>的道路等级；所述步骤S6中,每个蜜源被引领蜂选择的概率如下所示：          <Image id="icf0004" he="161" wi="199" file="FDA0002540935020000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中P<Sub>i</Sub>表示蜜源i被选择的概率；f<Sub>i</Sub>表示蜜源i的适应度函数值。</td>   <td>G06Q10/04;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              程海杰;                   张权       </td>   <td>中山大学</td>   <td>一种基于非对称度量学习的异构行人再识别方法</td>   <td>广东省</td>   <td>CN109635728B</td>   <td>2020-10-13</td>   <td>本发明公开了一种基于非对称度量学习的异构行人再识别方法,该方法将不同模态下的深度特征进行非对称度量,步骤是：使用两个不共享参数的稀疏自编码器分别将不同模态深度特征投影到共享空间,同时引入全局约束和局部约束去约束不同模态深度特征间的距离,使不同模态特征间的类内距离减小和类间距离增加；将全局约束和局部约束的约束结果作为监督信号反向传播到训练网络中用于修正各个参数。本发明通过缩小不同模态间模态差距,使网络尽可能地忽略模态信息而更加关注身份信息,从而提高行人特征表达力和行人匹配精确度。</td>   <td>1.一种基于非对称度量学习的异构行人再识别方法,其特征在于,包括步骤：在训练模型过程中,输入两种模态下的行人图像,分别提取深度特征；将不同模态下的深度特征进行非对称度量,步骤是：使用两个不共享参数的稀疏自编码器分别将不同模态深度特征投影到共享空间,同时引入全局约束和局部约束去约束不同模态深度特征间的距离,使不同模态特征间的类内距离减小和类间距离增加；将全局约束和局部约束的约束结果作为监督信号反向传播到训练网络中用于修正各个参数；根据深度特征计算全局特征和局部特征的损失,以全局损失、局部损失、以及非对称度量中的全局约束和局部约束之和达到最小化为目标去优化训练模型；将不同模态下的深度特征进行非对称度量,步骤是：首先,将提取的深度特征划分为F<Sup>B</Sup>和F<Sup>R</Sup>两组,<Image id="icf0001" he="70" wi="459" file="FDA0002538650360000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image><Image id="icf0002" he="71" wi="411" file="FDA0002538650360000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中B、R分别代表RGB模态、IR模态,f<Sub>i</Sub><Sup>m</Sup>表示第i个深度特征向量；接着,将两组特征F<Sup>B</Sup>和F<Sup>R</Sup>,分别通过两个不共享参数的稀疏自编码器SAE,每一个稀疏自编码器由两个全连接层构成,分别作为编码器E和解码器D,编码器E负责将不同模态特征投影到共享空间,解码器D负责将编码的特征重新映射到与输入特征空间大小一致的空间；继而,构建重构损失,记为l<Sub>r</Sub>,用于约束SAE的输出和输入,使其尽可能保持一致：l<Sub>r</Sub>＝||f<Sup>B</Sup>,D<Sup>B</Sup>(E<Sup>B</Sup>(f<Sup>B</Sup>))||<Sub>2</Sub>+||f<Sup>R</Sup>,D<Sup>R</Sup>(E<Sup>R</Sup>(f<Sup>R</Sup>))||<Sub>2</Sub>；f<Sup>B</Sup>,E<Sup>B</Sup>,D<Sup>B</Sup>代表模态B的特征、编码器和解码器；f<Sup>R</Sup>,E<Sup>R</Sup>,D<Sup>R</Sup>代表模态R的特征、编码器和解码器；最后,在共享空间,引入全局约束用于约束不同模态特征分布间的差距,引入局部约束用于减小不同模态特征间的类内距离和增加类间距离,并将上述约束结果作为监督信号反向传播回训练模型去修正各个参数；全局约束用于约束不同模态特征分布间的差距,记为l<Sub>global</Sub>＝W(E<Sup>B</Sup>(f<Sup>B</Sup>),E<Sup>R</Sup>(f<Sup>R</Sup>))<Sup>2</Sup>,其中W满足对于任意给定的两个分布X＝N(m<Sub>X</Sub>,C<Sub>X</Sub>)和Y＝N(m<Sub>Y</Sub>,C<Sub>Y</Sub>),m,C分别代表X分布和Y分布的均值和方差,有<Image id="icf0003" he="48" wi="700" file="FDA0002538650360000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>局部约束用于减小不同模态特征间的类内距离和增加类间距离,记为l<Sub>local</Sub>＝(max(d(f,p))-min(d(f,n))+α),p∈A(f)、n∈B(f),A(f)、B(f)分别代表与特征f有相同身份信息和不同身份信息的特征集合,d(·,·)代表两个特征间的欧式距离,α是用于控制正样本对和负样本对间距的超参数。</td>   <td>G06K9/00;G06K9/66</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李梦婷;              李翔;              印鉴;              刘威;                   余建兴       </td>   <td>中山大学</td>   <td>一种基于人脸高维特征的性别预测方法</td>   <td>广东省</td>   <td>CN111753641A</td>   <td>2020-10-09</td>   <td>本发明提供一种基于人脸高维特征的性别预测方法,该方法基于ResNet100层的CNN网络结构的人脸识别模型衍生出的性别预测方法,首先通过对百万数量级人脸识别数据的训练,得到人脸高维特征表述的方式,再通过十万张标注性别的人脸照片,利用一个浅层的网络,训练出可以通过人脸照片就可以判断出性别的模型；本发明完全复用了人脸识别过程中的高维特征的计算结果,只需要很小的计算量就可以得到性别的预测结果,同时还可以保持非常高的预测精度。</td>   <td>1.一种基于人脸高维特征的性别预测方法,其特征在于,包括以下步骤：S1：采用100层的卷积神经网络ResNet作为主干网络,使用百万人脸数据集MS1M进行人脸识别模型的训练,生成一个可以使用的深度人脸模型,并把通过100层的参数权重固定,这样每张人脸照片在通过这100层的主干网络之后,都可以生成一个高维度的向量；S2：基于S1生成的主干网络,使用10万张带有性别标注的人脸照片输入到主干网络中,把通过主干网络生成的高维向量作为特征,训练一个两层的浅层网络,生成一个预测男女的二分类模型；S3：将S1和S2生成的两个模型部署到服务器中,同时提供人脸识别服务和性别预测服务,实时返回人脸照片对应的身份以及性别属性。</td>   <td>G06K9/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘威;              何枷瑜;              王海明;              朱怀杰;              余建兴;              印鉴;                   邱爽       </td>   <td>中山大学</td>   <td>一种基于时空图信息最大化模型的路段特征表示学习算法</td>   <td>广东省</td>   <td>CN111754019A</td>   <td>2020-10-09</td>   <td>本发明提供一种基于时空图信息最大化模型的路段特征表示学习算法,该方法考虑路段状态的时间性,深入挖掘了路段的时间信息,并采用了最大化互信息机制,把路段信息、时间信息和交通信息三者之间相互影响、相互作用的关系提取并利用到基于神经网络的学习算法中。得到的路段表示更好的反映实时的全局交通情况,并学习到路段间上下游实时的依赖关系,大大提高了旅行时间预测的精度。</td>   <td>1.一种基于时空图信息最大化模型的路段特征表示学习算法,其特征在于,包括以下步骤：S1：从路网中提取路段属性,生成路段初始向量,并基于历史数据中的轨迹构造时间邻接矩阵；S2：对交通状况采用CNN和max-pooling操作,提取对应的交通状态/流表示；S3：将S1、S2和S3的数据输入到编码器进行训练,获得实时的路段表示；S4：将得到的路段表示作为目标,通过全连接层得到路段的动态表示。</td>   <td>G06Q10/04;G06Q50/14;G06F16/29;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         梁姗姗;              袁进;              钟培勋;              钟菁;              李新宇;                   张军       </td>   <td>中山大学</td>   <td>一种基于角膜共聚焦图像的菌丝筛查系统</td>   <td>广东省</td>   <td>CN111754457A</td>   <td>2020-10-09</td>   <td>本发明公开了一种基于角膜共聚焦图像的菌丝筛查系统,包括：图像获取模块、诊断模块和菌丝可视化模块；图像获取模块用于获取待检测角膜共聚焦图像；诊断模块用于将待检测角膜共聚焦图像输入至预设的菌丝诊断模型中,以使菌丝诊断模型,对待检测角膜共聚焦图像进行图像特征提取,并根据所提取的图像特征判断检测角膜共聚焦图像是否均在菌丝；菌丝可视化模块用于在判断待检测角膜共聚焦图像存在菌丝时,从待检测角膜共聚焦图像提取菌丝区域,生成包含菌丝区域的可视化图像。通过实施本发明实施例,能自动进行对角膜进行菌丝检测,并在有菌丝时进行可视化展示；一方面提高了检测效率,另一方面无需依赖医生经验,避免了由于经验不足造成的误诊。</td>   <td>1.一种基于角膜共聚焦图像的菌丝筛查系统,其特征在于,包括：图像获取模块、诊断模块和菌丝可视化模块；所述图像获取模块,用于获取待检测角膜共聚焦图像；所述诊断模块,用于将所述待检测角膜共聚焦图像输入至预设的菌丝诊断模型中,以使所述菌丝诊断模型,对所述待检测角膜共聚焦图像进行图像特征提取,并根据所提取的图像特征判断所述检测角膜共聚焦图像是否存在菌丝；所述菌丝可视化模块,用于在判断所述待检测角膜共聚焦图像存在菌丝时,从所述待检测角膜共聚焦图像提取菌丝区域,生成包含所述菌丝区域的可视化图像。</td>   <td>G06T7/00;G06T7/11;G06T7/136;G06T5/30;G06N3/04;G06N3/08;G16H15/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李翔;              李梦婷;              印鉴;              刘威;                   余建兴       </td>   <td>中山大学</td>   <td>一种面向无感场景的人脸识别方法</td>   <td>广东省</td>   <td>CN111738059A</td>   <td>2020-10-02</td>   <td>本发明提供一种面向无感场景的人脸识别方法,该方法先通过对百万数量级的高质量的人脸识别数据的训练,得到人脸高维特征表述的方式,在实际场景中,再通过对上传人脸照片质量可量化的评估,结合高维特征向量和底库中其他特征向量的相似度,以及灵活动态调整参数阈值的方式,识别其真实身份,提高无感场景下的人脸识别率；本方法通过对抓拍照片的质量评估,结合高维度特征向量相似度的阈值设置等技术,可以在无感场景下大幅度提高人脸识别的准确率。</td>   <td>1.一种面向无感场景的人脸识别方法,其特征在于,包括以下步骤：S1：人脸向量计算并匹配底库；S2：人脸照片质量评估和目标相似度结果判断；S3：基于S2输出的评估结果,对照片进行不同的后续处理,如果是识别或者注册成功,则还要加入到一个临时队列中,对接下来照片进行宽松阈值的识别判断,从而降低一人由于多张不同角度照片造成识别错误的概率,将不同阶段的结果存储到对应表中。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林凯荣;              李文静;                   梁汝豪       </td>   <td>中山大学</td>   <td>基于GIS-神经网络集成的山洪灾害风险区划及预测方法</td>   <td>广东省</td>   <td>CN108280553B</td>   <td>2020-10-02</td>   <td>本发明涉及一种基于GIS-神经网络集成的山洪灾害风险区划及预测方法,包括：S1.利用关联规则挖掘山洪灾害中风险因子与风险等级之间的关联关系,辨识风险因子,构建定量化的山洪灾害风险评价指标体系；S2.采用层次分析法确定危险性和易损性指标体系及其权重,生成各要素图层；S3.利用ArcGIS将山洪灾害危险性和易损性分布图层叠加得到山洪灾害风险分布图；S4.采用ISO最大似然法聚类及自下而上区域合并与自上而下定性分析结合的方法,形成山洪灾害风险区划；S5.利用Elman神经网络分析评价指标与风险等级、灾情数据之间的非线性关系,构建山洪灾害风险评价及损失预估模型。本发明解决了变化环境下山洪灾害评估中的空间尺度不确定性问题,可广泛用于山洪灾害风险评估。</td>   <td>1.一种基于GIS-神经网络集成的山洪灾害风险区划及预测方法,其特征在于,包括以下步骤：S1.利用关联规则挖掘山洪灾害中风险因子与风险等级之间的关联关系,辨识风险因子,构建定量化的山洪灾害风险评价指标体系；首先构建山洪灾害风险指标因子数据库,用项集A来表示,如式①所示,将山洪灾害的风险等级用项集B来表示,如式②所示,同时,考虑径流量在暴雨山洪灾害发生过程中所起表征作用,以网格化后流域的典型断面为研究对象,将河段径流量变化幅度划分阈值,用项集C来表示,如式③所示,由此构建用于山洪灾害“风险因子-径流量变化-灾害等级”不确定性分析的事物集；A＝{A<Sub>1</Sub>,A<Sub>2</Sub>,A<Sub>3</Sub>,...,A<Sub>m</Sub>}                       ①B＝{B<Sub>1</Sub>,B<Sub>2</Sub>,B<Sub>3</Sub>,...,B<Sub>n</Sub>}                        ②C＝{C<Sub>1</Sub>,C<Sub>2</Sub>,C<Sub>3</Sub>,...,C<Sub>n</Sub>}                        ③其中,A为山洪灾害的环境条件因素项集,A<Sub>1</Sub>,A<Sub>2</Sub>,A<Sub>3</Sub>,…,Am分别对应于某个山洪灾害案例的环境条件因素状态的组合；B为山洪灾害的风险等级项集,B<Sub>1</Sub>,B<Sub>2</Sub>,B<Sub>3</Sub>,…,Bn表征了山洪灾害的风险特征；C为典型断面径流量的变化幅度项集,C<Sub>1</Sub>,C<Sub>2</Sub>,C<Sub>3</Sub>,…,Cn表征了山洪灾害案例中径流量的变化特征；关联规则用X＝&gt;Y来表示,其中X、Y是项集,I是所有项集的总和,X∈I,Y∈I,X∩Y＝Φ,X＝&gt;Y表示在数据库的事件中,包含项集X的事件同时也包含项集Y的可能性；然后,在遍历所有项集的基础上,利用式④和式⑤进行关联规则分析,挖掘不同灾害环境组合条件项与不同灾害等级项之间的强关联规则；进一步地,探究其间的物理成因,与关联规则的分析结果进行验证；进而对各暴雨山洪灾害风险因子在径流量变化、暴雨山洪灾害风险评价中的作用大小及其不确定性进行定量分析,构建相应的风险评价指标体系,为研究更加符合暴雨山洪灾害成因机理和客观规律的风险评价模型提供科学合理的物理基础；          <Image id="icf0001" he="88" wi="700" file="FDA0002619198090000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0002" he="92" wi="700" file="FDA0002619198090000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,式④为支持度的计算式,表示在数据库中项集X和项集Y同时出现的概率,D<Sub>X∪Y</Sub>为包含X且包含Y的元组数,D表示总元组数；式⑤为置信度的计算式,表示在出现项集X的事件中,项集Y也出现的概率,D<Sub>X∪Y</Sub>为包含X且包含Y的元组数,D<Sub>X</Sub>表示包含X的元组数；S2.采用层次分析法确定危险性和易损性指标体系及其权重,生成各要素图层；所述的S2步骤具体包括：S21.建立层次递阶结构,层次分析的结构模型分为三层,从最高层到最底层依次是目标层,准则层和变量层；S22.构建判断矩阵；判断矩阵是针对上一层次中某元素而言,评定该层次中各元素间相对重要程度的判断；S23.计算判断矩阵的最大特征向量与特征根；首先,计算判断矩阵每行所有元素的几何平均值<Image id="icf0003" he="78" wi="80" file="FDA0002619198090000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>          <Image id="icf0004" he="163" wi="689" file="FDA0002619198090000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        得到<Image id="icf0005" he="80" wi="482" file="FDA0002619198090000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>然后,将<Image id="icf0006" he="78" wi="58" file="FDA0002619198090000025.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>归一化,即计算：          <Image id="icf0007" he="248" wi="700" file="FDA0002619198090000026.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        得到<Image id="icf0008" he="79" wi="483" file="FDA0002619198090000027.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>即为所求特征向量的近似值,也是各因素的相对权重ω；最后,计算判断矩阵的最大特征值λ<Sub>max</Sub>：          <Image id="icf0009" he="151" wi="383" file="FDA0002619198090000028.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,<Image id="icf0010" he="78" wi="125" file="FDA0002619198090000029.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为向量Aω的第i个元素；S24.计算判断矩阵一致性指标,并检验其一致性；引入一致性比率CR：          <Image id="icf0011" he="125" wi="192" file="FDA0002619198090000031.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0012" he="124" wi="278" file="FDA0002619198090000032.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,n为判断矩阵阶数,CI为一致性指标,CR为随机一致性比率,RI为随机一致性指标；若CR&lt;0.1,判断矩阵具有很好一致性,判断合理；若CR＝0.1,判断矩阵具有较好一致性,判断较合理；若CR&gt;0.1,判断矩阵不符合一致性原则,需重新调整S25.通过加权综合计算得到准则层C层指标的相对权重；通过步骤S21至S24的计算,得到一层指标相对于上一层指标的权重,再计算指标层各指标相对于目标层的总权重,其计算公式为：目标层对准则层的相对权重为：<Image id="icf0013" he="94" wi="665" file="FDA0002619198090000033.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>准则层对指标层的相对权重为：<Image id="icf0014" he="93" wi="700" file="FDA0002619198090000034.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>I＝1,2,...,k；则,指标层对目标层的总权重为：          <Image id="icf0015" he="155" wi="388" file="FDA0002619198090000035.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        得到的<Image id="icf0016" he="77" wi="522" file="FDA0002619198090000036.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为指标层相对于目标层的总权重；S3.利用ArcGIS将山洪灾害危险性和易损性分布图层叠加得到山洪灾害风险分布图；S4.采用ISO最大似然法聚类以及自下而上区域合并与自上而下定性分析结合的方法,形成山洪灾害风险区划；S5.利用Elman神经网络分析评价指标与风险等级、灾情数据之间的非线性关系,构建山洪灾害风险评价及损失预估模型；所述的S5步骤中Elman神经网络的输入u(k-1)∈R<Sup>r</Sup>,输出y(k)∈R<Sup>m</Sup>,隐含层输出x(k)∈R<Sup>n</Sup>,承接层输出x<Sub>c</Sub>(k)∈R<Sup>n</Sup>,Elman网络的数学模型为：x(k)＝f(W<Sup>1</Sup>x<Sub>c</Sub>(k)+W<Sup>2</Sup>u(k-1))x<Sub>c</Sub>(k)＝αx<Sub>c</Sub>(k-1)+x(k-1)y(k)＝g(W<Sup>3</Sup>x(k))其中,W<Sup>1</Sup>∈R<Sup>n×n</Sup>、W<Sup>2</Sup>∈R<Sup>n×r</Sup>、W<Sup>3</Sup>∈R<Sup>m×n</Sup>分别为承接层到隐含层、输入层到隐含层、隐含层到输出层的连接权矩阵；f(·)和g(·)分别为隐含层神经元和输出层神经元的激发函数组成的非线性向量函数；设第k步网络的实际输出为<Image id="icf0017" he="77" wi="135" file="FDA0002619198090000041.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>则误差指标函数为：          <Image id="icf0018" he="76" wi="700" file="FDA0002619198090000042.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        所述的S5步骤中利用Elman神经网络,构建山洪灾害风险评价及损失预测模型包括以下步骤：S51.数据准备,将各评价指标原始数据作为输入数据,利用ArcGIS自然间断法将各指标数据分为5个灾害风险等级并作为风险评价Ⅰ,将基于历史灾情评价的损失风险评估结果作为损失评价集Ⅱ,将风险评价集Ⅰ和损失评价集Ⅱ作为输出数据,随机选取训练样本和验证样本,训练样本和验证样本的比例为3:1；S52.数据预处理,采用premnmx函数对输入和输出数据进行归一化处理至[-1,1]区间内；S53.Elman神经网络构建,选择单隐含层的神经网络即三层Elman神经网络来进行模拟灾害风险评价和灾害损失预估,其中,隐含层传输函数选择非线性的双曲正切函数tanh或tansig,其公式为          <Image id="icf0019" he="127" wi="517" file="FDA0002619198090000043.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        输出层使用线性传输函数purelin,训练函数选择带有动量项的自适应学习算法traingdx；S54.Elman神经网络训练,根据S3步骤构建的神经网络,分别对灾害风险评价和灾害损失预估利用训练输入数据进行调试,traingdx训练方法设定训练步数为10000步,期望训练精度根据不同的数据输入,选择0.008-0.05不等,通过比较模拟值和实际值的拟合程度,确定模型隐含层节点；S55.Elman神经网络预测,将验证输入数据分别输入到训练好的灾害风险评价和灾害损失预估模型中进行预测,分别得到山洪灾害风险评价结果和山洪灾害损失预估结果,应用上述训练好的网络,将各指标图层栅格值作为输入数据,模拟灾害风险及损失,利用ArcGIS输出山洪灾害风险评价等级图和灾害损失预测分布图；S56.误差分析与泛化验证,通过误差函数评价模拟效果,并更换训练数据和验证数据,对模型的泛化能力进行验证。</td>   <td>G06Q10/04;G06Q10/06;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         余阳;              朱李设;                   吴晓鹏       </td>   <td>广州天源信息科技股份有限公司;中山大学</td>   <td>一种面向精准营销的电信用户消费画像的构建方法</td>   <td>广东省</td>   <td>CN107358269B</td>   <td>2020-10-02</td>   <td>本发明涉及一种面向精准营销的电信用户消费画像的构建方法,包括以下步骤：对用户话单数据做数据预处理,得到用户消费行为数据；将用户话单数据和新套餐计费规则作为离线计费引擎输入,计算用户在新套餐下消费额,利用新旧套餐下消费额给用户标定类标签；训练新套餐的随机森林模型；根据随机森林模型筛选重要特征；针对每个重要特征,计算新套餐的非潜在用户和潜在用户的分裂点；利用重要特征和相应分裂点构建潜在用户的用户画像。本发明利用离线计费引擎,计算用户在新套餐下消费额,为用户标类标签,利用随机森林模型筛选重要特征并构建用户画像,利于新套餐的精准营销。</td>   <td>1.一种面向精准营销的电信用户消费画像的构建方法,其特征在于,包括以下步骤：步骤S1、对用户话单数据做数据预处理,得到用户消费行为数据；步骤S2、将用户话单数据和新套餐计费规则作为离线计费引擎的输入,计算用户在新套餐下消费额,利用新旧套餐下消费差额给用户标定类标签；步骤S3、利用用户消费行为数据和所标定的相应类标签,训练新套餐的随机森林模型；步骤S4、通过随机森林模型筛选重要特征,得到重要特征集合；步骤S5、针对每个重要特征,计算新套餐的非潜在用户和潜在用户的分裂点；步骤S6、利用重要特征和相应分裂点构建新套餐的潜在用户的用户画像；所述步骤S4中,通过随机森林模型筛选重要特征时,计算特征重要性的方法如下：若随机森林中共有n棵树,特征个数为m：S41、对于n棵树中的每棵树以及m个特征中的每个特征,计算特征重要性；特征i在树j中的重要性记为Imp(i_j)＝sum(gain*num),1≤i≤m,1≤j≤n,其中gain是树j中在特征i分裂的节点的信息增益,num是经过该节点的实例个数；S42、将每棵树中的m个特征的重要性归一化到总和为1,得到每棵树的重要性向量,树j的重要性向量记为vj；S43、对步骤S42所求得的n个向量求均值,得到n棵树的平均重要性向量v＝(v1+v2+……+vn)/n；S44、将平均重要性向量中的m个元素归一化到总和为1；两次归一化使用的方法都是Z-score标准化方法；所述步骤S5的方法如下：对于给定样本集D和重要特征A,假定重要特征A在样本集D上出现n个不同的取值,将这些值从小到大排序,记为{a<Sup>1</Sup>,a<Sup>2</Sup>,……,a<Sup>n</Sup>},选取[a<Sup>i</Sup>,a<Sup>i+1</Sup>)的中位点(a<Sup>i</Sup>+a<Sup>i+1</Sup>)/2为分裂点,分裂点将{a<Sup>1</Sup>,a<Sup>2</Sup>,……,a<Sup>n</Sup>}划分为大于分裂点和小于分裂点两类,记为+、-两类；对于{a<Sup>1</Sup>,a<Sup>2</Sup>,……,a<Sup>n</Sup>}每两个相邻点,都计算得到一个分裂点,共n-1个分裂点；选择信息增益最大的分裂点a作为最终的分裂点：          <Image id="icf0001" he="41" wi="700" file="FDA0002491146290000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image></td>   <td>G06K9/62;G06Q30/02;G06Q50/30</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李立;              蒋新华;              崔春艳;              李姣;                   谢菲       </td>   <td>中山大学肿瘤防治中心(中山大学附属肿瘤医院、中山大学肿瘤研究所)</td>   <td>基于机器学习的乳腺钼靶及MR图像影像组学模型的建立方法</td>   <td>广东省</td>   <td>CN111739033A</td>   <td>2020-10-02</td>   <td>本发明提供一种基于机器学习的乳腺钼靶及MR图像影像组学模型的建立方法,包括(1)乳腺钼靶和MR图像收集、(2)建立基于机器学习的乳腺钼靶和MR图像分割算法、(3)建立乳腺钼靶和MR图像的影像组学分析方法和风险预测模型。步骤(2)是先完成1000例乳腺钼靶和MR图像病灶区域及其他感兴趣区域的手动勾画,利用该组数据,建立训练样本和验证样本；用机器学习算法进行肿瘤区域和非肿瘤区域的特征参数筛选和组织分类算法的筛选；使用筛选出的特征参数及组织分类算法,通过机器学习建立分类器预测模型,完成对病灶区域或感兴趣区域内的像素/体素与背景像素/体素的分类区分,从而实现对图像的分割。</td>   <td>1.一种基于机器学习的乳腺钼靶及MR图像影像组学模型的建立方法,其特征在于,包括如下步骤：(1)乳腺钼靶和MR图像收集采用影像工作站从医学图像存储及传输系统存储服务器调阅影像,乳腺钼靶和MR图像,格式均为DICOM图像格式,图像均为数字图像；并在图像正式处理之前进行预处理,去除干扰结果的噪声和杂质等；(2)建立基于机器学习的乳腺钼靶和MR图像分割算法首先由经验丰富的影像科医生完成1000例乳腺钼靶和MR图像病灶区域及其他感兴趣区域的手动勾画,利用该组数据,建立训练样本和验证样本；用人工神经网络,贝叶斯网络,支持向量机和决策树学习算法进行肿瘤区域和非肿瘤区域的特征参数筛选和组织分类算法的筛选；使用筛选出的特征参数及组织分类算法,通过机器学习建立分类器预测模型,完成对病灶区域或感兴趣区域内的像素/体素与背景像素/体素的分类区分,从而实现对图像的分割；同时用其余1000例乳腺钼靶和MR图像上手动勾画的靶区作为验证集,以测试自动化分割算法的准确性并以此进一步优化分割算法；(3)建立乳腺钼靶和MR图像的影像组学分析方法和风险预测模型进行乳腺钼靶和MR图像的特征提取,提取特征后,使用到机器学习方法对放射组学特征进行筛选及降维,使用机器学习方法进行预后预测；将训练集及验证集以4：1随机划分；以训练集建立模型,以验证集进行模型验证、评价。</td>   <td>G06T7/11;G06K9/32;G06K9/62;G16H30/20;G16H50/30;G06N20/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘楠;              王玥琪;              李晓丽;              陈川;              郑子彬;              严强;                   李辉忠       </td>   <td>深圳前海微众银行股份有限公司;中山大学</td>   <td>一种联邦学习模型的训练方法及装置</td>   <td>广东省</td>   <td>CN111723947A</td>   <td>2020-09-29</td>   <td>本发明公开了一种联邦学习模型的训练方法及装置,包括：客户端获取服务器广播的第k-1次迭代的全局模型参数,其中,k为正整数,再将全局模型参数作为设有正则化约束的本地模型参数,使用本地数据进行第k次迭代训练,得到第k次迭代训练的本地模型参数,其中,正则化约束是根据服务器的全局模型参数和客户端的本地模型参数确定的,通过正则化约束优化模型中的梯度,进而减小极端数据对本地模型参数训练的影响,提高本地模型参数对非独立同分布数据训练的准确率,然后将第k次迭代训练的本地模型参数发送给服务器,以使服务器更新第k次迭代的全局模型参数,提高全局模型参数对非独立同分布数据训练的准确率。</td>   <td>1.一种联邦学习模型的训练方法,其特征在于,包括：客户端获取服务器广播的第k-1次迭代的全局模型参数；所述k为正整数；所述客户端将所述全局模型参数作为设有正则化约束的本地模型参数,使用本地数据进行第k次迭代训练,得到第k次迭代训练的本地模型参数；所述正则化约束是根据所述服务器的全局模型参数和所述客户端的本地模型参数确定的；所述客户端将所述第k次迭代训练的本地模型参数发送给所述服务器,以使所述服务器更新第k次迭代的所述全局模型参数。</td>   <td>G06N20/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李煜政;              刘智斌;              陈川;              刘楠;              郑子彬;              严强;                   李辉忠       </td>   <td>深圳前海微众银行股份有限公司;中山大学</td>   <td>一种应用于区块链的联邦学习方法及装置</td>   <td>广东省</td>   <td>CN111723946A</td>   <td>2020-09-29</td>   <td>本发明公开了一种应用于区块链的联邦学习方法及装置,其中方法为：第一委员会节点获取来自任一非委员会节点的第一本地模型信息；所述第一委员会节点根据所述第一委员会节点的本地验证数据集和所述第一本地模型信息,确定所述第一委员会节点对所述非委员会节点的第一验证结果；所述第一委员会节点将所述第一验证结果发送至各第二委员会节点；若所述第一委员会节点确定所述各委员会节点对所述第一本地模型信息达成共识,则至少根据所述第一本地模型信息,更新联邦学习模型。上述方法应用于金融科技(Fintech)时,共识所述第一本地模型信息后才准予参与联邦学习模型的训练,从而可以及时发现区块链节点联合作恶。</td>   <td>1.一种应用于区块链的联邦学习方法,其特征在于,适用于包括各委员会节点和各非委员会节点的区块链,所述方法包括：第一委员会节点获取来自任一非委员会节点的第一本地模型信息；所述第一本地模型信息是基于所述非委员会节点的本地测试数据集训练得到的；所述第一委员会节点为所述各委员会节点中任一委员会节点；所述第一委员会节点根据所述第一委员会节点的本地验证数据集和所述第一本地模型信息,确定所述第一委员会节点对所述非委员会节点的第一验证结果；所述第一委员会节点将所述第一验证结果发送至各第二委员会节点；所述第一验证结果用于结合各第二验证结果供所述各委员会节点共识所述第一本地模型信息；所述各第二委员会节点为所述各委员会节点中除所述第一委员会节点之外的委员会节点；所述各第二验证结果是根据所述各第二委员会节点的本地验证数据集和所述第一本地模型信息得到的验证结果；若所述第一委员会节点确定所述各委员会节点对所述第一本地模型信息达成共识,则至少根据所述第一本地模型信息,更新联邦学习模型。</td>   <td>G06N20/20;G06F16/27</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李熙莹;                   李国鸣       </td>   <td>中山大学</td>   <td>一种基于时空分类的动态背景差分检测方法、系统及装置</td>   <td>广东省</td>   <td>CN107578424B</td>   <td>2020-09-29</td>   <td>本发明公开了一种基于时空分类的动态背景差分检测方法、系统及装置,方法包括：在时间序列上通过分组采样对图像中的每个像素建立对应的背景模型,并根据待检测像素对背景模型内的像素进行分类,得到粗糙的前景掩模图像；以粗糙的前景掩模图像中的前景像素点为中心,将中心像素点设定邻域范围内的像素点进行分类,并根据设定邻域范围内与中心像素点同类的像素点中属于背景像素点的数目,将中心像素点修正为背景像素点或继续保持为前景像素点。本发明采用了分组采样的方法,增强了对动态背景描述的能力；只采用了与中心像素点同类的像素点来确定前景像素点是否为真实的前景像素点,有利于提高检测的正确率。本发明可广泛应用于运动目标检测领域。</td>   <td>1.一种基于时空分类的动态背景差分检测方法,其特征在于：包括以下步骤：在时间序列上通过分组采样对图像中的每个像素建立对应的背景模型,并根据待检测像素对背景模型内的像素进行分类,得到粗糙的前景掩模图像；以粗糙的前景掩模图像中的前景像素点为中心,将中心像素点设定邻域范围内的像素点进行分类,并根据设定邻域范围内与中心像素点同类的像素点中属于背景像素点的数目,将中心像素点修正为背景像素点或继续保持为前景像素点,从而得到准确的前景掩模图像；所述根据待检测像素对背景模型内的像素进行分类,得到粗糙的前景掩模图像这一步骤,具体包括：在背景模型C中寻找出所有与待检测像素同类的像素,并将所有与待检测像素同类的像素数目记为T,所述与待检测像素同类的像素满足：<Image id="icf0001" he="124" wi="386" file="FDA0002443490040000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中,c<Sub>t</Sub>为待检测像素,ε为给定的第一阈值,<Image id="icf0002" he="108" wi="96" file="FDA0002443490040000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为背景模型C的第j个像素样本,j＝1,2,…,m；判断数目T是否大于给定的第二阈值f<Sub>t</Sub>,若是,则将c<Sub>t</Sub>判定为背景像素点,反之,则将c<Sub>t</Sub>判定为前景像素点,最终得到粗糙的前景掩模图像。</td>   <td>G06T7/215;G06T7/246;G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周凡;              张富为;              王若梅;                   林格       </td>   <td>中山大学</td>   <td>一种基于时空一致性的新闻视频拆条方法</td>   <td>广东省</td>   <td>CN111709324A</td>   <td>2020-09-25</td>   <td>本发明公开了一种基于时空一致性的新闻视频拆条方法。本发明首先通过标注一部新闻视频为参考系新闻视频；然后把待拆条的新闻视频与参考系新闻视频进行时空一致性对应,保存所得到的预切割点与预切割图；再利用人脸检测,对预切割点与预切割图中属于新闻视频开始和结束部分的画面进行删选,得到精确的切割点以及对应的切割图；最后根据精确的切割点进行新闻视频切割,将新闻视频进行拆条。本发明采用时空一致性算法来进行新闻视频的拆条,简化了当前新闻视频拆条的流程,缓解了当前新闻视频标注数据量不足的情况。由于仅需人工标注单个视频,因而减少了重复劳动且提高了视频拆条的准确性,提高了新闻视频拆条的效率。</td>   <td>1.一种基于时空一致性的新闻视频拆条方法,其特征在于,所述方法包括：对随机选取的一部新闻视频进行标注,得到一部参考系新闻视频；把待拆条的新闻视频与所述参考系新闻视频,进行时空一致性对应,即逐帧进行相似度匹配与双阈值检测,当相似度大于设定阈值A时为相似,再当相似度大于设定阈值B(B&gt;A)时,保存所得到的预切割点与预切割图；利用人脸检测,对所述预切割点与预切割图中属于新闻视频开始和结束部分的画面进行删选,得到精确的切割点以及对应的切割图；根据所述精确的切割点进行新闻视频切割,然后利用语音识别、分词工具来进行关键词处理,并进行切割点与对应语音文本的保存,将新闻视频进行拆条。</td>   <td>G06K9/00;G06K9/62;G10L15/26;G06F40/289</td>  </tr>        <tr>   <td>中国专利</td>   <td>         段凯;              陈晓宏;              刘丙军;                   赵铜铁钢       </td>   <td>中山大学</td>   <td>二元水权核算交易方法及系统</td>   <td>广东省</td>   <td>CN111709618A</td>   <td>2020-09-25</td>   <td>本发明提供的一种二元水权核算交易方法及系统,通过构建区域水资源与水环境模拟模型,建立了河道外取水总量控制与河道内排污总量控制之间的动态函数关系,创新性地将区域水量水质模拟纳入水权价值的核算中,能够通过辨识人类取水排污过程对区域水资源可利用量和水环境质量的影响,反映一定来水条件和水质管理目标下可分配取水量与排污量的转换关系,为科学衡量取水权与排污权交易价值、实现取水总量与排污总量的协同管控提供一种简单有效的解决办法。</td>   <td>1.二元水权核算交易方法,其特征在于,包括以下步骤：S1：获取调查区域内的水功能区划、行政区划及流域水系分布,建立区域水资源系统的取水排污概化模型；S2：获取调查区域内水文气象条件、水资源可利用量、水资源开发利用现状与水环境现状,针对调查区域的主要流量与水质控制断面,建立水量平衡方程与污染物平衡方程；S3：获取并测算调查区域内耗水率、排污系数、污染物降解系数特征值,得到综合排污系数；S4：结合调查区域的污水排放标准与水环境质量标准,计算现状取水、排污条件下的可分配新增取水量与最大取水总量；S5：建立在一定来水条件与水质控制目标下新增/削减取水量与削减/新增排污量的函数关系,核算用水户进行取水权与排污权交易的价值标准与调控范围。</td>   <td>G06Q10/06;G06Q40/04;G06Q50/06;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              康丹青;              吴凯;                   朱俊勇       </td>   <td>中山大学</td>   <td>一种基于多任务学习的表面缺陷检测方法</td>   <td>广东省</td>   <td>CN111696079A</td>   <td>2020-09-22</td>   <td>本发明公开了一种基于多任务学习的表面缺陷检测方法,包括：获取具有标注信息的输入图像并将输入图像分为训练集图像和测试集图像；对训练集图像进行剪切,得到缺陷实例并对缺陷实例进行增强,得到增强图像；构建表面缺陷检测网络并将测试集图像和增强图像输入表面缺陷检测网络,得到表面缺陷数据。通过使用本发明,可解决缺陷样本不足问题的同时提高工业缺陷检测的速度和精度。本发明作为一种基于多任务学习的表面缺陷检测方法,可广泛应用于工业视觉缺陷检测领域。</td>   <td>1.一种基于多任务学习的表面缺陷检测方法,其特征在于,包括以下步骤：获取具有标注信息的输入图像并将输入图像分为训练集图像和测试集图像；对训练集图像进行剪切,得到缺陷实例并对缺陷实例进行增强,得到增强图像；构建表面缺陷检测网络并将测试集图像和增强图像输入表面缺陷检测网络,得到表面缺陷数据。</td>   <td>G06T7/00;G06T7/11;G06T5/00;G06K9/62;G01N21/88</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李晓丽;              车春江;              李煜政;              陈川;              郑子彬;              严强;                   李辉忠       </td>   <td>深圳前海微众银行股份有限公司;中山大学</td>   <td>一种基于联邦学习的模型训练的方法及装置</td>   <td>广东省</td>   <td>CN111695696A</td>   <td>2020-09-22</td>   <td>本发明公开了一种基于联邦学习的模型训练的方法及装置,包括：终端获取中央服务器第k次迭代的第一矩阵参数和第二矩阵参数,其中,第一矩阵参数和第二矩阵参数是中央服务器对全局模型参数矩阵进行分解得到的,k为自然数,以此减少终端的模型的参数,降低终端的模型训练时运行所需的内存消耗,然后终端使用训练样本进行训练,确定出第一矩阵参数的更新梯度、第二矩阵参数的更新梯度和偏置参数的更新梯度,通过将第一矩阵参数的更新梯度和偏置参数的更新梯度发送至中央服务器和/或将第二矩阵参数的更新梯度和偏置参数的更新梯度发送至所述中央服务器,减少计算数据,以使中央服务器更新全局模型参数矩阵。</td>   <td>1.一种基于联邦学习的模型训练的方法,其特征在于,包括：终端获取中央服务器第k次迭代的第一矩阵参数和第二矩阵参数；所述第一矩阵参数和所述第二矩阵参数是所述中央服务器对全局模型参数矩阵进行分解得到的；所述k为自然数；所述终端使用训练样本对所述第一矩阵参数和所述第二矩阵参数进行训练,确定出所述第一矩阵参数的更新梯度、所述第二矩阵参数的更新梯度和偏置参数的更新梯度；所述终端将所述第一矩阵参数的更新梯度和所述偏置参数的更新梯度发送至所述中央服务器和/或将所述第二矩阵参数的更新梯度和所述偏置参数的更新梯度发送至所述中央服务器,以使所述中央服务器更新所述全局模型参数矩阵。</td>   <td>G06N20/00;G06N3/04;G06N3/08;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张小虎;              林彬;              甘叔玮;              王杰;                   杨夏       </td>   <td>中山大学</td>   <td>一种基于成像特性的空间弱小多目标检测方法及系统</td>   <td>广东省</td>   <td>CN111696096A</td>   <td>2020-09-22</td>   <td>本发明公开一种基于成像特性的空间弱小多目标检测方法,首先通过中值滤波来平滑图像的噪声；再通过卷积操作寻找图像的高亮区域以获得所有局部最大像素值点；然后根据局部最大像素值点在卷积图像中的分布特征,设定多个子图半径,以局部最大像素值点为中心按子图半径在卷积图像内提取多个子图,构建与子图相同尺度且满足高斯分布的核函数；接着计算目标特性参数,通过该目标特性参数可准确提取各个局部最大像素值点的特征；最后根据目标特性参数,计算局部最大像素值点的MLTC值,根据MLTC值,确定待检测图像中的目标点,并根据MLTC值剔除所述目标点中的重复目标点,获得待检测图像中的空间弱小目标,该方法可实现高数量、高精度的目标检测,检测过程简单高效。</td>   <td>1.一种基于成像特性的空间弱小多目标检测方法,其特征在于,包括：获取带有多个空间弱小目标的待检测图像；对所述待检测图像进行中值滤波,获得平滑图像；根据预先设置的卷积核对所述平滑图像进行卷积处理,获得卷积图像内的局部最大像素值点；根据所述局部最大像素值点在所述卷积图像中的分布特征,设定多个子图半径,以所述局部最大像素值点为中心按所述子图半径在所述卷积图像内提取多个子图,构建与所述子图相同尺度且满足高斯分布的核函数；根据所述核函数以及所述局部最大像素值点,确定所述子图的目标特性参数中的能量响应度、能量集中度,以及根据所述子图,确定所述目标特性参数中的能量转移度；根据所述目标特性参数,计算局部最大像素值点的MLTC值,根据所述MLTC值,确定所述子图中的目标点,并根据所述MLTC值剔除所述目标点中的重复目标点,获得所述待检测图像中的空间弱小目标。</td>   <td>G06T7/00;G06T5/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谢子维;              魏朋旭;              詹宗沅;                   林倞       </td>   <td>中山大学</td>   <td>基于角点引导级联沙漏网络结构学习的真实图像超分辨率模型及方法</td>   <td>广东省</td>   <td>CN111696033A</td>   <td>2020-09-22</td>   <td>本发明公开了一种基于角点引导级联沙漏网络结构学习的真实图像超分辨率模型及方法,模型包括：多尺度特征提取单元,利用级联沙漏网络结构提取输入图像的多尺度信息的特征；分区域重建单元,利用不同深度的多尺度特征分别重建多个初始的超分辨率图像；分区域监督单元,利用角点检测算法将高分辨率图像解耦为平坦、边缘和角点区域,分别监督各个初始的超分辨率图像；角点引导重建单元,利用提取到的图像各区域信息；梯度加权约束单元,基于图像的梯度信息来加权损失函数,加强角点区域的拟合能力。本发明能够避免一幅图像的所有区域被同等对待,最终将重建得到的三个结果加权融合成更符合人类视觉感受的超分辨率图像,有效地提升图像质量。</td>   <td>1.基于角点引导级联沙漏网络结构学习的真实图像超分辨率模型,其特征在于,包括多尺度特征提取单元、分区域重建单元、分区域监督单元、角点引导重建单元以及梯度加权约束单元,所述多尺度特征提取单元和分区域监督单元均与分区域重建单元连接,所述分区域重建单元和梯度加权约束单元均与角点引导重建单元连接；所述多尺度特征提取单元,用于通过级联沙漏网络结构提取输入图像的多尺度信息的特征；所述分区域重建单元,用于通过不同深度的多尺度特征分别重建多个初始的超分辨率图像；所述分区域监督单元,用于通过角点检测算法将高分辨率图像解耦为平坦、边缘和角点区域,分别监督各个初始的超分辨率图像；所述角点引导重建单元,用于通过提取到的图像各区域信息,尤其是角点信息来引导重建超分辨率图像；所述梯度加权约束单元,用于通过图像的梯度信息来加权损失函数,从而约束模型的拟合方向,加强角点区域的拟合能力。</td>   <td>G06T3/40;G06T5/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;                   张权       </td>   <td>中山大学</td>   <td>一种基于深度特征正交分解的可见光-近红外行人再识别方法</td>   <td>广东省</td>   <td>CN111695470A</td>   <td>2020-09-22</td>   <td>本发明公开了一种基于深度特征正交分解的可见光-近红外行人再识别方法,包括：获取训练图像集并提取图像特征,得到全局特征和模态特征；根据全局特征和模态特征进行柱坐标系的特征分解,得到行人身份特征和视角特征；对行人身份特征、视角特征和模态特征计算特征损失函数并根据特征损失函数优化训练模型；基于训练模型对输入图像进行识别。通过使用本发明,可解决现实应用场景中常见的识别率下降的问题,提高行人再识别方法模型的抗干扰和自适应的能力。本发明作为一种基于训练模型对输入图像进行识别方法,可广泛应用于计算机视觉领域。</td>   <td>1.一种基于深度特征正交分解的可见光-近红外行人再识别方法,其特征在于,包括以下步骤：获取训练图像集并提取图像特征,得到全局特征和模态特征；根据全局特征和模态特征进行柱坐标系的特征分解,得到行人身份特征和视角特征；对行人身份特征、视角特征和模态特征计算特征损失函数并根据特征损失函数优化训练模型；基于训练模型对输入图像进行识别。</td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘智勇;              陈晓宏;              谢宇莹;              林凯荣;              涂新军;                   张清涛       </td>   <td>中山大学</td>   <td>一种旱涝急转风险评估方法</td>   <td>广东省</td>   <td>CN111680912A</td>   <td>2020-09-18</td>   <td>本发明公开了一种旱涝急转风险评估方法,其以水文气象变量的为基础,计算其连续N个时间段的平均时间序列为P<Sub>t</Sub>,再从中提取滞后一个N时间段的时间序列为P<Sub>t-1</Sub>,然后根据时间序列P<Sub>t</Sub>和P<Sub>t-1</Sub>,利用阈值水平法确定干旱和洪涝的阈值,此时基于copula联合分布函数,构建P<Sub>t</Sub>和P<Sub>t-1</Sub>之间的联合概率分布函数,并进一步构建P<Sub>t</Sub>和P<Sub>t-1</Sub>之间的旱转涝和涝转旱的条件分布函数模型,以实现对旱转涝和涝转旱的风险概率评估；即通过上述方式建立模型后,旱涝急转风险评估方法能有效地评估某一地区某一时期发生旱转涝灾害的可能性,也可以根据未来模拟预测的气象数据,预测未来气候情景下该地区发生此类风险的可能性,从而为抵抗旱涝灾害和风险管理决策提供准确可靠的依据。</td>   <td>1.一种旱涝急转风险评估方法,其特征在于,包括以下步骤：S1,对于某水文气象变量,计算其连续N个时间内的平均时间序列为P<Sub>t</Sub>,提取滞后一个N时间段的时间序列P<Sub>t-1</Sub>,其中所述P<Sub>t</Sub>为旱涝急转事件的第一个时间序列,所述P<Sub>t-1</Sub>为旱涝急转事件的第二个时间序列；S2,定义干旱和洪涝,即根据时间序列所述P<Sub>t</Sub>和所述P<Sub>t-1</Sub>,利用阈值水平法确定干旱和洪涝的阈值；S3,基于copula联合分布函数,构建所述P<Sub>t</Sub>和所述P<Sub>t-1</Sub>之间的联合概率分布函数；S4,基于构建的联合概率分布函数,进一步构建所述P<Sub>t</Sub>和所述P<Sub>t-1</Sub>之间的旱转涝和涝转旱的条件分布函数模型,以实现对旱转涝和涝转旱的风险概率评估。</td>   <td>G06Q10/06;G06Q50/26;G06F17/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郭裕兰;              张永聪;              陈铭林;                   傲晟       </td>   <td>中山大学</td>   <td>一种基于激光雷达点云数据的三维目标检测方法</td>   <td>广东省</td>   <td>CN111681212A</td>   <td>2020-09-18</td>   <td>本发明公开了一种基于激光雷达点云数据的三维目标检测方法,所述方法根据激光雷达点云的数据特征,采取一种稠密的数据表达形式,以得到稠密的特征并将三维特征转换为二维特征,有效提高运算效率并提升运算精度。</td>   <td>1.一种基于激光雷达点云数据的三维目标检测方法,其特征在于,所述方法包括以下步骤：将点云表示成密集形的表面图,图中行数为K,其中K是激光雷达的通道数；给定一个激光雷达点p＝(x,y,z,r,)l,其中(x,y,z),r和l∈{0,...,K-1}分别表示位置、反射率和点所在层数；点p位于表面图S<Sup>h×w</Sup>的网格(h,w)中,其中h＝l,<Image id="icf0001" he="155" wi="456" file="FDA0002501493520000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表面图根据场景的表面,将三维点投影到二维网格中,对于表面图的每个网格(h,w),通过平均网格内的所有点,获得质心点<Image id="icf0002" he="103" wi="291" file="FDA0002501493520000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>(h,w)网格内的深度的计算如下：          <Image id="icf0003" he="90" wi="287" file="FDA0002501493520000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        然后可以获得表面深度图D<Sub>map</Sub>＝{d}∈R<Sup>H×W</Sup>；表面深度图将深度信息存储在每个网格中；基于体素特征编码层的网格特征编码器,体素特征编码层处理表面图的每个网格以生成该网格的特征,从而生成规则的2D表曲面特征图<Image id="icf0004" he="92" wi="272" file="FDA0002501493520000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>若网格没有任何点,则使用零填充；且网格特征编码器不执行体素特征编码层中的随机采样；具有N种不同分辨率的表面图,即S<Sup>H×W</Sup>,<Image id="icf0005" he="131" wi="507" file="FDA0002501493520000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>由网格特征编码器对其分别进行独立处理,生成三个表面特征图,即S<Sup>H×W</Sup>,<Image id="icf0006" he="124" wi="512" file="FDA0002501493520000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>然后,由特征串接得到一个多尺度表面特征F∈R<Sup>3C×H×W</Sup>：          <Image id="icf0007" he="83" wi="700" file="FDA0002501493520000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        此多尺度表面特征用作后续模块的初始输入；具有表面特征卷积模块,并通过增加一个低分辨率输出的网络反卷积层来获得全分辨率输出；表面特征卷积模块生成的<Image id="icf0008" he="86" wi="271" file="FDA0002501493520000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>中的前视图特征具有与其输入表面特征F相同的分辨率,但特征的维度不同；具有基于深度表面图的前视图特征从前视图到鸟瞰图的视图转换模块,不同对象的深度不同,但是从2d前视伪图像中获取的绝对深度是不相等的；从俯视图特征中得到物体的深度,并在视图变换后对高度进行回归。从热图H<Sup>O</Sup>派生的点表示俯视图中检测对象中心的位置,即,x,z,而参数图P<Sup>O</Sup>包含对象的参数,检测网络由一个公共特征提取器和两个分支,即热图分支和参数分支组成。</td>   <td>G06T7/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              杜灵双;                   李昊曦       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>基于深度学习的人脸识别和年龄合成联合模型的构建方法</td>   <td>广东省</td>   <td>CN106650653B</td>   <td>2020-09-15</td>   <td>本发明提供一种基于深度学习的人脸识别和年龄合成联合模型的构建方法,该方法通过对输入的一对图像进行对齐和PCA和LDA降维的预处理；并通过一个经过训练得到的自动编码器,得到用于身份表示的特征和不同年龄段表示的特征共6组,然后对6对结果经过平行CNN,输出图像相似度,之后加权融合得到匹配结果；该发明对单独的人脸识别或者年龄检测以及共同任务均能得到很好的效果,对光照、姿势影响下的人脸识别也额能取得很好的效果；由于区分开了年龄与人脸身份的特征,因此对跨年龄的人脸识别也具有鲁棒性。并且,可视要求而定调整一些参数和权值,因此非常有灵活性。</td>   <td>1.一种基于深度学习的人脸识别和年龄合成联合模型的构建方法,其特征在于,包括以下步骤：S1：对图片进行切片预处理：根据双眼中心进行对齐,采用PCA和LDA的方式进行降维达到增大类间差距的目的；S2：编码：通过训练数据得到一个自动编码器对输入特征向量进行编码,该编码器的目的是将原图特征通过某种编码方式合成新的特征,用于表达身份或者年龄的相关信息,对任何输入的图片,该编码器将生成六组不同的表达：第一组为身份表达,对原图特征减去平均脸后的映射编码,反映个体的身份的稳定信息；第二组至第六组分别为幼年、少年、成年、中年、老年五个年龄段下原图片的合成图片的表达,这一部分的编码过程与上述相似,不同的是输入是原图信息,这五组编码器的作用是模拟老化过程来合成特定年龄段的图片,然后通过损失函数和一定的约束规则来控制消除年龄对身份表达的影响,即在年龄合成中起到重要作用的特征,降低它在身份表达中起的作用；S3：对每对图像进行身份匹配验证：测试图像与一幅训练图像作为一对,由编码器得到的六对特征分别通过平行CNN,softmax层将给出输入的一对特征的相似度的大小；令I<Sub>a</Sub>,I<Sub>b</Sub>为一对输入图像,则相似得分表示为：s(I<Sub>a</Sub>,I<Sub>b</Sub>)＝softmax(Ws|o(I<Sub>a</Sub>)-o(I<Sub>b</Sub>)|+b<Sub>s</Sub>)其中,o()表示CNN中全连接层的输出,Ws和b<Sub>s</Sub>为softmax层的参数；对这六个结果进行加权平均得到验证结果,其中身份表达占的比重较大；五对年龄合成的相似度表示在五个年龄段里的相似度,作为参考因素,占的比重比身份表达要小,由此将得到两幅图像匹配的概率：score＝as<Sub>1</Sub>+(1-a)(s<Sub>2</Sub>+s<Sub>3</Sub>+s<Sub>4</Sub>+s<Sub>5</Sub>+s<Sub>6</Sub>)；S4：对所有特征得到的相似得分结果进行余弦相似度融合,得到最终的结果。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苟超;              郝坤坤;              卓莹;                   熊宸       </td>   <td>中山大学</td>   <td>基于深度对抗网络的带边界标注信息乳腺肿块图生成方法</td>   <td>广东省</td>   <td>CN111667491A</td>   <td>2020-09-15</td>   <td>本发明提供一种基于深度对抗网络的带边界标注信息乳腺肿块图生成方法,该方法首先从含有肿块病变的乳腺钼靶图片以及对应的肿块标注图片上提取肿块图片和肿块分割图片,并将提取的图片缩放到统一尺寸；然后利用这些提取的图片,根据肿块的形状、大小、边缘、密度来设计肿块图片生成器；最后根据设计的肿块图片生成器,在健康的乳腺钼靶图片上生成形状、大小、边缘、位置都具有多样性的肿块和其对应的肿块标注图片。本发明克服了现有肿块生成方法的不足,充分利用了肿块的病理信息来生成肿块,解决了基于人工智能实现自动诊断过程中医疗图像数据有限且难以标注的问题,大力推动现有人工智能乳腺辅助诊断的研究,具有重大临床意义与实际应用价值。</td>   <td>1.一种基于深度对抗网络的带边界标注信息乳腺肿块图生成方法,其特征在于,包括以下步骤：S1：利用乳腺钼靶数据集中所有含有肿块病变的乳腺钼靶图片以及对应的肿块标注图片,分别提取肿块图片和肿块分割图片,并将提取的图片缩放到统一尺寸；S2：利用步骤S1提取的图片,根据肿块的形状、大小、边缘、密度来设计肿块图片生成器G2,使该生成器能够生成高质量的多样性丰富的肿块图片；S3：利用肿块分割图片,设计肿块分割图片生成器G1来模拟肿块分割区域的形状和大小,并生成无限多的肿块分割图片；S4：利用步骤S3生成的肿块分割图片,模拟其对应于健康乳腺钼靶图片上的位置,并生成肿块标注图片,重复步骤S1中的操作得到提取的健康乳腺组织图片和肿块分割图片；S5：利用步骤S2设计的肿块图片生成器G2在步骤S4中提取到的健康乳腺组织图片上生成肿块,并把生成的肿块还原到健康乳腺钼靶图片的原始位置。</td>   <td>G06T7/11;G06T7/12;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         唐晓颖;              黄念微;                   李婧媛       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>低磁场磁共振图像的皮层下结构子区域的自动分割方法</td>   <td>广东省</td>   <td>CN107492104B</td>   <td>2020-09-15</td>   <td>本发明涉及一种低磁场磁共振图像的皮层下结构子区域的自动分割方法,包括以下步骤：(1)基于已进行子区域分割的皮层下结构进行模板的构建；(2)将低磁场磁共振图像中待进行子区域分割的皮层下结构A进行基于平移和旋转的刚性变换；(3)将步骤(2)进行刚性变换的皮层下结构A通过高度形变微分同胚度量映射算法进行形变,得到形变后的皮层下结构B；(4)对皮层下结构B进行基于提升采样的表面细化；(5)找到皮层下结构B表面与模板表面之间距离最短的各对顶点对,然后将模板各个顶点所属的子区域赋给经过表面细化后的皮层下结构A表面相应的顶点,皮层下结构A根据顶点完成子区域的分割。</td>   <td>1.低磁场磁共振图像的皮层下结构子区域的自动分割方法,其特征在于：包括以下步骤：(1)基于已进行子区域分割的皮层下结构进行模板的构建,所述皮层下结构包括海马体、杏仁核、丘脑、尾状核和壳核；所述步骤(1)进行模板的构建的具体过程为：对于海马体和杏仁核,首先通过专家对高磁场磁共振图像中的皮层下结构进行子区域的分割,然后基于专家已进行分割的皮层下结构进行模板的构建；对于丘脑、尾状核和壳核,是通过功能磁共振图像以及弥散张量图像得到的其子区域分割结果,然后基于分割结果进行模板的构建；(2)将低磁场磁共振图像中待进行子区域分割的皮层下结构进行基于平移和旋转的刚性变换；(3)将步骤(2)进行刚性变换的皮层下结构通过高度形变微分同胚度量映射算法进行形变,得到形变后的皮层下结构；(4)对形变后的皮层下结构进行基于提升采样的表面细化；(5)找到表面细化后的皮层下结构表面与模板表面之间距离最短的各对顶点对,然后将模板各个顶点所属的子区域赋给经过表面细化后的皮层下结构表面相应的顶点,表面细化后的皮层下结构根据顶点完成子区域的分割；所述高磁场磁共振图像为7T或者11T的磁场磁共振图像,所述低磁场磁共振图像为1.5T或者3T的磁场磁共振图像。</td>   <td>G06T7/11</td>  </tr>        <tr>   <td>中国专利</td>   <td>         梁姗姗;                   刘自弛       </td>   <td>中山大学</td>   <td>一种用于眼底图像拼接的图像预处理方法</td>   <td>广东省</td>   <td>CN111652805A</td>   <td>2020-09-11</td>   <td>本发明为一种用于眼底视网膜图像拼接的图像预处理方法,所述方法包括以下步骤：S1获取若干的彩色眼底视网膜图像；S2对所获取的彩色眼底视网膜图像进行裁剪；S3针对S2裁剪后的图像数据进行筛选,筛选掉透光图像以及模糊图像；S4针对S3筛选后的图像进行图像相似度评价,将所在眼底视网膜区域相近的图像归为同一类别；S5针对S4分类完成后的图像,进行类间图像的清晰度评价,筛选出每一类中最清晰的图像,作为图像拼接的数据。本发明的有益效果在于,设计了一套完整的图像预处理方案,能有效的从大量彩色眼底视网膜图像中筛选出图像数据,进行图像拼接。从而能使得在单张图像中显示更加完整的眼底结构,提高医生诊断效率。</td>   <td>1.一种用于眼底图像拼接的图像预处理方法,其特征在于,所述方法包括以下步骤：S1获取若干的彩色眼底视网膜图像；S2对所获取的彩色眼底视网膜图像进行裁剪；S3针对S2裁剪后的图像进行筛选,筛选出透光图像以及模糊图像；S4针对S3筛选后的图像进行图像间相似度评价,按图像所在眼底视网膜区域,对图像进行分类；S5针对S4分类后的图像,进行类间图像清晰度评价；S6保留S5每一类中最清晰的图像数据,用于图像拼接。</td>   <td>G06T3/40;G06T7/00;G06T7/13</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王嘉辉;              孙梓瀚;              郭祥;              江灏;                   蔡志岗       </td>   <td>中山大学</td>   <td>一种基于前后融合成像的三维重建系统及方法</td>   <td>广东省</td>   <td>CN111652967A</td>   <td>2020-09-11</td>   <td>本发明公开一种基于前后融合成像的三维重建系统,包括框体、融合成像模块、三维重建模块,框体存在承载空间,所述的承载空间用于放置物品；融合成像模块拍摄物品的前景图像和物品的后景图像,融合成像模块设置在框体的内侧,融合成像模块与框体连接；三维重建模块通过物品的前景图像和物品的后景图像,结合得到物品的三维模型,三维重建模块与融合成像模块电连接。本发明还公开了一种基于前后融合成像的三维重建方法,包括以下步骤：S1：在承载空间中放置标定物,根据标定物调整融合成像模块的内参数和外参数以及调整三维重建模块的坐标变换矩阵；S2：取出标定物,在标定物的位置上放置目标物品,通过调整后三维重建系统形成物品的三维模型。</td>   <td>1.一种基于前后融合成像的三维重建系统,用于形成物品的三维模型,其特征在于,包括框体、融合成像模块、三维重建模块,其中,所述的框体存在承载空间,所述的承载空间用于放置物品；所述的融合成像模块拍摄物品的前景图像和物品的后景图像,融合成像模块设置在框体的内侧,融合成像模块与框体连接；所述的三维重建模块通过物品的前景图像和物品的后景图像,结合得到物品的三维模型,三维重建模块与融合成像模块电连接。</td>   <td>G06T17/00;H04N13/239;H04N5/225;H04N5/235</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王嘉辉;              李子健;              魏杨燊;              张佰君;                   蔡志岗       </td>   <td>中山大学</td>   <td>一种基于图像高频信息的无参考图像质量评价方法</td>   <td>广东省</td>   <td>CN111652854A</td>   <td>2020-09-11</td>   <td>本发明公开一种基于图像高频信息的无参考图像质量评价方法,包括以下步骤：对待评价图像I使用低通滤波,得到二次退化图像I<Sub>deg</Sub>；对原始待评价图像I和退化图像I<Sub>deg</Sub>进行计算,分别得到图像的梯度图I<Sub>grad</Sub>和退化图像的梯度图I<Sub>grad,deg</Sub>；对梯度图I<Sub>grad</Sub>中所有的像素值求平均值,得到第一结果；对梯度图I<Sub>grad</Sub>进行分块操作,建立分块集合{B}；计算分块集合{B}中所有分块的像素值之和S<Sub>k</Sub>；对集合C按照S的大小进行倒序排列,得到新的集合D；寻找集合D中的前L个元素和它们对应的分块B,找到在梯度图I<Sub>deg,grad</Sub>中相同位置的分块B<Sub>deg</Sub>；对L个分块对B,B<Sub>deg</Sub>求结构相似度,求均值后取倒数,得到第二结果；别对第一结果和第二结果乘上合适的系数α和β后相加,得到代表图像质量的结果Quality(I)。</td>   <td>1.一种基于图像高频信息的无参考图像质量评价方法,其特征在于,包括以下步骤：S1：定义I为待评价的失真图像；定义I(i,j)表示I的中心坐标(i,j)的像素值；其中1≤i≤M,1≤j≤N,所述的M为I的宽度,N为I的高度；S2：对I进行低通滤波,得到二次退化图像I<Sub>deg</Sub>；S3：对I使用梯度算子进行滤波,得到关于I的梯度信息图I<Sub>grad</Sub>,定义I<Sub>grad</Sub>(i,j)表示I<Sub>grad</Sub>的中心坐标(i,j)处的像素值；对I<Sub>deg</Sub>使用梯度算子进行滤波,得到二次退化图像的梯度信息图I<Sub>grad,deg</Sub>,定义I<Sub>deg,grad</Sub>(i,j)表示I<Sub>deg,grad</Sub>的中心坐标(i,j)处的像素值；S4：定义G(i,j)为I<Sub>grad</Sub>中所有像素值的平均值,得到代表图像全局结构信息质量的第一结果；S5：对I<Sub>grad</Sub>进行分块操作,分块操作采取滑动窗口的方式,窗口从左上角开始,按行、按列的优先级顺序进行滑动,对应的分块记做B<Sub>x,y</Sub>,所述的x代表在行方向上为第x个分块,所述的y代表在列方向上为第y个分块,定义左上角的分块为B<Sub>1,1</Sub>,定义{B}为所有分块的集合；S6：定义S<Sub>k</Sub>为{B}中所有分块的像素值之和,其中1≤k≤P,P为分块的数量；定义集合C＝{(B<Sub>k</Sub>,S<Sub>k</Sub>)},即每个分块与对应的像素值之和；S7：对集合{(B,S)}按照S的大小进行倒序排列,得到集合D；S8：选择集合D中的前L个元素,得到L个元素中对应每个元素中的分块B,寻找L个元素中对应每个元素在I<Sub>deg,grad</Sub>中相同位置的分块B<Sub>deg</Sub>,得到L个分块对B,B<Sub>deg</Sub>；S9：对分块对B,B<Sub>deg</Sub>求结构相似度,得到结构相似度的求均值,再取倒数,所述的倒数定义为第二结果；是预设值S10：分别对第一结果和第二结果乘上系数α和β后相加,得到代表图像质量的结果Quality(I),所述的α是预设值；所述的β是预设值；S11：接合专家设定的判断规则,根据Quality(I)得到对图像质量的评价结果。</td>   <td>G06T7/00;G06T7/13</td>  </tr>        <tr>   <td>中国专利</td>   <td>         邓夏君;              王若梅;                   周凡       </td>   <td>中山大学</td>   <td>基于图神经网络的全景分割方法、系统、设备及存储介质</td>   <td>广东省</td>   <td>CN111428726B</td>   <td>2020-09-11</td>   <td>本发明公开了一种基于图神经网络的全景分割方法,包括：从图片中提取多个目标特征；通过实例分割头部网络以得到图片的前景类别概率、背景类别概率及掩膜结果,通过语义分割头部网络以得到图片的初步语义分割结果；通过前景类别概率对新前景图进行处理以生成实例分类结果,并根据掩膜结果从实例分类结果中提取目标实例分割掩膜；通过背景类别概率及初步语义分割结果对新背景图进行处理以生成目标语义分割结果；采用启发式算法对目标实例分割掩膜及目标语义分割结果进行融合,生成全景分割结果。本发明还公开了一种基于图神经网络的全景分割系统、计算机设备及计算机可读存储介质。采用本发明,可利用物体之间的相互关系优化图片的全景分割效果。</td>   <td>1.一种基于图神经网络的全景分割方法,其特征在于,包括：通过ResNet-50网络及FPN网络对图片进行特征提取,以提取多个目标特征；通过实例分割头部网络并根据所述目标特征以得到图片的前景类别概率、背景类别概率及掩膜结果,通过语义分割头部网络并根据所述目标特征以得到图片的初步语义分割结果；通过前景图神经网络对原始前景图进行处理以生成新前景图,通过所述前景类别概率对所述新前景图进行处理以生成实例分类结果,并根据所述掩膜结果从所述实例分类结果中提取目标实例分割掩膜；具体地,通过前景图神经网络对原始前景图进行节点特征的传播及节点表示的更新,以生成新前景图；对所述前景类别概率中的每一行向量分别进行归一化处理,将每一归一化结果分别作为实例注意力系数,将每一实例注意力系数与新前景图分别相乘以生成加权结果,将每一行的加权结果相加并求均值以生成实例一维向量；将所述实例一维向量与初步特征图中对应的行向量进行拼接,以生成新实例特征图；通过全连接层对所述新实例特征图进行处理,以生成实例分类结果；提取实例分类结果中每一行的概率最大值,根据所述概率最大值提取候选区域对应的类别,并根据所述类别提取对应的掩模结果以得到目标实例分割掩膜；其中,所述候选区域由RPN网络对所述目标特征进行处理后生成,所述候选区域进行池化处理后生成候选区域特征,所述初步特征图由全连接层对所述候选区域特征进行处理后生成；通过背景图神经网络对原始背景图进行处理以生成新背景图,通过所述背景类别概率及初步语义分割结果对所述新背景图进行处理以生成目标语义分割结果；采用启发式算法对所述目标实例分割掩膜及目标语义分割结果进行融合,生成全景分割结果。</td>   <td>G06K9/34;G06K9/32;G06T7/194;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         齐志新;              沈雪娇;                   张慧       </td>   <td>中山大学</td>   <td>基于Landsat时间序列遥感影像的城市更新区域监测方法</td>   <td>广东省</td>   <td>CN111062368B</td>   <td>2020-09-11</td>   <td>本发明公开了基于Landsat时间序列遥感影像的城市更新监测方法,包括以下步骤：利用研究区t<Sub>1</Sub>和t<Sub>2</Sub>连续两个年份的Landsat卫星影像计算土地开发指数,利用t<Sub>1</Sub>年份的Landsat卫星影像计算归一化植被指数、归一化水体指数；基于土地开发指数,通过阈值法提取出t<Sub>1</Sub>和t<Sub>2</Sub>年份间的土地开发区域；基于植被指数和水体指数,通过阈值法提取t<Sub>1</Sub>年份的植被和水体区域；对提取的土地开发区域、植被区域、水体区域进行叠加分析,从土地开发区域中去除由水体和植被区域转变而来的误差,从而提取出t<Sub>1</Sub>到t<Sub>2</Sub>年份的城市更新区域,再通过滤波降噪算法去除/降低噪声,得到t<Sub>1</Sub>到t<Sub>2</Sub>年份最终的城市更新区域。利用本方法,可以实现1984年至今任意年份、任意区域的城市更新范围提取。</td>   <td>1.基于Landsat时间序列遥感影像的城市更新区域监测方法,其特征在于,包括以下步骤：获取t<Sub>2</Sub>年份和上一年份t<Sub>1</Sub>年份的Landsat卫星遥感影像；通过t<Sub>1</Sub>和t<Sub>2</Sub>两个年份的Landsat卫星影像计算土地开发指数LDI,通过t<Sub>1</Sub>年份的Landsat卫星影像计算植被指数NDVI、水体指数NDWI；根据土地开发指数LDI通过阈值法提取t<Sub>2</Sub>年份基于t<Sub>1</Sub>年份的土地开发区域；根据植被指数NDVI和水体指数NDWI通过阈值法提取t<Sub>1</Sub>年份Landsat卫星影像上的植被和水体区域；对土地开发指数LDI、植被指数NDVI、水体指数NDWI进行叠加分析,从土地开发区域中去除由水体和植被转变而来的区域,即城市扩张区域,保留土地开发区域中由建筑物转变而来的区域,从而筛选出土地开发区域中全部的城市更新区域,即为t<Sub>1</Sub>到t<Sub>2</Sub>年份的城市更新区域；针对提取的城市更新区域通过滤波降噪算法去除/降低噪声、保持边界平滑,得到t<Sub>1</Sub>到t<Sub>2</Sub>年份的最终城市更新区域；土地开发指数LDI通过以下子步骤进行计算：根据t<Sub>1</Sub>年份Landsat卫星影像中的红光波段和蓝光波段计算t<Sub>1</Sub>年份的城市待建用地指数CLI<Sub>t1</Sub>；根据t<Sub>2</Sub>年份的Landsat卫星影像中的红光波段和蓝光波段计算t<Sub>2</Sub>年份的城市待建用地指数CLI<Sub>t2</Sub>；根据待建用地指数CLI<Sub>t2</Sub>和待建用地指数CLI<Sub>t1</Sub>计算土地开发指数LDI；待建用地指数CLI进行如下计算：          <Image id="icf0001" he="127" wi="260" file="FDA0002606199370000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,Red表示Landsat卫星影像中的红光波段；Blue表示Landsat卫星影像中的蓝光波段；土地开发指数LDI进行如下计算：          <Image id="icf0002" he="106" wi="269" file="FDA0002606199370000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image></td>   <td>G06K9/00;G06T5/00;G06T7/136;G06T7/187</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              何炜雄;                   谢晓华       </td>   <td>中山大学</td>   <td>基于深度网络增强的特定场景下行人检测器自动学习方法</td>   <td>广东省</td>   <td>CN108549852B</td>   <td>2020-09-08</td>   <td>本发明公开了一种基于深度网络增强的特定场景下行人检测器自动学习方法,步骤是：在服务器端使用通用的数据集训练出第一神经网络和第二神经网络,第二神经网络部署到嵌入式设备中；通过嵌入式设备对当前场景的图像进行捕捉,获得新增图像样本,传送到服务器端；在服务器端利用之前训练好的第一神经网络对新增图像样本进行测试,根据测试得分对样本进行标注；估计当前高度行人检测框的大小,剔除正样本中检测框和估计的大小有明显差异的样本,保留剩余样本；服务器端对第二神经网络进行调优；将调优后的第二神经网络模型从服务器端重新部署到嵌入式设备中。本发明可以在特定场景下快速得到精准的行人检测模型。</td>   <td>1.基于深度网络增强的特定场景下行人检测器自动学习方法,其特征在于,包括步骤：(1)在服务器端使用通用的数据集训练出第一神经网络和第二神经网络,第二神经网络用于部署到嵌入式设备中；(2)通过嵌入式设备在进行行人检测的工作过程中,对当前场景的图像进行捕捉,获得新增图像样本,传送到服务器端；(3)在服务器端利用之前训练好的第一神经网络对新增图像样本进行测试,利用第一神经网络的测试得分对样本进行标注；(4)对嵌入式设备当前高度下行人检测框的大小进行估计,计算正样本中检测框和估计的行人检测框的差异值,若差异值超过阈值,则进行剔除,保留剩余样本；(5)服务器端利用上述剩余样本对第二神经网络进行调优；(6)将调优后的第二神经网络模型从服务器端重新部署到嵌入式设备中；步骤(4)中,判断是否剔除样本的步骤是：对于正样本集合P下的每一个样本{l<Sub>i</Sub>,s<Sub>i</Sub>},通过以下准则确定是否从正样本集合中剔除：若|x<Sub>1</Sub>-x<Sub>r</Sub>|＞γ*w,则将{l<Sub>i</Sub>,s<Sub>i</Sub>}从P剔除,加入到N；若|x<Sub>1</Sub>-x<Sub>r</Sub>|*γ＜w,则将{l<Sub>i</Sub>,s<Sub>i</Sub>}从P剔除,加入到N；若|y<Sub>l</Sub>-y<Sub>r</Sub>|＞γ*h,则将{l<Sub>i</Sub>,s<Sub>i</Sub>}从P剔除,加入到N；若|y<Sub>l</Sub>-y<Sub>r</Sub>|*γ＜h,则将{l<Sub>i</Sub>,s<Sub>i</Sub>}从P剔除,加入到N；若<Image id="icf0001" he="72" wi="669" file="FDA0002440176930000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>则将{l<Sub>i</Sub>,s<Sub>i</Sub>}从P剔除,加入到N；若<Image id="icf0002" he="72" wi="675" file="FDA0002440176930000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>则将{l<Sub>i</Sub>,s<Sub>i</Sub>}从P剔除,加入到N；将上述操作之后得到的正样本集合设为P<Sub>1</Sub>,负样本集合设为N<Sub>1</Sub>,γ、<Image id="icf0003" he="69" wi="41" file="FDA0002440176930000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>均为预先设定的、大于1的参数。</td>   <td>G06K9/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              郭彤彤;                   李中华       </td>   <td>中山大学</td>   <td>一种自动驾驶图像语义分割优化方法</td>   <td>广东省</td>   <td>CN111639524A</td>   <td>2020-09-08</td>   <td>本发明公开了一种自动驾驶图像语义分割优化方法,该方法设计了一种使用标签来辅助激活的AAM模块,通过分割标签对网络提取的特征进行修正,使得同类物体提取出来的特征具有近似相同的值。将AAM模块集成到分割模型的编码器与解码器中间,通过训练得到一个性能比基准模型更好的模型,称为教师网络；通过知识迁移将教师网络基于AAM模块的所学知识迁移到分割模型中,从而提升其分割性能。本发明能够很好地挖掘分割标签的信息来提高分割模型的性能,并且无需修改网络结构,具有很强的应用价值。</td>   <td>1.一种自动驾驶图像语义分割优化方法,其特征在于,构建一教师-学生学习网络,其中教师网络是指编码器、AAM模块、解码器训练得到的分割模型,学生网络为仅包括编码器、解码器的基准模型,通过知识迁移将学习好的教师网络的知识迁移到学生网络,进而训练学生网络；在训练教师网络过程中将AAM模块集成到基准模型的编码器和解码器中间,得到教师网络分割模型；其中,AAM模块中没有可学习参数,其执行下述4个步骤：将编码器输出的多通道高层语义特征图按通道的维度求平均,获得每个像素位置的平均特征值,进而得到单通道平均特征图；将单通道平均特征图逐像素的和训练集分割标签相乘,得到单通道前辅助性特征激活图；所述训练集分割标签中属于不同的目标的像素有不同的激活等级；将单通道前辅助性特征激活图与激活因子相乘,得到最终辅助性特征激活图；所述激活因子在训练过程中会随着训练次数的增加逐渐减小直至为零；将最终辅助性特征激活图与多通道高层语义特征图相加,作为解码器的输入。</td>   <td>G06K9/00;G06K9/34;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         蔡庆玲;              裴海军;              梁伟霞;                   吕律       </td>   <td>中山大学</td>   <td>一种为有向有序多类不平衡数据分类的样本合成方法</td>   <td>广东省</td>   <td>CN107766875B</td>   <td>2020-09-08</td>   <td>本发明公开了一种为有向有序多类不平衡数据分类的样本合成方法,包括：Step1、构建初始数据集；Step2、基础训练；Step3、基础测试；Step4、计算再合成指数；Step5、构建增长训练数据集；Step6、增长训练；Step7、增长测试；Step8、错误率判断。本发明解决了医学信息等有向有序多类不平衡数据分类的序列性和方向性问题,确保增长分类模型更倾向于错误分类代价为正及代价敏感度高的分类等级,以降低分类错误造成的代价,并且,本发明的样本合成方法不需要人工设立代价的先验概率,其可以缺省自动赋予不同的代价敏感因子即类不平衡指数和错误分类代价因子,解决了人工设立代价先验概率准确性难以确保的问题。</td>   <td>1.一种为有向有序多类不平衡数据分类的样本合成方法,其特征在于：所述的样本合成方法包括以下步骤：Step1、构建初始数据集：将采集得到的医学诊断结果数据的集合记录为样本数据集S,从所述样本数据集S中,采用不放回抽样方式依次随机抽取N<Sub>t</Sub>个、N<Sub>s</Sub>个、N<Sub>gs</Sub>个样本,以依次生成基础训练数据集S<Sub>t</Sub>、基础测试数据集S<Sub>s</Sub>、增长测试数据集S<Sub>gs</Sub>,其中,所述样本数据集S中的样本总数量为N<Sub>t</Sub>+N<Sub>s</Sub>+N<Sub>gs</Sub>个,且所述样本数据集S中的样本均为有向有序多类不平衡数据,每一个所述样本均包含属性数据向量X和对应该属性数据向量X的正确分类等级c,该属性数据向量X是由多个属性数据组成的有序序列,该正确分类等级c为自然数；所述有向有序多类不平衡数据是指其分类具有序列性和方向性的数据,序列性是指：数据的分类是有等级区分的,方向性是指：与正确的分类等级相比,数据的分类等级偏大与分类等级偏小所需付出的代价不相同；Step2、基础训练：使用所述基础训练数据集S<Sub>t</Sub>,训练通用分类算法uCM<Sub>b</Sub>,以生成基础分类模型CM<Sub>b</Sub>,该基础分类模型CM<Sub>b</Sub>表达的是所述属性数据向量X与所述正确分类等级c之间的对应关系；Step3、基础测试：将所述基础训练数据集S<Sub>t</Sub>和基础测试数据集S<Sub>s</Sub>组成原始样本集S<Sub>p</Sub>＝(S<Sub>t</Sub>,S<Sub>s</Sub>),并将所述原始样本集S<Sub>p</Sub>中每一个样本的属性数据向量X分别代入所述基础分类模型CM<Sub>b</Sub>,以计算出对应的分类等级,记为分类等级计算值c’；Step4、计算再合成指数：按照以下公式一、公式二和公式三,计算所述原始样本集S<Sub>p</Sub>中每一个样本的再合成指数β<Sub>c</Sub>：μ<Sub>ci</Sub>＝n<Sub>ci</Sub>/((N<Sub>t</Sub>+N<Sub>s</Sub>+N<Sub>gs</Sub>)÷m)                       [公式一]式中,μ<Sub>ci</Sub>表示所述原始样本集S<Sub>p</Sub>中第i个样本x<Sub>i</Sub>的类不平衡指数,i为整数且1≤i≤N<Sub>t</Sub>+N<Sub>s</Sub>,样本x<Sub>i</Sub>的正确分类等级c记为c<Sub>i</Sub>,n<Sub>ci</Sub>表示所述样本数据集S中正确分类等级c为c<Sub>i</Sub>的样本数量,m表示所述样本数据集S中的样本所划分的等级数；λ<Sub>i</Sub>＝c<Sub>i</Sub>’-c<Sub>i</Sub>                                                   [公式二]式中,λ<Sub>i</Sub>表示所述原始样本集S<Sub>p</Sub>中第i个样本x<Sub>i</Sub>的错误分类代价因子,样本x<Sub>i</Sub>在所述Step3中计算出的分类等级计算值c’记为c<Sub>i</Sub>’；          <Image id="icf0001" he="215" wi="700" file="FDA0002540442580000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,β<Sub>ci</Sub>表示所述原始样本集S<Sub>p</Sub>中第i个样本x<Sub>i</Sub>的再合成指数,β<Sub>ci</Sub>’为中间计算值,ρ<Sub>1</Sub>和ρ<Sub>2</Sub>均为预设的分类等级权重系数,且ρ<Sub>2</Sub>≤ρ<Sub>1</Sub>；Step5、构建增长训练数据集：使所述原始样本集S<Sub>p</Sub>中的每一个样本均合成数量与其再合成指数β<Sub>c</Sub>相等的增长训练样本,并用所述原始样本集S<Sub>p</Sub>中的全部样本所合成的增长训练样本作为元素生成增长训练数据集S<Sub>gt</Sub>；其中,所述原始样本集S<Sub>p</Sub>中第i个样本x<Sub>i</Sub>合成其β<Sub>ci</Sub>个增长训练样本x<Sub>ij</Sub>的方法为：首先,按照k最近邻算法,从所述原始样本集S<Sub>p</Sub>的正确分类等级c等于为c<Sub>i</Sub>的样本中找出与所述样本x<Sub>i</Sub>最近邻的k个样本,k为所述k最近邻算法中的预设值,然后,在所述k个样本中随机选一个出来,记为样本x<Sub>j</Sub>,最后,用所述样本x<Sub>i</Sub>与所述样本x<Sub>j</Sub>合成所述β<Sub>ci</Sub>个增长训练样本x<Sub>ij</Sub>,即：所述样本x<Sub>i</Sub>合成的增长训练样本x<Sub>ij</Sub>同样包含由多个属性数据有序组成的属性数据向量X和对应该属性数据向量X的正确分类等级c,并且,所述β<Sub>ci</Sub>个增长训练样本x<Sub>ij</Sub>的正确分类等级c均取值为所述样本x<Sub>i</Sub>的正确分类等级c<Sub>i</Sub>,所述增长训练样本x<Sub>ij</Sub>与所述样本x<Sub>i</Sub>中组成它们属性数据向量X的属性数据数量和属性排序相同,且对于所述样本x<Sub>i</Sub>、样本x<Sub>j</Sub>和所述β<Sub>ci</Sub>个增长训练样本x<Sub>ij</Sub>位于同一个属性排序的属性数据来说,所述β<Sub>ci</Sub>个增长训练样本x<Sub>ij</Sub>的属性数据取值为在所述样本x<Sub>i</Sub>的属性数据取值与所述样本x<Sub>j</Sub>的属性数据取值之间的β<Sub>ci</Sub>个随机值；Step6、增长训练：使用所述增长训练数据集S<Sub>gt</Sub>,训练所述基础分类模型CM<Sub>b</Sub>,以生成增长分类模型CM<Sub>g</Sub>,该增长分类模型CM<Sub>g</Sub>表达的是所述属性数据向量X与所述正确分类等级c之间的对应关系；Step7、增长测试：将所述增长测试数据集S<Sub>gs</Sub>中每一个样本的属性数据向量X分别代入所述增长分类模型CM<Sub>g</Sub>,以计算出对应的分类等级,记为增长测试分类等级计算值c”,并且,将所述增长测试数据集S<Sub>gs</Sub>中每一个样本的正确分类等级c与其增长测试分类等级计算值c”进行对比,如果所述正确分类等级c与其增长测试分类等级计算值c”相等,则将对应的样本归属于增长测试正确测试数据集S<Sub>grt</Sub>,否则,将对应的样本归属于增长测试错误测试数据集S<Sub>ger</Sub>；Step8、错误率判断：计算错误率R<Sub>err</Sub>＝N<Sub>ger</Sub>/N<Sub>gs</Sub>,N<Sub>ger</Sub>为所述增长测试错误测试数据集S<Sub>ger</Sub>所包含样本的数量,N<Sub>gs</Sub>为所述增长测试数据集S<Sub>gs</Sub>所包含样本的数量；如果满足R<Sub>err</Sub>≤Ac,Ac为预设的最大错误率,则停止学习,并认定所述增长分类模型CM<Sub>g</Sub>能够正确表达出所述属性数据向量X与所述正确分类等级c之间的对应关系,否则,学习次数加1,并重新进行学习,即返回所述Step1以重新执行所述Step1至Step8,直至所述学习次数的累加值达到预设的最大学习次数L<Sub>max</Sub>时,停止学习,并认定最后一次学习所生成的增长分类模型CM<Sub>g</Sub>能够正确表达出所述属性数据向量X与所述正确分类等级c之间的对应关系。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈武辉;              高振量;                   郑子彬       </td>   <td>中山大学</td>   <td>一种基于区块链及动态网络服务质量的数据商品交易方法</td>   <td>广东省</td>   <td>CN111626717A</td>   <td>2020-09-04</td>   <td>本发明提出一种基于区块链及动态网络服务质量的数据商品交易方法,包括：S1.计算服务供应商从发送数据商品至任意买家节点接收到数据商品所花费的时间,作为最初的网络服务质量；S2.进行商品交易,收集买家报价集,确定数据商品的最终买家分配方案和成交价格；S3.服务供应商通过奖励激励中继节点,中继节点提交传输凭证至区块链,获取收益；S4.根据步骤S3中数据商品从发送至买家节点接收所花费的时间记录,更新网络服务质量；S5.判断数据商品交易是否完成,若是,结束；否则,返回S2进入下一轮的数据商品交易。本发明提出的基于区块链及动态网络服务质量的数据商品交易方法,提高网络服务质量,有效保障数据交易的安全隐私。</td>   <td>1.一种基于区块链及动态网络服务质量的数据商品交易方法,其特征在于,至少包括：S1.计算服务供应商从发送数据商品至任意买家节点接收到数据商品所花费的时间,作为最初的网络服务质量；S2.进行商品交易,服务供应商收集购买同一数据商品的买家报价集,确定数据商品的最终买家分配方案和成交价格；S3.服务供应商通过奖励激励中继节点,向买家传输数据商品,中继节点提交传输凭证至区块链,获取收益；S4.根据步骤S3中数据商品从发送至买家节点接收所花费的时间记录,更新网络服务质量；S5.判断数据商品交易是否完成,若是,结束；否则,返回步骤S2进入下一轮的数据商品交易。</td>   <td>G06Q20/06;G06Q20/38;G06Q30/02;G06Q30/08;G06Q40/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>              孙雪冬       </td>   <td>中山大学</td>   <td>基于有向超图的个性化学习路径的抽取与拼接方法及系统</td>   <td>广东省</td>   <td>CN106600065B</td>   <td>2020-09-04</td>   <td>本发明公开了一种基于有向超图的个性化学习路径的抽取与拼接方法及系统。该方法根据知识元之间的依赖关系、利用有向超图理论对相关领域知识进行建模,获得相关知识图谱；根据学习者的知识背景和知识目标对相关知识图谱进行相应的处理,得到个性化知识图谱；利用超图的性质和模型上所附加的知识语义,通过逆向回溯找到从开始知识元集到学习目标的个性化学习路径集；给出逆向回溯中学习背景、多输入/输出以及多目标的处理规则；并给出从变化发展的角度进行路径优选的算法。本发明能根据学习者的知识目标及背景给出相对稳定的、优化的学习内容及知识路径；并能根据学习目标和知识背景的变化进行学习内容和学习路径调整。</td>   <td>1.一种基于有向超图的个性化学习路径的抽取与拼接方法,其特征在于：具体包括以下步骤：步骤1,根据知识元之间的关系,利用有向超图建立相关知识图谱；步骤1中,根据知识元之间的逻辑关系及有向超图的性质,采用有向超图对相关知识进行建模；根据分析问题的需要,并借鉴课堂教学中知识元的表述,根据知识元间的聚集程度,在进行知识建模时,加入知识单元描述,它既能用来描述课程,也能用来描述学科领域,用包含不同节点和边的不同超边来描述这种聚集性,并把这个模型称为基于有向超图的知识图谱；步骤2,基于相关知识图谱,根据学习者知识目标和知识背景创建个性化的知识图谱；步骤2中,在基于有向超图的知识图谱的基础上,加上学习者描述,主要描述目标知识元和背景知识元,分别用包含不同节点的超边来描述学习者的知识目标和知识背景,这样就得到了用于个性化优化的知识图谱,简称个性化知识图谱,形式化描述为：个性化知识图谱.PKHM＝(KV,KE),KV＝(KGV,KUV),KE＝(UKE,IE,BE,BKE,OKE,ODKE),KUV＝(BKV,KOV),其中,PKHM为知识模型,KV为超图模型中的节点,表示模型中的知识元；KE为超图模型中的边,KGV为图形中的一般知识元,KUV为用来描述学习者的知识元,BKV为背景知识元,KOV为目标知识元；UKE表示知识单元,是课程或领域,内部包含的节点为知识单元包含的知识元；IE表示课程内部知识元之间的依赖关系,超边表示逻辑“AND”,不同的边描述逻辑“XOR”；BE表示不同知识单元之间的知识元之间的依赖关系；BKE表示学习者的背景知识,内部包含学习者的背景知识元；OKE为目标集,内部包含不同的知识目标；ODKE为不同的知识目标,内部包含不同的目标知识元；目标集：学习者的学习目标由一个或多个知识元集构成,每个集合包含一组知识元,并且知识元之间的关系满足集合的特点,表示完成一项任务目标需要的知识元；不同的集合之间可能存在交集,表示完成不同的任务目标可能需要相同的知识元,如果一个知识元属于多个集合的交集,则说明这个知识元出现的频率较高；步骤3,基于个性化知识图谱,以目标集为起点,采用逆向搜索的方式来进行可达路径的抽取和拼接；步骤4,根据模型的语义进行学习路径的优选；步骤4中,学习是一个知识积累的过程,已获得的知识是进一步学习的基础,因此,在进行学习过程优化时,既要考虑当前的学习效果,又要考虑到后续的学习情况；对于给定两个等效路径片段,在进行路径片段选择时,根据模型的语义以及学习过程本身的特点,在相应的个性化知识图谱上选择节点数少、并且出现频率高的路径片段,可形式化描述为：等效路径片段选择规则：对于给定的个性化有向超图,假设有k个目标节点,Obj<Sub>1</Sub>,Obj<Sub>2</Sub>,…,Obj<Sub>k</Sub>,且出现的频率分别为ns<Sub>1</Sub>,ns<Sub>2</Sub>,…,ns<Sub>k</Sub>,两个等效路径P<Sub>1</Sub>和P<Sub>2</Sub>,包含的节点分别为P<Sub>1</Sub>＝{kv<Sub>11</Sub>,kv<Sub>12</Sub>,……,kv<Sub>1m</Sub>},P<Sub>2</Sub>＝{kv<Sub>21</Sub>,kv<Sub>22</Sub>,……,kv<Sub>2n</Sub>},m,n分别为这两个等效路径的节点数,如果<Image id="icf0001" he="128" wi="700" file="FDA0002452850680000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>则选择路径P<Sub>2</Sub>,其中,w<Sub>1i</Sub>表示目标节点对应的路径是否通过P<Sub>1</Sub>的kv<Sub>1i</Sub>,当不通过P<Sub>1</Sub>时,w<Sub>1i</Sub>＝0；当通过P<Sub>1</Sub>时,w<Sub>1i</Sub>＝1；w<Sub>2j</Sub>表示目标节点对应的路径是否通过P<Sub>2</Sub>的kv<Sub>2j</Sub>,当不通过P<Sub>2</Sub>时,w<Sub>2j</Sub>＝0；当通过P<Sub>2</Sub>时,w<Sub>2j</Sub>＝1；步骤5,监测学习目标及知识背景,如果学习目标发生变化,则进行新路径集与原路径集相关性判断,并进行相应处理；如果相关知识背景发生变化,则根据原知识路径的执行情况进行处理；步骤5中,当学习目标发生变化时,以新目标为起点,根据步骤3进行逆向搜索,获得所有可能的路径集；之后判断新的路径集与原路径是否存在交集；如果不存在交集,则根据步骤4对新的可能的路径集进行路径的优选,并以新的路径作为学习内容及学习路径的依据；如果存在交集,则要根据原路径的执行情况进行处理,如果交集的某些部分在原路径中已执行,则在新的路径集中作为背景知识处理；之后,对处理后的可能路径集依据步骤4进行路径的优选；如果交集部分在原路径中未执行,则原路径对新路径无影响,对新路径依据步骤4进行优选；当相关知识背景发生变化时,重复步骤2,构建面向新知识背景的知识图谱,并根据原路径的执行情况进行处理；如果与改变相关的知识元已执行完,则改变的知识背景对知识路径无影响；如果与改变相关地知识元未执行,则重复步骤3与步骤4,构建新的知识路径。</td>   <td>G06Q10/04;G06Q50/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赵福利;              梁达安;              衣杨;              王馥君;                   吴慧英       </td>   <td>中山大学</td>   <td>基于IEO信息科学学科专业本科人才培养质量评价方法</td>   <td>广东省</td>   <td>CN111612291A</td>   <td>2020-09-01</td>   <td>本发明为基于IEO信息科学学科专业本科人才培养质量评价方法,提供一种在IEO评价模型的基础上,学生质量的输入基准值和学生所在环境的激励因素加权求和,得到学生质量的期望值。随后利用学生质量的评价值和学生质量的期望值作差值求商运算,求出学生质量的成长值。最后使用学生质量的评价值和学生质量的成长值作求和运算,得到学生质量的综合输出值。</td>   <td>1.一种基于IEO信息科学学科专业本科人才培养质量评价方法,其特征在于,包括以下步骤：S1：提出计算学生质量的五个变量值：学生质量的输入基准值I<Sub>eva</Sub>、学生所在环境的激励因素E<Sub>mov</Sub>、学生质量的期望输出值O<Sub>exp</Sub>、学生质量的评价值O<Sub>cal</Sub>、学生质量的成长值G<Sub>dev</Sub>；S2：构建步骤S1中五个变量值来计算学生质量的综合输出值O<Sub>com</Sub>的模型。</td>   <td>G06Q10/06;G06Q50/20;G06F17/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         朱勉春;              许曼玲;                   戴宪华       </td>   <td>中山大学</td>   <td>一种基于比率自适应池化的多尺度特征物体检测算法</td>   <td>广东省</td>   <td>CN111612065A</td>   <td>2020-09-01</td>   <td>本发明涉及一种基于比率自适应池化的多尺度特征物体检测算法。包括以下步骤：(1)采集大量的图像,按照一定比例划分为训练集和测试集,并对训练集进行预处理；(2)将训练集输入到一个预训练好的神经网络(ResNet50)进行特征提取,获得对应的feature map；(3)将RPN嵌入在RAP结合FPN结构中生成不同尺度特征并对RPN进行训练；(4)将步骤(3)生成的不同尺度的ROI进行RoI Pooling,然后计算损失、分类、更细节的边框回归；(5)将测试集图像输入到训练好的检测模型中输出检测结果。本发明的方法能有效的缓解FPN在融合过程中损失语义问题,提升检测精度。</td>   <td>1.一种基于比率自适应池化的多尺度特征物体检测算法,对传统的自上而下特征金字塔网络融合结构进行改进,所述方法包括：将待测图像输入到卷积神经网络(ResNet50),C<Sub>x</Sub>代表ResNet每一个模块生成的特征图C＝{C<Sub>2</Sub>,C<Sub>3</Sub>,C<Sub>4</Sub>,C<Sub>5</Sub>},整体框架保留了FPN原来的结构,横向连接是C经过1×1卷积降低为维度到256记为M＝{M<Sub>2</Sub>,M<Sub>3</Sub>,M<Sub>4</Sub>,M<Sub>5</Sub>},然后经过最顶层的M<Sub>5</Sub>经过RAP模块进行增强,输出记为P<Sub>5</Sub>；P<Sub>5</Sub>经过上采样与横向连接的M<Sub>4</Sub>融合,输出记为P<Sub>4</Sub>。依次操作,直到输出最后一层特征图P<Sub>2</Sub>,完成增强过程。P＝{P<Sub>2</Sub>,P<Sub>3</Sub>,P<Sub>4</Sub>,P<Sub>5</Sub>}送入后续的检测。其中在RAP增强的具体操作是：FPN的最顶层特征M<Sub>5</Sub>经过一个比率自适应,池化方式为自适应平均池化,这里我们选用的池化系数为α＝[0.1,0.2,0.3],输出的分辨率不同的三个特征图记为{A,B,C}；然后{A,B,C}经过3×3卷积后再经过双线性差值上采样输出记为{E,F,G},恢复到原来的输入分辨率大小；最后三个分辨率相同的特征图进行特征拼接,再经过1×1卷积降低通道数到256,输出最后的增强结果。至此,比率自适应池化过程构建完成。</td>   <td>G06K9/62;G06K9/46;G06K9/32;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王高平;              许曼玲;                   戴宪华       </td>   <td>中山大学</td>   <td>一种引入注意力机制的人脸图像修复方法</td>   <td>广东省</td>   <td>CN111612718A</td>   <td>2020-09-01</td>   <td>本发明涉及一种引入注意力机制的人脸图像修复方法,该方法包括：(1)获取原始数据集并进行图像预处理,得到我们需要的人脸图像数据集并合理划分整理为测试集与数据集。(2)将训练数据集输入到引入上下文关注层的图像修复模型中进行训练,该模型在生成器网络中引入了两个并行的编码器网络,一个编码器网络用于进行卷积操作提取高级特征图像,另一个编码器用于引入上下文关注层网络,用于实现前景区域与背景区域之间的长程关联。(3)将测试数据集输入到训练好的人脸修复模型中,测试训练好的修复模型对于缺损人脸图像的修复能力。本发明中引入上下文关注层后,解决了卷积神经网络由于感受野大小有限导致修复模型无法充分利用背景区域信息的问题,实现了背景信息与前景区域的长程关联,充分利用了背景区域信息对前景区域进行填充。引入上下文关注层后,修复模型在一些细节纹理上取得了更好的修复效果,总体上也提升了人脸图像的修复效果。</td>   <td>1.一种引入注意力机制的人脸图像修复方法,其特征在于,包括以下步骤：(1)人脸图像采集,本发明中使用到的数据集来自于CelebA人脸数据集,我们从中随机挑选出40000张人脸图像。(2)人脸图像划分,将选出的40000张人脸图像按照7比1的比例分成训练集与测试集,两部分中不包含相同的图像,训练集用于训练人脸图像修复模型,测试集用于测试训练好的修复模型的修复效果。(3)人脸图像预处理,由于CelebA人脸数据集中包含许多背景信息,本发明中对原始CelebA图像进行人脸检测并剪切出人脸区域部分,并将得到的人脸区域部分调整为128×128的人脸图像,得到的人脸图像为本发明中的真实图像；对得到的人脸图像中心区域进行64×64的掩码处理得到缺损图像数据集,用于之后修复模型的训练与测试。(4)训练修复模型,将经过预处理得到的128×128的人脸图像训练集输入到本发明中的修复模型中进行训练,经过生成器网络与判别器网络之间的对抗训练,不断提升生成器网络拟合样本的能力,保存最终训练好的修复模型。(5)测试训练好的修复模型,将经过预处理的人脸测试集图像输入训练好的修复模型中,保存生成的修复图像,并从主观视觉感受和客观相似性评价指标两方面与真实图像进行对比,得到的结果代表最终修复模型对缺损人脸图像的修复能力。</td>   <td>G06T5/00;G06T5/40;G06T7/11;G06T7/194;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈楚城;                   戴宪华       </td>   <td>中山大学</td>   <td>一种基于候选区域面积和宽高的自适应特征块提取方法</td>   <td>广东省</td>   <td>CN111611998A</td>   <td>2020-09-01</td>   <td>本发明涉及一种基于候选区域面积和宽高的自适应特征块提取方法,包括：(1)利用特征提取网络对输入图像进行特征提取；(2)通过特征金字塔网络构建检测特征图；(3)利用区域建议网络获取候选区域；(4)通过自适应特征块提取方法获取候选区域的最终特征块；(5)获取检测模型的分类误差和回归误差,训练模型；(6)利用训练好的检测模型获取测试图像的目标位置和类别。本发明的方法可以有效利用多个检测特征图的信息,补充感受野信息的同时获取细节信息,从而提高检测模型在具有悬殊宽高比的目标上的检测性能,最终提高检测模型的整体检测。</td>   <td>1.一种基于候选区域面积和宽高的自适应特征块提取方法,其特征在于包括如下步骤：(1)图像划分,将图像划分成训练集和测试集两部分,两个部分不存在相同的图像,训练集用来训练检测模型,测试集用来评估检测模型的性能,且训练集和测试集中不仅包含图像,还包含图像中目标对象的位置信息和类别信息；(2)图像预处理,包括随机上下翻转、随机左右翻转和随机光照改变等,其中随机上下翻转、随机左右翻转和随机光照改变只针对训练集,特别的,当进行随机上下翻转和随机左右翻转的时候,目标对象的坐标信息也需要做出相应的变化；(3)训练检测模型,将经过图像预处理后的训练集中的图像和标签信息输入到基于自适应特征块提取的带有特征金字塔网络的Faster R-CNN的检测模型中进行训练,获取各图像中目标对象的预测框和类别,并与实际的标签信息中的真实框和类别进行对比,计算出回归损失和分类损失,其中回归损失和分类损失包含了区域建议网络的损失还有第二阶段的损失,然后采用多学习任务的方法,利用带动量的梯度下降算法进行训练；(4)测试检测模型,将测试集中的图像输入到训练好的基于自适应特征块提取的带有特征金字塔网络的Faster R-CNN的检测模型中进行检测,获得测试图像中目标对象的位置和类型,并统计检测模型的检测精度。</td>   <td>G06K9/32;G06K9/46;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张贺晔;                   张卫卫       </td>   <td>中山大学</td>   <td>冠状动脉特异性钙化检测方法及装置</td>   <td>广东省</td>   <td>CN111612756A</td>   <td>2020-09-01</td>   <td>本申请提供了一种冠状动脉特异性钙化检测方法及装置,包括：利用人工智能模型的自学习能力,建立冠状动脉多视角医学图像的多视角图像特征与冠状动脉中的钙化分割结果和钙化量化结果之间的对应关系；获取患者的当前冠状动脉多视角医学图像的当前多视角图像特征；通过对应关系,确定与当前多视角图像特征对应的当前冠状动脉中的钙化分割结果和钙化量化结果；具体地,确定与多视角图像特征对应的当前冠状动脉中的钙化分割结果和钙化量化结果,包括：将对应关系中与当前多视角图像特征相同的多视角图像特征所对应的冠状动脉中的钙化分割结果和钙化量化结果,确定为当前冠状动脉中的钙化分割结果和钙化量化结果。同时实现分割和量化,节省冗余工作。</td>   <td>1.一种冠状动脉特异性钙化检测方法,其特征在于,包括：利用人工智能模型的自学习能力,建立冠状动脉多视角医学图像的多视角图像特征与冠状动脉中的钙化分割结果和钙化量化结果之间的对应关系；其中,所述多视角至少包括三个位置不同的视角；获取患者的当前冠状动脉多视角医学图像的当前多视角图像特征；通过所述对应关系,确定与所述当前多视角图像特征对应的当前冠状动脉中的钙化分割结果和钙化量化结果；具体地,确定与所述多视角图像特征对应的当前冠状动脉中的钙化分割结果和钙化量化结果,包括：将所述对应关系中与所述当前多视角图像特征相同的多视角图像特征所对应的冠状动脉中的钙化分割结果和钙化量化结果,确定为所述当前冠状动脉中的钙化分割结果和钙化量化结果。</td>   <td>G06T7/00;G06T7/11;G16H50/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张贺晔;                   郭宜锋       </td>   <td>中山大学</td>   <td>一种基于条件生成对抗网络的MRI成像方法及装置</td>   <td>广东省</td>   <td>CN111612865A</td>   <td>2020-09-01</td>   <td>本申请提供了一种基于条件生成对抗网络的MRI成像方法及装置,包括：利用人工神经网络的自学习能力,建立欠采样MRI数据与MRI图像的图像特征之间的对应关系；具体地,人工神经网络通过依据欠采样MRI数据生成的模拟MRI数据和欠采样MRI数据生成模拟MRI图像的图像特征；依据MRI图像的图像特征和模拟MRI图像的图像特征建立对应关系；获取当前受检测者的当前欠采样MRI数据；通过对应关系,确定与当前欠采样MRI数据对应的当前MRI图像的图像特征；具体地,确定与当前欠采样MRI数据对应的当前MRI图像的图像特征,包括：将对应关系中与当前欠采样MRI数据相同的欠采样MRI数据所对应的MRI图像的图像特征,确定为当前MRI图像的图像特征。实现更好的重建细节。</td>   <td>1.一种基于条件生成对抗网络的MRI成像方法,应用于将压缩感知磁共振成像装置获取的欠采样MRI数据进行成像,其特征在于,包括：利用人工神经网络的自学习能力,建立欠采样MRI数据与MRI图像的图像特征之间的对应关系；具体地,人工神经网络通过依据所述欠采样MRI数据生成的模拟MRI数据和所述欠采样MRI数据生成模拟MRI图像的图像特征；依据所述MRI图像的图像特征和所述模拟MRI图像的图像特征建立所述对应关系；获取当前受检测者的当前欠采样MRI数据；通过所述对应关系,确定与所述当前欠采样MRI数据对应的当前MRI图像的图像特征；具体地,确定与所述当前欠采样MRI数据对应的当前MRI图像的图像特征,包括：将所述对应关系中与所述当前欠采样MRI数据相同的欠采样MRI数据所对应的MRI图像的图像特征,确定为所述当前MRI图像的图像特征。</td>   <td>G06T11/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              卓嘉璇;                   陈泽宇       </td>   <td>中山大学</td>   <td>一种基于集中学习与深度网络学习的遮挡行人再识别方法</td>   <td>广东省</td>   <td>CN108596211B</td>   <td>2020-08-28</td>   <td>本发明公开了一种基于集中学习与深度网络学习的遮挡行人再识别方法,该方法通过遮挡模拟器从原始未遮挡训练样本生成多种类型遮挡训练样本,生成的遮挡训练样本与原始训练样本组成联合训练集用于模型的训练,同时添加遮挡与非遮挡分类损失到行人分类损失中去,用多任务损失函数代替以往的单任务损失函数,有效地应对遮挡下行人再识别的问题,使得深度网络学习特征的时候考虑遮挡与非遮挡的先验信息进行特征的提取。实验表明,本发明能较大幅度地提高现有的深度网络在遮挡行人再识别上的性能,具有广泛的应用价值。</td>   <td>1.一种基于集中学习与深度网络学习的遮挡行人再识别方法,其特征在于,包括步骤：S1.建立一个遮挡模拟器,从原始的未遮挡行人图像中生成各种不同类型遮挡的行人图像,生成的遮挡行人图像组成遮挡行人集合；S2.将生成的遮挡行人图像与原始完整的行人图像合并,联合训练一个带有多任务损失函数的深度网络,即集中学习框架,在这个框架下,深度网络通过不断前向传播和后向调整,实现对图像中行人部位提取特征并再进行分类的功能；训练直至深度网络的参数收敛得到网络模型；S3.在步骤S2得到的网络模型的基础上进一步训练真实的遮挡行人图像和非遮挡行人图像,得到最后的网络模型；S4.使用步骤S3中得到的网络模型对目标行人图像及行人图像库中的行人图像分别进行特征的提取,然后将从目标行人图像中提取的特征依次与从图像库中的行人图像中提取的特征进行匹配,基于匹配的结果确定目标行人图像的身份。</td>   <td>G06K9/62;G06K9/46</td>  </tr>        <tr>   <td>中国专利</td>   <td>         朱庆棠;              戚剑;              闫立伟;              姚执;                   刘小林       </td>   <td>中山大学附属第一医院</td>   <td>人体周围神经束型结构的可视化三维重建方法</td>   <td>广东省</td>   <td>CN108022291B</td>   <td>2020-08-28</td>   <td>本发明提供一种人体周围神经束型结构的可视化三维重建方法,包括以下步骤：获取人体周围神经并制备周围神经样本；将该周围神经样本浸泡于液体中；设置Micro-MRI的扫描参数,应用Micro-MRI对该周围神经样本进行扫描,获取该周围神经样本在该液体环境下的图像数据；基于该图像数据进行三维重建周围神经样本的结构模型。本发明的方法确保在不破坏周围神经的形态和理化性质的前提下获得高质量的扫描图像,以获得精准的周围神经束型的可视化三维模型。</td>   <td>1.一种人体周围神经束型结构的可视化三维重建方法,包括以下步骤：S1、获取人体周围神经,并对所述周围神经进行修剪以去除周围游离脂肪组织,然后将修剪后的周围神经切段即得周围神经样本；S2、将所述周围神经样本直接装入离心管,并在所述离心管中注满液体以浸泡所述周围神经样本,所述液体为蒸馏水或造影剂,然后封闭所述离心管,并排空气泡；S3、将装有所述液体和浸泡于所述液体中的所述周围神经样本的所述离心管放入Micro-MRI头线圈,固定后安装所述线圈；设置Micro-MRI的扫描参数,并应用Micro-MRI对浸泡于所述液体中的所述周围神经样本进行扫描,获取所述周围神经样本在所述液体环境下的图像数据；S4、基于所述图像数据进行三维重建所述周围神经样本的可视化结构模型。</td>   <td>G06T17/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李喆超;              张一帆;                   吴维刚       </td>   <td>中山大学</td>   <td>一种基于缓存的虚拟机启动方法</td>   <td>广东省</td>   <td>CN106933654B</td>   <td>2020-08-28</td>   <td>本发明提供的方法的物理节点在收到申请虚拟机的请求时,首先通过查找自身或兄弟节点中是否缓存有目标镜像,若缓存有目标镜像且自身或兄弟节点能够满足资源要求的话则直接在自身或兄弟节点中启动虚拟机。因此本发明提供的方法能够最大程度上减少镜像传输的发生。而后,步骤S5和S6从各个源点选取和中间交换机的选择来规划传输路径,这优化了整个网络上的带宽利用率。因此,本发明提供的方法能够有效地提高传输速度。再者,步骤S8在启动虚拟机后,还包括有判断是否需要缓存目标镜像的内容。这使得目标物理所在簇包括的物理节点中肯定缓存有目标镜像。为后续的虚拟机启动减少镜像传输的发生。因此本发明提供的方法能够有效地加快虚拟机的启动速度。</td>   <td>1.一种基于缓存的虚拟机启动方法,其特征在于：包括以下步骤：S1.物理节点A收到申请虚拟机的请求,如果物理节点A缓存有目标镜像并且物理节点A的资源满足申请的需求,则直接利用目标镜像在物理节点A的本地启动虚拟机；否则执行步骤S2；S2.查找物理节点A的父节点,然后将父节点的儿子节点中与物理节点A处于同一个簇并且缓存有目标镜像的节点加入到列表PM_list中；S3.a)PM_list不为空,且PM_list中存在物理节点B的资源满足申请的需求,则利用物理节点B缓存的目标镜像在物理节点B启动虚拟机；b)PM_list为空,则从物理节点A父节点的儿子节点中选择资源能够满足申请需求的物理节点C作为虚拟机放置的节点,然后执行步骤S4；c)若a)、b)两种情况均无法找到资源能够满足申请需要的物理节点,则将这个请求转发给其他簇的物理节点进行处理；S4.从物理节点C出发,递归地搜索物理节点C的父节点的儿子节点缓存的镜像信息,得到所有缓存有目标镜像的物理节点的簇；S5.遍历步骤S4得到的簇,根据簇中数据传输的拥挤程度从相应的簇中选择相应的物理节点作为数据源；S6.循环胖树中的所有层,在每一层的所有同等地位的交换机中选择工作负载最小的交换机构建从数据源到物理节点C的传输路径,然后将目标镜像通过传输路径从数据源传输至物理节点C；S7.物理节点C利用目标镜像在本地启动虚拟机；然后判断其自身是否还具有足够的空间来存放目标镜像,若是则直接缓存目标镜像,否则执行步骤S8；S8.计算物理节点C所在簇包括的物理节点缓存有的各种镜像的出现频率,若目标镜像的出现频率高于其余镜像的出现频率,则不缓存目标镜像,否则将出现频率最高的镜像进行删除,然后将目标镜像缓存进物理节点C所在簇包括的物理节点中。</td>   <td>G06F9/455</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;                   马伟       </td>   <td>中山大学</td>   <td>基于深度局部特征的人脸属性识别方法</td>   <td>广东省</td>   <td>CN107169455B</td>   <td>2020-08-28</td>   <td>本发明提供的方法是基于局部区域特征来进行人脸属性识别的,局部区域特征与全部区域特征相比更有鲁棒性和判别力；再者,利用降维和多尺度特征融合,比传统的降维方式和仅使用最后一层特征的方式,能得到更多有用的深度层级视觉特征；利用局部区域定位、分类和局部属性识别,能够更好的找到各属性所对应的人脸关键特征区域。</td>   <td>1.基于深度局部特征的人脸属性识别方法,其特征在于：包括以下步骤：一、训练阶段S1.对于训练集中的每一幅人脸图像,利用深度卷积神经网络的不同卷积层计算得到多个尺度下的特征图；S2.利用得到的多个尺度下的特征图和人脸图像N个局部区域的边界框类标,分别为N个局部区域训练相应的局部区域定位网络；其中N为大于2的整数；S3.根据训练集中的每一幅人脸图像的多尺度特征图和N个局部区域的边界框类标,计算各个局部区域的多尺度特征图；S4.将计算得到的各个局部区域的多尺度特征图进行降维,然后将降维后得到的各个局部区域的特征图按其局域区域所在的位置拼接在一起然后进行特征融合；S5.将步骤S4得到的融合特征送入各局部区域的识别网络,训练局部属性分类器；二、测试阶段S6.利用步骤S1提取测试集中人脸图像多个尺度下的特征图,然后利用步骤S2得到的局部区域定位网络从多个尺度下的特征图中定位出N个局部区域；S7.利用定位出的N个 局部区域的边界框类标及人脸图像多个尺度下的特征图计 算各个局部区域的多尺度特征图；S8.对计算得到的各个局部区域的多尺度特征图进行降维,然后将降维后得到的各个局部区域的特征图按其局域区域所在的位置拼接在一起然后进行特征融合；S9.利用步骤S5训练好的局部属性分类器对融合特征进行属性识别,得到各局部区域相关的人脸属性,将各个局部区域相关的人脸属性组合起来,得到测试集中人脸图像的人脸属性；所述人脸图像N个局部区域包括：额头至头顶的区域、眉毛区域、眼睛区域、鼻子区域、嘴巴区域、下巴区域、脖颈区域、面部区域、头部区域；所述步骤S2的局部区域定位网络在进行边界框回归训练时,利用候选边界框和真值边界框的重叠率和9个局部区域固有的位置关系作为约束条件：记第i个局部区域边界框的左上角坐标(x<Sub>i</Sub>,y<Sub>i</Sub>),宽和高分别为w<Sub>i</Sub>,h<Sub>i</Sub>；对一给定的像素点(x',y'),记它与第i个局部区域边界框(x<Sub>i</Sub>,y<Sub>i</Sub>,w<Sub>i</Sub>,h<Sub>i</Sub>)左上角、右下角的坐标差值分别为：(Δx<Sub>li</Sub>,Δy<Sub>li</Sub>)和(Δx<Sub>ri</Sub>,Δy<Sub>ri</Sub>),则显然：Δx<Sub>li</Sub>＝x'-x<Sub>i</Sub>；Δy<Sub>li</Sub>＝y'-y<Sub>i</Sub>；Δx<Sub>ri</Sub>＝x<Sub>i</Sub>+w<Sub>i</Sub>-x'；Δy<Sub>ri</Sub>＝y<Sub>i</Sub>-h<Sub>i</Sub>-y'若记该点与真值边界框<Image id="icf0001" he="94" wi="300" file="FDA0002389256420000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>左上角、右下角的坐标差值分别为<Image id="icf0002" he="93" wi="227" file="FDA0002389256420000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0003" he="93" wi="249" file="FDA0002389256420000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>真值边界框与候选边界框重叠区域的宽、高分别为Δx<Sub>i</Sub>、Δy<Sub>i</Sub>,则可得：候选边界框的面积为：S<Sub>i</Sub>＝(Δx<Sub>li</Sub>+Δx<Sub>ri</Sub>)×(Δy<Sub>li</Sub>+Δy<Sub>ri</Sub>)真值边界框的面积为：<Image id="icf0004" he="93" wi="639" file="FDA0002389256420000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>候选边界框与真值边界框重叠面积为：S<Sub>∩i</Sub>＝Δx<Sub>i</Sub>×Δy<Sub>i</Sub>其中,<Image id="icf0005" he="97" wi="700" file="FDA0002389256420000025.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>而且,若重叠面积为0,即若<Image id="icf0006" he="76" wi="564" file="FDA0002389256420000026.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>或<Image id="icf0007" he="79" wi="593" file="FDA0002389256420000027.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>则Δx<Sub>i</Sub>＝0,Δy<Sub>i</Sub>＝0；则可以计算出,第i个局部区域候选边界框与真值边界框的重叠率为：          <Image id="icf0008" he="148" wi="358" file="FDA0002389256420000028.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        则对于任何一个局部区域i,真值边界框的约束关系在loss函数中可以表示为<Image id="icf0009" he="79" wi="700" file="FDA0002389256420000029.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>此外根据9个局部区域的标记规则和人脸图像固有的生理构造,各局部区域边界框之间存在如下约束关系：记额头至头顶的区域、眉毛区域、眼睛区域、鼻子区域、嘴巴区域、下巴区域、脖颈区域、面部区域、头部区域9个局部区域的边界框坐标分别为(x<Sub>i</Sub>,y<Sub>i</Sub>,w<Sub>i</Sub>,h<Sub>i</Sub>),其中i＝1,2,...,9,则存在：y<Sub>1</Sub>+h<Sub>1</Sub>＝y<Sub>2</Sub>；x<Sub>2</Sub>＝x<Sub>3</Sub>；y<Sub>2</Sub>＝y<Sub>3</Sub>；x<Sub>3</Sub>＜x<Sub>4</Sub>＜x<Sub>4</Sub>+w<Sub>4</Sub>＜x<Sub>3</Sub>+w<Sub>3</Sub>；y<Sub>3</Sub>＜y<Sub>4</Sub>＜y<Sub>5</Sub>；y<Sub>5</Sub>+h<Sub>5</Sub>＝y<Sub>6</Sub>；y<Sub>6</Sub>+h<Sub>6</Sub>＝y<Sub>7</Sub>；y<Sub>8</Sub>+h<Sub>8</Sub>＝y<Sub>9</Sub>+h<Sub>9</Sub>；x<Sub>8</Sub>＝x<Sub>9</Sub>；w<Sub>8</Sub>＝w<Sub>9</Sub>；y<Sub>9</Sub>＝y<Sub>1</Sub>记像素点(x',y')与第i个局部区域边界框(x<Sub>i</Sub>,y<Sub>i</Sub>,w<Sub>i</Sub>,h<Sub>i</Sub>)左上角、右下角的坐标差值分别为：(Δx<Sub>li</Sub>,Δy<Sub>li</Sub>)和(Δx<Sub>ri</Sub>,Δy<Sub>ri</Sub>),则以上约束关系可对应转化为：Δy<Sub>r1</Sub>＝-Δy<Sub>l2</Sub>；Δx<Sub>l2</Sub>＝Δx<Sub>l3</Sub>；Δy<Sub>l2</Sub>＝Δy<Sub>l3</Sub>；-Δx<Sub>l3</Sub>＜-Δx<Sub>l4</Sub>＜Δx<Sub>r4</Sub>＜Δx<Sub>r3</Sub>；Δy<Sub>l3</Sub>＞Δy<Sub>l4</Sub>＞Δy<Sub>l5</Sub>；Δy<Sub>r5</Sub>＝-Δy<Sub>l6</Sub>；Δy<Sub>r6</Sub>＝-Δy<Sub>l7</Sub>；Δy<Sub>r8</Sub>＝-Δy<Sub>l9</Sub>；Δx<Sub>l8</Sub>＝Δx<Sub>l9</Sub>；Δx<Sub>l8</Sub>+Δx<Sub>r8</Sub>＝Δx<Sub>l9</Sub>+Δx<Sub>r9</Sub>；Δy<Sub>l9</Sub>＝Δy<Sub>l1</Sub>定义条件函数<Image id="icf0010" he="82" wi="700" file="FDA0002389256420000031.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>          <Image id="icf0011" he="87" wi="700" file="FDA0002389256420000032.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>则：以Δy<Sub>r1</Sub>＝-Δy<Sub>l2</Sub>；Δy<Sub>r5</Sub>＝-Δy<Sub>l6</Sub>；Δy<Sub>r6</Sub>＝-Δy<Sub>l7</Sub>；Δy<Sub>r8</Sub>＝-Δy<Sub>l9</Sub>作为约束,取相应loss函数为L”<Sub>loc1i</Sub>＝β<Sub>1</Sub>(i)|Δy<Sub>ri</Sub>+Δy<Sub>li+1</Sub>|以Δy<Sub>l2</Sub>＝Δy<Sub>l3</Sub>；-Δx<Sub>l3</Sub>＜-Δx<Sub>l4</Sub>；Δx<Sub>r4</Sub>＜Δx<Sub>r3</Sub>；Δy<Sub>l3</Sub>＞Δy<Sub>l4</Sub>作为约束,取相应loss函数为L”<Sub>loc2i</Sub>＝β<Sub>3</Sub>(i)(|Δy<Sub>li-1</Sub>-Δy<Sub>li</Sub>|+θ(Δx<Sub>li</Sub>-Δx<Sub>li+1</Sub>)+θ(Δx<Sub>ri</Sub>-Δx<Sub>ri+1</Sub>)+θ(Δy<Sub>li</Sub>-Δy<Sub>li+1</Sub>))以Δx<Sub>l2</Sub>＝Δx<Sub>l3</Sub>；Δx<Sub>l8</Sub>＝Δx<Sub>l9</Sub>作为约束,取相应loss函数为L”<Sub>loc3i</Sub>＝β<Sub>2</Sub>(i)|Δx<Sub>li</Sub>-Δx<Sub>li+1</Sub>|以Δy<Sub>l9</Sub>＝Δy<Sub>l1</Sub>作为约束,取loss函数为L”<Sub>loc4i</Sub>＝β<Sub>4</Sub>(i)(|Δy<Sub>li</Sub>-Δy<Sub>li-8</Sub>|+|Δx<Sub>ri-1</Sub>-Δx<Sub>ri</Sub>|)以-Δx<Sub>l4</Sub>＜Δx<Sub>r4</Sub>；Δy<Sub>l4</Sub>＞Δy<Sub>l5</Sub>作为约束,取相应loss函数为L”<Sub>loc5i</Sub>＝β<Sub>5</Sub>(i)(θ(Δx<Sub>ri</Sub>+Δx<Sub>li</Sub>)+θ(Δy<Sub>li</Sub>-Δy<Sub>li+1</Sub>))以上约束均为两个局部区域相对位置关系的局部约束,组合起来,即可做为整个人脸9个局部区域间的边界框约束条件：L”<Sub>loci</Sub>＝L”<Sub>loc1i</Sub>+L”<Sub>loc2i</Sub>+L”<Sub>loc3i</Sub>+L”<Sub>loc4i</Sub>+L”<Sub>loc5i</Sub>＝β<Sub>1</Sub>(i)|Δy<Sub>ri</Sub>+Δy<Sub>li+1</Sub>|+β<Sub>3</Sub>(i)(|Δy<Sub>li-1</Sub>-Δy<Sub>li</Sub>|+θ(Δx<Sub>li</Sub>-Δx<Sub>li+1</Sub>)+θ(Δx<Sub>ri</Sub>-Δx<Sub>ri+1</Sub>)+θ(Δy<Sub>li</Sub>-Δy<Sub>li+1</Sub>))+β<Sub>2</Sub>(i)|Δx<Sub>li</Sub>-Δx<Sub>li+1</Sub>|+β<Sub>4</Sub>(i)(|Δy<Sub>li</Sub>-Δy<Sub>li-8</Sub>|+|Δx<Sub>ri-1</Sub>-Δx<Sub>ri</Sub>|)+β<Sub>5</Sub>(i)(θ(Δx<Sub>ri</Sub>+Δx<Sub>li</Sub>)+θ(Δy<Sub>li</Sub>-Δy<Sub>li+1</Sub>))。</td>   <td>G06K9/00;G06K9/62;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;                   黄嘉胜       </td>   <td>中山大学</td>   <td>一种基于图神经网络的群体行为识别方法</td>   <td>广东省</td>   <td>CN111598032A</td>   <td>2020-08-28</td>   <td>本发明公开了一种基于图神经网络的群体行为识别方法,方法包括以下步骤：特征提取,对单位时间内视频段的个体视觉特征进行提取,获取每个人的特征表达与整个场景特征表达；生成虚图,根据得到的每个人的特征表达与场景特征表达生成全连接的无向图,在无向图中引入虚节点,生成虚图；图神经网络更新,对虚图进行图神经网络更新；构建图神经网络,根据图神经网络层构建图神经网络模型；群体行为识别,将完整的虚图导入到图神经网络,对预测类标和真实类标进行误差计算。本发明定义了一种新型的基于虚节点的图神经网络,可以学习到视频中丰富的时间空间特征,从而帮助对视频中的群体行为进行准确的识别。</td>   <td>1.一种基于图神经网络的群体行为识别方法,其特征在于,包括以下步骤：特征提取,对单位时间视频端的个体进行检测,并依据检测到个体的位置,在空间和时间上进行视频切片,然后将视频切片输入到三维残差卷积网络进行特征提取,获取每个个体的特征表达与整个场景特征表达；生成虚图,根据得到的每个个体的特征表达与场景特征表达,将每个个体视为图的一个节点,计为实节点,将所有实节点两两相连,得到全连接的无向图,在无向图中引入虚节点,将虚节点与原图中的节点连接,形成虚图；对多个图进行虚拟节点引入形成的虚图进行图神经网络的更新,更新后的图神经网络层具有充分的特征表达能力；构建图神经网络,表达图神经网络层,根据图神经网络层的表达式构建图神经网络模型；群体行为识别,将完整的虚图导入到图神经网络模型,进行非线性变换归一化处理,对预测类标和真实类标进行误差计算。</td>   <td>G06K9/00;G06K9/46;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张贺晔;                   郭宜锋       </td>   <td>中山大学</td>   <td>一种基于生成对抗网络的磁共振成像方法及装置</td>   <td>广东省</td>   <td>CN111598966A</td>   <td>2020-08-28</td>   <td>本申请提供了一种基于生成对抗网络的磁共振成像方法及装置,包括：利用人工神经网络的自学习能力,建立欠采样MRI数据与MRI图像的图像特征之间的对应关系；具体地,确定相邻时序对应的数据段之间的相关性；确定欠采样MRI数据中的目标空间特征；依据相关性和目标空间特征,确定欠采样MRI数据与MRI图像的图像特征之间的对应关系；获取当前受检测者的当前欠采样MRI数据；通过对应关系,确定与当前欠采样MRI数据对应的当前MRI图像的图像特征；具体地,确定与当前欠采样MRI数据对应的当前MRI图像的图像特征,包括：将对应关系中与当前欠采样MRI数据相同的欠采样MRI数据所对应的MRI图像的图像特征,确定为当前MRI图像的图像特征。避免了在重建过程中产生大量残余噪声。</td>   <td>1.一种基于生成对抗网络的磁共振成像方法,应用于将压缩感知磁共振成像装置获取的欠采样MRI数据进行成像,所述欠采样MRI数据包含多段依据时序获取及排列的数据段,其特征在于,包括：利用人工神经网络的自学习能力,建立欠采样MRI数据与MRI图像的图像特征之间的对应关系；具体地,确定相邻时序对应的所述数据段之间的相关性；确定所述欠采样MRI数据中的目标空间特征；依据所述相关性和所述目标空间特征,确定所述欠采样MRI数据与MRI图像的图像特征之间的对应关系；获取当前受检测者的当前欠采样MRI数据；通过所述对应关系,确定与所述当前欠采样MRI数据对应的当前MRI图像的图像特征；具体地,确定与所述当前欠采样MRI数据对应的当前MRI图像的图像特征,包括：将所述对应关系中与所述当前欠采样MRI数据相同的欠采样MRI数据所对应的MRI图像的图像特征,确定为所述当前MRI图像的图像特征。</td>   <td>G06T11/00;G06N3/04;G06N3/08;G01R33/48;G01R33/54;G01R33/56</td>  </tr>        <tr>   <td>中国专利</td>   <td>         余伟江;              梁小丹;              肖侬;                   林倞       </td>   <td>中山大学</td>   <td>一种视觉问答任务实现方法及系统</td>   <td>广东省</td>   <td>CN111598118A</td>   <td>2020-08-28</td>   <td>本发明公开了一种视觉问答任务实现方法及系统,该方法包括：步骤S1,对输入图片提取视觉特征X<Sub>o</Sub>,对输入的已知语句以及输入的候选回答语句进行特征提取,得到已知语言特征X<Sub>q</Sub>和候选答案特征X<Sub>c</Sub>；步骤S2,基于视觉因果关系推理更新每一个视觉特征,得到更新后的视觉实体特征X<Sub>g</Sub>；步骤S3,将更新后的视觉实体特征X<Sub>g</Sub>作为引导特征,对候选的候选答案特征X<Sub>c</Sub>进行引导选择出视觉敏感的回答特征X<Sub>V</Sub>；步骤S4,将已知语言特征X<Sub>q</Sub>作为引导特征,对候选答案特征X<Sub>c</Sub>进行引导选择出语言敏感的回答特征X<Sub>L</Sub>；步骤S5,将步骤S3和步骤S4产生的两种特征进行融合,进而预测最后的模型结果,输出正确的回答。</td>   <td>1.一种视觉问答任务实现方法,包括如下步骤：步骤S1,对输入图片提取视觉特征X<Sub>o</Sub>,对输入的已知语句以及输入的候选回答语句进行特征提取,得到已知语言特征X<Sub>q</Sub>和候选答案特征X<Sub>c</Sub>；步骤S2,基于视觉因果关系推理更新每一个视觉特征,得到更新后的视觉实体特征X<Sub>g</Sub>；步骤S3,将更新后的视觉实体特征X<Sub>g</Sub>作为引导特征,对候选的候选答案特征X<Sub>c</Sub>进行引导选择出视觉敏感的回答特征X<Sub>V</Sub>；步骤S4,将已知语言特征X<Sub>q</Sub>作为引导特征,对候选答案特征X<Sub>c</Sub>进行引导选择出语言敏感的回答特征X<Sub>L</Sub>；步骤S5,将步骤S3和步骤S4产生的两种特征进行融合,进而预测最后的模型结果,输出正确的回答。</td>   <td>G06K9/62;G06F16/332;G06N5/04;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              罗维冰;              陈荣军;              谢舜道;              邓雅文;              朱雄泳;                   曾衍瀚       </td>   <td>中山大学</td>   <td>适用于可视化二维码的全参考图像质量评价方法及系统</td>   <td>广东省</td>   <td>CN111598837A</td>   <td>2020-08-28</td>   <td>本发明公开了适用于可视化二维码的全参考图像质量评价方法及系统,针对可视化二维码的固有特点,选择颜色、对比度、梯度三个方面的失真和畸变,作为可视化二维码质量评价方案的参考指标。通过对可视化二维码颜色相似度、对比度相似度和梯度相似度分别进行计算,并通过最终的加权平均得到可视化二维码图像质量评价结果。该评价方法解决了目前的经典图像质量评价方法不适合可视化二维码的问题,将可视化二维码图像是彩色图像,编码过程中产生的图像结构、对比度失真等特点考虑进方案中,更加有针对性的评价可视化二维码编码图像质量,更好的指导可视化二维码方案的优化和改进更好的评价其特点,对编码方案的优化和改进提供更好的参考依据。</td>   <td>1.适用于可视化二维码的全参考图像质量评价方法,其特征在于,包括以下步骤：S1.获取待评价的可视化二维码图像,根据编码方案中数据区域的设置,获取其可视化二维码数据区域图像；S2.将获取的可视化二维码数据区域图像转换成RGB彩色模型；S3.基于所述RGB彩色模型,获取所述可视化二维码数据区域图像和原始可视化二维码图像的R、G、B三个通道的像素值,并计算可视化二维码图像的颜色相似度；S4.根据可视化二维码数据区域图像的模块数和图像大小,对可视化二维码数据区域图像和原始可视化二维码图像进行模块划分,并对两张图像的R、G、B三个通道同时以编码模块为单位进行对比度相似度计算,得到可视化二维码图像的对比度相似度；S5.对可视化二维码数据区域图像和原始可视化二维码图像的R、G、B三个通道图像分别获取梯度图像,计算可视化二维码图像的梯度相似度；S6.将所述可视化二维码图像颜色相似度、对比度相似度和梯度相似的指标合并,得到可视化二维码图像质量评价结果。</td>   <td>G06T7/00;G06T7/44;G06T7/90;G06K9/62;G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林夏娜;              倪江群;                   张木水       </td>   <td>中山大学</td>   <td>一种基于图像降噪的抗打印拍摄图像数字水印方法</td>   <td>广东省</td>   <td>CN111598761A</td>   <td>2020-08-28</td>   <td>本发明提供的一种基于图像降噪的抗打印拍摄图像数字水印方法,采用生成对抗网络构建一个能够抵抗图像在打印拍摄过程中可能存在的噪声攻击的图像降噪层,能够抵抗多种噪声的同时攻击,实现更好的降噪效果,保证图像具有高保真度；并且将图像降噪层加入到整个水印嵌入与提取的训练框架中,图像降噪层实现了抵抗噪声攻击的功能,在一定程度上保证了鲁棒性,使得水印编解码器可以更专注于提高水印嵌入后的视觉效果、水印的检测准确率和嵌入容量,从而实现鲁棒性、视觉效果和嵌入容量三种指标的均衡。</td>   <td>1.一种基于图像降噪的抗打印拍摄图像数字水印方法,其特征在于,包括以下步骤：S1：对打印拍摄过程中的噪声进行建模,构建噪声层；根据噪声层对生成式对抗网络GANs进行对抗训练,构建图像降噪层；S2：将原始图像和经过噪声层的噪声图像作为对抗训练后的图像降噪层的输入,得到一个对畸变过程具有一定鲁棒性的降噪层；S3：对降噪层进行预训练并构建水印编码器、水印解码器；S4：随机生成水印信息,将原始图像和水印信息输入水印编码器中,将水印信息嵌入到原始图像中,输出水印图像；S5：将水印图像经过噪声层和预训练的降噪层,模拟水印图像的畸变过程和降噪过程,得到降噪后的水印图像；S6：利用水印解码器对降噪后的水印图像进行解码,得到解码后的水印信息；S7：根据水印信息和解码后的水印信息判断解码正确率,若符合训练标准,则完成抗打印拍摄图像数字水印方法；若否,使用交叉熵函数对水印解码器的解码器正确率进行训练,返回执行步骤S4。</td>   <td>G06T1/00;G06T5/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄晓;              万林鸿;                   倪江群       </td>   <td>中山大学</td>   <td>一种生成式鲁棒图像隐写方法</td>   <td>广东省</td>   <td>CN111598762A</td>   <td>2020-08-28</td>   <td>本发明提供一种生成式鲁棒图像隐写方法,包括：构建图像数据集,并对图像数据集进行预处理；构建并初始化深度学习网络架构；采用联合-精调式方法训练深度学习网络架构,得到网络架构模型；利用网络架构模型生成载密伪图并进行秘密通信,完成图像隐写过程。本发明提供的图像隐写方法,通过利用生成对抗网络StyleGAN,将秘密信息的嵌入过程融入到图像的生成过程中,构建一种能承担较大容量秘密信息并具备一定鲁棒性的生成式图像隐写架构,从而得到的生成式图像隐写方法具有嵌入的容量较大、生成的图像质量好、载密图像统计不可检测性强、实用性高等优点,并克服了现有的生成式图像隐写生成的载密图像质量差、嵌入容量低下、信息提取准确率不高等问题。</td>   <td>1.一种生成式鲁棒图像隐写方法,其特征在于,包括以下步骤：S1：构建图像数据集,并对图像数据集进行预处理；S2：构建并初始化深度学习网络架构；S3：采用联合-精调式方法训练深度学习网络架构,得到网络架构模型；S4：利用网络架构模型生成载密伪图并进行秘密通信,完成生成式鲁棒图像隐写过程。</td>   <td>G06T1/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马争鸣;              陈李创凯;              甘伟超;              冯伟佳;                   刘洁       </td>   <td>中山大学</td>   <td>一种基于HSIC最大化的张量子空间学习算法</td>   <td>广东省</td>   <td>CN111582321A</td>   <td>2020-08-25</td>   <td>本发明是研究多维数据的降维问题。本发明采用一个张量表示一个多维数据集,其中张量的前面各维表示多维数据的各个维度,而最后一维则表示数据集所包含的数据的个数。由于张量与矩阵的模式积可以改变张量某个维的大小,因此,本发明提出基于张量模式积的张量数据降维模型,模式积的矩阵是可选的,可以根据不同的准则确定。本发明根据降维前后两个张量之间HSIC最大化的准则确定模式积的矩阵。HSIC把两个数据集变换到两个再生核希尔伯特空间(RKHS)上,然后利用两个RKHS之间的HS算子衡量两个变换后的数据集的统计依赖性。本发明的优势是RKHS是可选的,人们可以根据给定的数据集,选择降维效果最好的RKHS。</td>   <td>1.一种基于HSIC最大化的张量子空间学习算法,其特征在于：A.提出一种基于HSIC最大化的子空间学习框架,HSIC准则是衡量两个再生核希尔伯特空间(RKHS)中的数据集之间的数据依赖性。数据依赖是通过已有数据样本的信息有效地反映了流形的几何结构,使核函数在映射的同时能够保持流形原有的几何结构；B.将这个子空间学习框架运用到张量数据当中；C.考虑到张量数据维度较高,会出现维数灾难的问题,直接处理张量数据会比较复杂。所以将张量数据映射到RKHS上并对其进行降维处理；D.用一个张量表示一个多维数据集,利用基于张量模式积的张量数据的降维模型进行降维；E.降维后的数据集采用内积核作为再生核进而产生RKHS；F.原有数据集的再生核是可选的,不同的再生核产生不同的RKHS；G.利用两个RKHS之间的HS算子来衡量两个数据集之间的统计依赖性；H.采用HSIC最大化的准则来确定模式积的矩阵,进而确定子空间的标准正交基；I.本专利的算法适用于机器学习领域中的人脸识别和物体分类等数据集。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马争鸣;              刘佳;              高婷;              雷文洁;                   刘洁       </td>   <td>中山大学</td>   <td>一种基于局部线性嵌入和模式积的张量降维算法</td>   <td>广东省</td>   <td>CN111582322A</td>   <td>2020-08-25</td>   <td>本发明解决张量的降维问题。局部线性嵌入是流形学习的重要算法,它根据高维局部数据之间线性关系保持不变的原则进行数据降维,在降维过程中保持数据的几何信息。故本发明为解决一些张量降维方法未考虑数据的非线性结构的问题,提出基于局部线性嵌入对张量进行降维的算法。由于张量与矩阵的模式积可以改变张量某阶的大小,本发明根据降维张量与一系列矩阵的模式积逼近高维张量的原则进行数据降维,进一步,本发明提出模式积的迭代解法,该解法兼顾张量数据各阶的差异和联系。本发明融合上述二种算法,提出一种基于局部线性嵌入和模式积的张量降维算法,在降维过程中即保持局部性质又考虑全局性质,达到更好的降维效果。</td>   <td>1.一种基于局部线性嵌入和模式积的张量降维算法,其主要特征在于：A.采用一个张量表示一个多维数据集,其中张量的前面各阶表示多维数据的各个维度,而最后一阶则表示数据集所包含的数据的个数；B.针对张量数据的特点,构建基于局部线性嵌入的张量数据降维算法,它根据高维局部数据之间线性关系保持不变的原则进行数据降维；C.由于张量与矩阵的模式积可以改变张量某阶的大小,构建基于模式积的张量数据降维算法,它根据降维张量与一系列矩阵的模式积逼近高维张量的原则进行数据降维；D.提出模式积的迭代解法,相比全局或模式PCA解法,迭代解法兼顾张量数据各阶的差异和联系；E.基于局部线性嵌入的张量降维算法是一个局部性质保持的降维算法,而基于模式积的张量降维算法则是一个全局性质保持的降维算法,将这两种算法融合,提出基于局部线性嵌入和模式积的张量数据降维算法。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         梁姗姗;              岳孟挺;              李新宇;                   张军       </td>   <td>中山大学</td>   <td>基于深度学习的视网膜层和积液区域的层分割方法及系统</td>   <td>广东省</td>   <td>CN111583291A</td>   <td>2020-08-25</td>   <td>本发明公开了一种基于深度学习的视网膜层和积液区域的层分割方法及系统,所述方法包括：获取医疗系统中各节点区域的视网膜OCT数据集,将该数据集划分为预训练数据集及测试数据集,对预训练数据集中的数据进行随机平移,得到训练数据集；根据构建的分割网络及对应的损失函数对分批次送入该分割网络的训练数据集中的数据进行前向传播,得到分割预测图；根据联合损失函数公式计算分割预测图与专家像素级标记图像进行one-hot编码后的标准概率图之间的联合损失值,将联合损失值反向传播,通过预设周期长度的迭代训练得到分割网络模型；通过测试数据集对分割网络模型进行测试,以验证分割网络模型的可靠性。本发明能够提高分割网络泛化能力与类别分割准确率。</td>   <td>1.一种基于深度学习的视网膜层和积液区域的层分割方法,其特征在于,所述方法包括如下步骤：获取医疗系统中各节点区域的视网膜OCT数据集,将所述视网膜OCT数据集划分为预训练数据集及测试数据集,并对所述预训练数据集中的数据进行随机平移,以得到训练数据集；根据构建的改进型Unet分割网络及该分割网络对应的损失函数,对分批次送入该分割网络的训练数据集中的数据进行前向传播,以得到分割预测图；根据联合损失函数公式计算分割预测图与专家像素级标记图像进行one-hot编码后的标准概率图之间的联合损失值,将所述联合损失值进行反向传播,并通过预设周期长度的迭代训练得到分割网络模型；通过所述测试数据集对分割网络模型进行测试,以验证所述分割网络模型的可靠性。</td>   <td>G06T7/143;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄敏;              钱宇翔;                   周锦荣       </td>   <td>中山大学</td>   <td>一种个体行程时间短期预测方法、系统、装置及存储介质</td>   <td>广东省</td>   <td>CN111582563A</td>   <td>2020-08-25</td>   <td>本发明公开了一种个体行程时间短期预测方法、系统、装置及存储介质,方法包括：构建个体出行数据集；计算各个时间窗下各个路段的交通状态；获取每个驾驶员在所述各个交通状态下的驾驶行为,确定每个驾驶员在各个交通状态下的驾驶偏好；根据所述各个时间窗下各个路段的交通状态对下一时间窗的交通状态进行预测；基于预测得到的下一时间窗的交通状态,结合所述驾驶员在各个交通状态下的驾驶偏好,确定驾驶员的路段行程时间预测值；对所述路段行程时间预测值进行误差分析,确定驾驶员的驾驶行为概率。本发明考虑了不同交通状态的驾驶员偏好的不同,提高了驾驶员偏好的预测准确率,可广泛应用于交通数据处理技术领域。</td>   <td>1.一种个体行程时间短期预测方法,其特征在于,包括：获取每个驾驶员在每个时间窗对应路段下的行程时间,构建个体出行数据集；基于每个时间窗下的平均行程时间及对应的出行交通量,计算各个时间窗下各个路段的交通状态；获取每个驾驶员在所述各个交通状态下的驾驶行为,确定每个驾驶员在各个交通状态下的驾驶偏好；根据所述各个时间窗下各个路段的交通状态对下一时间窗的交通状态进行预测；基于预测得到的下一时间窗的交通状态,结合所述驾驶员在各个交通状态下的驾驶偏好,确定驾驶员的路段行程时间预测值；对所述路段行程时间预测值进行误差分析,确定驾驶员的驾驶行为概率。</td>   <td>G06Q10/04;G06K9/62;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马争鸣;              陈皓青;              袁雪敬;              林远平;                   刘洁       </td>   <td>中山大学</td>   <td>一种基于重构误差的局部线性嵌入算法</td>   <td>广东省</td>   <td>CN111563537A</td>   <td>2020-08-21</td>   <td>本发明提出了一种基于重构误差的局部线性嵌入算法,可以在一定程度上解决解决传统的局部线性嵌入算法在面对局部曲率过大的数据不能准确地进行降维的问题,取得更加好的降维效果。本发明在对数据流形划分局部之后,首先计算出每一个局部上邻域点对中心点的拟合关系并且求得邻域点拟合中心点的重构误差,再根据重构误差的大小和各邻域点与中心点的距离的关系构建出调整矩阵,对原拟合关系进行调整,得到新的重构权重,在降维过程中保持新的重构权重不变得到原数据的低维结果。本发明在原理上可以保留局部线性嵌入有效的结果而仅仅对结果欠佳的局部进行调整,因此可以在大部分情况比原本的局部线性嵌入算法有更好的效果。</td>   <td>1.一种基于重构误差的局部线性嵌入算法,其特征在于：A.提出了一种基于重构误差的局部线性嵌入算法,并且提出一种对权重矩阵进行调整的调整矩阵框架<Image id="icf0001" he="135" wi="279" file="FDA0002454756210000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>能够有效解决不理想局部特征的情况,明显提升传统局部线性嵌入算法的效果。B.对局部线性重构是将中心点映射到邻域点的超平面的本质进行了说明,并且在此基础上提出可以根据重构误差及点之间的欧式距离,仅仅对局部线性重构失效的局部进行调整,而不影响原本效果良好的重构权重,能够变化调整幅度的一种调整矩阵<Image id="icf0002" he="281" wi="700" file="FDA0002454756210000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>C.使用K近邻准则分割局部,将局部投影到邻域点组成的超平面进行线性重构。D.从调整矩阵与线性重构权重的积得到调整之后的重构权重。E.保持经过调整的重构权重进行低维嵌入。F.利用瑞利熵(Rayleigh quotient)定理,通过求取特征值和特征向量的方式求出低维嵌入的结果。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马争鸣;              张国凯;              黄海东;              张舒婕;                   刘洁       </td>   <td>中山大学</td>   <td>一种基于局部同胚与全局子空间投影距离最小的张量降维算法</td>   <td>广东省</td>   <td>CN111563538A</td>   <td>2020-08-21</td>   <td>本发明提出了一种基于局部同胚与全局子空间投影距离最小的张量降维算法,用于对高维的张量数据简化,使处理器能够更加迅捷地处理海量数据。本发明运用K近邻准则将原始高维的张量数据分割一个个局部,同时在局部同胚准则下,以保持每个局部从原始高维张量数据映射到切空间的连续依赖关系不变。由于张量与矩阵的模式积可以改变维数的大小,因此本发明是学习一个子空间,使得高维张量数据与其子空间投影距离最小,即降维数据之间方差最大化,这有利于每个低维数据个体的判别分析。本发明结合局部同胚与全局子空间投影距离最小两个准则,充分考虑了高维张量局部的非线性几何结构和全局的分布信息,在降维过程中能更好地保持数据的内在几何关系,防止重要特征的丢失。</td>   <td>1.一种基于局部同胚和子空间学习的非负张量数据降维算法,其特征在于：A.提出了一种基于局部同胚与全局子空间投影距离最小的算法框架,结合该两个准则,即考虑了数据在全局分布信息和局部的流形结构,在降维过程中能更好保持数据的内在几何关系,减少重要特征的损失；B.考虑到张量数据具有非线性结构,本发明提出了局部同胚准则下的降维算法,使得数据投影过程中能够保持原有数据之间关系,学习数据几何结构；C.对原有高维张量数据运用K近邻分割成一个个局部,获得每一个局部对应的选择矩阵和选择向量；D.将高维局部进行中心化处理,再将其映射到切空间,获得低维上投影的局部坐标；E.应用仿射矩阵,将局部坐标旋转对齐进行排列,求取同胚矩阵；F.由于张量与一个矩阵的模式积可以改变张量某个维度的大小,我们要求张量与其在全局子空间上的投影距离最小,即降维张量满足方差最大化,有利于数据个体判别分析；G.求得投影矩阵与原始高维张量进行模式积,得到核心张量。我们将此算法应用到图像识别与图像处理中。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马争鸣;              何健信;              彭欣雅;                   刘洁       </td>   <td>中山大学</td>   <td>一种基于希尔伯特-施密特独立准则子空间学习的域自适应方法</td>   <td>广东省</td>   <td>CN111563539A</td>   <td>2020-08-21</td>   <td>一种基于希尔伯特-施密特独立准则子空间学习的域自适应方法。本发明提出了一种全新的域自适应降维方法,把源域数据和目标域数据的输入样本共同映射到再生核希尔伯特空间,然后再通过投影矩阵映射到另外一个再生核希尔伯特空间即子空间中,最后,基于希尔伯特-施密特独立准则,计算源域数据样本与目标域数据样本在子空间中的HSIC值,使得源域与目标域数据相关依赖性最大化,实现降维。本发明提出将核学习,子空间学习以及希尔伯特-施密特独立准则三者结合起来,计算出最优化的投影矩阵,使得降维效果最优化。</td>   <td>1.一种基于希尔伯特-施密特独立准则子空间学习的域自适应方法,其特征在于：A.令<Image id="icf0001" he="82" wi="700" file="FDA0002454811210000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示数据样本特征空间,其中,X<Sub>s</Sub>表示源域数据样本,X<Sub>t</Sub>表示目标域数据样本,N表示数据样本总个数,且源域数据X<Sub>s</Sub>和目标域数据X<Sub>t</Sub>的边缘概率分布不同；通过核学习,把源域和目标域的数据样本映射到再生核希尔伯特空间H<Sub>1</Sub>＝span{φ(x<Sub>1</Sub>),…,φ(x<Sub>N</Sub>)},其中,φ(x)＝k(·,x)∈H<Sub>1</Sub>是H<Sub>1</Sub>上的核函数,定义为线性核函数<Image id="icf0002" he="88" wi="586" file="FDA0002454811210000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>φ(x<Sub>1</Sub>),…,φ(x<Sub>N</Sub>)分别对应X中的N个样本；通过在H<Sub>1</Sub>上计算内积得到核矩阵K；B.构造H<Sub>1</Sub>的有限维子空间H<Sub>2</Sub>：<Image id="icf0003" he="81" wi="700" file="FDA0002454811210000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中,W是一个投影矩阵,把H<Sub>1</Sub>映射到其子空间即再生核希尔伯特空间H<Sub>2</Sub>,得到H<Sub>1</Sub>在H<Sub>2</Sub>中的投影：<Image id="icf0004" he="79" wi="700" file="FDA0002454811210000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中,<Image id="icf0005" he="86" wi="235" file="FDA0002454811210000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是H<Sub>2</Sub>的一组基,且满足线性无关约束：<Image id="icf0006" he="217" wi="700" file="FDA0002454811210000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>C.根据H<Sub>2</Sub>的线性无关基<Image id="icf0007" he="80" wi="272" file="FDA0002454811210000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>得到源域数据X<Sub>s</Sub>在子空间H<Sub>2</Sub>的投影坐标表示：<Image id="icf0008" he="77" wi="533" file="FDA0002454811210000018.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>目标域数据X<Sub>t</Sub>在子空间H<Sub>2</Sub>的投影坐标表示：<Image id="icf0009" he="85" wi="607" file="FDA0002454811210000019.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中N<Sub>s</Sub>是源域数据个数,N<Sub>t</Sub>是目标域数据个数,N<Sub>s</Sub>+N<Sub>t</Sub>＝N；D.根据内积计算的方式,分别得出源域数据在子空间H<Sub>2</Sub>的核矩阵<Image id="icf0010" he="66" wi="63" file="FDA00024548112100000110.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和目标域数据在子空间H<Sub>2</Sub>的核矩阵<Image id="icf0011" he="67" wi="91" file="FDA00024548112100000111.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>E.由于进行了两次核映射,为了最大化源域与目标域数据间的相关依赖性,这里使用了希尔伯特-施密特独立准则计算<Image id="icf0012" he="65" wi="68" file="FDA00024548112100000112.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0013" he="70" wi="64" file="FDA00024548112100000113.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>在子空间H<Sub>2</Sub>中的相关依赖性,记为：HSIC(X<Sub>s</Sub>,X<Sub>t</Sub>)；然后,得到无监督的目标函数：<Image id="icf0014" he="131" wi="602" file="FDA00024548112100000114.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>s.t.W<Sup>T</Sup>KW＝I<Sub>d</Sub>,其中,C表示去中心化矩阵,求解目标函数并得到最优情况下的投影矩阵W；对于输入的数据样本X,令k<Sub>x</Sub>＝[k(x<Sub>1</Sub>,x) k(x<Sub>2</Sub>,x)…k(x<Sub>N</Sub>,x)]<Sup>T</Sup>,则X的降维表示为：<Image id="icf0015" he="90" wi="204" file="FDA00024548112100000115.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>因此,对原数据的降维完成。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马争鸣;              吴荟彬;              陈成;              梁傲琨;                   刘洁       </td>   <td>中山大学</td>   <td>一种基于局部同胚与子空间学习的非负张量数据降维算法</td>   <td>广东省</td>   <td>CN111563540A</td>   <td>2020-08-21</td>   <td>本发明提出了一种基于局部同胚与子空间学习的非负张量数据降维算法,用于对高维的张量数据简化,使处理器能够更加迅捷地处理海量数据。本发明运用K近邻准则将原始高维的张量数据分割一个个局部,同时在局部同胚准则下,以保持每个局部从原始高维张量数据映射到切空间的连续依赖关系不变。由于张量与矩阵的模式积可以改变维数的大小,因此本发明是学习一个非负的子空间,这有利于实现对每个低维数据个体的非负性的保持。本发明结合局部同胚与子空间学习,充分考虑了高维张量局部的非线性几何结构和全局的分布信息,在降维过程中能更好地保持数据的内在几何关系,防止重要特征的丢失。</td>   <td>1.一种基于局部同胚和子空间学习的非负张量数据降维算法,其特征在于：A.提出一种基于局部同胚和子空间学习的非负张量降维框架,在子空间学习的基础上加了局部同胚项,局部同胚项是通过已有数据样本的信息有效地反映了流形的几何结构,使子空间学习在降维的同时能够保持流形原有的几何结构；B.考虑到张量数据的非线性结构,直接在张量数据上建模进行子空间学习会让流形保持的原有的几何结构丢失,所以将张量数据映射局部同胚操作后保留原本的流形几何结构；C.先对原始数据求取局部同胚矩阵；D.再初始化子空间矩阵,然后初始化核心张量；E.固定核心张量,更新子空间矩阵,运用乘法更新法则及KKT条件先求得子空间中的一组非负基；F.固定子空间矩阵,同样运用乘法更新法则及KKT条件更新核心张量；G.求得的核心张量进行最后一阶模式展开,展开后的每一行代表一个样本数据,我们将此算法应用到图像识别和图片分类当中。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              李昊曦;              顾建权;                   谢斯岳       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于贝叶斯概率与神经网络的图像分割方法</td>   <td>广东省</td>   <td>CN106846321B</td>   <td>2020-08-18</td>   <td>本发明公开一种基于贝叶斯概率与神经网络的图像分割方法,包括：为每个像素定义一个后验概率能量函数,通过利用基于贝叶斯概率计算的形态学膨胀方法对每个像素的相邻像素进行计算,然后对该像素的进行膨胀,使具有相似或者相同的后验概率分布的像素作为对分割分类的神经网络的输入。通过将该像素及其自适应的相邻像素输入到一个多层的神经网络中,提取判别性的特征判断是否为待分割图像的前景或背景,达到有效分割的效果。</td>   <td>1.一种基于贝叶斯概率与神经网络的图像分割方法,其特征在于,包括以下步骤：(1)对于图像的每一个像素,计算该像素的后验概率能量函数,得到该像素的能量值；(2)以该像素为中心,向该像素的邻域扩展,利用贝叶斯概率计算其相邻像素与该像素的相邻关系,选择后验概率大的M个像素；(3)设定高斯分布函数作为被选择的M个像素的条件概率密度函数,然后使用K-means聚类算法对M个像素进行聚类,包括中心点的初始化和聚类收敛；(4)根据聚类结果,选择类别大的一类的聚类中心点为中心,计算N个到该中心最近的像素点作为神经网络的输入数据；(5)将最终被选取的N个像素点作为神经网络的输入数据,提取判别性特征,根据SVM分类器判断该像素属于前景还是背景,实现图像分割。</td>   <td>G06T7/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈刚;              凌晔华;              何涛;              何晟宇;              张余;              孟海涛;                   黄凯       </td>   <td>中山大学</td>   <td>一种基于FPGA实现的二值神经网络立体视觉匹配方法</td>   <td>广东省</td>   <td>CN111553296A</td>   <td>2020-08-18</td>   <td>本发明涉及一种基于FPGA实现的二值神经网络立体视觉匹配方法,包括以下步骤：步骤一：获取双目匹配灰度图像中的像素的周期性输入流；步骤二：从像素中获取图像块；步骤三：将步骤二中的图像块输入预设权值与参数的二值神经网络获取二值的特征向量；步骤四：将特征向量在最大搜索视差内进行代价计算,获得匹配代价；步骤五：将代价输入半全局代价聚合进行代价聚合,得到聚合后的代价；步骤六：在聚合后的代价中选择代价最小的位置作为视差；步骤七：对选择的视差进行一致性检测和视差细致化计算,得到视差图,并且按周期逐个输出像素的视差值。通过二值化的方法可以有效的降低网络的计算和存储资源,从而可以将高精度的立体匹配网络部署到FPGA中。</td>   <td>1.一种基于FPGA实现的二值神经网络立体视觉匹配方法,其特征在于,包括以下步骤：步骤一：获取双目匹配灰度图像中的像素的周期性输入流；步骤二：从像素中获取用于计算代价的图像块；步骤三：使用二值化策略将卷积神经网络进行二值化处理得到二值神经网络；将步骤二中的图像块输入预设权值与参数的二值神经网络获取二值的特征向量；步骤四：将特征向量在最大搜索视差内进行代价计算,获得匹配代价；步骤五：将代价输入半全局代价聚合进行代价聚合,得到聚合后的代价；步骤六：在聚合后的代价中选择代价最小的位置作为视差；步骤七：对选择的视差进行一致性检测和视差细致化计算,得到视差图,并且按周期逐个输出像素的视差值。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08;G06T1/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         高燕;              武志毅;              袁泉;                   李文龙       </td>   <td>中山大学;广州地铁设计研究院股份有限公司</td>   <td>针对地质环境风险的轨道交通选线评估及成本优化方法</td>   <td>广东省</td>   <td>CN111553509A</td>   <td>2020-08-18</td>   <td>本发明涉及一种针对地质环境风险的轨道交通选线评估及成本优化方法,具体包括以下步骤：步骤S1：获取地铁修建区域的基本数据,进行线路区间的选线区段划分和选线方案编号；步骤S2：对完成区段划分和方案编号的线路区间构建风险评价目标函数、风险处理成本目标函数和风险处理工期目标函数；步骤S3：通过动态可变模糊评价模型计算线路区间的地质风险等级；步骤S4：根据地质风险等级,通过启发式算法计算出线路区间的风险评价、成本、工期多目标最优的地铁线路方案。与现有技术相比,本发明具有提高轨道交通路线的安全性和稳定性、通过对轨道交通选线方案所涵盖地质风险的等级及处理风险成本、工期的评估,进而减少轨道交通施工成本等优点。</td>   <td>1.一种针对地质环境风险的轨道交通选线评估及成本优化方法,其特征在于,具体包括以下步骤：步骤S1：获取地铁修建区域的基本数据,根据所述地铁修建区域的基本数据进行线路区间的选线区段划分和选线方案编号；步骤S2：对完成所述选线区段划分和选线方案编号的线路区间构建风险评价目标函数、风险处理成本目标函数和风险处理工期目标函数；步骤S3：通过动态可变模糊评价模型计算所述线路区间的地质风险等级；步骤S4：根据所述地质风险等级,通过启发式算法计算出所述线路区间的风险评价、成本、工期多目标最优的地铁线路方案。</td>   <td>G06Q10/04;G06Q10/06;G06Q50/30</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;              蔡佳辉;              王金鹏;              陈嘉敏;                   林佳玲       </td>   <td>中山大学;广州智慧城市发展研究院</td>   <td>一种基于通道信息共享残差模块的实时语义分割方法</td>   <td>广东省</td>   <td>CN111553921A</td>   <td>2020-08-18</td>   <td>本发明涉及计算机视觉领域,公开了一种基于通道信息共享残差模块的实时语义分割方法,其包括步骤：将特征图X通过二维通道信息共享残差模块经通道分裂进行分组操作,分成X1和X2两组；将分组X1连续经过两个不带空洞卷积的3*1和1*3的卷积核进行卷积操作,再经过带空洞卷积的3*1和1*3卷积核进行卷积操作,得到输出Y1；将输出Y1和输入X2进行拼接,再进行一系列带空洞卷积和不带空洞卷积的3*1和1*3的卷积核进行卷积操作,输出Y2；拼接Y1和Y2后,将各通道洗牌打乱；将实时语义分割网络中的编码器学习到的语义特征映射到高分辨率的特征图上,获得密集预测。该方法可对输入的特征图进行实时精确地分割,有效降低了整个网络的参数量,提高了计算效率,提高了特征图的实时分割精度。</td>   <td>1.一种基于通道信息共享残差模块的实时语义分割方法,其特征在于,包括以下步骤：S1、将特征图X通过二维通道信息共享残差模块经通道分裂进行分组操作,分成两组,分别为X1和X2；S2、将第一个分组X1连续经过两个不带空洞卷积的3*1和1*3的卷积核进行卷积操作,再经过带空洞卷积的3*1和1*3卷积核进行卷积操作,得到第一组分组的输出Y1；S3、将输出Y1和第二分组的输入X2进行拼接,再进行一系列带空洞卷积和不带空洞卷积的3*1和1*3的卷积核进行卷积操作,输出Y2；S4、拼接Y1和Y2后,将各通道洗牌打乱；S5、将实时语义分割网络中的编码器学习到的语义特征映射到高分辨率的特征图上,获得密集预测。</td>   <td>G06T7/10;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         高成英;                   李文盛       </td>   <td>中山大学</td>   <td>一种双手姿势的实时检测方法</td>   <td>广东省</td>   <td>CN111539288A</td>   <td>2020-08-14</td>   <td>本发明公开了一种双手姿势的实时检测方法,通过采用2d关节点位置和3d关节点位置进行双手姿势重建,能够重建出两只手的骨架模型,即使是复杂交互的双手姿势也能够清楚地构建,解决了现有技术存在的无法对复杂交互的双手姿势进行检测的问题,同时,通过采用2d关节点位置和3d关节点位置进行拟合的方式,能够降低重建两只手骨架模型的运算难度,提升重建双手骨架模型的速度,从而保证了检测双手姿势的实时性,从而解决了现有技术存在的难以实现实时性的问题。</td>   <td>1.一种双手姿势的实时检测方法,所述方法基于单目摄像机,其特征在于：所述方法具体包括以下步骤：步骤S1,通过单目摄像机捕捉到双手单帧图像,将所述单帧图像输入到图像分割网络进行分割,分割出包括左手、右手和背景三种类别的分割结果；步骤S2,依据分割结果提取出包括左手2d关节点位置的左手热度图和包括右手2d关节点位置的右手热度图；步骤S3,依据包括左手2d关节点位置的左手热度图和包括右手2d关节点位置的右手热度图,计算出左手3d关节点位置和右手3d关节点位置；步骤S4,将左手2d关节点位置和左手3d关节点位置与左手骨架模型进行拟合,并将右手2d关节点位置和右手3d关节点位置与右手骨架模型进行拟合,得到左右手骨架模型的参数,从而得到双手的姿势。</td>   <td>G06K9/00;G06K9/34;G06K9/46;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         冯诗睿;              吴恙;              李冠彬;              林倞;                   肖侬       </td>   <td>中山大学</td>   <td>一种用于具现化场景问答任务的动作决策模型及方法</td>   <td>广东省</td>   <td>CN111539292A</td>   <td>2020-08-14</td>   <td>本发明公开了一种用于具现化场景问答任务的动作决策模型及方法,所述模型包括：预训练特征提取模组单元,用于对当前时间节点下的多模态输入特征分别进行提取；特征融合单元,用于将由多模态中提取出来的各个特征进行融合形成融合特征；融合特征解码单元,用于将当前时间节点融合特征向量解码为动作空间下的概率分布序列；时序融合动作决策单元,用于将当前及先前时间节点所获得的动作空间下的概率分布序列进行融合,根据融合得到的动作决策向量中的最大值对应的动作做出当前的动作决策。</td>   <td>1.一种用于具现化场景问答任务的动作决策模型,包括：预训练特征提取模组单元,用于对当前时间节点下的多模态输入特征分别进行提取；特征融合单元,用于将由多模态中提取出来的各个特征进行融合形成融合特征；融合特征解码单元,用于将当前时间节点融合特征向量解码为动作空间下的概率分布序列；时序融合动作决策单元,用于将当前及先前时间节点所获得的动作空间下的概率分布序列进行融合,根据融合得到的动作决策向量中的最大值对应的动作做出当前的动作决策。</td>   <td>G06K9/00;G06K9/34;G06K9/62;G06F16/332;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              张达良;              陈荣军;              谢舜道;              朱雄泳;                   曾衍瀚       </td>   <td>中山大学</td>   <td>一种基于区块链和大数据技术的商品溯源系统</td>   <td>广东省</td>   <td>CN111539750A</td>   <td>2020-08-14</td>   <td>本发明公开了一种基于区块链和大数据技术的商品溯源系统,包括数据采集及查询模块,区块链溯源系统及大数据集群服务环境；数据采集及查询模块用于获取及查询商品溯源信息数据,区块链溯源系统用于对商品核心数据进行上链保存,大数据集群服务环境用于对商品详情进行存储；其中商品核心数据为所述商品溯源信息数据中的预设定的核心部分,商品详情为商品溯源信息数据的所有数据。本发明将商品核心数据提交到区块链溯源系统进行上链保存,有效防止被恶意篡改,区块链账本的去中心化分布可明显提高商品重要数据的容灾能力；同时将商品详情传输到大数据系统中进行存储,提高数据管理效率,减小区块链账本的存储压力,提高系统交易的吞吐量。</td>   <td>1.一种基于区块链和大数据技术的商品溯源系统,其特征在于,包括：数据采集及查询模块,以及分别与其连接的区块链溯源系统、大数据集群服务环境；所述数据采集及查询模块用于获取及查询商品溯源信息数据,所述区块链溯源系统用于对商品核心数据进行上链保存,所述大数据集群服务环境用于对商品详情进行存储；其中商品核心数据为所述商品溯源信息数据中的预设定的核心部分,商品详情为所述商品溯源信息数据的所有数据。</td>   <td>G06Q30/00;G06F21/64</td>  </tr>        <tr>   <td>中国专利</td>   <td>         曾怡瑞;              马争鸣;              李冠彬;                   林倞       </td>   <td>中山大学</td>   <td>一种对抗鲁棒的图像显著性检测方法及系统</td>   <td>广东省</td>   <td>CN111539916A</td>   <td>2020-08-14</td>   <td>本发明公开了一种对抗鲁棒的图像显著性检测方法及系统,该方法包括：步骤S1,在原始图像上,针对显著性检测的对抗攻击,基于迭代梯度的方法生成针对显著性检测的对抗攻击样本作为系统的输入图像；步骤S2,以步骤S1中得到的对抗样本作为输入,使用基于能量的生成模型重建输入图像,利用神经网络近似能量函数进行似然建模,生成去除对抗噪声的重建图像；步骤S3,将步骤S2得到的重建图像作为骨干网络的输入并产生密集标记的显著图,本发明可提高现有密集标记方法的鲁棒性并维持效率。</td>   <td>1.一种对抗鲁棒的图像显著性检测方法,包括如下步骤：步骤S1,在原始图像上,针对显著性检测的对抗攻击,基于迭代梯度的方法生成针对显著性检测的对抗攻击样本作为系统的输入图像；步骤S2,以步骤S1中得到的对抗样本作为输入,使用基于能量的生成模型重建输入图像,利用神经网络近似能量函数进行似然建模,生成去除对抗噪声的重建图像；步骤S3,将步骤S2得到的重建图像作为骨干网络的输入并产生密集标记的显著图。</td>   <td>G06T7/00;G06K9/46</td>  </tr>        <tr>   <td>中国专利</td>   <td>         洪思宇;              郭裕兰;              符智恒;                   黄小红       </td>   <td>中山大学</td>   <td>基于多任务网络的单目深度估计与表面法向量估计方法</td>   <td>广东省</td>   <td>CN111539922A</td>   <td>2020-08-14</td>   <td>本发明公开了基于多任务网络的单目深度估计与表面法向量估计方法,所述方法包括以下步骤：采用高分辨率网络作为骨干网络收集多尺度信息；通过高分辨率网络输出了不同分辨率的特征,并对特征分别进行独立上采样后获得与原分辨率相同的特征图；将获得的特征图串接得到一个多尺度表面特征,生成多尺度融合特征；将多尺度融合特征分为2个分支特征,并输入至互相关注意力机制交互模块,获得学习相关性的互相关矩阵；把输入到每个分支特征的1x1连续卷积层,再通过softmax操作得到两个互相关注意力图并利用注意力图上有利于交互的部分获得新的融合特征；重复步骤S5获得特定任务的特征信息后,最终得到单目深度估计和表面法向量估计结果。</td>   <td>1.基于多任务网络的单目深度估计与表面法向量估计方法,其特征在于,所述方法包括以下步骤：S1采用高分辨率网络作为骨干网络收集多尺度信息；S2通过高分辨率网络输出了不同分辨率的特征,并对特征分别进行独立上采样后获得与原分辨率相同的特征图；S3将获得的特征图串接得到一个多尺度表面特征,生成多尺度融合特征；S4将多尺度融合特征分为2个分支特征,并输入至互相关注意力机制交互模块,获得学习相关性的互相关矩阵；S5把输入到每个分支特征的连续卷积层,再通过softmax操作得到两个互相关注意力图并利用注意力图上有利于交互的部分获得新的融合特征；S6重复步骤S5获得特定任务的特征信息后,最终得到单目深度估计和表面法向量估计结果。</td>   <td>G06T7/00;G06T7/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         吴博文;              谢震宇;              梁小丹;              董浩业;                   林倞       </td>   <td>中山大学</td>   <td>一种基于单张图片的运动转移方法及系统</td>   <td>广东省</td>   <td>CN111539262A</td>   <td>2020-08-14</td>   <td>本发明公开了一种基于单张图片的运动转移方法及系统,所述方法包括：步骤S1,对源视频利用人体姿态估计器获得对应的姿势序列S<Sub>pose</Sub>,对目标人物图片I使用人体解析器获取对应的人体解析分割图I<Sub>parsing</Sub>,根据源视频对应的姿势序列S<Sub>pose</Sub>以及人体解析分割图I<Sub>parsing</Sub>,生成目标视频的人体语义解析图<Image he="91" wi="232" file="DDA0002436272650000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>步骤S2,根据生成人体语义解析图以及目标人物外观图片前景I<Sub>a</Sub>,生成目标视频的前景<Image he="88" wi="109" file="DDA0002436272650000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>步骤S3,通过修护后的背景图bg、步骤S2中生成的前景<Image he="88" wi="83" file="DDA0002436272650000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>以及步骤S2中生成的前一帧前景<Image he="89" wi="116" file="DDA0002436272650000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>预测出前景掩膜,并通过该前景掩膜融合前后景得到最终目标视频中的帧x<Sup>t</Sup>。</td>   <td>1.一种基于单张图片的运动转移方法,包括如下步骤：步骤S1,对源视频利用人体姿态估计器获得对应的姿势序列S<Sub>pose</Sub>,对目标人物图片I使用人体解析器获取目标人物图片I所对应的人体解析分割图I<Sub>parsing</Sub>,根据源视频对应的姿势序列S<Sub>pose</Sub>以及目标人物图片I所对应的人体解析分割图I<Sub>parsing</Sub>,并生成目标视频的人体语义解析图<Image id="icf0001" he="95" wi="224" file="FDA0002436272620000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>步骤S2,根据步骤S1中生成的目标视频的人体语义解析图<Image id="icf0002" he="92" wi="204" file="FDA0002436272620000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>以及目标人物外观图片前景I<Sub>a</Sub>,生成目标视频的前景<Image id="icf0003" he="93" wi="110" file="FDA0002436272620000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>步骤S3,通过修护后的背景图bg、步骤S2中生成的前景<Image id="icf0004" he="86" wi="89" file="FDA0002436272620000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>以及步骤S2中生成的前一帧前景<Image id="icf0005" he="94" wi="119" file="FDA0002436272620000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>预测出前景掩膜fg_mask<Sup>t</Sup>,并通过该前景掩膜融合前后景得到最终目标视频中的帧x<Sup>t</Sup>。</td>   <td>G06K9/00;G06K9/34;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         江明;                   武晓鸽       </td>   <td>中山大学</td>   <td>一种用于分布式大规模多天线系统的半监督学习定位方法</td>   <td>广东省</td>   <td>CN111523571A</td>   <td>2020-08-11</td>   <td>本发明提供一种用于分布式大规模多天线系统的半监督学习定位方法,包括：生成两个不同的数据集,包括训练数据集和坐标隶属度集；利用半监督学习算法估计高斯混合模型GMM参数,根据训练数据集对GMM进行初始化；基于半监督期望极大EM算法对GMM参数进行迭代估计,完成GMM的训练；根据训练完成的GMM和坐标隶属度集,完成对目标接收信号强度RSS数据对应的位置信息的估计。通过分析不同天线分布情况下的系统性能,可证明本发明提供的定位方法可实现较高的定位精度；同时,该方法可以有效降低训练集的采样成本,并且仍可达到较高的定位精度；可有效保证位置估计结果的唯一性,为解决二维平面定位和三维空间定位的问题提供了一种有效的通用方法。</td>   <td>1.一种用于分布式大规模多天线系统的半监督学习定位方法,其特征在于,包括以下步骤：S1：生成两个不同的数据集,包括训练数据集和坐标隶属度集；S2：利用半监督学习算法估计高斯混合模型GMM参数,根据训练数据集对GMM进行初始化；S3：基于半监督期望极大EM算法对GMM参数进行迭代估计,完成GMM的训练；S4：根据训练完成的GMM和坐标隶属度集,完成对目标接收信号强度RSS数据对应的位置信息的估计。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张小虎;              叶雪辀;              甘叔玮;                   林彬       </td>   <td>中山大学</td>   <td>基于张角序列的像机标定异面控制点自动匹配方法</td>   <td>广东省</td>   <td>CN111524191A</td>   <td>2020-08-11</td>   <td>本发明公开一种基于张角序列的像机标定异面控制点自动匹配方法,包括：获取控制点和相机光心三维空间坐标以及控制点在像素坐标系下的二维图像坐标；获取三维控制点两两相对于相机光心的夹角,形成第一角度序列矩阵,获取该矩阵中的最大值；筛选出距离最远的两个二维控制点,得到像机的等效焦距；基于等效焦距得到二维控制点两两相对于相机坐标系原点的夹角,形成第二角度序列矩阵；基于第一角度序列矩阵与第二角度序列矩阵得到每一个三维控制点与所有二维控制点的距离矩阵；基于距离矩阵完成像机标定异面控制点的匹配。利用张角序列对于序列的不一致具有鲁棒性,同时除了像点误差、物点误差及光心初值误差以外,张角序列不会额外引入其他误差。</td>   <td>1.一种基于张角序列的像机标定异面控制点自动匹配方法,其特征在于,包括如下步骤：步骤1,获取所有控制点和相机光心在世界坐标系下的三维空间坐标以及所有控制点在像素坐标系下的二维图像坐标；步骤2,计算世界坐标系下所有三维控制点两两之间P<Sub>i</Sub>、P<Sub>j</Sub>相对于光心O<Sub>w</Sub>的夹角∠P<Sub>i</Sub>O<Sub>w</Sub>P<Sub>j</Sub>,形成N×N的第一角度序列矩阵[X<Sub>ij</Sub>]<Sub>N×N</Sub>,并获取矩阵[X<Sub>ij</Sub>]<Sub>N×N</Sub>中的最大值α<Sub>max</Sub>,其中,N为控制点的总数,i＝1,2,3···,j＝1,2,3···,i≠j；步骤3,筛选出像素坐标系下距离最远的两个二维控制点p<Sub>a</Sub>、p<Sub>b</Sub>,根据α<Sub>max</Sub>与点p<Sub>a</Sub>、p<Sub>b</Sub>的二维图像坐标得到像机的等效焦距f；步骤4,基于等效焦距f得到所有二维控制点两两之间p<Sub>i</Sub>、p<Sub>j</Sub>相对于相机坐标系原点O<Sub>c</Sub>的夹角∠p<Sub>i</Sub>O<Sub>c</Sub>p<Sub>j</Sub>,形成N×N的第二角度序列矩阵[x<Sub>ij</Sub>]<Sub>N×N</Sub>；步骤5,基于矩阵[X<Sub>ij</Sub>]<Sub>N×N</Sub>与矩阵[x<Sub>ij</Sub>]<Sub>N×N</Sub>得到每一个三维控制点与所有二维控制点的距离,形成N×N的距离矩阵；步骤6,基于距离矩阵完成像机标定异面控制点的匹配。</td>   <td>G06T7/80;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              彭志锋;                   黄梓轩       </td>   <td>中山大学</td>   <td>一种基于Fredholm学习和对抗学习的域适应方法</td>   <td>广东省</td>   <td>CN111523680A</td>   <td>2020-08-11</td>   <td>本发明为一种基于Fredholm学习和对抗学习的域适应方法,所述方法包括：特征提取,对源域数据X<Sup>s</Sup>和目标域数据X<Sup>t</Sup>用同一个特征提取器提取特征；领域鉴别,在得到被提取特征后,要鉴别这些特征属于哪个域；其中,所述领域鉴别分为两个阶段：(1)Fredholm特征的获取；(2)域鉴别器进行鉴别；样本分类,使用两个分类器,分类模块接收特征提取器提取到的特征,输入到全连接层中计算,并经过softmax后分别从源域分类器C<Sup>s</Sup>和目标域分类器C<Sup>t</Sup>输出分类结果。本发明的有益效果在于,提出的基于Fredholm学习和对抗学习的域适应方法在图像分类上能够达到更好的分类效果。</td>   <td>1.一种基于Fredholm学习和对抗学习的域适应方法,其特征在于,所述方法包括：S1特征提取,对源域数据X<Sup>s</Sup>和目标域数据X<Sup>t</Sup>用同一个特征提取器提取特征；S2领域鉴别,在得到被提取特征后,要鉴别这些特征属于哪个域；其中,所述领域鉴别分为两个阶段：(1)Fredholm特征的获取；(2)域鉴别器进行鉴别；S3样本分类,使用两个分类器,分类模块接收特征提取器提取到的特征,输入到全连接层中计算,并经过softmax后分别从源域分类器C<Sup>s</Sup>和目标域分类器C<Sup>t</Sup>输出分类结果。</td>   <td>G06N20/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘宁;                   黄立峰       </td>   <td>中山大学</td>   <td>一种作用于目标检测系统的行人图像检测方法</td>   <td>广东省</td>   <td>CN111523478A</td>   <td>2020-08-11</td>   <td>本发明公开了一种作用于目标检测系统的行人图像检测方法,包括以下步骤：在目标检测系统中构建行人图像检测模型,采集待检测的行人图像,将待检测的行人图像输入到所述行人图像检测模型,所述行人图像检测模型输出行人图像检测结果；通过将包括有伪装图案的行人图像和不含有伪装图案的行人图像作为行人图像检测模型的训练数据,使得行人图像检测模型能够具备识别伪装图案的能力,同时,通过在目标检测系统中构建所述行人图像检测模型,目标检测系统通过针对行人图像检测模型的检测结果做出相应的措施,能够提高目标检测系统的防御能力和检测能力,避免出现漏检或者错检的情况。</td>   <td>1.一种作用于目标检测系统的行人图像检测方法,其特征在于：包括以下步骤：步骤S1,在目标检测系统中构建行人图像检测模型,所述行人图像检测模型的输入端为行人图像,所述行人图像检测模型的输出端为行人图像检测结果,其中,所述行人图像包括含有伪装图案的行人图像和不含有伪装图案的行人图像；步骤S2,采集待检测的行人图像；步骤S3,将待检测的行人图像输入到所述行人图像检测模型；步骤S4,所述行人图像检测模型输出行人图像检测结果。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              甘宇康;              李冠彬;                   王青       </td>   <td>中山大学</td>   <td>一种RGBD图像语义分割方法</td>   <td>广东省</td>   <td>CN107403430B</td>   <td>2020-08-07</td>   <td>本发明提供了一种RGBD图像语义分割方法,包括以下步骤：S1、采集训练样本的数据；S2、构建可配置的深度模型,并将训练样本的数据输入深度模型,以对深度模型进行训练；S3、获取需要进行语义分割的彩色图及其对应的深度图,利用训练后的深度模型对彩色图和深度图进行分析,预测RGBD图像中每个像素所属的物体类别；S4、根据S3的结果,形成并输出预测的图像语义分割图。本发明利用深层次的卷积神经网络和长短时记忆网络以及大数据,能有效地融合彩色图像和深度图像的特征,并且能有效地挖掘图像中的上下文信息,拥有很高的准确率。</td>   <td>1.一种RGBD图像语义分割方法,其特征在于,包括以下步骤：S1、采集训练样本的数据；S2、构建可配置的深度模型,并将训练样本的数据输入深度模型,以对深度模型进行训练；S3、获取需要进行语义分割的彩色图及其对应的深度图,利用训练后的深度模型对彩色图和深度图进行分析,预测RGBD图像中每个像素所属的物体类别；S4、根据S3的结果,形成并输出预测的图像语义分割图；其中,所述深度模型包括三个依次串联的子网络；第一子网络用于对彩色图像和深度图像数据的基础表达进行提取和学习,包括用于提取彩色图的特征的第一卷积神经网络,以及用于提取深度图的特征的第二卷积神经网络；第二子网络用于融合彩色图和深度图的特征以及学习图像的全局上下文信息,包括用于提取彩色图的上下文信息的第一长短时记忆网络,用于提取深度图像的上下文信息的第二长短时记忆网络,以及用于融合彩色图和深度图的上下文信息的第三长短时记忆网络；第三子网络包括第三卷积神经网络,用于融合局部特征和全局特征,进而预测图像像素所属的物体类别；彩色图依次经过第一卷积神经网络、第一长短时记忆网络处理后汇入第三长短时记忆网络；深度图依次经过第二卷积神经网络、第二长短时记忆网络处理后汇入第三长短时记忆网络；第三子网络根据第二子网络的输出和第一卷积神经网络的输出层叠得到的特征,输出RGBD图像中每个像素属于每一物体类别的概率值,进而预测RGBD图像像素所属的物体类别,最终输出图像语义分割图。</td>   <td>G06T7/10;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭晓军;              冯大鹏;              梁小丹;              王焕宇;              杨陈如诗;                   杨梦雨       </td>   <td>中山大学</td>   <td>一种基于多源数据知识迁移的三维物体检测框架</td>   <td>广东省</td>   <td>CN111507222A</td>   <td>2020-08-07</td>   <td>本发明提供了一种基于多源数据知识迁移的三维物体检测框架,通过将图像特征提取单元所提取处的图像特征输出,使感兴趣目标选择单元根据图像特征,输出感兴趣目标的点云数据至点云特征提取单元,由点云特征所从点云数据中提取点云特征,然后,在知识迁移单元中,使图像特征学习点云特征并更新图像特征提取单元的参数,而三维目标参数预测单元根据所述图像特征和点云特征更新所述图像特征提取单元和点云特征提取单元的参数,最后,由更新后的图像特征提取单元重新提取图像特征至三维目标参数预测单元,由三维目标参数预测单元根据所述图像特征,推算并输入三维参数,由此,提供了基于二维图像的三维物体检测的检测精度。</td>   <td>1.一种基于多源数据知识迁移的三维物体检测框架,其特征在于,包括以下步骤：S1、图像特征提取单元从图像中提取第一图像特征,并将所述第一图像特征输出至感兴趣目标选择单元、知识迁移单元和三维目标参数预测单元；S2、所述感兴趣目标选择单元根据所述第一图像特征,生成一系列的感兴趣目标的二维包围盒,以从点数空间中提取相应区域的点云数据以输出至点云特征单元；S3、所述点云特征提取单元从所述点云数据中提取点云特征,并将所述点云特征输出至所述知识迁移单元和三维目标参数预测单元；S4、所述知识迁移单元计算所述图像特征与所述点云特征两者之间的余弦相似度,并对所述余弦相似度进行处理,以更新所述图像特征提取单元的参数；S5、所述三维目标参数预测单元根据所述图像特征、所述点云特征生成三维包围盒,并输出所述三维包围盒的九个自由度参数,之后还通过反向传播更新所述图像特征提取单元、所述点云特征提取单元的参数；S6、二维检测器从所述图像中提取目标的候选边界框,并将所述候选边界框发送至所述图像特征提取单元；S7、所述图像特征提取单元从所述候选边界框中提取第二图像特征,并将所述第二图像特征输出至所述感兴趣目标选择单元、以及所述三维目标参数预测单元；S8、所述感兴趣目标选择单元根据所述第二图像特征,生成相应的二维包围盒,并输出所述相应的二维包围盒的中心坐标至所述三维目标参数预测单元；S9、所述三维目标参数预测单元根据所述第二图像特征、以及所述相应的二维包围盒的中心点坐标,生成相应的三维包围盒,并输出所述相应的三维包围盒的九个自由度参数。</td>   <td>G06K9/00;G06K9/32;G06K9/46;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王金桥;              林佳玲;              胡建国;              唐明;              朱贵波;                   蔡佳辉       </td>   <td>杰创智能科技股份有限公司;广州智慧城市发展研究院;中山大学</td>   <td>一种基于多接受野的交替更新网络的场景分割方法</td>   <td>广东省</td>   <td>CN111507984A</td>   <td>2020-08-07</td>   <td>本发明公开了一种基于多接受野的交替更新网络的场景分割方法,包括以下步骤：S1、将输入图像通过一个预训练好的卷积神经网络,进行特征图的提取；S2、通过预训练好的集合网络空洞金字塔模块,得到包含高层语义信息的特征图；S3、在步骤S2中得到的特征图的基础上对逐个像素计算分类的损失,获得分割热图。本发明的方法使用空洞金字塔池化网络进行场景的语义分割,集合网络空洞金字塔模块一方面能对特征图进行充分利用,改善网络中信息的流动,减少模型参数,从而达到压缩模型的效果,另一方面又结合扩张卷积的方法,从而增加了卷积核的接受野尺寸,以实现对场景图里面不同大小目标的分割,具有鲁棒性强、计算效率高等特性。</td>   <td>1.一种基于多接受野的交替更新网络的场景分割方法,其特征在于,包括以下步骤：S1、将输入图像通过一个预训练好的卷积神经网络,进行特征图的提取；S2、通过预训练好的集合网络空洞金字塔模块,得到包含高层语义信息的特征图；S3、在步骤S2中得到的特征图的基础上对逐个像素计算分类的损失,获得分割热图。</td>   <td>G06T7/10;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄华兵;                   王先伟       </td>   <td>中山大学</td>   <td>一种城市内涝风险快速评估方法及系统</td>   <td>广东省</td>   <td>CN111507375A</td>   <td>2020-08-07</td>   <td>本发明公开了一种城市内涝风险快速评估方法及系统,以内涝发生快慢为衡量风险的标准：对于给定内涝点,降雨之后达到危险积水深度所需的时间越短,留给应急响应的时间越有限,则相应的风险越大。以上方案只需要设计暴雨、DEM、土地利用/覆盖和排水系统设计标准即可计算,不涉及复杂的水动力模型,对排水管网数据要求低,基于GIS平台即可完成计算。本发明方法及系统解决了现有内涝风险评估对基础数据及操作人员的建模能力要求高、且其存在计算效率低、实用性差的问题。</td>   <td>1.一种城市内涝风险快速评估方法,其特征在于,包括以下步骤：S1.对于目标区域,根据DEM识别潜在的内涝发生位置,划定每一个内涝发生位置的汇水范围,并构成相应的小流域；S2.计算每个所述小流域的属性,包括其汇水面积A、平均坡度S、积水体积V；S3.设定降雨情景,根据水量平衡模型,考虑包括排水系统设计标准、土壤渗透和蒸散发的影响,计算在地表形成积水的内涝雨量R<Sub>i</Sub>；S4.以所述小流域为单元,以内涝发生快慢作为衡量标准,计算目标区域的内涝风险Risk：Risk＝ln(R<Sub>i</Sub>×A×S<Sup>1/2</Sup>/V)。</td>   <td>G06K9/62;G06Q10/06;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         何明光;              施丹莉;                   何嘉健       </td>   <td>中山大学中山眼科中心</td>   <td>一种基于头部运动和注视行为数据的行为识别系统、装置和方法</td>   <td>广东省</td>   <td>CN111507281A</td>   <td>2020-08-07</td>   <td>本发明公开了一种基于头部运动和注视行为数据的行为识别系统、装置和方法,包括并不限于附着在眼镜、头箍、耳机支架上的轻量信息采集装置和深度学习系统。通过信息采集装置采集头部运动和注视方向的运动姿态,如三轴加速度、三轴角速度、三轴欧拉角、三轴磁力等,输入到深度学习系统进行降噪、分割、特征提取、特征融合,进行单一或复合行为预测,从而超越通过传统的基于身体传感器所能识别的站立、行走、跑动、坐下等简单运动行为,实现实时、精细地识别与人类视觉活动相关的行为,如阅读、使用电脑、使用手机、交谈、娱乐、思考等复杂行为特征。</td>   <td>1.一种基于头部运动和注视行为数据的行为识别系统,其特征在于：包括轻量信息采集装置和深度学习系统,所述的信息采集装置采集注视方向的运动姿态,该信息包括三轴加速度、三轴角速度、三轴欧拉角、三轴磁力,输入到深度学习系统进行降噪、分割、特征提取、特征融合,进行单一或复合行为预测。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08;G01C23/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈龙;              谢国栋;              崔明月;                   黄凯       </td>   <td>中山大学</td>   <td>基于CUDA实现的实时立体匹配及优化的方法</td>   <td>广东省</td>   <td>CN107316324B</td>   <td>2020-08-04</td>   <td>本发明涉及计算机视觉的技术领域,更具体地,涉及基于CUDA实现的实时立体匹配及优化的方法。本发明是一种利用CUDA并行化处理来对左右输入图像进行密集的立体匹配,并且生成实时的视差图的方法。包括：对左右两图做census转换,生成一个字符序列,用汉明码距得到一个初始的cost,动态规划每个像素点8条路径,取一个最短的路径和来得到最终的cost,得到最初的一个密集的视差图；对左图利用k-means算法进行超像素分割,得到一个个超像素平面块,利用超像素平面拟合来优化初始视差进行优化。本发明还涉及到多任务GPU并行加速,具体涉及在NVIDA的CUDA架构并行实现多个任务,属于GPGPU计算领域。通过GPU多线程处理优化极大的缩小了计算时间,得到实时的视差图。</td>   <td>1.基于CUDA实现的实时立体匹配及优化的方法,其特征在于,包括以下步骤：S1.选定一个框大小,对左右图像除边缘部分的每个像素都做census转换为字符序列；S2.在给定的视差范围内,选定左图像素点,遍历右图范围内各个字符序列,计算汉明码距,得到初始的cost；S3.路径聚合,通过动态规划找到每一个点的路径聚合代价最小,得到能量函数；S4.用WTA选取能量函数最小所对应的视差值；S5.左右一致性检查进行后期校验,得到初始的密集视差图；S6.将左图每个像素由RGB颜色空间转换为CIElab颜色空间；S7.通过k-means聚合算法划分出超像素平面块,迭代几次直至收敛,并融合超像素平面块；S8.对每个超像素平面多次采样,计算平面参数；S9.用得到的平面参数计算得到新的视差值；S10.对新的视差做插值运算,减小黑色块,平滑视差图。</td>   <td>G06T7/33;G06T7/55;G06T1/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         梅晗;                   张东       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种双层圆环麦克风阵列语音增强方法</td>   <td>广东省</td>   <td>CN108447499B</td>   <td>2020-08-04</td>   <td>本发明公开了一种双层圆环麦克风阵列语音增强方法,根据声场环境的不同采用不同的波束形成方法,提高了麦克风阵列的对噪声的鲁棒性,其中当声场环境为散射噪声场时,在不同频段分别采用一阶和二阶差分波束形成器项组合的方法进行语音增强,可以在不牺牲白噪声增益的情况下获得较高的方向性因数。</td>   <td>1.一种双层圆环麦克风阵列语音增强方法,其特征在于：包括以下步骤：A、采集通道的时域信号；B、对采集到的通道的时域信号进行分帧、加窗后得到通道的接收信号的频域表示；C、对通道的接收信号的频域表示进行短时傅里叶变换,得到整个麦克风阵列接收的信号的频率子带；D、根据静音段不同通道间的相干函数对声场环境进行判定；E、当声场环境为非相关噪声场时,在子带上采用一阶差分波束形成器进行波束形成得到权向量；当声场环境为散射噪声场时,在低频段的子带上采用一阶差分波束形成器进行波束形成得到权向量,在高频段的子带上采用二阶差分波束形成器进行波束形成得到权向量；所述权向量的构建表达式为：          <U>h</U>(ω)＝<U>ψ</U><Sup>H</Sup>(ω)[<U>ψ</U>(ω)<U>ψ</U><Sup>H</Sup>(ω)]<Sup>-1</Sup>b<Sub>N+1</Sub>；其中,<Image id="icf0001" he="122" wi="581" file="FDA0002534109680000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>b<Sub>N,n</Sub>为预设的参数,用于构建理想的N阶差分阵列波束,N＝1或2,当声场环境为非相关噪声场时,N取1；当声场环境为散射噪声场时,N取1和2两个值并分别得到不同的权向量；<U>ψ</U>(ω)为导向矩阵,且<U>ψ</U>(ω)＝[ψ<Sub>1</Sub>(ω)ψ<Sub>2</Sub>(ω)]<Sup>T</Sup>,其中ψ(ω)的表达式为；          <Image id="icf0002" he="146" wi="700" file="FDA0002534109680000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,<Image id="icf0003" he="143" wi="700" file="FDA0002534109680000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>j<Sub>n</Sub>()  为第一类n阶贝塞尔函数,p＝1,2；F、根据权向量计算得到输出信号；G、将输出信号进行变换后得到增强后的语音信号。</td>   <td>G10L21/0216;G10L21/0232;G10L21/0264</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;                   黄嘉胜       </td>   <td>中山大学</td>   <td>基于图卷积网络和长短时记忆网络的篮球比赛进球事件预测方法</td>   <td>广东省</td>   <td>CN111488815A</td>   <td>2020-08-04</td>   <td>本发明公开了一种基于图卷积网络和长短时记忆网络的篮球比赛进球事件预测方法,包括下述步骤：S1、对篮球比赛视频单位时间视频频段的个体进行检测,依据检测到的个人位置,在空间和时间上进行视频切片,再将切片后的视频送进三维残差卷积网络进行特征提取；S2、构建基于图卷积神经网络的篮球进分时间预测模型；S3、基于图卷积神经网络和长短时记忆神经实现对一段篮球视频下一单位长度进球事件的预测。本发明定义了新型的图卷积神经网络,能有效捕捉人与人的关系,有效地考虑到了边权重的重要信息,并将场景全局特征作为模型输入,使得模型能从局部到全局刻画视频特征,从而获得一个更加完整的篮球比赛行为描述,进而有效地预测未来进球事件。</td>   <td>1.基于图卷积网络和长短时记忆网络的篮球比赛进球事件预测方法,其特征在于,包括下述步骤：S1、对篮球比赛视频单位时间视频频段的个体进行检测,依据检测到的个人位置,在空间和时间上进行视频切片,再将切片后的视频送进三维残差卷积网络进行特征提取；S2、构建基于图卷积神经网络的篮球进分时间预测模型,所述图卷积神经网络用于对具有节点和边的图模型进行特征变换和表征,从而使得每个节点不仅包含该节点所具有特征,同时包含与该节点相邻节点的信息；S3、基于图卷积神经网络和长短时记忆神经实现对一段篮球视频下一单位长度进球事件的预测,先将输入的T秒长的篮球视频,按单位时间长度1秒分为T段视频片段,对于每个视频片段,用图卷积网络进行特征提取,最后将T个视频片段的特征按顺序输入长短时记忆神经网络进行预测。</td>   <td>G06K9/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         唐晓颖;              邓丽洁;              袁进;                   黄海香       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学;中山大学中山眼科中心</td>   <td>超像素提取过渡期片状角膜溃烂区域的方法</td>   <td>广东省</td>   <td>CN108510470B</td>   <td>2020-08-04</td>   <td>本发明公开了一种超像素提取过渡期片状角膜溃烂区域的方法,包括步骤：S1.图像预处理；S2.超像素分割；S3.超像素特征提取；S4.SVM分类；S5.分割结果自动修正。在超像素分割的基础上,结合基于支持向量机(SVM)的自动分类以及基于线性回归的自动形态运算,有效地解决了过渡期片状角膜溃烂难以精准分割的问题。该方法可以在数据不断更新与增多的过程中,实现对算法模型的自我更新和完善,来达到适应更多角膜溃烂类型的目的,也为其他相关检测的算法设计提供借鉴,为多类型角膜溃烂实现溃烂区域的自动提取以及基于人工智能的辅助诊断提供有力基础。</td>   <td>1.一种超像素提取过渡期片状角膜溃烂区域的方法,其特征在于,包括以下步骤：S1.图像预处理：对输入图像构建角膜椭圆模型进行分割,提取角膜区域图像,对角膜区域图像进行滤波去噪；S2.超像素分割：基于超像素分割算法,对角膜区域图像进行超像素分块；S3.超像素特征提取：对每一个超像素块所包含的像素点求R、G、B各通道以及坐标位置(X,Y)的均值,得到<Image id="icf0001" he="94" wi="316" file="FDA0002530235330000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>构建5维特征矩阵；S4.SVM分类：利用所提取的超像素特征,对每个超像素进行基于SVM线性分类器的自动分类,得到初步分割结果；S5.分割结果自动修正：基于SVM分类的初步分割结果,进行腐蚀或膨胀操作的形态运算,获得准确分割结果；其中所述步骤S4中具体包括步骤：S4-1.训练数据：基于自动并配以手动校正的方法来精准分割100张过渡期片状角膜溃烂染色图片,在其分割结果的基础上,在每张样本图片上随机选择角膜溃烂区域中的1000个像素点以及角膜内非溃烂区域中的1000个像素点,分别设置训练标签为0和1,并提取各个像素点的R、G、B值及位置坐标信息,总共得到200000个训练样本点,并以R、G、B值及坐标信息X、Y的5维向量作为样本特征输入；S4-2.训练模型：选用SVM线性分类器,设置迭代次数为10000,获得预测模型；S4-3.预测：以待分割图像中的每一个超像素块作为样本输入,每个超像素块中所有像素点的R,G,B各通道亮度均值以及横、纵坐标均值作为预测样本特征输入,预测得到每个超像素块的标签并将其作为该超像素块中各个像素点的标签,得到SVM分割结果。</td>   <td>G06T7/00;G06T7/11;G06T7/155;G06T7/90;G06T5/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         邓楚富;              陈志广;              瞿毅力;              苏琬琪;              肖侬;                   卢宇彤       </td>   <td>中山大学</td>   <td>一种基于距离对抗生成网络的领域自适应方法及系统</td>   <td>广东省</td>   <td>CN111476771A</td>   <td>2020-07-31</td>   <td>本发明公开了一种基于距离对抗生成网络的领域自适应方法及系统,本发明方法包括获取目标域的医学图像x<Sub>T</Sub>；将目标域的医学图像x<Sub>T</Sub>输入预先训练好的距离对抗生成网络,通过距离对抗生成网络中的目标域转换器G<Sub>T</Sub>将目标域的医学图像x<Sub>T</Sub>投影成中间表示m<Sub>T</Sub>；通过距离对抗生成网络中的分割器Seg对中间表示m<Sub>T</Sub>进行分割得到分割图l<Sub>T,f</Sub>；将得到的分割图l<Sub>T,f</Sub>作为目标域的医学图像x<Sub>T</Sub>的目标域分割标签输出。本发明能够解决不同医学图像模态的域自适应问题,使得在任意模态训练完成的数据集可以很好地运用在其他模态上,从而极大地提高医学图像处理模型的泛化能力,减轻图像处理模型对数据集的模态依赖。</td>   <td>1.一种基于距离对抗生成网络的领域自适应方法,其特征在于实施步骤包括：获取目标域的医学图像x<Sub>T</Sub>；将目标域的医学图像x<Sub>T</Sub>输入预先训练好的距离对抗生成网络,通过距离对抗生成网络中的目标域转换器G<Sub>T</Sub>将目标域的医学图像x<Sub>T</Sub>投影成中间表示m<Sub>T</Sub>；通过距离对抗生成网络中的分割器Seg对中间表示m<Sub>T</Sub>进行分割得到分割图l<Sub>T,f</Sub>；将得到的分割图l<Sub>T,f</Sub>作为目标域的医学图像x<Sub>T</Sub>的目标域分割标签输出。</td>   <td>G06T7/00;G06T7/10;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨帆;              姚建华;              范新娟;              刘海玲;              陆唯佳;                   周昵昀       </td>   <td>腾讯科技(深圳)有限公司;中山大学附属第六医院</td>   <td>基于深度学习的病理图像处理方法、模型训练方法及装置</td>   <td>广东省</td>   <td>CN111462036A</td>   <td>2020-07-28</td>   <td>本申请公开了一种基于深度学习的病理图像处理方法,包括：获取待处理的目标对象的病理图像；对待处理的目标对象的病理图像进行分块处理,以得到分块图像集合；对分块图像集合进行特征分类提取处理,得到N个第一特征集合、N个第二特征集合以及N个第三特征集合；生成与目标对象所对应的目标融合特征；调用病理图像分析模型对目标融合特征进行分析处理,以输出待处理的目标对象的病理分析结果。本申请还公开了一种模型训练方法。本申请可以直接利用病理图像预测病灶转移的风险,相较于生物检测技术,能够减少等待检测结果的时间,提升检测效率,还能避免因实验误差和操作误差等不可控因素带来的误差,从而提供更为精准的检测结果。</td>   <td>1.一种基于深度学习的病理图像处理方法,其特征在于,包括：获取待处理的目标对象的病理图像；对所述待处理的目标对象的病理图像进行分块处理,以得到分块图像集合,其中,所述分块图像集合包括N个分块图像,所述N为大于或等于1的图像；对所述分块图像集合进行特征分类提取处理,得到N个第一特征集合、N个第二特征集合以及N个第三特征集合,其中,所述第一特征集合包括至少一个与分块图像形态关联的特征,所述第二特征集合包括至少一个与细胞核形态关联的特征,所述第三特征集合包括至少一个与细胞核位置关联的特征；将所述经分类提取处理后的N个第一特征集合、N个第二特征集合以及N个第三特征集合按预设规则进行特征融合处理,生成与所述目标对象所对应的目标融合特征,其中,所述融合特征包括P个元素,所述P为大于1的整数；调用病理图像分析模型对所述目标融合特征进行分析处理,以输出所述待处理的目标对象的病理分析结果。</td>   <td>G06T7/00;G06T7/66;G06K9/00;G06K9/32;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         沈鸿;                   吴家淮       </td>   <td>中山大学</td>   <td>基于批装箱问题的虚拟机调度方法</td>   <td>广东省</td>   <td>CN106648834B</td>   <td>2020-07-28</td>   <td>本发明公开了一种基于批装箱问题的虚拟机调度方法,该方法包括以下步骤：S1：虚拟机调度器定期接收用户提交的新的虚拟机请求,同时收集系统中每台物理机上运行的虚拟机的状态信息,包括即将结束运行的虚拟机信息；S2：针对新的每一组虚拟机请求,虚拟机调度器采用批装箱算法进行调整,得出新的虚拟机与物理机的对应关系表；S3：虚拟机调度器比较算法调度前后对应关系表之间的差异,制定并发送迁移指令给指定物理机,相关物理机根据指令完成虚拟机迁移。与现有技术相比,本发明方法关闭了空闲服务器以降低能耗,同时有效减少了虚拟机迁移次数提高了分配效率。</td>   <td>1.基于批装箱问题模型的虚拟机调度方法,其特征在于,包括下述步骤：S1：虚拟机调度器定期接收用户提交的新的虚拟机请求,同时收集系统中每台物理机上运行的虚拟机的状态信息,包括即将结束运行的虚拟机信息；S2：针对新的虚拟机请求和发生变化的虚拟机信息,虚拟机调度器采用装箱算法进行调整,得出新的虚拟机与物理机的对应关系表；S3：虚拟机调度器比较算法调度前后对应关系表之间的差异,制定并发送迁移指令给指定物理机,相关物理机根据指令完成虚拟机迁移；所采用的装箱算法为在线批装箱算法,该在线批装箱算法处理的物品是一批批到达,而不是一个个到达的,也就是说,在制定调度策略时,我们除了知道已运行在物理机上的虚拟机负载信息外,另外还知道即将进行分配的一组虚拟机请求序列。</td>   <td>G06F9/455</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李勇;              王鲁平;              张志勇;              梁建雄;              丘昌镇;                   王亮       </td>   <td>中山大学</td>   <td>一种红外无人机目标检测方法及系统</td>   <td>广东省</td>   <td>CN111222511B</td>   <td>2020-07-24</td>   <td>本发明公开一种红外无人机目标检测方法,包括：对采集的红外图像进行预处理获得预处理图像,对预处理图像进行目标聚类获得红外图像中疑似目标的外形及位置；采用差分盒维数法对采集的红外图像进行处理,在图像中包含天空和地面背景时获得分形特征图；利用分形特征图提取出天空与地面的分界线,即天地线；去掉处于天地线以下的全部疑似目标,将天地线以上的天空区域内的疑似目标作为无人机待判目标；根据疑似目标的外形及预处理图像的局部灰度与预设参考特征的相似度判断,在待判目标中识别出无人机目标。并在此基础上,提供一种用于红外无人机目标检测系统,用于解决现有技术中虚警率高、漏检、难以做到实时处理等问题,提高检测能力。</td>   <td>1.一种红外无人机目标检测方法,其特征在于,包括：步骤S1,对采集的红外图像进行预处理获得预处理图像,对预处理图像分割、连通域标记获得红外图像中疑似目标的外形及位置；所述红外图像为红外图像序列,通过FPGA对所述红外图像序列中的每一帧采用帧号进行标记,根据所述帧号通过多核DSP对所述红外图像序列进行流水线处理；步骤S2,对采集的所述红外图像采用差分盒维数法进行处理,在所述红外图像中包含天空和地面背景时获得分形特征图；利用分形特征图提取出天空与地面的分界线,即天地线；所述步骤S2包括：步骤S21,将红外图像分成多个<Image id="icf0001" he="27" wi="58" file="198650DEST_PATH_IMAGE001.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>大小的子图像,将每个<Image id="icf0002" he="27" wi="58" file="747705DEST_PATH_IMAGE001.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>的子图像又分成多个<Image id="icf0003" he="20" wi="37" file="119781DEST_PATH_IMAGE002.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>大小的子区域,令尺度系数为：<I>w=s/M</I>,然后计算每个子图像的分形维数D；<I>M、s</I>分别为方形子图像和方形子区域的边长,单位均为像素；步骤S22,对每个子图像的分形维数用阈值进行二值化处理获得分形特征图；步骤S23,在分形特征图中,若当前行灰度值为255的白点个数比下一行白点的个数明显小时,则认为当前行为天地交界线；步骤S3,去掉处于天地线以下的全部疑似目标,将天地线以上的天空区域内的疑似目标作为待判目标；步骤S4,根据疑似目标的外形及预处理图像的局部灰度与预设参考特征的相似度判断,在待判目标中识别出无人机目标。</td>   <td>G06K9/32;G06K9/34;G06K9/40;G06T7/48</td>  </tr>        <tr>   <td>中国专利</td>   <td>         康显桂;              黎恩磊;                   何自强       </td>   <td>中山大学</td>   <td>基于自回归模型系数检测定位语音片段内平滑处理的方法</td>   <td>广东省</td>   <td>CN111445924A</td>   <td>2020-07-24</td>   <td>本发明提出一种基于自回归模型系数检测定位语音片段内平滑处理的方法,包括：S1.构造原始语音集和平滑语音集；S2.提取原始语音集的AR系数作为原始语音特征集；提取平滑语音集的AR系数作为平滑语音特征集；S3.分别随机筛选出原始语音特征集样本和平滑语音特征集样本,训练出SVM支持向量机分类器；S4.选取待测语音,将待测语音进行分帧,对每一帧待测语音信号分别提取AR系数,作为待测语音特征集；S5.利用训练好的SVM支持向量机分类器对待测语音特征集进行分类,判断信号是否经过平滑处理,若经过平滑处理,则定位平滑处理的位置。本发明提出的方法不需要借助频率信息,减少了检测过程的计算量,提高了检测定位的准确率。</td>   <td>1.一种基于自回归模型系数检测定位语音片段内平滑处理的方法,其特征在于,至少包括：S1.构造原始语音集和平滑语音集；S2.提取原始语音集的AR系数作为原始语音特征集；提取平滑语音集的AR系数作为平滑语音特征集；S3.分别从原始语音特征集和平滑语音特征集中随机筛选出原始语音特征集样本和平滑语音特征集样本,训练SVM支持向量机分类器；S4.选取待测语音,将待测语音进行分帧,对每一帧待测语音信号分别提取AR系数,作为待测语音特征集；S5.利用训练好的SVM支持向量机分类器对待测语音特征集进行分类,判断信号是否经过平滑处理,若经过平滑处理,则定位平滑处理的位置。</td>   <td>G10L25/51;G10L25/03;G10L25/27</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李中华;                   何冠男       </td>   <td>中山大学</td>   <td>基于分簇竞争的防碰撞方法、移动阅读器、可读存储介质</td>   <td>广东省</td>   <td>CN111444736A</td>   <td>2020-07-24</td>   <td>本发明公开了一种基于分簇竞争的防碰撞方法、移动阅读器、可读存储介质,所述防撞方法是每一轮执行包括以下阶段：竞争簇首RFID移动阅读器阶段、簇间通信资源竞争阶段、簇内通信资源竞争阶段、RFID移动阅读器换簇阶段。本发明从RFID系统通信资源最优化分配的角度出发,先确定簇首阅读器,簇首阅读器代表本簇竞争簇通信资源,簇内的阅读器竞争再对本簇的通信资源进行再竞争。从而实现对通信资源的分层竞争,提高了竞争通信资源的效率,也提升了通信资源的利用效率,最大化RFID系统的标签询问效率。</td>   <td>1.一种基于分簇竞争的防碰撞方法,其特征在于：所述防撞方法是每一轮执行包括以下阶段：竞争簇首RFID移动阅读器阶段、簇间通信资源竞争阶段、簇内通信资源竞争阶段、RFID移动阅读器换簇阶段；其中所述竞争簇首RFID移动阅读器阶段：RFID移动阅读器在服务器广播簇首竞争信号的同步下通过相互随机发送竞争信号的形式对簇首RFID移动阅读器进行竞争,成为簇首的RFID移动阅读器向通信范围内的阅读器广播簇信息,接收到簇信息的RFID移动阅读器加入到簇中；所述簇间通信资源竞争阶段：簇首RFID移动阅读器代表本簇竞争通信资源,各个簇首RFID移动阅读器根据竞争规则对各类通信资源信息进行竞争,竞争结束后,簇首RFID移动阅读器向本簇的阅读器广播簇间竞争获得的通信资源信息；所述簇内通信资源竞争阶段：簇内的RFID移动阅读器通过竞争规则对在簇间获得的通信资源进行竞争,竞争成功并获得通信资源的阅读器开始询问标签；RFID移动阅读器换簇阶段：在簇内竞争通信资源失败或由于移动性远离本簇簇首通信范围的RFID移动阅读器在本簇和相邻簇首阅读器的辅助下,进行换簇操作；换簇成功之后,根据新簇通信资源的利用情况对通信资源展开再竞争,以实现对通信资源的高效利用。</td>   <td>G06K7/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘芳;              张振源;              蔡振华;                   苏屹宏       </td>   <td>中山大学</td>   <td>一种面向NVM的差异化服务的边缘缓存拍卖方法</td>   <td>广东省</td>   <td>CN111445318A</td>   <td>2020-07-24</td>   <td>本发明公开了一种面向NVM的差异化服务的边缘缓存拍卖方法,包括以下步骤：S1：构建边缘缓存模型,S2：定义用户j感兴趣内容集合为S<Sub>j</Sub>,其中,<Image he="73" wi="491" file="DDA0002400676710000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>定义对内容i感兴趣的用户集合为Ω<Sub>i</Sub>；S3：将用户对内容的需求定义为用户估值,并利用用户估值构建用户估值的联合概率密度分布函数；S4：构建用户收益目标函数；S5：将差异化服务因素、NVM因素引入边缘缓存模型,构建服务提供商的收益的数学目标函数,ER为服务提供商的收益；S6：将服务提供商所在系统分为两个阶段,求解最大化的服务提供商的收益。本发明通过联合多因素构建互联网场景边缘缓存模型,实现了满足用户不同需求和考虑NVM磨损成本的同时,服务提供商的收益最大化。</td>   <td>1.一种面向NVM的差异化服务的边缘缓存拍卖方法,其特征在于,包括以下步骤：S1：构建边缘缓存模型,所述边缘缓存模型包括：m个内容发布者,服务提供商,n个用户,其中m、n为正整数,所述内容发布者用于向服务提供商发布视频,所述用户用于向服务提供商请求视频；S2：定义用户j感兴趣内容集合为S<Sub>j</Sub>,其中,<Image id="icf0001" he="72" wi="489" file="FDA0002400676680000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>定义对内容i感兴趣的用户集合为Ω<Sub>i</Sub>,其中,<Image id="icf0002" he="65" wi="274" file="FDA0002400676680000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>设定<Image id="icf0003" he="143" wi="362" file="FDA0002400676680000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中,|S<Sub>j</Sub>|表示使用户j感兴趣的内容个数,|Ω<Sub>i</Sub>|表示对内容i感兴趣的用户个数；S3：将用户对内容的需求定义为用户估值,并利用用户估值构建用户估值的联合概率密度分布函数；S4：构建用户收益目标函数；S5：将差异化服务因素、NVM因素引入边缘缓存模型,构建服务提供商的收益的数学目标函数,ER为服务提供商的收益,所述NVM即非易失性内存/存储；S6：将服务提供商所在系统分为两个阶段,求解最大化的服务提供商的收益。</td>   <td>G06Q30/08;H04L29/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;                   杨焕       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>基于目标检测的多任务及临近信息融合的深度学习方法</td>   <td>广东省</td>   <td>CN109101932B</td>   <td>2020-07-24</td>   <td>本发明公开了基于目标检测的多任务及临近信息融合的深度学习方法,包括输入图片,利用卷积神经网络提取图像特征,并生成目标候选框；将所述目标候选框经过区域候选网络,提取出目标预测框；将目标预测框进行特征提取和特征池化,再进行边框回归、方向预测、目标检测分类,得到初步检测结果；将初步检测结果与目标候选框融合并进入RO I池化层和通过第二全连接层,得到最终检测结果；其中,目标检测分类是利用一个目标预测框与其临近的其他目标预测框的信息关系而重新定义该目标预测框的置信度分数；算法采用多任务输出模式。本发明在提高目标检测的速度的同时,确保了目标检测的准确性,达到实时目标检测的要求。</td>   <td>1.基于目标检测的多任务及临近信息融合的深度学习方法,其特征在于,包括以下步骤：输入经初始化带有真实框的图片,利用预训练好的卷积神经网络提取图像特征,并生成目标候选框；将所述目标候选框经过区域候选网络,提取出目标预测框；将目标预测框经过卷积层进行特征提取和经过池化层进行特征池化,再经过第一全连接层进行初步边框回归、目标预测框与真实框之间的方向预测、初步目标检测分类,得到包含经筛选后的目标预测框的初步检测结果；将所述初步检测结果和目标候选框融合并进入ROI池化层,并通过第二全连接层来进行最终边框回归和最终目标检测分类,得到包含已分类图片的最终检测结果；其中,所述初步目标检测分类和最终目标检测分类是利用一个目标预测框与其临近的其他目标预测框的信息关系而重新定义该目标预测框的置信度分数。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;              晏斌;              邓成谦;              林培祥;              黄家诚;                   李凯祥       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于防伪溯源系统的异常数据处理方法及系统</td>   <td>广东省</td>   <td>CN107038593B</td>   <td>2020-07-21</td>   <td>本发明提供一种基于防伪溯源系统的异常数据处理方法及系统,所述方法在用户通过防伪溯源平台查询商品真伪的过程中,当判定商品为假冒时,系统将得到的用户信息(性别、年龄)、查询信息(空间位置、时间)以及用户反馈的商品信息(价格、种类、用途)和购买途径信息(线上(网站、店铺)、线下(店铺))等进行数据预处理。接着对数据进行异常检测,然后对线下数据集利用基于距离分类方法进行分析,根据位置信息查找假冒源,或者针对线上数据集基于频率分类方法进行分析,达到阈值即判定为假冒源。本发明利用防伪溯源系统的信息进行有效挖掘,为用户、店铺管理者和政府监管部门提供有效参考。</td>   <td>1.一种基于防伪溯源系统的异常数据处理方法,其特征在于,包括以下步骤：S1：获取用户信息,查询产品真伪情况；获取用户信息具体为：获取用户输入的基本信息、商品信息、查询信息和购买途径,基本信息包括ID、性别、年龄,商品信息包括价格、种类、用途,查询信息包括空间位置、时间,购买途径为线上和线下,线上途径包括网站、店铺,线下途径为店铺；S2：根据获取的信息,利用数据清洗、数据集成、数据变换和数据归约方法对数据进行预处理；具体包括以下步骤：S2.1：检查用户输入的各项属性,若存在空值,则将记录删除；S2.2：对价格、时间输入统一格式：价格提取整数部分,删除小数部分和货币符号,时间保留年、月、日、时、分信息；S2.3：将数据依据预收的规则进行变换,包括对地理位置的经纬度依据一定的变换关系,扩大数据间的差异；S2.4：对短时间内反复出现的相同数据点,标记为异常行为,只记录1次；对用户与其反馈店铺反复成对出现的数据点,标记为异常行为,只记录为1次；S3：针对数据集进行异常检测,去除异常点干扰；具体包括以下步骤：S3.1：对输入数据D<Sub>i</Sub>进行处理,使用相同的半径,将输入数据划分成不同的类,当类中所包含的数量大于所设阈值K时,证明其不是离群点,将其删除；S3.2：使用FCM聚类算法对步骤S3.2获取到的数据进行聚类,数据集记为D＝(D<Sub>1</Sub>,D<Sub>2</Sub>,D<Sub>3</Sub>,..,D<Sub>c</Sub>),其中c为聚类数量,D<Sub>i</Sub>为具体类别中所含数量,r<Sub>i</Sub>为聚类半径,则根据以下公式计算密度Den：          <Image id="icf0001" he="135" wi="220" file="FDA0002474316060000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        根据Den的数值对其进行排序,密度较低的几个类,就最大可能包含离群点；S3.3：使用最近邻算法查找离群点；S3.4：在输入数据D<Sub>i</Sub>中,将步骤S3.3获取的离群点删除,结果即为有效点集；S4：针对线下数据集利用基于距离的方法找到最可疑假冒源；针对线上数据集采用基于频率的分类方法,找出最可疑假冒源；S5：标记不良店铺,并发送标记结果到数据库。</td>   <td>G06Q30/00;G06K9/62;G06F16/25;G06F16/29;G06F16/215;G06F16/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王国利;              黄旭;              郭雪梅;                   李中华       </td>   <td>中山大学</td>   <td>一种显微镜图像的处理方法</td>   <td>广东省</td>   <td>CN111435529A</td>   <td>2020-07-21</td>   <td>本发明公开了一种显微镜图像的处理方法,包括：从显微镜图像中提取具有有效信息的图像块；训练生成对抗网络重构具有有效信息的图像块,包括训练生成对抗网络模型、保存训练所训练好的模型及重构具有有效信息的图像块成为重构后的具有有效信息的图像块；以及拼接重构后的具有有效信息的图像块,成为处理后的显微镜图像。本发明所公开的显微镜图像的处理方法提供了一種医学显微镜图像超分辨问题的整体解决方案,可以加快网络的训练和整个显微镜图像重构的效率、可以将重构的误差最小化,可有效的提升操作者(例如是医生)的工作效率,且具有较强的鲁棒性和稳健性。</td>   <td>1.一种显微镜图像的处理方法,其特征在于,包括：从所述显微镜图像中提取具有有效信息的图像块；训练生成对抗网络重构所述具有有效信息的图像块,包括：训练所述生成对抗网络模型；保存所述训练所述生成对抗网络模型的步骤所训练好的生成对抗网络模型；及所述训练好的生成对抗网络模型以第一重构倍数放大重构所述具有有效信息的图像块,成为重构后的具有有效信息的图像块；以及拼接所述重构后的具有有效信息的图像块,成为处理后的显微镜图像。</td>   <td>G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陆遥;              信闫奇;              朱庆堂;              吕璐璐;                   刘小林       </td>   <td>中山大学</td>   <td>一种基于单个深度相机的手部骨架高精度三维重建方法</td>   <td>广东省</td>   <td>CN111429499A</td>   <td>2020-07-17</td>   <td>本发明涉及计算机视觉和计算机图形学技术领域,更具体地,涉及一种基于单个深度相机的手部骨架高精度三维重建方法。一种基于单个深度相机的手部骨架高精度三维重建方法,利用单个深度相机多角度采集手部深度数据和RGB数据,提取RGB图像中的手部骨架关键点并映射到对应的深度数据点,将多角度采集到的精确关键点匹配,同时利用手部骨架拓扑结构校正,得到高精度的手部骨架三维模型。本发明可以解决基于视觉图像序列三维重建方法中受距离、光线以及手部自身遮挡等因素影响下精度不足的问题,能够重建出真实的高精度的手部骨架三维模型。</td>   <td>1.一种基于单个深度相机的手部骨架高精度三维重建方法,其特征在于,包括以下步骤：S1：标定单个深度相机的深度数据、RGB数据内部参数以及多角度的外部位置参数,利用单个深度相机多角度采集手部的深度数据和RGB数据；S2：对各个角度采集到的手部RGB数据分割出该角度下的手部骨架关键点二维图,将二维手部骨架关键点映射到对应的深度数据中,得到该角度下的手部骨架关键点三维结构；S3：利用步骤S1中的多角度外部位置参数,将步骤S2中的各角度下的手部骨架关键点三维结构配准到同一个三维坐标系下,得到初步手部骨架三维模型；S4：利用手部骨架拓扑结构,对步骤S3中得到的初步手部骨架三维模型进行手指中心线校正、手部整体拓扑结构校正,得到高精度的手部骨架三维模型。</td>   <td>G06T7/55;G06T17/00;G06T5/00;G06T7/80</td>  </tr>        <tr>   <td>中国专利</td>   <td>         邓夏君;              王若梅;                   周凡       </td>   <td>中山大学</td>   <td>基于图神经网络的全景分割方法、系统、设备及存储介质</td>   <td>广东省</td>   <td>CN111428726A</td>   <td>2020-07-17</td>   <td>本发明公开了一种基于图神经网络的全景分割方法,包括：从图片中提取多个目标特征；通过实例分割头部网络以得到图片的前景类别概率、背景类别概率及掩膜结果,通过语义分割头部网络以得到图片的初步语义分割结果；通过前景类别概率对新前景图进行处理以生成实例分类结果,并根据掩膜结果从实例分类结果中提取目标实例分割掩膜；通过背景类别概率及初步语义分割结果对新背景图进行处理以生成目标语义分割结果；采用启发式算法对目标实例分割掩膜及目标语义分割结果进行融合,生成全景分割结果。本发明还公开了一种基于图神经网络的全景分割系统、计算机设备及计算机可读存储介质。采用本发明,可利用物体之间的相互关系优化图片的全景分割效果。</td>   <td>1.一种基于图神经网络的全景分割方法,其特征在于,包括：通过ResNet-50网络及FPN网络对图片进行特征提取,以提取多个目标特征；通过实例分割头部网络并根据所述目标特征以得到图片的前景类别概率、背景类别概率及掩膜结果,通过语义分割头部网络并根据所述目标特征以得到图片的初步语义分割结果；通过前景图神经网络对原始前景图进行处理以生成新前景图,通过所述前景类别概率对所述新前景图进行处理以生成实例分类结果,并根据所述掩膜结果从所述实例分类结果中提取目标实例分割掩膜；通过背景图神经网络对原始背景图进行处理以生成新背景图,通过所述背景类别概率及初步语义分割结果对所述新背景图进行处理以生成目标语义分割结果；采用启发式算法对所述目标实例分割掩膜及目标语义分割结果进行融合,生成全景分割结果。</td>   <td>G06K9/34;G06K9/32;G06T7/194;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              王晓;                   王广聪       </td>   <td>中山大学</td>   <td>一种基于深度学习和条件随机场的多目标跟踪方法</td>   <td>广东省</td>   <td>CN107122735B</td>   <td>2020-07-14</td>   <td>本发明公开了一种基于深度学习和条件随机场的多目标跟踪方法,主要包括下述步骤：用训练集训练深度网络；用网络提取目标的表观特征；在时间滑窗内将数据集给出的检测结果连接成可能的跟踪片段；将每个目标的跟踪片段集合作为点进行条件随机场建模；计算随机场中点与边的势能；利用联合树求解条件随机场得到滑窗内的跟踪结果；移动窗口,重复跟踪过程。主要贡献包括：(1)提出一种基于深度网络提取特征的多目标跟踪方法；(2)提出一种新的跟踪片段相似性度量方法；(3)实现一种基于跟踪片段集的半在线跟踪算法。通过利用深度网络提取表观特征,将检测结果连接为可能的跟踪片段,并利用条件随机场对片段进行关联,完成了多目标跟踪。</td>   <td>1.一种基于深度学习和条件随机场的多目标跟踪方法,其特征在于,包括步骤：S1、训练网络：利用给定的训练集,训练一个深度网络；S2、应用阶段：输入待处理视频的目标检测结果,对检测结果进行筛选,得到筛选后的检测结果；S3、提取特征：将筛选后的检测结果输入深度网络中,得到检测结果的表观特征；S4、产生跟踪片段：根据滑动窗口中检测结果和检测结果之间、目标与检测结果之间的表观特征相似度和位置关系,将可能为同一目标的检测结果连接成跟踪片段；S5、建模：将跟踪片段集合进行建模得到条件随机场模型,其中条件随机场中的点代表某一跟踪目标的可能的跟踪片段集合,集合的大小为跟踪片段的个数；点与点之间存在的边表示不同目标的跟踪片段之间共存的可能性；S6、计算：计算条件随机场中点的势能与边的势能；S7、求解：利用联合树算法求解,得到每个目标的跟踪片段索引,作为该滑动窗口内的跟踪结果；S8、滑动窗口,设定滑动步长,计算滑动窗口内的跟踪结果,依次循环,得到整个视频的跟踪结果；在步骤S4中,将表观相似度与运动相似度分别大于给定阈值的检测结果连接构成跟踪片段,其中表观相似度使用欧式距离度量,其数学公式为<Image id="icf0001" he="124" wi="700" file="FDA0002455571230000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中f(d<Sub>i</Sub>)<Sub>k</Sub>,f(d<Sub>j</Sub>)<Sub>k</Sub>分别为检测结果d<Sub>i</Sub>,d<Sub>j</Sub>表观特征的第k维；运动相似度用检测结果与跟踪片段在当前帧中检测框体的位置的交并比的平方来衡量,两个框体交并比的数学表达式为<Image id="icf0002" he="142" wi="414" file="FDA0002455571230000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中p,d分别为预测框体和检测框体,inner(p,d)为两框体的重叠面积,union(p,d)为两框体的联合面积；在步骤S6中,计算条件随机场中点的势能,其数学表达式为<Image id="icf0003" he="55" wi="700" file="FDA0002455571230000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中T为滑动窗口的长度,<Image id="icf0004" he="77" wi="90" file="FDA0002455571230000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为滑动窗口前已知的第m个跟踪目标,<Image id="icf0005" he="78" wi="119" file="FDA0002455571230000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为该跟踪目标在滑动窗口内的第x<Sub>m</Sub>条跟踪片段；其中<Image id="icf0006" he="55" wi="700" file="FDA0002455571230000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示跟踪目标与跟踪片段内检测结果的势能,第一项为表观相似度的势能,第二项为位置关系的势能,η为参数,<Image id="icf0007" he="80" wi="209" file="FDA0002455571230000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为该目标在滑动窗口内最后一次出现的时刻,o<Sup>2</Sup>为预测框体与检测框体交并比率的平方；f<Sub>p</Sub>(d<Sub>i</Sub>,d<Sub>j</Sub>)＝-D<Sub>A</Sub>(d<Sub>i</Sub>,d<Sub>j</Sub>)为某跟踪片段内两个检测框体的势能,<Image id="icf0008" he="57" wi="700" file="FDA0002455571230000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为高阶势能,ξ为计算预测框体与检测框体中心点之间的欧式距离,K为计算两检测结果的颜色直方图的欧式距离,γ,ε,θ为参数；在步骤S6中,不同目标跟踪片段间的势能主要惩罚不同目标的跟踪片段在某一时刻为连接了同一检测框体的情况,边的势能数学表达式为：          <Image id="icf0009" he="54" wi="700" file="FDA0002455571230000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中<Image id="icf0010" he="76" wi="230" file="FDA0002455571230000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为跟踪片段<Image id="icf0011" he="77" wi="116" file="FDA0002455571230000025.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>在时刻τ所连接的检测框体,<Image id="icf0012" he="74" wi="508" file="FDA0002455571230000026.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为指示函数,当两个跟踪片段在时刻τ所连接的是同一个检测框体时为1,否则为0,α,β为参数,其中β要超过一定值,且该值要足够大,以此避免框体的重复分配。</td>   <td>G06K9/00;G06K9/62;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              陈泽宇;                   卓嘉璇       </td>   <td>中山大学</td>   <td>基于循环对抗生成网络的行人图像遮挡检测方法</td>   <td>广东省</td>   <td>CN108573222B</td>   <td>2020-07-14</td>   <td>本发明公开了一种基于循环对抗生成网络的行人图像遮挡检测方法,步骤：(1)在无遮挡的行人图像中随机加入遮挡图像块,生成带遮挡的行人图像及对应的带遮挡标注的行人图像；(2)将上述带遮挡的行人图像及对应的带遮挡标注的行人图像作为两个数据域,训练循环对抗生成网络模型,构建遮挡图像与遮挡标注之间的映射；(3)对于当前输入的待标注图像,利用训练后的循环对抗生成网络模型去产生该图像对应的遮挡标注,同时对待标注图像进行分割,得到若干个超像素块,根据每个超像素块内遮挡标注的信息,采用区域生长方法得到最终的遮挡区域。本发明方法可以在缺少遮挡标注的情况下,针对不同类型的遮挡进行建模,得到像素级别的遮挡区域检测结果。</td>   <td>1.基于循环对抗生成网络的行人图像遮挡检测方法,其特征在于,包括步骤：(1)在无遮挡的行人图像中随机加入遮挡图像块,生成带遮挡的行人图像及对应的带遮挡标注的行人图像；所述的遮挡标注是指人为设定的遮挡图像块或者原图自带的遮挡物体在行人图像中的区域；(2)将上述带遮挡的行人图像及对应的带遮挡标注的行人图像作为两个数据域,训练循环对抗生成网络模型,构建遮挡图像与遮挡标注之间的映射；(3)对于当前输入的待标注图像,利用训练后的循环对抗生成网络模型去产生该图像对应的遮挡标注,同时对待标注图像进行分割,得到若干个超像素块,根据每个超像素块内遮挡标注的信息,采用区域生长方法得到最终的遮挡区域。</td>   <td>G06K9/00;G06K9/34</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄林冲;              黄帅;              梁禹;                   陶臣园       </td>   <td>中山大学</td>   <td>一种图像增强方法、装置及设备</td>   <td>广东省</td>   <td>CN107358586B</td>   <td>2020-07-14</td>   <td>本发明公开了一种图像增强方法,包括：对原图像进行二值化处理,将原图像分割背景类图像和目标类图像；根据分割后的图像的种类,对分割后的图像采用相应尺度的retinex算法计算处理；对计算处理后的图像中包括的亮度图像进行双边滤波处理,对计算处理后的图像中包括的反射图像进行小波去噪处理；对双边滤波处理后的图像和小波去噪处理后的图像合成并进行伽马校正,得到增强后的图像。通过对两个尺度进行变换,有利于减少卷积计算量,对亮度图像进行双边滤波,以及对反射图像进行小波去噪,有利于去除图像的噪声且较少的损失图像细节,通过对合成的图像进行伽马校正,有利于增强图像全局。</td>   <td>1.一种图像增强方法,其特征在于,所述图像增强方法包括：对原图像进行二值化处理,将原图像分割背景类图像和目标类图像；根据分割后的图像的种类,对分割后的图像采用相应尺度的retinex算法计算处理；对计算处理后的图像中包括的亮度图像进行双边滤波处理,对计算处理后的图像中包括的反射图像进行小波去噪处理；对双边滤波处理后的图像和小波去噪处理后的图像合成并进行伽马校正,得到增强后的图像。</td>   <td>G06T5/00;G06T5/50;G06T7/11</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王鲁平;              李勇;              张志勇;              梁建雄;              丘昌镇;                   王亮       </td>   <td>中山大学</td>   <td>一种基于嵌入式的红外复杂场景目标实时跟踪方法及系统</td>   <td>广东省</td>   <td>CN111415370A</td>   <td>2020-07-14</td>   <td>本发明公开一种基于嵌入式的红外复杂场景目标实时跟踪方法,包括：当收到外部通信单元发来的目标粗略位置后,从自此时算起的初始帧图像中识别出目标并找到目标的精确坐标；判断当前帧图像背景复杂程度；若图像背景复杂,则在后续帧到来后,采用核相关滤波跟踪算法对目标进行跟踪；若图像背景简单,则在后续帧到来后,采用基于灰度对比度的形心跟踪算法对目标进行跟踪。在此基础上提供一种基于嵌入式的红外复杂场景目标实时跟踪系统,用于解决现有技术中难于实时处理、目标跟踪易丢失、需要预先存储模板等问题,适用于导引头、红外告警、光电吊舱等红外搜索跟踪系统。</td>   <td>1.一种基于嵌入式的红外复杂场景目标实时跟踪方法,其特征在于,包括以下步骤：步骤S1,收到目标在初始帧图像中的粗略坐标；步骤S2,根据接收的跟踪命令及待跟踪目标的粗略坐标识别初始帧图像中目标,并得到目标的精确位置；包括：步骤S21,在采集的当前帧视频图像中读取以待跟踪目标粗略坐标位置为中心的预定尺寸的局部区域图像,进行高通滤波处理,阈值分割后获得目标增强二值图像；步骤S22,对目标增强二值图像进行连通域分析,获得疑似目标的外形及位置；步骤S23,根据疑似目标的外形的特征与阈值的比较结果,获得候选目标及数量；步骤S24,将面积最大的候选目标作为识别目标并输出识别目标精确位置坐标；步骤S4,在候选目标数量大于目标数量阈值时,认为图像背景复杂并根据收到的视频图像以及上次获得的跟踪目标位置坐标通过核相关滤波算法进行跟踪,获得本次跟踪目标位置坐标；步骤S5,在候选目标数量小于目标数量阈值时,认为图像背景简单并基于灰度对比度的形心跟踪获得本次跟踪目标位置坐标；步骤S6,重复步骤S1、S2、S4、S5,完成对最后一帧视频图像的目标跟踪。</td>   <td>G06T7/215;G06T7/136;G06T7/12</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡航通;              王伟;              陈立达;              阮思敏;              匡铭;              谢晓燕;                   吕明德       </td>   <td>中山大学附属第一医院</td>   <td>一种基于复合神经网络的超声造影视频数据分析方法</td>   <td>广东省</td>   <td>CN111402207A</td>   <td>2020-07-10</td>   <td>本发明公开了一种基于复合神经网络的超声造影(CEUS)视频数据分析方法,通过获取待分析肝脏病变的超声造影多期视频数据,从多期视频数据中提取出多个超声造影时序单元,并标注多个超声造影时序单元,再通过复合神经网络提取各时序单元的综合信息,并根据各时序单元的综合信息进行后续网络训练,得到针对肝脏病变判定的参数权重,根据网络参数和参数权重构建肝脏病变分析模型,最后将待分析肝脏病变的超声造影多期视频数据输入至肝脏病变分析模型,输出待分析肝脏病变的分析结果。采用本发明提供的实施例,不仅能够充分利用CEUS时序信息,还降低了对视频分析的计算机算力需求,从而能够快速地对待分析肝脏病变进行分析。</td>   <td>1.一种基于复合神经网络的超声造影视频数据分析方法,其特征在于,包括以下步骤：获取待分析肝脏病变的超声造影多期视频数据；从所述多期视频数据中提取出多个超声造影时序单元,并标注所述多个超声造影时序单元；通过复合神经网络提取各时序单元的综合信息,并根据所述各时序单元的综合信息进行后续网络训练,得到针对肝脏病变判定的参数权重,根据网络参数和所述参数权重构建肝脏病变分析模型；将所述待分析肝脏病变的超声造影多期视频数据输入至所述肝脏病变分析模型,输出所述待分析肝脏病变的分析结果。</td>   <td>G06T7/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王静雯;              刘江;                   袁进       </td>   <td>广州视源电子科技股份有限公司;中国科学院宁波工业技术研究院慈溪生物医学工程研究所;中山大学中山眼科中心</td>   <td>一种图像分级方法、装置、设备和存储介质</td>   <td>广东省</td>   <td>CN111402217A</td>   <td>2020-07-10</td>   <td>本发明公开了一种图像分级方法、装置、设备和存储介质。该方法包括：确定原始AS-OCT图像对应的原始三维图像；将原始三维图像对应的第一预设个数尺度的中间三维图像依次输入至对应预设3D卷积神经网络,得到对应的一维向量；根据第一预设个数的一维向量计算得到对应的输出结果；根据输出结果和预先配置的浑浊类别确定原始AS-OCT图像的浑浊程度。本发明通过从不同角度拍摄原始AS-OCT图像,可以提取并学习到图像中的更多特征,有效提升了网络分类的精度；同时,通过构建多尺度的3D卷积神经网络,以将原始三维图像对应的多个尺度的中间三维图像输入至对应预设3D卷积神经网络中,从而将全局特征和局部特征相融合有利于网络挖掘到更具有辨别性的特征信息。</td>   <td>1.一种图像分级方法,其特征在于,包括：确定原始眼前段光学相干断层扫描AS-OCT图像对应的原始三维图像；将所述原始三维图像对应的第一预设个数尺度的中间三维图像依次输入至对应预设3D卷积神经网络,得到对应的一维向量；根据所述第一预设个数的一维向量计算得到对应的输出结果；根据所述输出结果和预先配置的浑浊类别确定所述原始AS-OCT图像的浑浊程度。</td>   <td>G06T7/00;G06T17/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李中华;                   肖玉梅       </td>   <td>中山大学</td>   <td>考虑邻居探测的RFID阅读器防碰撞方法</td>   <td>广东省</td>   <td>CN111401091A</td>   <td>2020-07-10</td>   <td>本发明公开了一种考虑邻居探测的RFID阅读器防碰撞方法,其包括每轮依次执行的邻居探测、资源竞争、碰撞协调三个阶段。本发明从提高阅读器利用率的角度,阅读器根据邻居数量来选择竞争时序序号,使相对稀疏的邻居阅读器优先参与竞争,减少资源竞争阶段阅读器之间的碰撞。同时设计碰撞后的协调机制,使竞争阶段发生碰撞的阅读器通过交换和比较彼此的邻居数量,来决定胜出的阅读器。邻居数量较少的阅读器竞争成功,使更多的阅读器有机会被利用。本发明方法通过在邻居阅读器之间共享邻居阅读器数量信息,使更多的阅读器得到利用,从而提高RFID系统的标签识别效率。</td>   <td>1.一种考虑邻居探测的RFID阅读器防碰撞方法,其特征在于：所述的方法包括每轮依次执行的邻居探测、资源竞争、碰撞协调三个阶段；其中邻居探测阶段：每一个RFID移动阅读器发送信标信号来探测邻居阅读器,并对邻居阅读器数量进行估计；资源竞争阶段：所述RFID阅读器根据邻居阅读器的数量来选择竞争时序序号,并在自身的竞争时序序号与服务器发送的同步竞争命令的时序序号相等时,竞争通信资源；竞争成功的RFID阅读器开始识别标签,竞争失败且在资源竞争阶段所有邻居阅读器都竞争失败的RFID阅读器退避至碰撞协调阶段；碰撞协调阶段：进入碰撞协调阶段的RFID阅读器向邻居阅读器发送包含邻居阅读器数量的协调信号,等待一段时间后,若所述RFID阅读器未接收到其邻居阅读器发送的协调信号,则获取通信资源并进入识别状态；若所述RFID阅读器接收到其邻居阅读器发送的协调信号,则根据约定的协调规则决定胜出的阅读器,胜出的阅读器进入识别状态,被淘汰的阅读器进入休眠状态。</td>   <td>G06K7/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖义明;              郭正辉;              彭圣萌;              吴宛桦;              范辉阳;              雷震;              李嘉路;              华芮;              张游龙;                   李骁       </td>   <td>中山大学孙逸仙纪念医院;深圳市华嘉生物智能科技有限公司</td>   <td>前列腺癌病理图像中上皮细胞核的分割方法、装置和终端</td>   <td>广东省</td>   <td>CN111402267A</td>   <td>2020-07-10</td>   <td>本发明实施例公开了一种前列腺癌病理图像中上皮细胞核的分割方法、装置和终端,该方法包括：对获取的病理染色图像进行色彩空间转换,并基于转换后的彩色图像的一单通道图像进行细胞核分割；对细胞核分割彩色图像的每一单通道图像中的每一细胞核进行的初始尺寸图像和缩放图像进行区域分割以得到单通道区域图像,并对各单通道区域图像进行特征提取；将获取的该细胞核的单通道图像特征和多通道图像特征输入细胞核分类模型中进行细胞核分类,并根据分类结果确定病理染色图像中的上皮细胞核。本发明的技术方案可以很好地解决现有技术中很难对前列腺中的上皮细胞核进行准确分割的难题,从而提高对前列腺癌的病理诊断及严重程度等判断的准确性等。</td>   <td>1.一种前列腺癌病理图像中上皮细胞核的分割方法,其特征在于,包括：对获取的病理染色图像进行色彩空间转换,并基于所述转换得到的彩色图像的一单通道图像进行细胞核分割之后获取细胞核分割彩色图像；对所述细胞核分割彩色图像的每一单通道图像中的每一细胞核的初始尺寸图像和缩放至预设固定尺寸后的缩放图像分别进行区域分割以得到相应的单通道区域图像,并对所述单通道区域图像进行特征提取以获取对应细胞核的单通道图像特征,以及基于所述单通道图像特征获取对应的多通道图像特征；将所述对应细胞核的所述单通道图像特征和所述多通道图像特征输入细胞核分类模型中进行细胞核分类,并根据所述分类的结果确定所述病理染色图像中的上皮细胞核。</td>   <td>G06T7/11;G06T7/136;G06T7/45;G06T7/62;G06T7/90;G06K9/62;G06K9/46</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈武辉;              林晖;                   郑子彬       </td>   <td>中山大学</td>   <td>一种基于区块链的车联网数据交易方法</td>   <td>广东省</td>   <td>CN111402043A</td>   <td>2020-07-10</td>   <td>本发明公开一种基于区块链的车联网数据交易方法,包括：建立基于联盟区块链的数据交易网络拓扑结构,其包括：数台具有本地存储车辆数据且与授权的边缘服务器通信连接的车辆；数台边缘服务器,包括边缘层和区块链层,边缘层作为交易代理,采用双拍卖机制来执行车辆间数据交易的投标过程；作为区块链层,组成区块链网络,运行智能合约,为边缘层提供数据代理存储、投标过程的计算支持和区块链智能合同的共识；建立以买方效用最大化、卖方成本最小化、交易代理的社会福利最大化为市场均衡的第一目标函数,获取第一目标函数最优解。本发明为车联网中的数据交易提供一种安全、真实的方式。</td>   <td>1.一种基于区块链的车联网数据交易方法,其特征在于,包括：S10建立基于联盟区块链的数据交易网络拓扑结构,其包括：数台具有本地存储车辆数据且与授权的边缘服务器通信连接的车辆,在P2P数据交易中作为出售车辆数据的卖方、请求其他车辆数据的买方或既不出售又不购买数据的空闲车辆；数台边缘服务器,包括边缘层和区块链层,边缘层作为交易代理,采用双拍卖机制来执行车辆间数据交易的投标过程；作为区块链层,组成区块链网络,运行智能合约,为边缘层提供数据代理存储、投标过程的计算支持和区块链智能合同的共识；S20建立以买方效用最大化、卖方成本最小化、交易代理的社会福利最大化为市场均衡的第一目标函数,获取第一目标函数最优解。</td>   <td>G06Q40/04;G06Q30/08;G06Q30/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         聂琳;              王可泽;              林木得;              成慧;                   王青       </td>   <td>中山大学</td>   <td>一种单目彩色视频的三维人体关节点定位方法</td>   <td>广东省</td>   <td>CN107392097B</td>   <td>2020-07-07</td>   <td>本发明提供了一种单目彩色视频的三维人体关节点定位方法,包括以下步骤：S1、构建可配置的深度模型,并在该深度模型中引入时序信息；S2、采集训练样本,并利用训练样本学习出深度模型的参数；S3、利用S2中学习得到的参数对深度模型进行初始化,将需要进行三维人体关节点定位的单目彩色视频数据转化为连续多帧二维图像,输入深度模型以进行分析；针对每帧二维图像,输出其中人物的三维人体关节点坐标。本发明利用深度学习,构建深层次的卷积神经网络,来从大量的训练样本中自动学习出有效的时空特征,而不再依赖人工设计的先验条件和人体关节结构约束；通过学习出的有效特征,直接回归出人体的关节点位置。</td>   <td>1.一种单目彩色视频的三维人体关节点定位方法,其特征在于,包括以下步骤：S1、构建可配置的深度模型,并在该深度模型中引入时序信息；其中,所述深度模型包括互相串联的卷积神经网络和长短时记忆网络；所述卷积神经网络用于对视频数据进行逐帧处理,提取二维图像中人物的二维人体关节点特征,并将二维图像中人物的二维人体关节点特征转化到三维人体关节点坐标相关的特征空间；所述长短时记忆网络用于结合当前帧及其之前的连续多帧二维图像的特征信息,预测出当前帧二维图像的三维人体关节点坐标；S2、采集训练样本,并利用训练样本学习出深度模型的参数；所述训练样本包括：被转换成连续多帧二维图像的视频数据、每帧二维图像对应的真实的二维人体关节点坐标和三维人体关节点坐标；其中,视频数据和二维人体关节点坐标用于供深度模型学习出构建卷积神经网络的参数,视频数据和三维人体关节点坐标用于供深度模型学习出构建长短时记忆网络的参数；S3、利用S2中学习得到的参数对深度模型进行初始化,将需要进行三维人体关节点定位的单目彩色视频数据转化为连续多帧二维图像,输入深度模型以进行分析；针对每帧二维图像,输出其中人物的三维人体关节点坐标。</td>   <td>G06K9/00;G06K9/46;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         温武少;                   黎睿德       </td>   <td>中山大学</td>   <td>一种分布式云系统-云簇架构下的服务容器资源的分配方法与系统</td>   <td>广东省</td>   <td>CN111381936A</td>   <td>2020-07-07</td>   <td>本发明涉及云计算技术领域,具体为分布式云系统-云簇架构下的服务容器的资源分配方法及系统。分配方法包括步骤：本地微云系统根据终端用户提交的任务请求,估算完成任务需要构建的容器数量及相关的资源需求量,基于其本身运行的任务管理算法,决定终端用户的业务请求能否由本地微云系统服务。若本地微云系统有足够资源直接执行任务,则依据计算服务成本最低的策略创建虚拟机,并在新创建的虚拟机上创建容器提供服务；若无法服务,则基于双染色体遗传算法计算生成服务成本最低的跨微云容器资源分配方案,将部分或者全部任务转载到其它相邻微云系统上执行。本发明通过物理资源的跨微云调度,解决单个微云在业务高峰期资源不足的问题。</td>   <td>1.一种分布式云系统-云簇架构下的服务容器资源的分配方法,其特征在于,包括以下步骤：S1、本地微云系统收集其自身的资源情况、资源实际使用情况；云簇管理中心收集所有微云资源情况、实际资源使用情况；本地微云系统从云簇管理中心获取其相邻微云系统集的信息；S2、本地微云系统根据终端用户提交的任务请求,估算完成任务需要构建的容器数量及相关的资源需求量；S3、本地微云系统基于其本身运行的任务管理算法,决定一个终端用户的业务请求能否由本地微云系统服务；S4、若本地微云系统有足够资源直接执行任务,则依据计算服务成本最低的策略创建虚拟机,并在新创建的虚拟机上创建容器为用户提供服务；S5、若本地微云系统无法服务,则基于双染色体遗传算法计算生成服务成本最低的跨微云容器资源分配方案,将部分或者全部任务转载到其它相邻微云系统上执行；本地微云系统根据跨微云的容器资源分配方案,遵照跨微云服务请求流程请求相关邻居微云提供服务；相关邻居微云收到请求后,则确认请求并分配资源,创建虚拟机,并在新创建的虚拟机上创建容器为用户提供服务。</td>   <td>G06F9/455;G06F9/50;G06N3/12</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李阳辉;              康显桂;              胡建芳;                   林小拉       </td>   <td>中山大学</td>   <td>基于移动深度学习引擎的移动端人像智能背景替换方法</td>   <td>广东省</td>   <td>CN111369430A</td>   <td>2020-07-03</td>   <td>本发明提出一种基于移动深度学习引擎的移动端人像智能背景替换方法,至少包括以下步骤：S1.选取待训练的卷积神经网络模型；S2.在服务端训练卷积神经网络模型；S3.基于移动深度学习引擎,结合自适应多级模型选择策略,将卷积神经网络模型部署在移动端；S4.利用选择得出的最优卷积神经网络模型进行人像智能背景替换。本发明在移动设备上就能实现背景替换的功能,解决了因网络因素导致人像背景替换的处理效率和成功率低的问题；另外,在进行卷积神经网络模型移动端的部署时,结合了自适应多级模型选择策略,达到根据用户设备差异有效选择最优模型的目的,提升用户使用体验。</td>   <td>1.一种基于移动深度学习引擎的移动端人像智能背景替换方法,其特征在于,至少包括：S1.选取待训练的卷积神经网络模型；S2.在服务端训练卷积神经网络模型；S3.基于移动深度学习引擎,结合自适应多级模型选择策略,将卷积神经网络模型部署在移动端；S4.利用选择得出的最优卷积神经网络模型进行人像智能背景替换。</td>   <td>G06T3/00;G06T1/40;G06K9/34;G06K9/62;G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑子彬;              张桔;                   吴嘉婧       </td>   <td>中山大学</td>   <td>一种针对跨版本软件缺陷预测的数据噪声处理方法</td>   <td>广东省</td>   <td>CN111367808A</td>   <td>2020-07-03</td>   <td>本发明公开一种针对跨版本软件缺陷预测的数据噪声处理方法,包括通过余弦相似度计算数据集中样本件的相似性；基于样本件的相似性归类数据集中簇集合C；从簇集合C中对每簇判断其内部样本标签是否一致；若不一致,则提取簇集合C的样本容量；根据簇集合C的样本容量情况分别确定簇的本簇标签；采用本簇标签对样本的噪声重标记。本发明有效地解决软件缺陷预测中的数据降噪问题。</td>   <td>1.一种针对跨版本软件缺陷预测的数据噪声处理方法,其特征在于,包括如下步骤：S10通过余弦相似度计算数据集中样本件的相似性；S20基于样本件的相似性归类数据集中簇集合C；S30从簇集合C中对每簇判断其内部样本标签是否一致；S40若不一致,则提取簇集合C的样本容量；S50根据簇集合C的样本容量情况分别确定簇的本簇标签；S60采用本簇标签对样本的噪声重标记。</td>   <td>G06F11/36;G06K9/62;G06N20/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>              李立       </td>   <td>中山大学肿瘤防治中心(中山大学附属肿瘤医院、中山大学肿瘤研究所)</td>   <td>一种乳腺肿块图像加强识别方法及装置</td>   <td>广东省</td>   <td>CN111368877A</td>   <td>2020-07-03</td>   <td>本发明属于医疗技术领域,提供了一种乳腺肿块图像加强识别方法及装置,其中,方法包括：S01：获取待识别的乳腺图像,并将乳腺图像进行DCT变换；S02：将变换后的乳腺图像发送给数据处理模块,数据处理模块对乳腺图像进行DCT逆变换,然后,通过HOG特征提取算法进行特征提取；S03：计算待识别的乳腺图像特征与乳腺肿块样本图像的特征之间的相似度；S04：比较并判断是否为乳腺肿块；装置包括：图像获取模块、数据处理模块以及判断模块。本发明的一种乳腺肿块图像加强识别方法及装置,减少乳腺图像向数据处理模块传输过程中噪声或数据丢包的影响,从而提高还原乳腺图像的概率,提高乳腺图像肿块识别的准确性。</td>   <td>1.一种乳腺肿块图像加强识别方法,其特征在于,包括如下步骤：S01：获取待识别的乳腺图像,并将乳腺图像进行DCT变换；S02：将变换后的乳腺图像发送给数据处理模块,数据处理模块对乳腺图像进行DCT逆变换,得到待识别的乳腺图像,通过HOG特征提取算法对乳腺图像进行特征提取；S03：计算待识别的乳腺图像特征与乳腺肿块样本图像的特征之间的相似度；S04：将计算得到的相似度与设定的阈值比较,判断是否为乳腺肿块。</td>   <td>G06K9/62;G06K9/36;G06K9/40;G06K9/46;G06T7/11</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;              蔡佳辉;              王金鹏;              陈嘉敏;                   林佳玲       </td>   <td>中山大学;广州智慧城市发展研究院</td>   <td>一种基于三维交替更新网络的视频行为识别方法</td>   <td>广东省</td>   <td>CN111353394A</td>   <td>2020-06-30</td>   <td>本发明公开了一种基于三维交替更新网络的视频行为识别方法,涉及计算机视觉领域。该视频行为识别方法包括步骤：S1、将视频分为连续的帧,对数据集进行预处理；S2、对参与训练的视频片段执行数据增强操作；S3、将执行数据增强操作后的训练数据放入3D CliqueNet架构中进行训练,获得网络的预训练模型；S4、输入测试数据得到测试数据集的行为分类结果,对经过训练的网络进行测试。本发明的方法使用3D CliquNet来提取时空信息,该网络能最大化提升深度网络中的信息流的流动,可以减少训练困难以及更有效的利用参数。通过在Kinetics数据集上进行预训练,该方法具有较高的行为识别表现以及对于复杂环境具有更好的鲁棒性。</td>   <td>1.一种基于三维交替更新网络的视频行为识别方法,其特征在于,包括以下步骤：S1、将视频分为连续的帧,对数据集进行预处理；S2、对参与训练的视频片段执行数据增强操作；S3、将执行数据增强操作后的训练数据放入3D CliqueNet架构中进行训练,获得网络的预训练模型；S4、输入测试数据得到测试数据集的行为分类结果,对经过训练的网络进行测试。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         程晓;                   陈卓奇       </td>   <td>中山大学</td>   <td>一种极地可见光遥感自适应制图方法</td>   <td>广东省</td>   <td>CN111354054A</td>   <td>2020-06-30</td>   <td>本发明公开一种极地可见光遥感自适应制图方法,包括如下步骤：对卫星遥感影像进行太阳高度角订正；对太阳高度角订正后的卫星遥感影像进行非朗勃体订正,订正冰雪表面的非朗勃体效应；对经过太阳高度角订正、非朗勃体订正后的卫星遥感影像进行非线性彩色拉伸；对非线性彩色拉伸后的卫星遥感影像进行拼接,完成卫星遥感影像自适应制图。本发明提出了一套标准化、流程化的极地遥感影像自适应制图方法,通过对遥感图像进行太阳高度角订正、非朗勃体订正、非线性影像彩色拉伸、极地可见光遥感影像拼接,所获得的极地遥感影像亮度明显提高、无色差,且保留了原始影像中所有的细节信息,实现了对南北极环境的有效监测。</td>   <td>1.一种极地可见光遥感自适应制图方法,其特征在于,包括如下步骤：对卫星遥感影像进行太阳高度角订正；对太阳高度角订正后的卫星遥感影像进行非朗勃体订正,订正冰雪表面的非朗勃体效应；对经过太阳高度角订正、非朗勃体订正后的卫星遥感影像进行非线性彩色拉伸,包括：确定自适应非线性影像彩色拉伸函数,并基于自适应非线性拉伸函数对极地可见光卫星遥感影像进行非线性拉伸处理；对非线性彩色拉伸后的卫星遥感影像进行拼接,完成卫星遥感影像自适应制图。</td>   <td>G06T11/00;G06T3/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         衣杨;              胡攀;              邓小康;              张念旭;              谢韬;                   郑镇贤       </td>   <td>中山大学</td>   <td>一种基于显著轨迹空间信息的视频人体行为识别方法</td>   <td>广东省</td>   <td>CN106709419B</td>   <td>2020-06-30</td>   <td>本发明提供一种基于显著轨迹空间信息的视频人体行为识别方法,该方法重新定义了视频中轨迹的显著性,有效地剔除视频中背景和人体非运动部位的轨迹,留下了前景中运动显著性高的轨迹,这些轨迹误差更小,表达能力也更强；另外该方法将不同人体部位的运动部件以及交互物体区分开来,并通过多核学习来利用他们之间的空间和语义关系,提高了算法的识别效果。</td>   <td>1.一种基于显著轨迹空间信息的视频人体行为识别方法,其特性在于,包括以下步骤：S1：提取视频帧,构建图像金字塔,然后对视频进行超像素分割,在图像金字塔上计算光流,然后利用帧的颜色,空间分布,以及光流的对比性来计算动态和静态显著性,将它们融合为总的显著性；S2：将轨迹显著性定义为轨迹每点在组合显著性图像中显著性的均值；然后计算自适应阈值,当轨迹显著性小于阈值时,则认为是背景轨迹或者非运动区域的轨迹而予以删除,从而有效提取显著轨迹；S3：首先对视频的所有显著轨迹进行随机采样,然后对采样得到轨迹利用其空间信息进行AP聚类,得到不定数量的聚类中心,接着用k-means将聚类中心调整为固定的数目C,最后将视频所有的轨迹分类到距离最近的聚类中心去,从而得到了视频的轨迹分类；S4：对一个视频C个类的轨迹进行编码,得到了C个向量,向量就是视频的表示；所述步骤S1的过程如下：S11：对视频帧进行金字塔构建和超像素分割,对于图像金字塔的某一层而言,得到K个超像素；S12：计算超像素基于颜色对比的显著性：          <Image id="icf0001" he="103" wi="428" file="FDA0002462562470000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0002" he="135" wi="584" file="FDA0002462562470000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,K是超像素的数量,c<Sub>i</Sub>和c<Sub>j</Sub>表示两个超像素的颜色值,p<Sub>i</Sub>和p<Sub>j</Sub>是超像素中心位置的坐标,w<Sub>ij</Sub><Sup>(p)</Sup>是对颜色对比值进行校正的系数,σ<Sub>p</Sub>用来控制颜色对比显著性的范围,设置为0.25；S13：计算超像素基于空间分布对比的显著性：          <Image id="icf0003" he="105" wi="432" file="FDA0002462562470000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0004" he="121" wi="538" file="FDA0002462562470000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0005" he="94" wi="289" file="FDA0002462562470000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,Z<Sub>i</Sub>表示金字塔中某层的相邻像素的总量,其中w<Sub>ij</Sub><Sup>(c)</Sup>是对空间位置对比值的校正系数,σ<Sub>c</Sub>用来控制空间对比显著性的范围,取20,<Image id="icf0006" he="53" wi="55" file="FDA0002462562470000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是颜色c<Sub>i</Sub>的平均权重位置；S14：基于颜色对比和基于空间分布对比的显著性融合得到超像素的静态显著性：<Image id="icf0007" he="82" wi="374" file="FDA0002462562470000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>参数S<Sub>i</Sub>表示表示第i个点的静态显著性,其中<Image id="icf0008" he="73" wi="56" file="FDA0002462562470000018.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0009" he="77" wi="62" file="FDA0002462562470000019.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是基于颜色对比显著性U<Sub>i</Sub>和基于空间分布对比显著性D<Sub>i</Sub>归一化到了[0,1]后的值,k表示控制参数,用来控制颜色对比和空间的对比的相对强度,取值为1；S15：静态显著性能够有效地剔除视频中帧的背景区域,利用插值得到静态的显著性：          <Image id="icf0010" he="115" wi="391" file="FDA0002462562470000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0011" he="104" wi="700" file="FDA0002462562470000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中w<Sub>ij</Sub>是高斯权重,S<Sub>j</Sub>表示第j个像素的静态显著性,x<Sub>fi</Sub>表示帧f上的第i个像素,d<Sub>i</Sub>和d<Sub>j</Sub>是像素i和j的颜色值,q<Sub>i</Sub>和q<Sub>j</Sub>是像素i和j的位置,β和α均设置为30；S16：在未做超像素分割的金字塔图像上计算光流,然后利用某一像素点的所在的视频帧f<Sub>i</Sub>的平均光流值与该点光流值的对比,即卡方距离,得到动态显著性：C<Sub>d</Sub>(x<Sub>fi</Sub>)＝χ<Sup>2</Sup>(h(x<Sub>fi</Sub>),h(A(x<Sub>fi</Sub>))),其中,h(x<Sub>fi</Sub>)是像素点x<Sub>fi</Sub>处光流直方图的所有bin组成的向量,h(A(x<Sub>fi</Sub>))是全部h(x<Sub>fi</Sub>)的bin的平均值组成的向量；S17：结合动静态显著性,得到某一个像素点的显著性：          <Image id="icf0012" he="67" wi="700" file="FDA0002462562470000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,<Image id="icf0013" he="68" wi="45" file="FDA0002462562470000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0014" he="66" wi="51" file="FDA0002462562470000025.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是经过归一化后像素的显著性,a和b用来控制两种显著性的权重,均设置为1。</td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         罗洁;                   韦妙鸾       </td>   <td>中山大学</td>   <td>一种通过力台压力中心数据提取身体平衡振荡起始时间的方法</td>   <td>广东省</td>   <td>CN107169406B</td>   <td>2020-06-30</td>   <td>本发明涉及一种通过力台压力中心数据提取身体平衡振荡起始时间的方法,包括以下步骤：S1.基于力台压力中心数据重建身体平衡振荡信号；S2.特征点检测及其第一次筛选；S3.特征点的第二次筛选,将通过筛选的发生时刻最早的特征点对应的发生时刻确定为身体平衡振荡的起始时间。</td>   <td>1.一种通过力台压力中心数据提取身体平衡振荡起始时间的方法,其特征在于：包括以下步骤：S1.基于力台压力中心数据重建身体平衡振荡信号；S2.特征点检测及其第一次筛选；S3.特征点的第二次筛选,将通过筛选的发生时刻最早的特征点对应的发生时刻确定为身体平衡振荡的起始时间；所述步骤S1采用小波变换的方法重建身体平衡振荡信号,小波分解的具体层数根据力台压力中心数据的采样率以及身体平衡振荡信号的频带特征进行选取；所述步骤S2特征点检测及其第一次筛选的具体过程如下：S21.将身体平衡振荡信号分成t秒一段的若干片段,计算每个片段幅度的标准差,以其中位数的1.96倍作为阈值1；S22.将幅度高于阈值1的信号点作为特征点；S23.然后对步骤S22得到的特征点进行筛选,筛选条件如下：1)排除发生时刻在身体平衡振荡信号前10s内的特征点；2)以该特征点为起点的5s信号片段幅度的标准差小于阈值1,则该特征点应排除；所述步骤S3的具体过程如下：S31.对通过第一次筛选的特征点,计算以每一特征点为起点的t秒信号片段幅度的标准差,以其均值作为阈值2；S32.在通过第一次筛选的特征点中寻找幅度大于阈值2的点,以发生时刻最早的特征点对应的发生时刻为身体平衡振荡信号的起始时间。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;              王金鹏;              蔡佳辉;              林佳玲;                   陈嘉敏       </td>   <td>中山大学;广州智慧城市发展研究院</td>   <td>一种基于相似性损失的行为识别方法</td>   <td>广东省</td>   <td>CN111339886A</td>   <td>2020-06-26</td>   <td>本发明涉及计算机视觉识别领域,公开了一种基于相似性损失的行为识别方法,涉及计算机视觉识别领域。该行为识别方法包括步骤：将视频片段输入前馈网络,得到特征图以及对应的分类概率；计算任意两个配对样本的预测结果,根据预测结果计算分布之间的成对距离；根据成对距离和交叉熵损失计算整个的相似性损失。本发明的方法提出了一种新的相似性损失用来指导整个网络的学习目标,相似性损失可以简单的集成在任意一个基础网络之中,在没有额外引入参数和没有额外开销的前提下,该方法在数据集上取得了最优效果,验证了相似性损失的有效性。</td>   <td>1.一种基于相似性损失的行为识别方法,其特征在于,包括以下步骤：S1、将视频片段输入前馈网络,得到特征图以及对应的分类概率；S2、计算任意两个配对样本的预测结果,根据预测结果计算分布之间的成对距离；S3、根据成对距离和交叉熵损失计算整个的相似性损失。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张贺晔;                   张冬       </td>   <td>中山大学</td>   <td>冠状动脉狭窄的量化方法及装置</td>   <td>广东省</td>   <td>CN111340794A</td>   <td>2020-06-26</td>   <td>本申请提供了一种冠状动脉狭窄的量化方法及装置,包括：利用人工神经网络的自学习能力,建立冠状动脉多视角医学图像的多视角图像特征与冠状动脉狭窄的形态学参数之间的对应关系；获取患者的当前冠状动脉多视角医学图像的当前多视角图像特征；通过所述对应关系,确定与所述当前多视角图像特征对应的当前冠状动脉狭窄的形态学参数；具体地,确定与所述多视角图像特征对应的当前冠状动脉狭窄的形态学参数,包括：将所述对应关系中与所述当前多视角图像特征相同的多视角图像特征所对应的冠状动脉狭窄的形态学参数,确定为所述当前冠状动脉狭窄的形态学参数。基于多视角预测冠状动脉狭窄的形态学参数,缓解狭窄与其他血管重叠给狭窄量化带来的影响。</td>   <td>1.一种冠状动脉狭窄的量化方法,其特征在于,包括：利用人工神经网络的自学习能力,建立冠状动脉多视角医学图像的多视角图像特征与冠状动脉狭窄的形态学参数之间的对应关系；其中,所述多视角至少包括三个位置不同的视角；获取患者的当前冠状动脉多视角医学图像的当前多视角图像特征；通过所述对应关系,确定与所述当前多视角图像特征对应的当前冠状动脉狭窄的形态学参数；具体地,确定与所述多视角图像特征对应的当前冠状动脉狭窄的形态学参数,包括：将所述对应关系中与所述当前多视角图像特征相同的多视角图像特征所对应的冠状动脉狭窄的形态学参数,确定为所述当前冠状动脉狭窄的形态学参数。</td>   <td>G06T7/00;G06K9/46;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李中华;                   何冠男       </td>   <td>中山大学</td>   <td>基于标签数量的防撞资源竞争方法、移动阅读器、可读存储介质</td>   <td>广东省</td>   <td>CN111339794A</td>   <td>2020-06-26</td>   <td>本发明公开了一种基于标签数量的防撞资源竞争方法、移动阅读器、可读存储介质,其中所述的方法轮回执行以下阶段：竞争权值确定阶段、竞争权值交换阶段、通信资源竞争阶段。本发明从RFID系统通信资源最优化分配的角度出发,阅读器根据询问范围内的标签数量来确定竞争权值,通过与邻居阅读器的竞争权值比较来确定是否参与对通信资源的竞争,参与竞争的阅读器再通过一定的规则对通信资源进行竞争,以提升通信资源的利用效率,最大化RFID系统的标签询问效率。</td>   <td>1.一种基于标签数量的防碰撞资源竞争方法,其特征在于：所述的方法轮回执行以下阶段：竞争权值确定阶段、竞争权值交换阶段、通信资源竞争阶段；其中,所述的竞争权值确定阶段：每一个RFID移动阅读器对询问范围内的RFID电子标签数量进行估计；并根据估计的RFID电子标签数量设定阅读器的竞争权值；所述的竞争权值交换阶段：RFID移动阅读器在轮询服务器广播竞争权值交换信号的条件下,RFID移动阅读器通过竞争规则与邻居RFID移动阅读器交换和比较权值信息；并根据竞争权值的比较结果来判断阅读器是否进入通信资源竞争阶段；确定不进入通信资源竞争阶段的RFID移动阅读器本轮保持静默,下一轮接收到轮信号后,再重新参与竞争；所述的通信资源竞争阶段：RFID移动阅读器在轮询服务器广播竞争同步信号的条件下,对通信资源进行竞争,获得通信资源的RFID移动阅读器开始询问RFID电子标签,没有获得通信资源的RFID移动阅读器本轮保持静默,下一轮接收到轮信号后,再重新参与竞争。</td>   <td>G06K7/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         温武少;                   张朗淇       </td>   <td>中山大学</td>   <td>一种基于云簇的虚拟机借调方法</td>   <td>广东省</td>   <td>CN111338754A</td>   <td>2020-06-26</td>   <td>本发明涉及计算机技术领域,是基于云簇的虚拟机借调方法,包括步骤：微云接收终端计算机的任务请求,将任务请求转化为一定数量和规格的虚拟机申请；微云在本地资源充足时,直接服务任务请求；在本地资源不足时,计算每个邻居微云可提供的虚拟机数量以及任务成本；微云根据跨微云虚拟机借调算法生成虚拟机借调方案；依据虚拟机借调方案,微云向租借方案列表中的邻居微云进行虚拟机借调。本发明支持在本地微云本地资源不足时,按照跨微云的虚拟机调度算法,从空闲微云处进行虚拟机借调,满足本地的用户任务需求,达到了在微云之间共享闲置资源的目的。</td>   <td>1.一种基于云簇的虚拟机借调方法,其特征在于,包括以下步骤：S1、微云接收终端计算机的任务请求,将任务请求转化为一定数量和规格的虚拟机申请；S2、微云在本地资源充足时,直接服务任务请求；在本地资源不足时,计算每个邻居微云可提供的虚拟机数量以及任务成本；S3、微云根据跨微云虚拟机借调算法生成虚拟机借调方案；S4、依据虚拟机借调方案,微云向租借方案列表中的邻居微云进行虚拟机借调。</td>   <td>G06F9/455;H04L29/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         朱俊勇;              赖剑煌;              郑伟诗;                   逯峰       </td>   <td>中山大学</td>   <td>基于对数梯度直方图的广义光照不变人脸特征描述方法</td>   <td>广东省</td>   <td>CN105550646B</td>   <td>2020-06-26</td>   <td>本发明涉及基于对数梯度直方图的广义光照不变人脸特征描述方法,包括以下步骤：(1)将人脸图像做预处理,通过双眼中心坐标进行对齐、裁剪并归一化成固定大小；(2)对预处理的人脸图像作对数高斯差分滤波处理；(3)在滤波后的人脸图像上计算梯度幅值和梯度方向；(4)对梯度幅值和梯度方向作后处理；(5)将人脸图像均匀分割成若干小分块,在每个分块中根据梯度幅值和梯度方向计算梯度直方图,最终将所有分块的直方图合并作为人脸的光照不变特征。本发明适用于光照强度和光照方向变化的情况,在可见光至近红外波段涉及光源波长变化的情况下也同样适用,并且有着良好的识别性能和鲁棒性。</td>   <td>1.基于对数梯度直方图的广义光照不变人脸特征描述方法,其特征在于,包括以下步骤：(1)将人脸图像做预处理,通过双眼中心坐标进行对齐、裁剪并归一化成固定大小；(2)对步骤(1)的人脸图像作对数域的带通滤波处理,得到滤波后人脸图像；(3)根据步骤(2)所得到的滤波后人脸图像,分别计算其梯度幅值和梯度方向；(4)对步骤(3)的梯度幅值和梯度方向作后处理；(5)根据步骤(4)得到的处理后的梯度幅值和梯度方向,分块计算梯度直方图,最终将所有分块的梯度直方图合并作为人脸的光照不变特征。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈家炜;              陈静雯;              朝红阳;                   杨铭       </td>   <td>中山大学</td>   <td>图像去噪方法、装置、设备及存储介质</td>   <td>广东省</td>   <td>CN108198154B</td>   <td>2020-06-26</td>   <td>本发明公开了一种图像去噪方法,包括：获取待去噪图像的平滑块集；将所述平滑块集中的每一平滑块减去对应的均值,以得到噪声块集；获取所述噪声块集中的第一噪声图像；根据生成对抗网络对所述噪声块集进行噪声建模,以得到可生成与所述待去噪图像同类型噪声的生成器；根据所述生成器获取第二噪声图像；根据所述无噪声图像、所述第一噪声图像及所述第二噪声图像构造训练集；根据所述训练集及判别学习方法训练出图像去噪网络模型；将所述待去噪图像输入到所述图像去噪网络模型,获取去噪后的图像。本发明还提供了图像去噪装置、设备及存储介质。能提升对现实生活中的未知真实噪声的去噪效果,提高了去噪效率。</td>   <td>1.一种图像去噪方法,其特征在于,包括：获取噪声块集；获取所述噪声块集中的第一噪声图像；根据所述第一噪声图像和无噪声图像构造训练集；根据所述训练集及判别学习方法模型训练出图像去噪网络模型；将待去噪图像输入到所述图像去噪网络模型,获取去噪后的图像；其中,在所述获取所述噪声块集中的第一噪声图像之后,所述根据所述第一噪声图像和无噪声图像构造训练集之前,还包括：根据生成对抗网络对所述噪声块集进行噪声建模,以得到可生成与所述待去噪图像同类型噪声的生成器；根据所述生成器获取第二噪声图像；则根据所述第一噪声图像和无噪声图像构造训练集包括：根据所述无噪声图像、所述第一噪声图像及所述第二噪声图像构造训练集。</td>   <td>G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;              蔡佳辉;              王金鹏;              陈嘉敏;                   林佳玲       </td>   <td>中山大学;广州智慧城市发展研究院</td>   <td>一种基于结合时域通道相关性块的行为识别方法</td>   <td>广东省</td>   <td>CN111325145A</td>   <td>2020-06-23</td>   <td>本发明涉及计算机视觉领域,公开了一种基于结合时域通道相关性块的行为识别方法,通过空间全局平均池化操作对输入的初始特征图进行压缩,获得时域通道描述算子；将时域通道描述算子输入注意力模块获得时域通道全局非线性依赖；将注意力模块输出的张量赋值为经过特征选择后每个通道重要性的权重,通过残差连接将输入的初始特征图与注意力模块输出的张量逐通道相乘得到通道加权之后的特征图。本发明通过网络层有效的捕获时域-通道之间的相关信息,获得一个逐通道描述算子,通过乘法逐通道加权到之前的特征上,完成在通道维度上对原始特征的重新加权,通过将网络的计算资源更多的集中到对输出结果比较重要的特征通道中去。</td>   <td>1.一种基于结合时域通道相关性块的行为识别方法,其特征在于,包括以下步骤：S1、通过空间全局平均池化操作对输入的初始三维时空信号特征图进行压缩,获得一个时域通道描述算子；S2、将时域通道描述算子输入注意力模块获得时域通道全局非线性依赖；S3、将注意力模块输出的张量赋值为经过特征选择后每个通道重要性的权重,通过残差连接将所述步骤S1中输入的初始三维时空信号特征图与所述步骤S2中注意力模块输出的张量逐通道相乘得到通道加权之后的特征图。</td>   <td>G06K9/00;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;              王金鹏;              蔡佳辉;              林佳玲;                   陈嘉敏       </td>   <td>中山大学;广州智慧城市发展研究院</td>   <td>一种基于投票的时序关联模型的视频动作识别方法</td>   <td>广东省</td>   <td>CN111325149A</td>   <td>2020-06-23</td>   <td>本发明涉及计算机视觉领域,公开了一种基于投票的时序关联模型的视频动作识别方法,其包括步骤：S1、对卷积特征图进行空间池化；S2、使用大小为1的卷积核对执行了空间池化后的卷积特征图进行通道压缩；S3、使用1维的时域卷积层的三路分支对经过通道压缩后输出的卷积特征图进行不同膨胀率的一维时间卷积运算；S4、经过时序池化,将空间池化后的卷积特征图降维为特征向量；S5、将三路分支的预测结果分别相加,作为最后的分类结果。本发明的方法在对特征图进行特征提取时,可以捕获时间信息,而且在训练过程中能够快速收敛,同时能够在网络的任意深度集成,在较高的提升了模型表征能力的基础上,还很好地控制了计算开销和模型复杂度。</td>   <td>1.一种基于投票的时序关联模型的视频动作识别方法,其特征在于,包括以下步骤：S1、对卷积特征图进行空间池化；S2、使用大小为1的卷积核对执行了空间池化后的卷积特征图进行通道压缩；S3、使用1维的时域卷积层的三路分支对经过通道压缩后输出的卷积特征图进行不同膨胀率的一维时间卷积运算；S4、经过时序池化,将空间池化后的卷积特征图降维为特征向量；S5、将三路分支的预测结果分别相加,作为最后的分类结果。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         冯镇轩;                   谢逸       </td>   <td>中山大学</td>   <td>一种面向加密网络流的Web用户点击识别方法</td>   <td>广东省</td>   <td>CN111310796A</td>   <td>2020-06-19</td>   <td>本申请的技术方案公开了一种面向加密网络流的Web用户点击识别方法,属于网络的技术领域。本申请的面向加密网络流的Web用户点击识别方法,主要包括网络侧采集web流数据、构造训练样本、训练分类器产生识别模型、向分类器输入web流进行识别这四大步骤。本申请的面向加密网络流的Web用户点击识别方法,利用进出网络边界的流量,根据HTTP(S)请求产生的过程,从混杂的Web网络流中区分出用户点击和自动请求分别产生的HTTP(S)请求所对应的网络流,从而达到对用户点击的有效识别。</td>   <td>1.一种面向加密网络流的Web用户点击识别方法,其特征在于,包括以下步骤：步骤一、在网络侧采集数据,在web客户端注入自动脚本来执行模拟用户点击所产生的web网络流；步骤二、根据步骤一中自动脚本执行模拟用户点击所产生的web网络流来构造训练样本；步骤三、向分类器输入步骤二所构造的训练样本,训练分类器生成识别用户点击的识别模型；步骤四、利用步骤二构造训练样本所使用的步骤,将任意真实的用户点击所产生的web网络流构造为待识别样本,然后把待识别样本输入步骤三中训练好识别模型的分类器进行学习,最后由分类器输出识别结果。</td>   <td>G06K9/62;H04L29/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         徐永键;              徐广健;              陆许明;              谭洪舟;                   陈远川       </td>   <td>中山大学花都产业科技研究院;中山大学</td>   <td>一种用于消除网络声音传输的回音和啸叫的系统和方法</td>   <td>广东省</td>   <td>CN106782592B</td>   <td>2020-06-19</td>   <td>本发明公开了一种用于消除网络声音传输的回音和啸叫的系统和方法,一方面,本发明提供了一种用于消除网络声音传输的回音和啸叫的方法,实时将远端输入的音频信号进行降采样得到第一处理音频信号；实时将麦克风采集到的音响发出的声音和近端输入的声音的混合音频信号进行降采样得到第二处理音频信号；实时将第二处理音频信号中与第一处理音频信号相关的信号进行提取升采样得到第三处理音频信号；实时将麦克风采集到的混合音频中与第三处理音频信号相关的信号消除得到消除了回音和啸叫的输出音频信号。另一方面,本发明提供了一种用于消除网络声音传输的回音和啸叫的系统。本发明可有效消除网络声音传输的回音和啸叫,保证音频质量。</td>   <td>1.一种用于消除网络声音传输的回音和啸叫的方法,运用到音响(1)和麦克风(6),其特征在于,该方法包括如下处理步骤：步骤1.1：实时将远端输入的音频信号进行降采样得到设定采样率的第一处理音频信号；步骤1.2：实时将麦克风(6)采集到的音响(1)发出的声音和近端输入的声音的混合音频信号进行降采样得到设定采样率的第二处理音频信号；步骤1.3：实时将第二处理音频信号中与第一处理音频信号相关的音频信号进行提取升采样得到设定采样率的第三处理音频信号；步骤1.4：实时将麦克风(6)采集到的混合音频信号中与第三处理音频信号相关的音频信号消除得到消除了回音和啸叫的输出音频信号。</td>   <td>G10L21/0208;G10L21/0264</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陶宇;              郑伟诗;                   胡建芳       </td>   <td>中山大学</td>   <td>一种利用时空信息的快速视频目标物体分割方法</td>   <td>广东省</td>   <td>CN111291663A</td>   <td>2020-06-16</td>   <td>本发明公开了一种利用时空信息的快速视频目标物体分割方法,包括下述步骤：建立一个神经网络系统,在最前部为深度卷积神经网络CNN,对图像进行基本的特征提取,得到每一帧对应的特征图；连接一个循环神经网络RNN,该循环神经网络RNN用以充分利用视频每一帧空间上的相关性包含的信息,以及视频在每一帧时间相关性上所包含的信息,并将这些隐含信息提取到特征中,从而得到视频对应每一帧包含时空信息的新的特征图；连接一个用以进行二分类的神经网络层,对特征图进行二分类,得到前景部分和背景部分,从而实现对视频每一帧的目标物体分割。</td>   <td>1.一种利用时空信息的快速视频目标物体分割方法,其特征在于,包括下述步骤：建立一个神经网络系统,在最前部为深度卷积神经网络CNN,对图像进行基本的特征提取,得到每一帧对应的特征图；连接一个循环神经网络RNN,该循环神经网络RNN用以充分利用视频每一帧空间上的相关性包含的信息,以及视频在每一帧时间相关性上所包含的信息,并将这些隐含信息提取到特征中,从而得到视频对应每一帧包含时空信息的新的特征图；连接一个用以进行二分类的神经网络层,对特征图进行二分类,得到前景部分和背景部分,从而实现对视频每一帧的目标物体分割。</td>   <td>G06K9/00;G06K9/34;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;              陈嘉敏;              林佳玲;              蔡佳辉;                   王金鹏       </td>   <td>中山大学;广州智慧城市发展研究院</td>   <td>一种细粒度识别方法、终端设备及计算机可读存储介质</td>   <td>广东省</td>   <td>CN111291767A</td>   <td>2020-06-16</td>   <td>本发明公开了一种细粒度识别方法、终端设备及计算机可读存储介质,涉及计算机视觉技术领域。该方法包括步骤：目标检测,对输入图片进行卷积,得到特征图,标框出目标所在位置,使用目标掩膜对检测出的目标框周围的特征进行相应的反转；局部特征提取,对特征图进行卷积和全局最大池化,得到图片显著点,提取显著点的特征；全局特征提取,将目标掩膜后得到的结果与目标检测步骤得到的特征图进行点乘,得到新的特征图,并把新特征图作为残差网络的输入,经过卷积层逐步提取图片的全局特征；特征融合,将得到的局部特征和全局特征按权重进行融合。本发明的方法基于背景分离和显著点检测,具有鲁棒性强、计算效率高的优点,可进行精确的细粒度识别。</td>   <td>1.一种细粒度识别方法,其特征在于,包括以下步骤：S1、目标检测,对输入图片进行卷积,得到特征图,标框出目标所在位置,使用目标掩膜对检测出的目标框周围的特征进行相应的反转；S2、局部特征提取,对特征图进行卷积和全局最大池化,得到图片显著点,提取显著点的特征；S3、全局特征提取,将目标掩膜后得到的结果与目标检测步骤得到的特征图进行点乘,得到新的特征图,并把新特征图作为残差网络的输入,经过卷积核大小不同的卷积层逐步提取图片的全局特征；S4、特征融合,将得到的局部特征和全局特征按权重进行融合。</td>   <td>G06K9/46;G06K9/32;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         邓秋君;              张东;                   方圳河       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于图像轮廓特征的目标识别方法</td>   <td>广东省</td>   <td>CN107103323B</td>   <td>2020-06-16</td>   <td>本发明公开一种基于图像轮廓特征的目标识别方法,对模板图像和待测物体图像进行预处理生成二值图像；建立物体模板轮廓的特征库：提取物体模板二值图像的完整轮廓,在轮廓上等间隔取一定数目的特征点,利用特征点的上下文特征对物体轮廓进行描述；对待检测物体图像进行目标识别：提取待检测二值图像的轮廓边缘；并选取一定的特征点；对待测物体的形状方向进行转换变成模板的取向；用选定点的上下文特征描述转换取向后的待测物体轮廓；通过匹配代价来衡量待测物体与模板物体的相似度。本发明相对于现有的技术,解决了在轮廓匹配过程中的旋转不变性的问题,使轮廓匹配过程中的旋转具有不变性,并有效的应用于图像中的目标识别。</td>   <td>1.一种基于图像轮廓特征的目标识别方法,其特征在于,包括以下步骤：S1：图像二值化：对待测物体图像和若干的模板图像进行预处理生成二值化图像,所述模板图像为确定物体的图像,所述待测物体图像为待测物体的图像；S2：建立模板轮廓的特征库：提取二值化模板图像中物体的完整轮廓,在轮廓上等间隔取若干数目的特征点,利用特征点的上下文特征对物体轮廓进行描述；具体包括以下步骤：S2.1：提取二值化模板图像中物体的完整轮廓；S2.2：对二值化模板图像中物体的完整轮廓进行等间隔取样,选取一定数目的轮廓点,即特征点；S2.3：对选取轮廓上的点用上下文的特征进行描述,包括以下步骤：a)对于一幅二值图像,黑点表示字符,白点表示空白,这样就可以用黑点集合来表示一幅图像,即一幅图像可以表示为集合P＝{P<Sub>1</Sub>,P<Sub>2</Sub>,P<Sub>3</Sub>,...,P<Sub>N</Sub>}；b)计算形状轮廓点的上下文特征：对于整个特征点集P,分别以其中的每个特征点P<Sub>1</Sub>,P<Sub>2</Sub>,P<Sub>3</Sub>,...,P<Sub>N</Sub>作参考点,依次计算该点与其余的N-1个点构成的向量分布直方图,最终得到N个向量分布直方图,并以N*(N-1)大小的矩阵存储；这样,对于任一目标,可用N*(N-1)大小的矩阵表示其形状信息,N*(N-1)大小的矩阵即为点集P的形状上下文,它描述了整个轮廓形状的特征；由于描述信息量太大,所以采用极坐标系,将向量的极半径和极角离散化的在空间具体分布描述出来,并且使越临近P<Sub>i</Sub>的点权值越大；三角形包围的点的为极坐标的中心,对于每一个由r和θ确定的极坐标区域,如果该区域内包含的点越多,则其在由θ和log(r)组成的直角坐标系中对应的区域颜色越深点；以特征点集中任意一点P<Sub>i</Sub>为参考点,在P<Sub>i</Sub>为圆心、R为半径的局域内按对数距离间隔建立n个同心圆；将此区域沿圆周方向M等分,点P<Sub>i</Sub>到其它各点的向量相对位置简化为模板上各扇区内的点分布数；S3：待测图像特征描述：提取二值化待测物体图像的轮廓边缘,在轮廓上选取若干的特征点；对待测物体的形状方向进行转换变成模板的取向；用选定的特征点的上下文特征描述转换取向后的待测物体轮廓；具体包括以下步骤：S3.1：提取待测物体二值图像的完整轮廓；S3.2：对待测物体轮廓上的点进行等间隔取样,选取与S2.2当中相同数目的轮廓点,即特征点；S3.3：对待测物体的形状方向进行转换,使之与模板图像的取向相同；为了解决旋转不变性,对当中等间隔选取的m个轮廓点所组成的查询形状采用最佳拟合椭圆模型的方法用于标准化其尺度和取向；S3.4：对转换后的待测物体上的轮廓点选取上下文的特征,从而形成对轮廓整体的描述；S4：对待测物体图像的特征描述与模板图像中物体轮廓的特征描述进行匹配,通过匹配代价来衡量待测物体与模板物体的相似度,进而识别待测物体。</td>   <td>G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              陈子良;              王可泽;                   许瑞嘉       </td>   <td>中山大学</td>   <td>一种基于对抗学习的多源域适应迁移方法及系统</td>   <td>广东省</td>   <td>CN108256561B</td>   <td>2020-06-16</td>   <td>本发明公开了一种基于对抗学习的多源域适应迁移方法及系统,所述方法包括如下步骤：步骤一,使用各源域数据进行预训练并初始化目标模型的表示网络和分类器；步骤二,使用多源域数据与目标域数据进行多路对抗,更新目标模型的表示网络和多路判别器；步骤三,计算每个源域与目标域之间的对抗分数；步骤四,基于各源域的分类器和对抗分数对目标域进行分类；步骤五,选取高置信度的目标域伪样本微调目标模型的表示网络和分类器；步骤六,返回步骤二,进行步骤二-五,直至模型收敛或达到最大迭代次数时停止训练,本发明可不再依赖单一源域标签集合与目标域一致的假设,并且可有效地避免多源域适应过程中存在的负迁移现象。</td>   <td>1.一种基于对抗学习的多源域适应迁移方法,包括如下步骤：步骤一,获取多个源域的带标记的源域数据以及无标记的目标域数据,并使用各源域的源域数据对目标模型的表示网络和多路分类器进行预训练及初始化,各源域数据包括图像数据和对应标签,所述目标域数据包括图像数据；所述步骤一还包括,输入带标记的N个源域数据集以及输入无标记的目标域数据集,使用所有的源域数据集对领域无关的表示网络F和领域相关的多路分类器C进行目标模型的预训练,所述预训练的步骤具体为根据如下优化目标          <Image id="icf0001" he="61" wi="700" file="FDA0002461077390000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        更新目标模型中表示网络F和多路分类器C的参数,其中<Image id="icf0002" he="60" wi="83" file="FDA0002461077390000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示多路分类的损失函数,<Image id="icf0003" he="50" wi="37" file="FDA0002461077390000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示具体选取的损失函数类型,<Image id="icf0004" he="80" wi="68" file="FDA0002461077390000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示第sj路分类器,E表示所有样本损失值的期望,F(x)表示图像x经过表示网络F后的特征编码；步骤二,固定当前多路分类器的参数,引入目标域数据,使用所述多个源域的源域数据与目标域数据进行多路对抗,更新所述目标模型的表示网络和多路判别器；步骤三,基于每一路判别器的损失值计算对应源域与目标域之间的对抗分数；步骤四,基于各源域的多路分类器和对抗分数对目标域的样本进行分类,赋予伪标签；步骤五,选取高置信度的目标域伪样本微调所述目标模型的表示网络和多路分类器,获取在目标域上更加有效可分的特征编码；步骤六,返回步骤二,进行步骤二-五,直至模型收敛或达到最大迭代次数时停止训练。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              谢震宇;              梁小丹;                   董浩业       </td>   <td>中山大学</td>   <td>一种基于混合光流的视频虚拟试穿方法及装置</td>   <td>广东省</td>   <td>CN111275518A</td>   <td>2020-06-12</td>   <td>本发明公开了一种基于混合光流的视频虚拟试穿方法及装置,该方法包括：步骤S1,根据人体图像获得姿态热图,并对人体图像处理得到只保留头部和下半身区域的人体分割图像,将姿态热图、人体分割图像及对应的衣服图像生成目标姿态下的目标人体语义分割图；步骤S2,对人体图像和表示人体姿态的骨架图分别提取人体SMPL模型,并计算两个SMPL模型间的3D光流图；步骤S3,根据示例衣服图像和目标衣服图像的二进制掩模,利用渐进式修正网络预测两者之间的衣服光流图；步骤S4,根据人体分割图像,衣服图像,目标姿态热图,目标人体语义分割图和上一张合成视频帧,在3D光流图和衣服光流图指导下,利用特征融合网络合成当前试穿视频帧。</td>   <td>1.一种基于混合光流的视频虚拟试穿方法,包括如下步骤：步骤S1,根据人体图像获得表示目标姿态的姿态热图,并对所述人体图像处理得到只保留头部和下半身区域的人体分割图像,将所述姿态热图、人体分割图像以及对应的衣服图像利用生成器网络生成目标姿态下的目标人体语义分割图；步骤S2,对人体图像和表示人体姿态的人体姿态骨架图,分别提取其各自的人体SMPL模型,并通过3D顶点匹配的方法,计算两个SMPL模型间的3D光流图；步骤S3,根据示例衣服图像的二进制掩模和目标衣服图像的二进制掩模,利用渐进式修正网络预测示例衣服图像与目标衣服图像之间的衣服光流图；步骤S4,根据所述人体分割图像,衣服图像,目标姿态热图,目标人体语义分割图和上一张合成视频帧,在3D光流图和衣服光流图的指导下,利用特征融合网络合成当前试穿视频帧。</td>   <td>G06Q30/06;G06K9/00;G06K9/62;G06T7/11</td>  </tr>        <tr>   <td>中国专利</td>   <td>         蔡庆玲;              孙玮;              何鸿奇;              林进可;                   林满盈       </td>   <td>中山大学</td>   <td>用于人工神经网络训练的医学图像数据的生成方法及装置</td>   <td>广东省</td>   <td>CN111275686A</td>   <td>2020-06-12</td>   <td>本申请提供了一种用于人工神经网络训练的医学图像数据的生成方法及装置,所述方法包括：利用人工神经网络的自学习能力,建立基础医学图像的目标特征与扩展图像的图像特征之间的对应关系；其中,所述图像特征包括纹理特征和内容特征；获取当前基础医学图像的当前目标特征；通过所述对应关系,确定与所述当前目标特征对应的当前扩展图像的图像特征；具体地,确定与所述目标特征对应的当前扩展图像的图像特征,包括：将所述对应关系中与所述当前目标特征相同的目标特征所对应的扩展图像的图像特征,确定为所述当前扩展图像的图像特征,提高了生成的扩展图像的合理性；细节还原更加出色,增加了特征的多样性。</td>   <td>1.一种用于人工神经网络训练的医学图像数据的生成方法,其特征在于,包括：利用人工神经网络的自学习能力,建立基础医学图像的目标特征与扩展图像的图像特征之间的对应关系；其中,所述图像特征包括纹理特征和内容特征；获取当前基础医学图像的当前目标特征；通过所述对应关系,确定与所述当前目标特征对应的当前扩展图像的图像特征；具体地,确定与所述目标特征对应的当前扩展图像的图像特征,包括：将所述对应关系中与所述当前目标特征相同的目标特征所对应的扩展图像的图像特征,确定为所述当前扩展图像的图像特征。</td>   <td>G06T7/00;G06T7/13;G06T5/00;G06T11/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         阮思敏;              王伟;              陈立达;              胡航通;              李薇;              黄漾;              谢晓燕;              吕明德;                   匡铭       </td>   <td>中山大学附属第一医院</td>   <td>一种基于剪切波弹性成像超声组学深度分析方法及系统</td>   <td>广东省</td>   <td>CN111275706A</td>   <td>2020-06-12</td>   <td>本发明公开了一种基于剪切波弹性成像超声组学深度分析方法及系统,所述方法包括：针对不同疾病,利用超声医学声学经验获取标准化剪切波弹性图像；针对相应疾病模型,利用剪切波图像获取所在器官相应弹性超声组学数据；将所述弹性超声组学数据输入训练好的深度学习网络,并根据所述弹性超声组学数据调整神经元的连接权重、配比卷积和池化层,得到调整后的弹性超声组学数据,通过深度学习获取每个病变的分类评分；基于患者临床信息,检验指标,对结果深度学习弹性分类评分,通过机器学习分析,构建深度分析决策系统。本发明能够提高边界数据获取的可重复性及图像分析的适应性,并构建深度分析决策系统提高辅助分析结果的准确性。</td>   <td>1.一种基于剪切波弹性成像超声组学深度分析方法,其特征在于,包括：步骤S11,针对不同疾病,利用超声医学声学经验获取标准化剪切波弹性图像；步骤S12,针对相应疾病模型,利用剪切波图像获取所在器官相应弹性超声组学数据；步骤S13,将所述弹性超声组学数据输入训练好的深度学习网络,并根据所述弹性超声组学数据调整神经元的连接权重、配比卷积和池化层,得到调整后的弹性超声组学数据,通过深度学习获取每个病变的分类评分；步骤S14,基于患者临床信息,检验指标,对结果深度学习弹性分类评分,通过机器学习分析,构建深度分析决策系统。</td>   <td>G06T7/00;G06N3/08;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周凡;              苏卓;              林谋广;                   陈小燕       </td>   <td>中山大学</td>   <td>一种融合注意力机制的多帧视频超分辨率方法</td>   <td>广东省</td>   <td>CN111260560A</td>   <td>2020-06-09</td>   <td>本发明公开了一种融合注意力机制的多帧视频超分辨率方法,包括：采集视频数据并采用视频增强技术对视频数据进行训练以生成训练集及测试集；连接变形卷积特征对齐模块及特征重建模块以构成多帧超分辨率网络,采用训练集对多帧超分辨率网络进行训练；将3D卷积特征对齐模块加入多帧超分辨率网络中,采用训练集对多帧超分辨率网络进行训练；将特征融合模块加入多帧超分辨率网络中,采用训练集对多帧超分辨率网络进行训练；采用训练集对多帧超分辨率网络进行微调以生成多帧超分辨率模型；采用测试集对多帧超分辨率模型进行测试。本发明可通过对大数据的分析有效提高超分辨率效果。</td>   <td>1.一种融合注意力机制的多帧视频超分辨率方法,其特征在于,包括：S1,采集视频数据,并采用视频增强技术对所述视频数据进行训练以生成训练集及测试集；S2,构建变形卷积特征对齐模块及特征重建模块,并连接所述变形卷积特征对齐模块及特征重建模块以构成多帧超分辨率网络,采用所述训练集对所述多帧超分辨率网络进行训练；S3,构建3D卷积特征对齐模块,并将所述3D卷积特征对齐模块加入所述多帧超分辨率网络中,采用所述训练集对所述多帧超分辨率网络进行训练；S4,构建特征融合模块,并将所述特征融合模块加入所述多帧超分辨率网络中,采用所述训练集对所述多帧超分辨率网络进行训练；S5,采用所述训练集对所述多帧超分辨率网络进行微调以生成多帧超分辨率模型；S6,采用所述测试集对所述多帧超分辨率模型进行测试。</td>   <td>G06T3/40;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林浩添;              杨雅涵;              李睿扬;                   郭翀       </td>   <td>中山大学中山眼科中心</td>   <td>一种近视图像深度学习识别模型训练方法及系统</td>   <td>广东省</td>   <td>CN111259743A</td>   <td>2020-06-09</td>   <td>本发明涉及医学图像处理技术领域,更具体地涉及一种近视图像深度学习识别模型训练方法,包括以下步骤：采集眼外观图像；对所述眼外观图像进行预处理；以人脸识别大数据库VGG-Face中的人脸图像作为第一训练数据,对VGG-16网络模型进行预训练；以预处理后的所述眼外观图像作为第二训练数据,对预训练后的所述VGG-16网络模型进行训练,得到用于近视图像识别的深度学习模型。本发明提供一种近视图像深度学习识别模型训练方法,用于辅助使用者快速、准确地判断青少年近视情况。</td>   <td>1.一种近视图像深度学习识别模型训练方法,其特征在于,包括以下步骤：采集眼外观图像；对所述眼外观图像进行预处理；以人脸识别大数据库VGG-Face中的人脸图像作为第一训练数据,对VGG-16网络模型进行预训练；以预处理后的所述眼外观图像作为第二训练数据,对预训练后的所述VGG-16网络模型进行训练,得到用于近视图像识别的深度学习模型。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         梁凡;                   李正仁       </td>   <td>中山大学</td>   <td>一种图像特征信息帧内快速划分方法、系统及存储介质</td>   <td>广东省</td>   <td>CN111259826A</td>   <td>2020-06-09</td>   <td>本发明公开了一种图像特征信息帧内快速划分方法、系统及存储介质,方法包括：获取训练视频序列,对视频序列提取特征获取训练数据；通过所述训练数据训练支持向量机得到分类器；通过已训练完成的分类器对模式列表进行过滤,获取过滤后的模式列表；根据过滤后的模式列表,跳过编码模式的尝试过程,进行编码流程。该方法提取图像特征信息后,训练得到分类器,并使用分类器对待处理图像进行判断,对耗时较长的编码模式的尝试过程进行跳过,从而缩短了在进行视频编码过程中所花费的时间。本发明可广泛应用于机器视觉和模式识别技术领域。</td>   <td>1.一种图像特征信息帧内快速划分方法,其特征在于,包括以下步骤：获取训练视频序列,对视频序列提取特征获取训练数据；通过所述训练数据训练支持向量机得到分类器；通过已训练完成的分类器对模式列表进行过滤,获取跳过模式列表；根据所述跳过模式列表,跳过编码模式的尝试过程,进行编码流程。</td>   <td>G06K9/00;G06K9/62;G06T7/45;H04N19/11;H04N19/136;H04N19/96</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈承勃;                   权小军       </td>   <td>中山大学</td>   <td>基于多模态对齐与多向量表征的人格检测方法</td>   <td>广东省</td>   <td>CN111259976A</td>   <td>2020-06-09</td>   <td>本发明公开一种基于多模态对齐与多向量表征的人格检测方法,包括将语音和视频模态数据按每个epoch进行重采样；将数个样本及其文本模态数据输入模态内表征模块进行独立编码,得到语音序列、视频序列和文本序列；将语音序列、视频序列和文本序列输入模态间对齐表征模块以两两对齐交互后拼接,得到增强后的语音表征、视频表征和文本表征；将所有语音表征、所有视频表征和所有文本表征分别拼接得到语音向量、视频向量和文本向量,输入卷积神经网络转化为至少两类人格向量；将至少两类人格向量分别线性化后通过sigmoid函数映射得到至少两类人格特点的预测概率。本发明通过3个模态数据的两两交互增强模态表征,提高模型的辨别能力,得到更为精准的预测结果。</td>   <td>1.一种基于多模态对齐与多向量表征的人格检测方法,其特征在于,包括如下步骤：S10将语音和视频模态数据按每个epoch进行重采样,生成数个彼此具有差异性的样本；S20将数个样本及其文本模态数据输入模态内表征模块进行独立编码,得到语音序列、视频序列和文本序列；S30将语音序列、视频序列和文本序列输入模态间对齐表征模块,模态间对齐表征模块分别将语音序列、视频序列和文本序列两两对齐交互后拼接,得到增强后的语音表征、视频表征和文本表征；S40将所有语音表征拼接成语音向量,将所有视频表征拼接成视频向量,将所有文本表征拼接成文本向量,利用卷积神经网络分别将语音向量、视频向量和文本向量转化为至少两类人格向量；S50将至少两类人格向量分别线性化后通过sigmoid函数映射得到至少两类人格特点的预测概率。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         余志;              江倩殷;              李熙莹;                   张瑞娟       </td>   <td>中山大学;广东方纬科技有限公司</td>   <td>一种基于视频图像的车头间距测量方法及系统</td>   <td>广东省</td>   <td>CN106898023B</td>   <td>2020-06-09</td>   <td>本发明公开了一种基于视频图像的车头间距测量方法及系统,方法包括：确定摄像机参数；在待测量车辆上确定测量点；根据摄像机参数计算测量点的世界坐标；根据测量点的世界坐标计算每辆待测量车辆的车头位置；根据每辆待测量车辆的车头位置计算相邻两辆车辆的车头间距。本发明基于视频图像来测量车头间距,误差更小；只需确定摄像机参数、测量点及其世界坐标,再结合现有车辆定位、车牌定位和车辆分割的方法,即可实现车头间距的自动测量,不再需人工操作,通过自动化测量的方式大大减小了人工工作量且成本较低；能精确计算相邻两辆车辆的车头间距而不是一个车队或者一个路段上的平均车头间距,更加可靠。本发明可广泛应用于智能交通领域。</td>   <td>1.一种基于视频图像的车头间距测量方法,其特征在于：包括以下步骤：确定摄像机参数,所述摄像机参数指能确定并表示摄像机姿态以及位置的参数；在待测量车辆上确定测量点,其中,测量点为待测量车辆的车体上一个高度已知且距离待测量车辆车头距离已知的点；根据摄像机参数计算测量点的世界坐标；根据测量点的世界坐标计算每辆待测量车辆的车头位置；根据每辆待测量车辆的车头位置计算相邻两辆车辆的车头间距；所述摄像机参数包括摄像机距离地面的高度、摄像机在世界坐标系中的坐标、摄像机拍摄角、摄像机旋转角、摄像机俯仰角、摄像机焦距、摄像机光心距离摄像机光轴与地面交点之间的距离,所述确定摄像机参数的方法包括人工到拍摄现场进行测量法、从摄像机的安装要求获得法和图像标定法,其中,图像标定法包括以下步骤：根据车辆视频图像的类型选定典型图像：若车辆视频图像为视频,则选定的典型图像为视频中成像稳定的一帧图像,若车辆视频图像为卡口图像,则选定的典型图像为卡口图像中一张成像质量好的图像；根据典型图像采用设定的摄像机模型及标定方法进行参数标定,得到包含摄像机距离地面的高度h在内的摄像机参数以及摄像机在世界坐标系中的坐标Q<Sub>CAMERA</Sub>(X<Sub>CAM</Sub>,Y<Sub>CAM</Sub>,Z<Sub>CAM</Sub>),其中,X<Sub>CAM</Sub>、Y<Sub>CAM</Sub>和Z<Sub>CAM</Sub>分别为摄像机在世界坐标系中的坐标Q<Sub>CAMERA</Sub>的X轴、Y轴和Z轴分量；所述在待测量车辆上确定测量点这一步骤,其包括：建立车型测量信息库,所述车型测量信息库中存储有不同型号车辆上可能用到的测量点的信息,其中,车型测量信息库存储的测量点信息包括测量点的类型、测量点距离地面的高度dz<Sub>Q</Sub>和测量点距离车头的距离dh<Sub>Q</Sub>,测量点的类型包括车牌边缘、车辆下边缘、车窗下边缘、车窗上边缘和车顶后端；对待测量车辆进行车辆定位、车牌定位与车型识别,获得图像中待测量车辆的测量点的类型与坐标q(x<Sub>q</Sub>,y<Sub>q</Sub>),其中,x<Sub>q</Sub>和y<Sub>q</Sub>分别为测量点在图像坐标的x轴和y轴分量；根据测量点的类型与坐标q(x<Sub>q</Sub>,y<Sub>q</Sub>)在车型测量信息库中查找对应的dz<Sub>Q</Sub>与dh<Sub>Q</Sub>并进行记录。</td>   <td>G06T7/60</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;              蔡祎俊;              李昊昕;                   陈立       </td>   <td>中山大学</td>   <td>基于交互建模的第一人称视角视频交互行为识别方法</td>   <td>广东省</td>   <td>CN111241963A</td>   <td>2020-06-05</td>   <td>本发明公开了一种基于交互建模的第一人称视角视频交互行为识别方法,提出对摄像头佩戴者和交互者进行分离,分别学习其对应的静态外观和动态运动特征,再显式建模二者之间的交互关系。为了将交互者从背景中分离出来,利用一个注意力模型生成掩码,并用人体解析模型辅助注意力模型的学习；提出一个运动模块分别预测摄像头佩戴者对应和交互者对应的运动信息矩阵,并通过对下一帧的重构辅助运动模块的学习。最后,提出一个用于交互建模的对偶长短时记忆模块,并在此模块基础上显式地建模交互关系。本发明能很好地对第一人称视角的交互行为进行描述和识别,并在常用的第一人称视角交互行为研究数据集上取得当前较优的识别结果。</td>   <td>1.基于交互建模的第一人称视角视频交互行为识别方法,其特征在于,包括下述步骤：S1、将摄像头佩戴者和交互者显式分离,分别学习二者的行为特征,包括：S1.1、通过注意力模块将交互者从背景中分离出来；S1.2、分别提取和学习摄像头佩戴者和交互者的行为特征,所述行为特征包括静态外观特征和动态运动特征；所述静态外观特征为摄像头佩戴者看见的静态视觉内容的特征,即对应摄像头佩戴者的视频帧I<Sub>t</Sub>的全局外观特征,以及对应交互者的视频帧I<Sub>t</Sub>的局部外观特征；S1.3、运动特征学习,对于摄像头佩戴者,其运动信息即为摄像头运动信息,该运动信息对视频帧变化的影响是全局性的；对于交互者,其运动信息对视频帧变换的影响是局部的,通过一个密集的运动矩阵D∈R<Sup>H x W</Sup>来表示交互者的运动信息,并通过和注意力模块生成的掩码M<Sub>t</Sub><Sup>(3)</Sup>逐渐相乘,使运动矩阵D只作用于交互者而不作用于背景；S1.4、对于每一对相邻的视频帧I<Sub>t-1</Sub>,I<Sub>t</Sub>,通过上述的注意力模块和运动模块分别得到摄像头佩戴者对应的全局静态外观特征f<Sub>t</Sub><Sup>g,a</Sup>和运动特征f<Sub>t</Sub><Sup>g,m</Sup>,以及交互者对应的局部静态外观特征f<Sub>t</Sub><Sup>l,a</Sup>和运动特征f<Sub>t</Sub><Sup>l,m</Sup>,摄像头佩戴者的行为特征定义为f<Sub>t</Sub><Sup>ego</Sup>＝[f<Sub>t</Sub><Sup>g,a</Sup>,f<Sub>t</Sub><Sup>g,m</Sup>],交互者的行为特征定义为f<Sub>t</Sub><Sup>exo</Sup>＝[f<Sub>t</Sub><Sup>l,a</Sup>,f<Sub>t</Sub><Sup>l,m</Sup>],这两个特征将用于摄像头佩戴者和交互者之间的交互关系建模；S2、对偶交互关系建模；S2.1、构建用于交互建模的长短时记忆模块；S2.2、用于交互建模的长短时记忆模块通过把对偶模块在上一帧的输出作为当前帧的输入,显式建模了摄像头佩戴者和交互者的交互关系。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         程乐华;                   曾韵       </td>   <td>中山大学</td>   <td>情绪识别能力测评方法、装置、电子设备及存储介质</td>   <td>广东省</td>   <td>CN111241980A</td>   <td>2020-06-05</td>   <td>本发明提供了一种情绪识别能力测评方法、装置、电子设备及存储介质。其中的方法包括：收到用于触发测评系统的测评题集进行展示的指令时,展示单选题集中的一选择题；收到由被测者对当前展示的选择题进行作答时触发的选项所产生的指令时,确定当前展示的选择题的答题信息,并将当前展示的选择题更新为单选题集中的另一选择题；其中,更新后的选择题与更新前已展示的任一选择题不同；当已展示的选择题总数达到第一预设数量时,根据答题信息和预存的正确答案,确定用于表征被测者的情绪识别能力的测评结果。</td>   <td>1.一种情绪识别能力测评方法,其特征在于,应用于情绪识别能力的测评系统,所述测评系统配置有测评题集,所述测评题集包括单选题集,单选题集中的每一选择题均包括用于表征图中人物从平静到指定情绪的变化过程的面部表情动图、和用于供被测者择一选择的若干选项；所述指定情绪为以下之一：恐惧、愤怒、厌恶、快乐、悲伤和惊讶；所述若干选项中,不同选项对面部表情动图所表达的情绪的描述不同；所述方法包括：收到用于触发测评系统的测评题集进行展示的指令时,展示单选题集中的一选择题；收到由被测者对当前展示的选择题进行作答时触发的选项所产生的指令时,确定当前展示的选择题的答题信息,并将当前展示的选择题更新为单选题集中的另一选择题；其中,更新后的选择题与更新前已展示的任一选择题不同；当已展示的选择题总数达到第一预设数量时,根据答题信息和预存的正确答案,确定用于表征被测者的情绪识别能力的测评结果。</td>   <td>G06K9/00;G16H10/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王国利;              梁方殷;                   郭雪梅       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于线特征描述的无标识增强现实注册方法</td>   <td>广东省</td>   <td>CN106952312B</td>   <td>2020-06-05</td>   <td>本发明提供一种基于线特征描述的无标识增强现实注册方法,该方法利用颜色差分直方图获取当前帧中兴趣目标所在的局部区域,然后对当前帧中的兴趣目标使用高斯滤波来除去噪声和图像亮度调整；再对区域中目标进行轮廓特征提取,按检测到的线特征提取描述符,进行线特征匹配；再通过当前帧中二维和三维的对应关系计算虚实坐标系变换矩阵,获取采集当前帧的计算摄像机外部参数并确定摄像机位姿；最后根据计算摄像机计算的上一帧得到的计算摄像机外部参数和摄像机位姿以及当前帧中目标的线特征信息更新当前的投影矩阵,进而实现实时注册；该方法缩小了当前帧对于目标所在区域的检测范围,提高了目标检测的效率和虚实物体位置匹配的效率。</td>   <td>1.一种基于线特征描述的无标识增强现实注册方法,其特征在于,包括以下步骤：S1：利用颜色差分直方图获取当前帧中兴趣目标所在的局部区域；S2：对当前帧中的兴趣目标使用高斯滤波来除去噪声和图像亮度调整；S3：对区域中目标进行轮廓特征提取,按检测到的线特征提取描述符,进行线特征匹配；S4：通过当前帧中二维和三维的对应关系计算虚实坐标系变换矩阵,获取采集当前帧的计算摄像机外部参数并确定摄像机位姿；S5：根据计算摄像机计算的上一帧得到的计算摄像机外部参数和摄像机位姿以及当前帧中目标的线特征信息更新当前的投影矩阵,进而实现实时注册；所述步骤S1中：首先获取视频流中兴趣目标完整出现的第一帧,并以该帧作为关键帧序列,在该关键帧图像中进行目标所在感兴趣区域分割,以此关键帧作为模版图像,然后同样在当前帧中对感兴趣区域中检测目标,将目标所在区域进行框定；所述步骤S3的具体过程如下：对当前帧进行二值化处理,然后对目标进行Canny边缘检测,计算最小链码的长度及相应点的曲率,并删除小于该曲率值的链码,保留各条线段的端点计算出各条线段的中点,利用线特征的几何分布特性进行线特征描述：采用直线的两个端点,坐标分别记为(x<Sub>1</Sub>,y<Sub>1</Sub>)和(x<Sub>2</Sub>,y<Sub>2</Sub>),一般方程为<Image id="icf0001" he="125" wi="367" file="FDA0002437561380000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>用极点到直线的距离z和直线与极轴的夹角θ作为表示直线的参数,以这两个参数所建立的坐标系和图像中的直线存在一一对应的关系；极点方程为z＝ycosθ+xsinθ,对兴趣目标进行轮廓提取后,通过逼近算法对轮廓形状分析,然后对模版图像及待配准图像比较两者的轮廓矩：对轮廓上所有点进行积分运算获取形状特征,定义一个轮廓的(p,q)矩：          <Image id="icf0002" he="128" wi="497" file="FDA0002437561380000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,p对应x维度上的矩,q对应y维度上的矩,阶数表示对应部分的指数,对轮廓边界上所有像素进行求和；所述步骤S4中,计算摄像机外部参数,选取相邻4条直线的中点作为空间坐标的三维空间点,并获取摄像机内部参数K,具体地：采用RANSAC算法建立当前帧关键点m<Sub>c</Sub>和模版图像关键点m<Sub>s</Sub>间的对应关系：          <Image id="icf0003" he="72" wi="204" file="FDA0002437561380000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        采用Tukey M-估计算法计算摄像机的位姿,其初始值为当前帧的前一帧的视点参数,该算法利用最小化误差残差：          <Image id="icf0004" he="134" wi="258" file="FDA0002437561380000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        获得参数的最优估计,ρ为连续对称函数,r<Sub>i</Sub>＝||m<Sub>i</Sub>-λP<Sub>i</Sub>M<Sub>i</Sub>||为图像的反投影误差,通过迭代优化求解摄像机的旋转和平移矩阵。</td>   <td>G06T7/80;G06T7/543;G06T7/13;G06T7/12;G06T7/564;G06T7/44;G06T7/33;G06T19/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              李传俊;                   谢晓华       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种在垂直视角下基于深度学习的客流计数方法</td>   <td>广东省</td>   <td>CN107103279B</td>   <td>2020-06-05</td>   <td>本发明提供一种在垂直视角下基于深度学习的客流计数方法,该方法垂直视角下进行人流统计,相比于斜拍视角,这个视角更容易应对商场、超市、地铁等公共场所人流很密集的情况；提出利用深度学习检测头肩,利用深度学习强大的学习能力,不需要对视频进行背景建模和前景提取,也不需要对前景进行行人切割,能够更精确更鲁棒地检测到头肩信息；进行匹配跟踪利用的是深度卷积特征,相对于HOG、LBP等手动设计的特征,深度卷积特征有更好的表达能力,能够更好的应对各种场景；本发明是直接将某一层的深度卷积特征拿来做匹配,避免了特征的重复计算,使得更加省时。</td>   <td>1.一种在垂直视角下基于深度学习的客流计数方法,其特征在于,包括以下步骤：S1：在视频画面内绘制进出统计线；S2：利用深度学习方法在当前画面内进行行人头肩检测；S3：判断当前画面是否存在头肩,如果是则转到步骤S4；如果不是则转到步骤S2,对下一帧继续检测；S4：将当前帧检测到的头肩与跟踪列表里的头肩进行匹配更新；S5：判断跟踪目标起始点和终止点是否在进出线两侧,如果是则转到步骤S6；如果不是则转到步骤S2,对下一帧继续检测；S6：更新进出的人数信息,接着转到步骤S2,对下一帧继续检测；所述步骤S1中当行人先经过进线再经过出线,表示离开状态,反之,当行人先经过出线再经过进线,表示进入状态；所述步骤S2中使用SSD深度学习检测方法,先在应用场景进行数据学习训练,接着对采集的视频的每一帧进行头肩检测：          <Image id="icf0001" he="91" wi="700" file="FDA0002425905350000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中N表示在多个深度卷积层匹配到的框的个数,如果N＝0,设置损失函数L()为0,<Image id="icf0002" he="77" wi="208" file="FDA0002425905350000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是一个指示器,表示类别p上第i个检测框与第j个真实框匹配；定位损失L<Sub>loc</Sub>使用的是预测框l与真实框g的L1范数损失：          <Image id="icf0003" he="97" wi="700" file="FDA0002425905350000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        置信度损失L<Sub>conf</Sub>采用的是头肩与背景置信度c的softmax损失：          <Image id="icf0004" he="77" wi="700" file="FDA0002425905350000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        基于上述的损失函数训练出来的模型收敛后可用于行人的头肩检测,给模型输入一张图片后可得到图片中头肩框的坐标位置以及是否为头肩的置信度,接着对采集的视频的每一帧利用该模型进行行人头肩的检测,用深度学习的方法不需要对视频进行背景建模和前景提取,也不需要对前景进行行人切割；所述步骤S4中将当前帧检测到的头肩与跟踪列表里的头肩进行匹配更新,初始时跟踪列表为空,当检测到画面内有头肩信息时,将头肩信息加入到跟踪列表内,接着使用匈牙利算法对跟踪列表里的头肩与当前帧检测到头肩进行匹配,其中匈牙利算法首先利用距离进行限制,头肩之间质心距离超过一个肩部距离的直接不匹配,接着衡量头肩之间的匹配相似度采用深度卷积层的特征进行度量,将检测到的头肩位置映射回深度卷积层对应的位置,若基础网络是VGG则采用conv4_3层的特征,将头肩的深度卷积特征统一归一化到一个固定的尺寸使得两两之间利用L2范数求得一个匹配相似度：          <Image id="icf0005" he="87" wi="458" file="FDA0002425905350000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中L表示匹配相似度,f<Sub>last</Sub>跟踪列表的头肩在conv4_3层的特征,f<Sub>now</Sub>表示当前 帧检测到的头肩在conv4_3层的特征；通过匈牙利算法的最优匹配原则将头肩之间匹配相似度最高的匹配上,剩下没有匹配上的再做下述步骤的处理；1)如果跟踪列表里有头肩没有匹配上,先暂时保留,如果8帧之后仍旧没有匹配上则认为该跟踪目标已离开将它删除；2)如果当前帧检测到的头肩没有匹配上,则认为它可能是新出现的跟踪目标,如果后续8帧有出现了5帧以上,则确认它为新出现的目标,将它加入跟踪列表,如果后续8帧该目标出现的次数小于2次,则认为它是噪声,将它删除。</td>   <td>G06K9/00;G06T7/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;                   冯展祥       </td>   <td>中山大学</td>   <td>一种基于视角关联与深度网络学习的行人再识别方法</td>   <td>广东省</td>   <td>CN107480631B</td>   <td>2020-06-05</td>   <td>本发明提供了一种基于视角关联与深度网络学习的行人再识别方法,其通过对每个视角建立视角关联的深度网络从而提取视角相关的底层视角特征,通过迭代的跨视角欧氏距离约束和跨视角中心度量约束(ICV-ECCL)约束不同网络之间的特征以减少不同视角之间行人特征的差异。实验表明,本发明能较大幅度地提高现有的深度网络在行人再识别上的性能,具有广泛的应用价值。</td>   <td>1.一种基于视角关联与深度网络学习的行人再识别方法,其特征在于：包括以下步骤：S1.通过行人图像预训练一个深度网络,并将该深度网络作为深度网络的初始化模型；S2.为摄像头的各个监控视角分别以步骤S1的初始化模型为基础构建一个深度网络,然后分别使用各个监控视角下的行人图像对相应监控视角的深度网络进行训练,训练过程中,利用迭代的跨视角欧氏距离约束和跨视角中心度量约束方法对不同视角的深度网络进行联合训练,减少来自不同视角的行人图像间的特征差距,直至深度网络的参数收敛；其中所述的跨视角欧氏距离约束的作用是减少不同的深度网络最后一层特征输出之间的欧氏距离,来自两个视角的行人图像的深度特征为<Image id="icf0001" he="96" wi="256" file="FDA0002408051220000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中上标{1,2}表示对应的视角,下标{i,j}表示对应的行人身份,{k<Sub>i</Sub>,k<Sub>j</Sub>}对应每个行人在对应视角下的某个训练样本,令{K<Sub>i</Sub>,K<Sub>j</Sub>}表示每个行人的训练样本数目,则跨视角欧氏距离约束为：<Image id="icf0002" he="149" wi="700" file="FDA0002408051220000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>所述的跨视角中心度量约束用于保证同类训练样本特征与不同视角下类中心的特征接近,从而保证特征的跨视角鲁棒性；来自两个视角的行人图像深度特征为<Image id="icf0003" he="108" wi="287" file="FDA0002408051220000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>计算出对应每一类以及所有样本的类中心为<Image id="icf0004" he="83" wi="301" file="FDA0002408051220000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>下标{i,j}表示对应的行人身份,{k<Sub>i</Sub>,k<Sub>j</Sub>}对应每个行人在对应视角下的某个训练样本,令{K<Sub>i</Sub>,K<Sub>j</Sub>}表示每个行人的训练样本数目,{M}表示行人类别总数,则跨视角中心度量约束为：          <Image id="icf0005" he="66" wi="700" file="FDA0002408051220000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        S3.对于某个监控视角下的目标行人图像及行人图像库,首先使用相应监控视角的深度网络对目标行人图像及行人图像库中的行人图像分别进行特征的提取,然后将从目标行人图像中提取的特征依次与从图像库中的行人图像中提取的特征进行匹配,基于匹配的结果确定目标行人图像的身份。</td>   <td>G06K9/00;G06K9/62;G06K9/46;G06F16/583</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              彭林;              曾衍瀚;              廖裕兴;              张浩;              张鑫;              陈翔;              陈荣军;                   路崇       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;佛山市顺德区中山大学研究院;中山大学</td>   <td>基于时隙预测的ALOHA防碰撞方法</td>   <td>广东省</td>   <td>CN107506674B</td>   <td>2020-06-05</td>   <td>本发明公开一种基于时隙预测的ALOHA防碰撞方法。本发明提供的防碰撞方法通过对下一个时隙进行预测,并根据预测的结果来对标签的SC值进行调整,从而加速识别进程,因此本发明提供的方法能够有效提高RFID系统的识别效率。</td>   <td>1.一种基于时隙预测的ALOHA防碰撞方法,其特征在于：包括有以下步骤：(1)阅读器向识别范围内的标签发送Query(Q)指令,开始一个清点周期,同时,将初始Q赋值给参照变量Qprior,其中Q为时隙计数器参数值,参照变量Qprior用于判断识别过程中Q值是否发生变动,每个标签在[0,2<Sup>Q</Sup>-1]间随机产生一个整数SC并载入自身的时隙计数器中；(2)整数SC为0的标签立刻响应,并反向反射一个16位的随机数RN16给阅读器；而整数SC为1的标签回传其随机数RN16的首位数据给阅读器；(3)阅读器检测是否有碰撞发生：a.若当前时隙没有标签响应,即为空闲时隙,令Q<Sub>fp</Sub>＝Q<Sub>fp</Sub>-C,Q<Sub>fp</Sub>为Q的浮点形式,C的取值区间为0.1&lt;C&lt;0.5；b.若只有一个标签响应,即为有效时隙,令Q<Sub>fp</Sub>保持不变,阅读器向响应的标签发送ACK(RN16)指令,响应的标签收到正确的ACK指令,立即转换到应答状态,并将EPC码回送给阅读器,成功识别标签；c.若存在两个或两个以上的标签响应,即为碰撞时隙,此时令Q<Sub>fp</Sub>＝Q<Sub>fp</Sub>+C；(4)当前时隙识别结束后,阅读器通过round()函数操作,将Q<Sub>fp</Sub>四舍五入取整得到整数Q,若整数Q与整数Qprior相比其值发生了变化,阅读器发送QueryAdjust指令利用Q对Qprior进行更新,并重置标签的SC值,然后进入步骤(2)；否则阅读器将对SC＝1的标签所回送的随机数RN16的首位数据进行处理,并以此预测下一时隙标签分布情况：d.若(2)中整数SC为1的标签的数量为1,则阅读器发送QueryRep(SC-1)指令,继续识别阅读器范围内的剩余标签,进入步骤(2)；e.若(2)中整数SC为1的标签的数量为0,预测下一个时隙为空闲时隙,此时令Q<Sub>fp</Sub>＝Q<Sub>fp</Sub>-C；f.若(2)中整数SC为1的标签的数量为2个或2个以上,预测下一个时隙为碰撞时隙,此时令Q<Sub>fp</Sub>＝Q<Sub>fp</Sub>+C；(5)完成对下一个时隙的预测后,阅读器对Q<Sub>fp</Sub>四舍五入取整得到整数Q,若整数Q与整数Qprior相比其值发生了变化,阅读器向标签发送QueryAdjust指令利用Q对Qprior进行更新,并重置标签的SC值,然后进入步骤(2)；否则阅读器向标签发送QueryRep(SC-2)指令,继续识别阅读器工作范围内的剩余标签,进入步骤(2)。</td>   <td>G06K7/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谢吉松;                   倪江群       </td>   <td>中山大学</td>   <td>基于深度学习的大容量抗打印/拍摄盲水印系统及方法</td>   <td>广东省</td>   <td>CN111223034A</td>   <td>2020-06-02</td>   <td>本发明提供的基于深度学习的大容量抗打印/拍摄盲水印方法,包括编码步骤和解码步骤,待处理图像由编码器进行分块将水印嵌入图像分块后,由编码器进行还原拼接,并对还原后的图像进行图像视觉保真；采用检测步骤对含水印图像进行检测；解码器则将含有水印信息的图像恢复为未保真状态；并对未保真状态的图像进行分块；解码器使用GPU并行对图像分块进行水印信息提取,得到二进制比特序列,进而解析出水印信息；一方面充分利用了每块图像的冗余空间进行水印信息的嵌入,提高整体的嵌入容量；另一方面减小图像尺寸,并行地对所有分块图像进行神经网络的计算,充分利用GPU的并行加速能力,从而提高运行速度,并且提高了水印检测的鲁棒性和实时性。</td>   <td>1.基于深度学习的大容量抗打印/拍摄盲水印系统,包括编码器和解码器,其特征在于：所述编码器中设置有水印嵌入网络和视觉保真网络；所述解码器中设置有水印恢复网络和水印提取网络；其中：所述编码器将待处理图像进行分块后由水印嵌入网络把水印嵌入图像分块中；所述编码器将嵌有水印的图像分块进行还原拼接后,由所述视觉保真网络进行图像视觉保真；所述解码器通过所述水印恢复网络将待处理图像恢复为未保真状态并对未保真状态的图像进行分块,由所述水印提取网络进行水印提取。</td>   <td>G06T1/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李勇;              王鲁平;              张志勇;              梁建雄;              丘昌镇;                   王亮       </td>   <td>中山大学</td>   <td>一种红外无人机目标检测方法及系统</td>   <td>广东省</td>   <td>CN111222511A</td>   <td>2020-06-02</td>   <td>本发明公开一种红外无人机目标检测方法,包括：对采集的红外图像进行预处理获得预处理图像,对预处理图像进行目标聚类获得红外图像中疑似目标的外形及位置；采用差分盒维数法对采集的红外图像进行处理,在图像中包含天空和地面背景时获得分形特征图；利用分形特征图提取出天空与地面的分界线,即天地线；去掉处于天地线以下的全部疑似目标,将天地线以上的天空区域内的疑似目标作为无人机待判目标；根据疑似目标的外形及预处理图像的局部灰度与预设参考特征的相似度判断,在待判目标中识别出无人机目标。并在此基础上,提供一种用于红外无人机目标检测系统,用于解决现有技术中虚警率高、漏检、难以做到实时处理等问题,提高检测能力。</td>   <td>1.一种红外无人机目标检测方法,其特征在于,包括：步骤S1,对采集的红外图像进行预处理获得预处理图像,对预处理图像分割、连通域标记获得红外图像中疑似目标的外形及位置；步骤S2,对采集的所述红外图像采用差分盒维数法进行处理,在所述红外图像中包含天空和地面背景时获得分形特征图；利用分形特征图提取出天空与地面的分界线,即天地线；所述步骤S2包括：步骤S21,将红外图像分成多个<Image id="icf0001" he="23" wi="52" file="631905DEST_PATH_IMAGE001.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>大小的子图像,将每个<Image id="icf0002" he="23" wi="52" file="789217DEST_PATH_IMAGE001.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>的子图像又分成多个<Image id="icf0003" he="19" wi="33" file="176336DEST_PATH_IMAGE002.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>大小的子区域,令尺度系数为：<I>w=s/M</I>,然后计算每个子图像的分形维数D；<I>M、s</I>分别为方形子图像和方形子区域的边长,单位均为像素；步骤S22,对每个子图像的分形维数用阈值进行二值化处理获得分形特征图；步骤S23,在分形特征图中,若当前行灰度值为255的白点个数比下一行白点的个数明显小时,则认为当前行为天地交界线；步骤S3,去掉处于天地线以下的全部疑似目标,将天地线以上的天空区域内的疑似目标作为待判目标；步骤S4,根据疑似目标的外形及预处理图像的局部灰度与预设参考特征的相似度判断,在待判目标中识别出无人机目标。</td>   <td>G06K9/32;G06K9/34;G06K9/40;G06T7/48</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李冠彬;              梁子仲;              刘劲;                   林倞       </td>   <td>中山大学</td>   <td>一种基于依赖树的深度学习视觉问答方法及系统</td>   <td>广东省</td>   <td>CN111222533A</td>   <td>2020-06-02</td>   <td>本发明公开了一种基于依赖树的深度学习视觉问答方法及系统,该方法包括：对问句-图像对输入进行预处理,提取图像的目标区域特征记为图像特征R,生成问句的词语特征记为问句特征E；使用自关注方法分别关注到图像以及问句的显著部分,更新图像及问句特征；通过协同关注方法,将图像和问句特征交替作为引导,更新图像及问句特征；将问句解析成依赖树,根据词语类型剪枝,将问句特征按词语分配到每个树结点；利用图像特征,从依赖树叶子结点往根结点流动,根据结点词语类型分为物体关注模块以及关系构建模块,将图像以及问句特征在模块流动过程中更新；将图像和问句特征进行融合,并通过全连接层分类得到问题的答案。</td>   <td>1.一种基于依赖树的深度学习视觉问答方法,包括如下步骤：步骤S1,对问句-图像对的输入进行预处理,提取图像I的目标区域特征,记为图像特征R,以及生成问句Q的问句词语特征,记为问句特征E；步骤S2,使用自关注的方法分别关注到图像以及问句的显著部分,从而更新图像特征以及问句特征；步骤S3,通过协同关注的方法,将步骤S2得到的图像和问句特征交替作为引导,更新图像特征及问句特征；步骤S4,将问句解析成依赖树,根据词语的类型进行剪枝,留下的结点词语属于关系与物体两种类型,将步骤S3得到的问句特征按照词语分配到每个树结点上步骤S5,利用步骤S3中得到的图像特征,从步骤S4得到的依赖树叶子结点开始往根结点流动,根据结点词语类型分为物体关注模块以及关系构建模块,将图像特征以及问句特征在模块流动过程中进行更新；步骤S6,利用步骤S5根据依赖树流动更新过后的图像和问句特征进行融合,并通过全连接神经网络分类得到问题的答案,即得到类别概率分布。</td>   <td>G06K9/62;G06K9/20;G06K9/46;G06F16/31;G06F16/36;G06F40/289;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苏航;              李召国;                   张怡       </td>   <td>中山大学深圳研究院</td>   <td>一种基于鱼眼图像校正的汽车后视行人检测报警方法</td>   <td>广东省</td>   <td>CN111199177A</td>   <td>2020-05-26</td>   <td>本发明公开了一种基于鱼眼图像校正的汽车后视行人检测报警方法。本发明包括：使用鱼眼镜头对车辆后方信息进行采集；对采集到的鱼眼图像提取有效区；针对图像边缘的畸变进行再次处理；检测图像中的行人并对行人进行识别与分割,并对运动的行人进行预测；根据行人与车尾的位置及行人运动趋势判断是否进行报警。本发明使用鱼眼镜头采集车辆行驶信息,克服了传统摄像头可视范围有限,安装数量多的弊端；通过多种图像处理算法结合使用对鱼眼图像进行去噪和畸变处理,使得精确识别行人成为可能；使用行人识别算法,对行人位置和运动趋势进行分析,最终达到对车辆行驶安全进行提示和保护的作用。</td>   <td>1.一种基于鱼眼图像校正的汽车后视行人检测报警方法,其特征在于,所述方法包括：使用鱼眼镜头对车辆后方信息进行采集；预处理,对采集到的鱼眼图像使用球面坐标定位算法提取有效区；使用双线性插值算法对预处理后的图像针对图像边缘的畸变进行再次处理；使用背景帧差法检测图像中的行人并对行人进行识别与分割,并使用灰色模型对运动的行人进行预测；根据行人与车尾的位置及行人运动趋势判断是否进行报警。</td>   <td>G06K9/00;G06T5/00;G06T7/194;G06T7/254;G06T7/70</td>  </tr>        <tr>   <td>中国专利</td>   <td>         吴贺俊;                   李嘉豪       </td>   <td>中山大学</td>   <td>一种基于SEHM特征图序列的动作识别方法</td>   <td>广东省</td>   <td>CN106778576B</td>   <td>2020-05-26</td>   <td>本发明提供的动作识别方法在进行动作识别时,是以本发明提出的SEHM(segment energy history maps)特征图作为底层特征进行动作识别的。通过合理选择算法中时间片长度等参数,计算相应的SEHM特征图序列并应用到神经网络进行预测,能够在动作识别上实现离线识别和在线识别的功能。且由于构建的SEHM特征图是与动作整体姿态的前后变化相关的,因此可以充分地利用动作变化过程中的动作信息,提高动作识别的准确度。同时,在进行SEHM特征图计算时对原始数据进行了一定的压缩,方法的复杂度和硬件的要求较低,并能做到在线的实时动作识别。</td>   <td>1.一种基于SEHM特征图序列的动作识别方法,其特征在于：包括以下步骤：S1.针对视频中选定的时间段长度为N帧的深度图序列,将深度图序列中每一帧的深度图投影到三个正交坐标系的不同平面中,得到三个正交的视角图：正视图、侧视图和俯视图；S2.对于每个视角图下的深度图序列,计算其相邻两帧的差值作为能量图,其中每帧能量图代表着前后帧的距离变化；然后根据能量图的具体数值和设定的阈值将能量图分为三种状态图：向前状态的二进制图、向后状态的二进制图或静态的二进制图； 具体如下：          <Image id="icf0001" he="139" wi="700" file="FDA0002209203910000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中<Image id="icf0002" he="65" wi="127" file="FDA0002209203910000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为在视角图v下第t帧的能量图；ε为所设的阈值；<Image id="icf0003" he="68" wi="319" file="FDA0002209203910000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image><Image id="icf0004" he="63" wi="149" file="FDA0002209203910000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示后一帧减去前一帧的差值的绝对值；i＝1,2,3,分别代表向前状态的二进制图、向后状态的二进制图和静态的二进制图；第t帧的状态图<Image id="icf0005" he="68" wi="397" file="FDA0002209203910000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>通过一个三通道矩阵EM<Sub>t</Sub>进行表示；S3.执行步骤S2后分别得到三个视角图下的状态图序列；将三个视角图的N帧状态图序列分别按照前后顺序平均分为S个时间片,S＝N/K,其中K表示每个时间片的长度；对于每个视角图下的状态图序列,按照从前到后的顺序依次选取一个时间片的状态图序列进行SEHM特征图的计算：S31.设第p次选择进行计算的时间片的状态图序列是从N帧状态图序列的第(p-1)*K+1帧开始而在第p*K帧结束的,则该时间片的SEHM特征图由以下公式和步骤S32计算得到：SEHM<Sub>p</Sub>＝max(SEHM<Sub>p</Sub>,EM<Sub>(p-1)*K+k</Sub>·k)其中,k的初始值为1,SEHM<Sub>p</Sub>是初始值被设为零的三通道矩阵；S32.令k＝k+1然后执行步骤S31的公式直至k＞K,最后经过标准化处理后输出SEHM<Sub>p</Sub>作为第p次选择进行计算的时间片的SEHM特征图；S4.通过步骤S31、S32得到三个视角图下各个时间片的SEHM特征图；S5.将三个视角图下相互对应的时间片的SEHM特征图进行融合,得到融合的以时间片为单位的SEHM特征图；S6.融合后的各个时间片的SEHM特征图构成SEHM特征图序列,将SEHM特征图序列输入到神经网络中,神经网络输出一列代表各个动作可能性的概率向量P,根据输出的概率向量P确定当前N帧深度图序列的动作识别结果；分别对三个视角图下的N帧状态图序列进行SEHM特征图的计算,然后对计算得到的三个视角图下的SEHM特征图进行融合,得到全局SEHM特征图；所述步骤S6中,全局SEHM特征图与各个时间片的SEHM特征图构成SEHM特征图序列,将该SEHM特征图序列输入神经网络中进行动作识别。</td>   <td>G06K9/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         蔡跃新;              郑亿庆;              余晋刚;              李远清;                   刘楚       </td>   <td>中山大学孙逸仙纪念医院;华南理工大学</td>   <td>一种基于深度学习的具有智能图像分类诊断功能的可视耳内镜</td>   <td>广东省</td>   <td>CN111191684A</td>   <td>2020-05-22</td>   <td>本发明公开了一种基于深度学习的具有智能图像分类诊断功能的可视耳内镜,包括镜体、检测笔,所述镜体具有显示屏,所述镜体内置有图像分类诊断器,所述图像分类诊断器与所述检测笔通过数据传输线相连接,所述检测笔用于检测获得患者的耳内镜图像,并且将检测得到的耳内镜图像传输给所述的图像分类诊断器,所述图像分类诊断器包括构建模块、训练模块、验证模块和诊断模块,该可视耳内镜能够实现患者内耳镜的可视化检测以及智能诊断。</td>   <td>1.一种基于深度学习的具有智能图像分类诊断功能的可视耳内镜,包括镜体(1)、检测笔(2),所述镜体(1)具有显示屏(11),其特征在于,所述镜体(1)内置有图像分类诊断器,所述图像分类诊断器与所述检测笔(2)通过数据传输线(3)相连接,所述检测笔(2)用于检测获得患者的耳内镜图像,并且将检测得到的耳内镜图像传输给所述的图像分类诊断器,所述图像分类诊断器包括构建模块、训练模块、验证模块和诊断模块,其中,构建模块：用于从医院病例数据库中选取耳内镜图像构建耳内镜数据集,将数据集划分为测试集以及训练集；训练模块：用于加载预训练的神经网络模型,在所得到的训练集上微调预训练的神经网络模型,获得训练得到的神经网络模型；验证模块：用于在测试集上验证所述训练模块训练得到的神经网络模型的性能,筛选出最优神经网络模型；诊断模块：通过所述验证模块获得的最优神经网络模型,用于对检测笔(2)检测到的患者的耳内镜图像进行智能分类诊断,输出该耳内镜图像的分类诊断结果,通过显示屏(11)将患者的耳内镜图像以及该耳内镜图像的分类诊断结果显示出来,从而实现耳内镜的可视化检测以及智能诊断。</td>   <td>G06K9/62;G06N3/04;G06N3/08;A61B1/227</td>  </tr>        <tr>   <td>中国专利</td>   <td>              段凯       </td>   <td>中山大学</td>   <td>一种跨流域调水效率评估方法</td>   <td>广东省</td>   <td>CN111191886A</td>   <td>2020-05-22</td>   <td>本发明公开了一种基于区域水资源模拟的跨流域调水效率评估方法,该方法通过模拟调水工程对区域水资源压力的影响范围与强度,辨识复杂环境变化背景下跨流域调水对水资源受益区的正面影响与对水资源受损区的负面影响,系统地衡量复杂变化环境下的跨流域调水效率,提出了“调入比与调出比之差”、“水资源压力缓解指数”等调水效率评价指标,为不同水文地质与经济社会背景下不同跨流域调水效率的一致性量化比较提供了一种简单有效的解决方法。</td>   <td>1.一种跨流域调水效率评估方法,其特征在于,包括以下步骤：S1.根据评估目的设定跨流域调水工程的不同环境背景,分别收集各环境背景下流域径流过程的数据并计算对应的区域水资源量；S2.设定在不考虑跨流域调水工程的受水区与供水区下游影响时,跨流域调水效率的效率评价指标,根据所述效率评价指标构建跨流域调水效率的评价模型；S3.设定跨流域调水工程的受水区与供水区下游所受影响的影响评价指标,根据所述影响评价指标构建跨流域调水效率的总体评价模型；S4.将步骤S1收集的所述数据及区域水资源量输入步骤S2的评价模型或步骤S3的总体评价模型,从而得到相应的跨流域调水效率评估结果。</td>   <td>G06Q10/06;G06Q50/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         曾青青;              赵小蕾;              刘金秀;              陈怡华;                   赵泽慧       </td>   <td>中山大学新华学院;广州菁泽信息技术有限公司</td>   <td>一种具有引导学生学习的智慧班牌系统</td>   <td>广东省</td>   <td>CN111192177A</td>   <td>2020-05-22</td>   <td>本发明公开一种具有引导学生学习的智慧班牌系统,包括电子班牌、信息发布模块、师资管理模块、课程管理模块和教学情况管理模块,其中：信息发布模块进行校园信息公布；师资管理模块对教师信息进行管理；课程管理模块对课程相关信息进行管理；教学情况管理模块对课程后续信息进行管理；所述信息发布模块、师资管理模块、课程管理模块和教学情况管理模块中的信息均可在所述电子班牌中显示。通过电子班牌替换传统班牌,与班级黑板报,墙体宣传栏等,展示丰富的班级风采与文化；还能将以往的历史班级风采、班级文化数字化保存；相对于传统班牌,本发明不仅能展示更多的班级信息,更能加强师生沟通交流,引导学生学习的积极性。</td>   <td>1.一种具有引导学生学习的智慧班牌系统,其特征在于,包括电子班牌、信息发布模块、师资管理模块、课程管理模块和教学情况管理模块,其中：信息发布模块进行校园信息公布；师资管理模块对教师信息进行管理；课程管理模块对课程相关信息进行管理；教学情况管理模块对课程后续信息进行管理；所述信息发布模块、师资管理模块、课程管理模块和教学情况管理模块中的信息均可在所述电子班牌中显示。</td>   <td>G06Q50/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郭裕兰;              马燕妮;              刘浩;                   文贡坚       </td>   <td>中山大学</td>   <td>一种基于点全局上下文关系推理的点云语义分割方法</td>   <td>广东省</td>   <td>CN111192270A</td>   <td>2020-05-22</td>   <td>本发明为一种基于点全局上下文关系推理的点云语义分割方法,所述方法包括：获取训练集T和测试集V；构建深度学习与全局上下文推理的点云数据语义分割网络；使用多分类的交叉熵损失函数,作为点云语义分割网络的损失函数；使用训练集,对点云数据语义分割网络进行P轮有监督的训练；将测试集输入到训练好的网络模型中进行语义分割,得到每一个点的分割结果。本发明的有益效果在于,利用一种基于深度学习和全局上下文推理的方法来解决3D点云语义分割的全局信息提取不足问题。在深度学习的基础上,加入的全局上下文推理模块通过使用通道注意力机制建模各个特征通道之间的关系,通过图卷积进一步传递和聚合通道间关系的全局信息,能够获取全局信息从而精化点云语义分割的结果。</td>   <td>1.一种基于点全局上下文关系推理的点云语义分割方法,其特征在于,所述方法包括：步骤1)获取训练集T和测试集V：步骤2)构建深度学习与全局上下文推理的3D点云数据语义分割网络：步骤3)使用多分类的交叉熵损失函数,作为3D点云语义分割网络的损失函数；步骤4)使用训练集T,对3D点云数据语义分割网络进行P轮有监督的训练,P≥50；步骤5)将测试集V输入到训练好的网络模型中进行语义分割,得到每一个点的分割结果。</td>   <td>G06T7/10;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘红梅;              张凤君;                   卢伟       </td>   <td>中山大学</td>   <td>基于L0正则化和模糊核后处理的图像盲去模糊的方法</td>   <td>广东省</td>   <td>CN107146202B</td>   <td>2020-05-19</td>   <td>本发明公开一种基于L0正则化和模糊核后处理的图像盲去模糊的方法,在图像复原的最优化模型中引入关于图像梯度、模糊核像素以及模糊核梯度稀疏性的先验信息,并以L0正则项的形式表现；其次,对最优化计算所得的模糊核根据其客观特性进行后处理,人为干预弥补最优化模型带来的不足,使复原所得的模糊核和中间图像更符合现实,最终复原图像质量进一步提高；最后,采用半二次分裂方法求解最优化模型,解法简洁,减少计算量,同时结合金字塔模型分层计算,所以本发明具有较高的鲁棒性,适用范围广。</td>   <td>1.一种基于L0正则化和模糊核后处理的图像盲去模糊的方法,其特征在于,包括以下步骤：S1：判断输入的原始模糊图像是否为灰度图像,若不是,则变换为灰度图像；S2：构造最优化模型求解模糊核,在模型中引入L0正则项,模型如公式(1)所示：          <Image id="icf0001" he="76" wi="700" file="FDA0002398846460000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中β、μ及λ为权重参数,x是模糊图像,y是清晰图像,k是模糊核,*是卷积运算符,<Image id="icf0002" he="53" wi="48" file="FDA0002398846460000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示梯度运算；S3：对步骤S2得到的模糊核进行骨架提取,并根据各非零点到骨架的距离进行加权,重新计算模糊核中各点大小；S4：利用步骤S3得到的新的模糊核采用非盲去模糊方法,对原始模糊图像中每个通道进行复原,再将每个通道的复原结果进行合成求得最终复原图形；在步骤S2中根据该最优化模型结合金字塔算法,采用基于半二次分裂方法对公式(1)中模型进行求解得到模糊核；在步骤S2中处理过程如下：S2.1：根据人工输入的模糊核大小k_size,下采样因子,及规定的最小核尺寸k_min_size计算金字塔模型的层级数目；S2.2：将公式(1)拆解为两个子过程,如公式(2)(3)：          <Image id="icf0003" he="98" wi="568" file="FDA0002398846460000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0004" he="51" wi="700" file="FDA0002398846460000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        两公式交替迭代计算,可求解公式(1)；S2.3：采用半二次分裂算法,公式(2)变形为如公式(4)所示：          <Image id="icf0005" he="91" wi="700" file="FDA0002398846460000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,u为引入辅助变量,σ为引入辅助参数,当σ接近于无穷时,公式(2) 与公式(4)求解得到的一致,x、u相互独立,分别求解,其中,初始化u＝0；S2.4：根据公式(4)求解u,如公式(5)所示：          <Image id="icf0006" he="102" wi="544" file="FDA0002398846460000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        由于公式(5)是像素层面的最小化问题,通过公式(6)直接求解：          <Image id="icf0007" he="214" wi="518" file="FDA0002398846460000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        S2.5：根据S2.4结果,公式(4)求解x过程变为解最小二乘问题,如(7)所示：          <Image id="icf0008" he="96" wi="636" file="FDA0002398846460000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        该问题可直接在频域求解,求解过程如(8)：          <Image id="icf0009" he="145" wi="700" file="FDA0002398846460000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中F(·)和F<Sup>-1</Sup>(·)分别代表傅里叶变换和反傅里叶变换,<Image id="icf0010" he="79" wi="100" file="FDA0002398846460000025.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是F(·)的复共轭形式,<Image id="icf0011" he="98" wi="317" file="FDA0002398846460000026.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image><Image id="icf0012" he="66" wi="58" file="FDA0002398846460000027.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0013" he="71" wi="65" file="FDA0002398846460000028.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>分别代表水平差分[1,-1]以及垂直差分[1,-1]<Sup>T</Sup>,<Image id="icf0014" he="84" wi="669" file="FDA0002398846460000029.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>u<Sub>x</Sub>、u<Sub>y</Sub>分别为u的水平、垂直差分；S2.6：调整σ＝2σ,若未超过限定的最大值1e<Sup>5</Sup>,则根据S2.5求得的x作为输入,再次执行S2.4,否则,执行S2.7；S2.7：采用半二次分裂算法,公式(3)变形为如公式(9)所示：          <Image id="icf0015" he="64" wi="700" file="FDA00023988464600000210.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,a、b为引入辅助变量,α、γ为引入辅助参数,当α、γ接近于无穷时,公式(3)与公式(9)求解得到的一致,k、a、b相互独立,分别求解,其中,初始化a＝0,b＝0；S2.8：根据公式(9)求解a、b,如公式(10)(11)所示：          <Image id="icf0016" he="96" wi="536" file="FDA00023988464600000211.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0017" he="100" wi="562" file="FDA0002398846460000031.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        由于公式(10)、(11)是像素层面的最小化问题,通过公式(12)(13)直接求解：          <Image id="icf0018" he="214" wi="543" file="FDA0002398846460000032.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0019" he="219" wi="569" file="FDA0002398846460000033.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        S2.9：根据S2.8结果,公式(9)求解k过程变为解最小二乘 问题,如(14)所示：          <Image id="icf0020" he="74" wi="700" file="FDA0002398846460000034.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        该问题可直接在频域求解,求解过程如(15)：          <Image id="icf0021" he="124" wi="700" file="FDA0002398846460000035.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中F(·)和F<Sup>-1</Sup>(·)分别代表傅里叶变换和反傅里叶变换,<Image id="icf0022" he="77" wi="100" file="FDA0002398846460000036.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是F(·)的复共轭形式,<Image id="icf0023" he="96" wi="315" file="FDA0002398846460000037.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image><Image id="icf0024" he="70" wi="62" file="FDA0002398846460000038.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0025" he="72" wi="68" file="FDA0002398846460000039.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>分别代表垂直差分[1,-1]<Sup>T</Sup>以及水平差分[1,-1],<Image id="icf0026" he="86" wi="656" file="FDA00023988464600000310.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>b<Sub>x</Sub>、b<Sub>y</Sub>分别为b的水平、垂直差分,对(15)规定阈值k_threshold,若k像素值小于k_threshold则k像素值取0,最后对k归一化；S2.10：调整μ＝2μ、λ＝2λ,若未超过限定的最大值1e<Sup>5</Sup>,则根据S2.9求得的k作为输入,再次执行S2.8,否则,执行S2.11；S2.11：若金字塔模型中所有层次求解完成,则保留k,执行S3,否则金字塔模型中未达到最大求解层次；金字塔模型当前层求解完成：将y上采样到金字塔模型中下一层级的对应尺寸作为输入,执行S2.2,否则更新参数σ＝max{σ/1.1,1e<Sup>-4</Sup>},μ＝max{μ/1.1,1e<Sup>-4</Sup>},λ＝max{λ/1.1,1e<Sup>-4</Sup>},执行S2.2。</td>   <td>G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张宇光;              周煊;              陈星;              李悦;              范修伟;              夏显茁;                   陈亮       </td>   <td>中山大学</td>   <td>一种结合航班信息和价格序列的机票价格预测方法</td>   <td>广东省</td>   <td>CN111178978A</td>   <td>2020-05-19</td>   <td>本发明公开一种结合航班信息和价格序列的机票价格预测方法,包括采集历史航班特征和近期价格序列,区分出航班连续特征、航班离散特征、价格连续特征和价格离散特征,并对航班离散特征和价格离散特征进行独热编码；以机器学习模型分别建立航班特征预测模型和价格序列预测模型；将航班连续特征和编码后的航班离散特征输入航班特征预测模型训练并优化模型的航班权重；将价格连续特征和价格离散特征输入价格序列预测模型训练并优化模型的序列权重；基于航班特征预测模型输出的航班预测价格、价格序列预测模型输出的序列预测价格,结合优化后的航班权重α和序列权重β构建目标预测函数得到预测结果。本发明降低机票价格预测中出现的误差,提高准确率。</td>   <td>1.一种结合航班信息和价格序列的机票价格预测方法,其特征在于,包括：S10采集历史航班特征和近期价格序列,将历史航班特征中分别提取航班连续特征和航班离散特征,并对航班离散特征进行独热编码；将近期价格序列按连续特征分别提取价格连续特征和价格离散特征,并对价格离散特征进行独热编码；S20以机器学习模型分别建立航班特征预测模型和价格序列预测模型；S30将航班连续特征和编码后的航班离散特征输入航班特征预测模型训练并优化模型的航班权重α；将价格连续特征和价格离散特征输入价格序列预测模型训练并优化模型的序列权重β；S40基于航班特征预测模型输出的航班预测价格P<Sup>static</Sup>、价格序列预测模型输出的序列预测价格P<Sup>dynamic</Sup>,结合优化后的航班权重α和序列权重β构建目标预测函数得到预测结果。</td>   <td>G06Q30/02;G06Q50/30;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         曾青青;              赵小蕾;              张海;              陈怡华;                   赵泽慧       </td>   <td>中山大学新华学院;广州菁泽信息技术有限公司</td>   <td>一种具有交互性的学习服务系统</td>   <td>广东省</td>   <td>CN111179132A</td>   <td>2020-05-19</td>   <td>本发明公开了一种具有交互性的学习服务系统,包括人机交互模块、基础数据管理模块、学生信息管理模块和教学资源与情况管理模块,其中：人机交互模块可显示基础数据管理模块、学生信息管理模块和教学资源与情况管理模块的信息；基础数据管理模块实现层次信息和学习中心信息管理；学生信息管理模块实现招生信息、学生信息、学生选课信息、学生成绩和毕业论文管理；教学资源与情况管理模块实现课程信息、学生作业、学生分组、学生学习和教学情况管理。本发明以按需提供网络资源的模式,可以创造出更高的水平的效率和经济性。</td>   <td>1.一种具有交互性的学习服务系统,其特征在于,包括人机交互模块、基础数据管理模块、学生信息管理模块和教学资源与情况管理模块,其中：人机交互模块可显示基础数据管理模块、学生信息管理模块和教学资源与情况管理模块的信息；基础数据管理模块实现层次信息和学习中心信息管理；学生信息管理模块实现招生信息、学生信息、学生选课信息、学生成绩和毕业论文管理；教学资源与情况管理模块实现课程信息、学生作业、学生分组、学生学习和教学情况管理。</td>   <td>G06Q50/20;G06F16/958;H04L29/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林浩添;              李中文;              郭翀;                   林铎儒       </td>   <td>中山大学中山眼科中心</td>   <td>一种识别视网膜出血图像的人工智能方法及系统</td>   <td>广东省</td>   <td>CN111179258A</td>   <td>2020-05-19</td>   <td>本发明涉及医学图像处理领域,更具体地涉及一种识别视网膜出血图像的人工智能方法及系统,包括以下步骤：对卷积神经网络进行深度学习训练得到识别视网膜出血模型；将广域眼底图像输入到所述识别视网膜出血模型中,判断所述广域眼底图像是否出现视网膜出血；当判断存在视网膜出血时,在所述广域眼底图像上定位视网膜出血病灶部位。本发明依靠人工智能深度学习的敏感性和准确性对广域眼底图像的视网膜进行分析,使得视网膜出血的早期筛查更准确、更智能、更便携,有利于提高筛查效率,减少其对人群造成不可逆的损害。</td>   <td>1.一种识别视网膜出血图像的人工智能方法,其特征在于,包括以下步骤：对卷积神经网络进行深度学习训练得到识别视网膜出血模型；将广域眼底图像输入到所述识别视网膜出血模型中,判断所述广域眼底图像是否出现视网膜出血；当判断存在视网膜出血时,在所述广域眼底图像上定位视网膜出血病灶部位。</td>   <td>G06T7/00;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林庆健;                   张东       </td>   <td>中山大学</td>   <td>一种基于深度学习的人声检测算法</td>   <td>广东省</td>   <td>CN111179972A</td>   <td>2020-05-19</td>   <td>本发明涉及一种基于深度学习的人声检测算法。包括特征提取器、Resnet网络以及LSTM网络；所述的特征提取器用于从输入音频信号中提取梅尔频谱特征；所述的Resnet网络用于将连续输入的梅尔频谱特征在时间维度上进行压缩,将输入的T帧特征降低为T/8帧的同时保留人声检测的信息,从而减少后续LSTM的计算量；所述的LSTM网络采用两层的LSTM网络级联,输出连接全连接层后得到当前输入帧是否有人说话的预测,1表示有人说话,0则反之。本发明提供的一种基于深度学习的人声检测算法,加入了Resnet结构对信号在时间维度上进行压缩,从而减少后续LSTM网络的计算成本,相较于基于LSTM的实现方案而言,运算时间仅为其1/8左右,提高系统整体的运算速度。</td>   <td>1.一种基于深度学习的人声检测算法,其特征在于,包括特征提取器、Resnet网络以及LSTM网络；所述的特征提取器用于从输入音频信号中提取梅尔频谱特征；所述的Resnet网络用于将连续输入的梅尔频谱特征在时间维度上进行压缩,将输入的T帧特征降低为T/8帧的同时保留人声检测的信息,从而减少后续LSTM的计算量；所述的LSTM网络采用两层的LSTM网络级联,输出连接全连接层后得到当前输入帧是否有人说话的预测,1表示有人说话,0则反之。</td>   <td>G10L25/60;G10L25/12;G10L25/18;G10L25/24;G10L25/30;G10L25/45;G10L15/14</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              王弘焌;              王广润;                   张冬雨       </td>   <td>中山大学</td>   <td>一种行人重识别系统对抗样本生成方法及系统</td>   <td>广东省</td>   <td>CN111160217A</td>   <td>2020-05-15</td>   <td>本发明公开了一种行人重识别系统对抗样本生成方法及系统,该方法包括：S1,将原始图片输入至基于残差网络的生成器中,生成对抗扰动；S2,将对抗扰动按位加至原始图片,生成粗对抗样本I′,并将其与原始图片作特征连接,输入多阶段判别器以生成可控制对抗扰动点数量的二元掩模图；S3,将二元掩模图与对抗扰动按位乘法,并添加到原始图片中,生成扰动点数量可控的对抗样本<Image he="75" wi="63" file="DDA0002336945180000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>S4,将对抗样本<Image he="63" wi="39" file="DDA0002336945180000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>输入至待攻击的行人重识别模型<Image he="50" wi="65" file="DDA0002336945180000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>将模型的返回值作为特征混淆损失函数、对抗学习损失函数、经平滑的分类混淆函数及多尺度结构相似损失函数的输入；S5,多次迭代式地进行S1-S4的训练过程,更新生成器与多阶段判别器的参数。</td>   <td>1.一种行人重识别系统对抗样本生成方法,包括如下步骤：步骤S1,将原始输入图片I输入至基于残差网络的生成器<Image id="icf0001" he="75" wi="44" file="FDA0002336945150000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>中,生成对抗扰动<Image id="icf0002" he="70" wi="89" file="FDA0002336945150000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>步骤S2,将步骤S1中生成的对抗扰动<Image id="icf0003" he="62" wi="62" file="FDA0002336945150000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>按位加至原始输入图片I,生成粗对抗样本I′,并将其与原始输入作特征连接,输入至多阶段判别器<Image id="icf0004" he="61" wi="73" file="FDA0002336945150000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>以生成可控制对抗扰动点数量的二元掩模图<Image id="icf0005" he="69" wi="97" file="FDA0002336945150000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>步骤S3,将所述二元掩模图<Image id="icf0006" he="57" wi="70" file="FDA0002336945150000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>与所述对抗扰动<Image id="icf0007" he="71" wi="60" file="FDA0002336945150000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>施以按位乘法,并添加到原始输入图片I中,生成扰动点数量可控的对抗样本<Image id="icf0008" he="74" wi="65" file="FDA0002336945150000018.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>步骤S4,将步骤S3中生成的对抗样本<Image id="icf0009" he="77" wi="36" file="FDA0002336945150000019.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>输入至待攻击的行人重识别模型<Image id="icf0010" he="63" wi="66" file="FDA00023369451500000110.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>将模型的返回值作为特征混淆损失函数<Image id="icf0011" he="71" wi="246" file="FDA00023369451500000111.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>对抗学习损失函数<Image id="icf0012" he="69" wi="163" file="FDA00023369451500000112.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>经平滑的分类混淆函数<Image id="icf0013" he="78" wi="232" file="FDA00023369451500000113.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>及多尺度结构相似损失函数<Image id="icf0014" he="72" wi="98" file="FDA00023369451500000114.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>的输入；步骤S5,多次迭代式地进行步骤S1-S4的训练过程,更新所述生成器<Image id="icf0015" he="77" wi="38" file="FDA00023369451500000115.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>与多阶段判别器<Image id="icf0016" he="56" wi="52" file="FDA00023369451500000116.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>的参数,步骤S3中最后生成的对抗样本<Image id="icf0017" he="72" wi="44" file="FDA00023369451500000117.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>即为经过优化的可欺骗行人重识别系统的对抗样本。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;                   程海杰       </td>   <td>中山大学</td>   <td>一种基于生成对抗网络的漫画人物身份识别方法</td>   <td>广东省</td>   <td>CN111160264A</td>   <td>2020-05-15</td>   <td>本发明公开了一种基于生成对抗网络的漫画人物身份识别方法,包括：获取真实行人和漫画行人图片,建立包含真实行人的检索库和包含漫画行人的查询库；构建人脸检测对齐模型,对检索库和查询库中的图片进行人脸检测对齐；构建漫画生成模型,将真实行人和人脸转换成对应的漫画图片；构建漫画人物身份识别模型,提取检索库和查询库中图片的融合特征,计算漫画行人和真实行人融合特征间的相似度分数；计算漫画行人融合特征间的相似度分数,利用漫画行人间的相似度分数对漫画行人与真实行人间的相似度分数进行重排序,设定阈值获取查询库中漫画行人在检索库中所对应的真实行人。本发明对漫画人物身份识别具有精度高、速度快的优点。</td>   <td>1.一种基于生成对抗网络的漫画人物身份识别方法,其特征在于,包括步骤：步骤S1：获取真实行人和漫画行人图片,建立包含真实行人的检索库和包含漫画行人的查询库；步骤S2：构建人脸检测对齐模型,对检索库和查询库中的图片进行人脸检测对齐；步骤S3：构建漫画生成模型,将真实行人和人脸转换成对应的漫画图片；步骤S4：构建漫画人物身份识别模型,提取检索库和查询库中图片的融合特征,计算漫画行人和真实行人融合特征间的相似度分数；步骤S5：计算漫画行人融合特征间的相似度分数,利用漫画行人间的相似度分数对漫画行人与真实行人间的相似度分数进行重排序,设定阈值获取查询库中漫画行人在检索库中所对应的真实行人。</td>   <td>G06K9/00;G06F16/58;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林浩添;              李中文;              郭翀;                   林铎儒       </td>   <td>中山大学中山眼科中心</td>   <td>一种广域眼底图像质量控制的方法及人工智能系统</td>   <td>广东省</td>   <td>CN111161257A</td>   <td>2020-05-15</td>   <td>本发明涉及一种广域眼底图像质量控制的方法及人工智能系统,其中所述方法包括以下步骤：A1.接收广域眼底照相仪拍摄的广域眼底图片；A2.将所述广域眼底图片输入已训练的二分类模型,判断所述广域眼底图片是否为低质量图片；A3.当判断所述广域眼底图片为低质量图片时,发送重新拍摄提示；A4.判断发送重新拍摄提示的次数,若未超过预设的次数则返回步骤A1,若超过预设次数则发出转诊的提示。通过将拍摄好的广域眼底图片输入已训练的二分类模块进行广域眼底图片质量的判断,当图片被判断为低质量图片时,发送重新拍摄提示,从而实现对广域眼底图片的质量进行实时、准确的质量控制。</td>   <td>1.一种广域眼底图像质量控制的方法,其特征在于,包括以下步骤：A1.接收广域眼底照相仪拍摄的广域眼底图片；A2.将所述广域眼底图片输入已训练的二分类模型,判断所述广域眼底图片是否为低质量图片；A3.当判断所述广域眼底图片为低质量图片时,发送重新拍摄提示；A4.判断发送重新拍摄提示的次数,若未超过预设的次数则返回步骤A1,若超过预设次数则发出转诊提示。</td>   <td>G06T7/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;                   邓卓爽       </td>   <td>中山大学</td>   <td>一种基于深度学习的自然图像抠图方法</td>   <td>广东省</td>   <td>CN111161277A</td>   <td>2020-05-15</td>   <td>本发明公开了一种基于深度学习的自然图像抠图方法,其包括以下步骤：获取抠图数据集,并进行数据增强；搭建具有编码器-解码器结构的自然图像抠图模型,为保留细节信息,设计编码器使其下采样倍数为4,为弥补下采样倍数下降带来的感受野变小,引入空洞卷积扩大感受野,保存最大池化操作中最大像素位置,以便为上采样阶段提供位置信息；为解决多尺度问题,在编码器顶部连接一个空洞空间金字塔模块；在解码器中设计全局语境模块,用于融合所述编码器与解码器对应的高层特征；最后训练并测试。本发明在提取特征过程中保留更多细节信息,同时关联多尺度特征,使模型能捕抓到全局信息,有利于模型处理细节以及大面积透明物体,提升抠图质量。</td>   <td>1.一种基于深度学习的自然图像抠图方法,其特征在于,包括步骤：S1：获取抠图数据集,把所述抠图数据集中样本划分为训练集与测试集；S2：搭建具有编码器-解码器结构的自然图像抠图模型,该模型中所述编码器的下采样倍数为4,结合普通卷积和空洞卷积操作；编码器顶部连接一个空洞空间金字塔池化模块；所述解码器包含全局语境模块,用于融合所述编码器与解码器对应的高层特征；S3：初始化并训练模型,利用所述训练集中的alpha蒙版生成三元图,把原图与三元图作为模型输入,对模型进行训练,模型输出为预测的alpha蒙版,计算预测的alpha蒙版与真实alpha蒙版之间的误差,保存在所述测试集上表现最好的模型；S4：将需要测试的图片及对应的三元图输入到已训练好的自然图像抠图模型中,得到预测的alpha蒙版。</td>   <td>G06T7/11;G06T7/194;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         龚林;              张磊;              徐世友;                   陈曾平       </td>   <td>中山大学</td>   <td>基于ISAR图像的目标尺寸提取方法</td>   <td>广东省</td>   <td>CN111161341A</td>   <td>2020-05-15</td>   <td>本发明公开了一种基于ISAR图像的目标尺寸提取方法。该方法通过作出二维ISAR图像的一维高分辨像,使用恒虚警率检测法对一维高分辨像进行检测,并根据检测结果得到待测目标的第一次CFAR检测区域；对二维ISAR图像进行区域重确定,得到区域重确定后的二维ISAR图像,根据区域重确定后的二维ISAR图像外的图像噪声水平,对检测阈值进行更新,并再次通过恒虚警率检测法对一维高分辨像进行检测,从而在二维ISAR图像得到目标检测区域,最终得到目标的尺寸数据。通过使用本发明中的方法,能够减少目标检测的处理用时,提高了检测效率；还能够有效减少目标边缘漏检,提高了目标检测的精度。本发明可广泛应用于雷达技术领域内。</td>   <td>1.基于ISAR图像的目标尺寸提取方法,其特征在于,包括以下步骤：获取含待测目标的二维ISAR图像,作出所述二维ISAR图像距离维和方位维的一维高分辨像；通过恒虚警率检测法对所述一维高分辨像进行检测,根据检测结果在二维ISAR图像得到待测目标的第一次CFAR检测区域；基于所述第一次CFAR检测区域,对所述二维ISAR图像进行区域重确定,得到区域重确定后的二维ISAR图像,所述区域重确定后的二维ISAR图像小于原二维ISAR图像；根据所述二维ISAR图像中区域重确定后的二维ISAR图像外的图像噪声水平,对恒虚警率检测法中的检测阈值进行更新；基于更新后的检测阈值,再次通过恒虚警率检测法对所述一维高分辨像进行检测,根据检测结果在二维ISAR图像得到目标检测区域；基于所述目标检测区域,获取目标的尺寸数据。</td>   <td>G06T7/60;G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         万海;              欧佳玲;              曾娟;                   王宝亿       </td>   <td>中山大学</td>   <td>面向移动端虚拟现实与增强现实的场景图谱生成方法</td>   <td>广东省</td>   <td>CN111144492A</td>   <td>2020-05-12</td>   <td>本发明提供面向移动端虚拟现实和增强现实的场景图谱生成方法,涉及场景图谱领域。包括：利用场景图谱样本集,提取视觉关系组合、常识信息；通过目标检测模型检测出图像中物体的物体框及其物体类别；获取与物体对应的语义信息,根据常识信息进一步构造常识知识图,生成视觉关系f1；提取物体框区域的视觉信息,生成视觉关系f2；结合常识信息和视觉信息,通过注意力机制将f1、f2结合,生成场景图谱,获得并优化场景图谱生成模型。本发明在移动端运行场景图谱生成模型,能快速识别虚拟现实或增强现实画面中目标物体及物体之间的关系,相比服务器端检测具有更高的检测效率,为面向移动端虚拟现实和增强现实进行场景图谱生成提供技术可行性。</td>   <td>1.面向移动端虚拟现实和增强现实的场景图谱生成方法,其特征在于,包括以下步骤：S1、利用场景图谱样本集,提取并统计视觉关系组合,提取常识信息；S2、通过训练好的目标检测模型检测出输入场景图谱样本集图像中的物体,生成若干个物体框,并预测出物体框对应的物体类别；S3、根据步骤S2得到的物体类别获取与物体对应的语义信息,再根据步骤S1所提取的常识信息,进一步构造常识知识图,生成初步的视觉关系f1；S4、通过神经网络模型提取所述物体框的视觉信息,包括视觉特征、空间特征和语义特征,生成初步的视觉关系f2；S5、结合视觉信息和常识信息,通过注意力机制,将初步的视觉关系f1、初步的视觉关系f2的检测结果结合,进行场景图谱生成,并得到场景图谱生成模型；S6、虚拟现实与增强现实系统包括移动端、计算机显示终端,对得到的场景图谱生成模型进行常识知识图和模型参数的优化,从而获取能够嵌入移动端的模型,以现实场景图像作为输入,并传输给计算及显示终端；计算及显示终端接收到现实场景图像,进行场景图谱生成,抽取对应现实场景图的视觉关系,叠加在现实场景图像中,获得当前现实或虚拟场景的场景图谱。</td>   <td>G06K9/62;G06K9/00;G06T19/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              朱允全;                   谢晓华       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于颜色和距离聚类的多目标跟踪方法</td>   <td>广东省</td>   <td>CN106951841B</td>   <td>2020-05-12</td>   <td>本发明提供一种基于颜色和距离聚类的多目标跟踪方法,该方法减少了跟踪目标形变带来的影响,增加目标之间的区分度,并降低了对检测器的依赖程度；该方法结合前后帧的位置信息和目标的直方图信息通过对前景点进行聚类,并计算前景点得分,最后定位目标位置,并不断更新目标的直方图特征和位置信息。</td>   <td>1.一种基于颜色和距离聚类的多目标跟踪方法,其特征在于,包括以下步骤：S1：计算距离得分；S2：计算颜色得分；S3：加权得分；S4：聚类；S5：目标回归；S6：更新距离和直方图模型；所述步骤S1的具体过程如下：对当前帧前景中每一个像素点,根据其和前一帧各个目标的距离,计算其属于前一帧中各个目标的可能性作为该点的属于各个目标类别的得分,得分为一个向量,向量的维数表示可能的目标数：          <Image id="icf0001" he="76" wi="592" file="FDA0002400681250000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0002" he="233" wi="700" file="FDA0002400681250000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        d(p)＝||p-c<Sup>n</Sup>||<Sup>2</Sup>……(3)对于一帧图像x<Sub>t</Sub>,应用高斯混合背景建模得到运动区域的前景掩模m<Sub>t</Sub>,m<Sub>t</Sub>是一张二值图像,像素值为1代表的是运动区域,0代表的是背景区域,将把m<Sub>t</Sub>里面为1的像素点所对应x<Sub>t</Sub>的像素点统称为前景点,用集合S<Sub>t</Sub>表示；t为某一时刻,我们现有的信息是前一时刻t-1时,目标的个数N,其中c<Sup>n</Sup>代表的是第n个目标的中心,每一个目标称为每一个类别,对于前景里某一点p,则d(p)表示的是p和类别中心c<Sup>n</Sup>的欧氏距离,函数g(d)计算的是p属于每一个类别的距离得分,其中g函数中3σ等于该视角下目标在画面里面的宽度,对于某一类别n,如果点p距离类别中心大于3σ,该点属于该类别的可能性为0,所以距离得分也为0,如果距离小于3σ,则按照高斯函数计算其距离得分,f<Sub>dist</Sub>(p)表示前景点分别属于各个类别的距离得分,用N维列向量表示:          <Image id="icf0003" he="61" wi="700" file="FDA0002400681250000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image></td>   <td>G06K9/00;G06K9/38;G06K9/46;G06K9/62;G06T7/143</td>  </tr>        <tr>   <td>中国专利</td>   <td>         齐志新;              丁昱;                   张慧       </td>   <td>中山大学</td>   <td>基于夜间灯光及高分辨率遥感影像的自然保护区人类活动自动监测方法及系统</td>   <td>广东省</td>   <td>CN111144340A</td>   <td>2020-05-12</td>   <td>本发明公开了基于夜间灯光及高分辨率遥感影像的自然保护区人类活动自动监测方法,包括以下步骤：获取同一光源在不同时间的夜间灯光影像；对所有的夜间灯光影像进行联合分割,对同一光源在不同时间内产生的灯光区域归属为同一图像对象；提取所有的图像对象的属性；从图像对象中提取夜间灯光斑块图像；根据得到的夜间灯光区域对高分辨率遥感影像进行剪裁,剪裁后的遥感影像和灯光斑块图像范围相同；对与灯光斑块对应的高分辨遥感影像进行检测、分析,自动提取人类活动信息。本发明还公开了基于上述方法所对应的自然保护区人类活动自动监测系统,包括图像获取模块、图像分割模块、属性提取模块、灯光斑块提取模块、剪裁模块、分析模块。</td>   <td>1.基于夜间灯光及高分辨率遥感影像的自然保护区人类活动自动监测方法,其特征在于,包括以下步骤：获取同一光源在不同时间的夜间灯光影像；对所有的夜间灯光图像进行联合分割,对同一光源在不同时间内产生的灯光区域归属为同一图像对象；提取所有的图像对象的属性；从图像对象中提取夜间灯光区域；根据灯光区域图像对高分辨率遥感影像进行剪裁,剪裁后的遥感影像和灯光斑块图像范围相同；通过对裁剪后的高分辨率遥感影像进行检测、分析,结合夜间灯光区域,得到人类活动信息。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              彭林;              曾衍瀚;              陈翔;              廖裕兴;              张浩;              张鑫;              陈荣军;                   路崇       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;佛山市顺德区中山大学研究院;中山大学</td>   <td>基于二进制散列的ALOHA防碰撞方法</td>   <td>广东省</td>   <td>CN107590408B</td>   <td>2020-05-12</td>   <td>本发明提供的基于二进制散列的ALOHA防碰撞方法通过引入二进制散列处理来对发生碰撞的标签进行时分处理,整个方法过程中无需频繁地下发QueryAdjust或者QueryRep指令来进行重置,因此其识别效率与现有技术相比得到了提高。</td>   <td>1.基于二进制散列的ALOHA防碰撞方法,其特征在于：包括有以下步骤：(1)阅读器通过广播形式向位于读取范围内的待识别标签发送Query(Q)指令,开始一个清点周期,其中Q为时隙计数器参数值,每个标签在[0,2<Sup>Q</Sup>-1]间随机产生一个整数SC并载入自身的时隙计数器中；(2)整数SC为0的标签立刻响应,并反向反射一个随机数RN16给阅读器；(3)阅读器检测是否有碰撞发生：a.若当前时隙没有标签响应,即为空闲时隙,令Q<Sub>fp</Sub>＝Q<Sub>fp</Sub>-C,Q<Sub>fp</Sub>为Q的浮点形式,C的取值区间为0.1&lt;C&lt;0.5；b.若只有一个标签响应,即为有效时隙,令Q<Sub>fp</Sub>保持不变,阅读器向响应的标签发送ACK(RN16)指令,响应的标签收到正确的ACK指令,立即转换到应答状态,并将EPC码回送给阅读器,成功识别标签；c.若存在两个或两个以上的标签响应,即为碰撞时隙,此时判断响应的标签的数量是否大于碰撞处理门限,若是则令Q<Sub>fp</Sub>＝Q<Sub>fp</Sub>+C,然后进入步骤(4),若否则令Q<Sub>fp</Sub>＝Q<Sub>fp</Sub>+C,然后进行二进制散列处理：c1.将响应的标签的整数SC随机置0或置1,其余未响应的标签的整数SC加1；c2.进行二进制散列处理后,若整数SC为0的标签数还大于或等于2,即执行步骤c1；若进行二进制散列处理后整数SC为0的标签数为1,此时执行步骤b；(4)当前时隙识别结束后,阅读器对Q<Sub>fp</Sub>四舍五入取整得到整数Q’,若整数Q’与整数Q相比其值发生了变化,阅读器发送QueryAdjust指令更新Q值,并重置所有标签的整数SC值；否则发送QueryRep指令使标签的整数SC值自减,继续识别后续标签,进入步骤(2)。</td>   <td>G06K7/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;                   陈殷齐       </td>   <td>中山大学</td>   <td>一种面向视频拼接的透视畸变矫正方法</td>   <td>广东省</td>   <td>CN111127361A</td>   <td>2020-05-08</td>   <td>本发明公开一种面向视频拼接的透视畸变矫正方法,包括采集具有运动关系且不同时刻拍摄的数张图像；识别图像的特征点并在图像间进行2-近邻匹配生成特征点匹配对；构造无透视畸变的参考平面,以此建模坐标位置向量：定义特征点及其在特征点匹配对的位移矢量；构建坐标位置向量的能量函数,能量函数包括特征点在其特征点匹配对的位移矢量与所有特征点在各自特征点匹配对的位移矢量均值的平行约束、特征点在其特征点匹配对中X坐标位移差与Y坐标位移差总和的约束、特征点在其特征点匹配对中垂直自由度约束；采用稀疏线性解算器最小化能量函数得到矫正的坐标位置向量。本发明在视频拼接时矫正了图像中远处景物的失真现象。</td>   <td>1.一种面向视频拼接的透视畸变矫正方法,其特征在于,包括如下步骤：S10采集具有运动关系且不同时刻拍摄的数张图像；S20识别图像中的特征点并在图像间进行2-近邻匹配生成特征点匹配对；S30构造无透视畸变的参考平面,以此建模X0Y坐标位置向量v：定义图像中的特征点p＝w<Sup>T</Sup>v,其中p表示特征点,w为双线性插值权重,矫正后图像中的特征点位置为<Image id="icf0001" he="78" wi="239" file="RE-FDA0002414275390000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image><Image id="icf0002" he="76" wi="147" file="RE-FDA0002414275390000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>分别表示经过相同单应变换后的p和v；定义每个特征点在其特征点匹配对的位移矢量<Image id="icf0003" he="78" wi="324" file="RE-FDA0002414275390000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中<Image id="icf0004" he="78" wi="72" file="RE-FDA0002414275390000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示参考图像上的第i个特征点的坐标位置,<Image id="icf0005" he="77" wi="74" file="RE-FDA0002414275390000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示源图像上的第i个特征点的坐标位置,以图像中第一张图像为参考图像,其他图像为源图像；S40构建图像坐标位置向量v的能量函数：E(v)＝E<Sub>p</Sub>(v)+αE<Sub>d</Sub>(v)+βE<Sub>a</Sub>(v),其中E<Sub>p</Sub>(v)为每个特征点在其特征点匹配对的位移矢量与所有特征点在各自特征点匹配对的位移矢量均值的平行约束,E<Sub>d</Sub>(v)为每个特征点在其特征点匹配对中基于平面仿射坐标的X坐标位移差与Y坐标位移差总和的约束,E<Sub>a</Sub>(v)为每个特征点在其特征点匹配对中垂直自由度约束,α为平衡E<Sub>d</Sub>(v)的权重系数,β为平衡E<Sub>a</Sub>(v)的权重系数；S50采用稀疏线性解算器最小化能量函数E(v),以优化v得到矫正坐标位置向量<Image id="icf0006" he="60" wi="76" file="RE-FDA0002414275390000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image></td>   <td>G06T5/00;G06T3/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谢晓华;              宋展仁;                   赖剑煌       </td>   <td>中山大学</td>   <td>一种基于深度表示学习跟动态匹配的行人再识别方法</td>   <td>广东省</td>   <td>CN111126198A</td>   <td>2020-05-08</td>   <td>本发明公开了一种基于深度表示学习跟动态匹配的行人再识别方法,用于判别在不同时间或者区域的行人身份。包括：构建特征提取模型,用于提取全局、局部特征,利用全局特征、局部特征进行联合学习；实现不同行人局部特征之间的动态匹配,使用三元组损失函数进行学习模型；取检索库和查询库中行人图片的全局特征,计算查询库行人和检索库行人的全局特征间的相似度分数,并利用相似度分数进行排序,获取查询库中行人在检索库中所对应的行人。本发明利用全局特征跟局部特征进行联合学习,其中实现了局部特征之间的对齐,这样使得模型学习到的全局特征同时关注了局部信息跟全局信息。缓解了行人再识别中的局部不对齐问题,提升了模型再识别的性能。</td>   <td>1.一种基于深度表示学习跟动态匹配的行人再识别方法,其特征在于,包括步骤：步骤S1：获取不同摄像头下的行人图片,构建行人检索库和查询库；步骤S2：构建局部特征、全局特征提取模型,利用全局特征、局部特征进行联合学习；步骤S3：实现不同行人局部特征之间的动态匹配,使用三元组损失函数进行模型学习；步骤S4：提取检索库和查询库中行人图片的全局特征,计算查询库行人和检索库行人的全局特征间的相似度分数,并利用相似度分数进行排序,获取查询库中行人在检索库中所对应的行人。</td>   <td>G06K9/00;G06K9/34;G06K9/46;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈跃欣;              陈童欣;              丘昌鑫;              黄春丰;              莫伟健;              李国城;                   蔡旭东       </td>   <td>中山大学新华学院</td>   <td>一种盲人服务系统</td>   <td>广东省</td>   <td>CN111126341A</td>   <td>2020-05-08</td>   <td>本发明涉及一种盲人服务系统,包括摄像头、客户端、服务端和播音器；其中：摄像头输出端与客户端的输入端电性连接,客户端的输出端与服务端的输入端电性连接,服务端的输出端与客户端的输入端电性连接,客户端的输出端与播音器的输入端电性连接。本发明提供一种盲人服务系统,摄像头获取图像信息,通过服务端将图像信息转化为语音信息,再通过播音器播报给用户,借助此操作系统,可实时传递用户信息和环境信息,运用于多种盲人设备,节约成本,还可通过客户端集中管理。</td>   <td>1.一种盲人服务系统,其特征在于,包括摄像头(1)、客户端(2)、服务端(3)和播音器(4)；其中：摄像头(1)输出端与客户端(2)的输入端电性连接,客户端(2)的输出端与服务端(3)的输入端电性连接,服务端(3)的输出端与客户端(2)的输入端电性连接,客户端(2)的输出端与播音器(6)的输入端电性连接。</td>   <td>G06K9/00;G06T7/90;G10L13/02</td>  </tr>        <tr>   <td>中国专利</td>   <td>         袁文平;                   刘伟       </td>   <td>中山大学</td>   <td>基于水体指数变异系数的水稻亚像元识别方法</td>   <td>广东省</td>   <td>CN107273797B</td>   <td>2020-05-08</td>   <td>本发明公开了一种基于水体指数变异系数的水稻亚象元识别方法,包括如下步骤：S1.构建水体指数、植被指数、地表温度时序数据集；S2.剔除研究区域内非耕地像元；S3.根据地表温度确定作物生长季的时期；S4.计算各像元生长季内水体指数的变异系数；S5.确定水体指数变异系数与水稻种植面积比例的关系；S6.依据水体指数的变异系数进行水稻种植面积比例计算。该方法利用水稻的需水特性,根据水体指数变异系数随水稻种植面积比例的增加而降低,二者具有显著的线性相关性。通过设计水体指数的变异系数,用于水稻亚像元识别并计算种植面积的比例,具有不依赖于先验知识、鲁棒性好、分类精度高、识别能力强等特点。</td>   <td>1.一种基于水体指数变异系数的水稻亚象元识别方法,其特征在于,包括如下步骤：S1.构建水体指数、植被指数、地表温度时序数据集；S2.剔除研究区域内非耕地像元；S3.根据地表温度确定作物生长季的时期；S4.计算各像元生长季内水体指数的变异系数；S5.确定水体指数的变异系数与水稻种植面积的比例的关系；S6.依据水体指数的变异系数进行水稻种植面积比例计算；所述水体指数的变异系数为耕地作物生长季水体指数的标准偏差与平均值的比值。</td>   <td>G06K9/00;G06Q50/02</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;                   陈禹亘       </td>   <td>中山大学</td>   <td>一种基于图卷积网络的偏瘫步态分类方法</td>   <td>广东省</td>   <td>CN111104902A</td>   <td>2020-05-05</td>   <td>本发明公开了一种基于图卷积网络的偏瘫步态分类方法,包括下述步骤：S1、获取待分析的步态数据,对该步态数据进行预处理；S2、基于图卷积网络构建检测网络模型,并训练该网络模型；S3、将步态数据按关节点结构输入训练好的模型中,得到最后一层的卷积结果；S4、获取最后一层的卷积结果,按特征权重比进行缩放；S5、根据得到的特征缩放结果,在全连接层中进行计算,分别得到该目标是健康人和是偏瘫患者的分。本发明利用图卷积神经网络自动提取关节点数据特征,增强了抗噪性能,使得分类速度和分类精度有大幅度提高。</td>   <td>1.一种基于图卷积网络的偏瘫步态分类方法,其特征在于,包括下述步骤：S1、获取待分析的步态数据,对该步态数据进行预处理；S2、基于图卷积网络构建检测网络模型,并训练该网络模型；所述建立图卷积网络包括：特征邻接矩阵,将输入图像中有联系的特征点连接起来；建立步态图,直接跟步态数据进行矩阵相乘；建立好步态图以后,对每一个节点初始化权重W,对于每一个输入的矩阵H,将矩阵H、图卷积网络G、初始化权重W进行相乘,即可得到每一个节点的特征,在这个过程中,图卷积利用了关节点间的连接信息,能够更好地提取特征,最后得到特征图；训练网络模型的步骤如下：将步态数据输入神经网络,获取分类结果后计算结果与真实值的差距,使用随机梯度下降和反向传播的方法调整网络参数,逐步缩小检测值与真实值的差距；S3、将步态数据按关节点结构输入训练好的模型中,得到最后一层的卷积结果；S4、获取最后一层的卷积结果,按特征权重比进行缩放；S5、根据得到的特征缩放结果,在全连接层中进行计算,分别得到该目标是健康人和是偏瘫患者的得分。</td>   <td>G06K9/00;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              陈家鑫;                   谢晓华       </td>   <td>中山大学</td>   <td>基于双目视觉的高低分辨率融合立体匹配方法</td>   <td>广东省</td>   <td>CN111105452A</td>   <td>2020-05-05</td>   <td>本发明公开了一种基于双目视觉的高低分辨率融合立体匹配方法,包括步骤：输入左右两张高分辨率图片和立体匹配的参数；图像进行预处理,计算左右图中视差搜索范围内的每一对像素点的视差代价,得到视差代价矩阵；视差代价矩阵下采样,得到降维后的视差代价矩阵；降维后的视差代价矩阵进行视差代价聚合,得到最佳匹配点；根据最佳匹配点进行匹配,得到视差图。本发明在预处理和视差代价计算步骤使用清晰的高分辨率图片,可得到准确的视差代价,在视差代价聚合和后处理步骤按照低分辨率图片的流程处理,可减少计算所需的时间。本发明通过立体匹配的准确率测试和耗时测试,表明本方法在准确率和耗时方面相较于现有技术,具有既准确又耗时少的优点。</td>   <td>1.基于双目视觉的高低分辨率融合立体匹配方法,其特征在于,包括步骤：输入双目相机拍摄得到的左右两张高分辨率图片以及立体匹配的参数；对图像进行预处理,计算左右图中处于视差搜素范围内的每一对像素点的视差代价,得到视差代价矩阵；视差代价矩阵下采样,得到降维后的视差代价矩阵；针对降维后的视差代价矩阵进行视差代价聚合,得到最佳匹配点；根据上述最佳匹配点进行匹配,得到视差图。</td>   <td>G06T7/55;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑子彬;                   刘子璐       </td>   <td>中山大学</td>   <td>一种基于币安热地址的比特币交易提现金额预测方法</td>   <td>广东省</td>   <td>CN111105239A</td>   <td>2020-05-05</td>   <td>本发明公开一种基于币安热地址的比特币交易提现金额预测方法,获取币安热地址历史的交易数据并补充其中数据缺失；从交易数据划分出转出交易数据为用户提现数据,聚合预定时段内的用户提现数据,将聚合后的用户提现数据以时间点A为界线,取过去时间段的用户提现数据为特征信息,预测时间段的用户提现数据为标签信息,构造交易信息详情的样本；将交易信息详情的样本划分出训练集和验证集,采用训练集训练深度学习模型,获取模型对未来时间用户提现数据的预测值,通过预测值与标签信息的差值调整深度学习模型的权重参数,直至得到稳定收敛；将验证集输入训练后的深度学习模型获取验证集的交易提现预测值。本发明提高了区域链交易的风险控制力。</td>   <td>1.一种基于币安热地址的比特币交易提现金额预测方法,其特征在于,包括：S10获取币安热地址历史所有的交易数据,补充交易数据中数据缺失；S20将补充后的交易数据按转入/转出行为划分出用于转出方储存的交易数据为用户提现数据,聚合预定时段内的用户提现数据,将聚合后的用户提现数据以时间点A为界线,取时间点A的过去时间内的用户提现数据为特征信息,时间点A预测时间段的用户提现数据为标签信息,构造交易信息详情的样本；S30将交易信息详情的样本划分出训练集和验证集,采用训练集训练深度学习模型,获取模型对未来时间用户提现数据的预测值,通过预测值与标签信息的差值调整深度学习模型的权重参数,直至得到稳定收敛训练后的深度学习模型；S60将验证集输入训练后的深度学习模型获取验证集的交易提现预测值。</td>   <td>G06Q20/40;G06N3/04;G06N3/08;G06Q20/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖正首;              黄林冲;                   黄帅       </td>   <td>中山大学</td>   <td>钻孔布置方案的鲁棒性评估方法和装置</td>   <td>广东省</td>   <td>CN111080020A</td>   <td>2020-04-28</td>   <td>本发明实施例提供一种钻孔布置方案的鲁棒性评估方法和装置。方法包括：根据钻孔布置方案,获得目标场地中钻孔位置土体参数；根据贝叶斯和随机场理论,获得目标场地各点土体参数的概率分布函数；通过马尔科夫蒙特卡洛抽样,获得目标场地各点土体参数的样本；基于目标场地各点土体参数的样本,通过映射函数计算出评估钻孔布置方案的鲁棒性的表征参数,进而可以依据该参数判断勘察工程的钻孔布置方案是否需要进行优化。因此,本发明提供的钻孔布置方案的鲁棒性评估方法和装置,有助于优化勘察工程的钻孔布置方案,节约勘察工程施工成本,提升勘察工程执行速度。</td>   <td>1.一种钻孔布置方案的鲁棒性评估方法,其特征在于,包括：S1、根据钻孔布置方案,获得目标场地中钻孔位置土体参数；S2、根据贝叶斯和随机场理论,获得目标场地各点土体参数的概率分布函数；S3、通过马尔科夫蒙特卡洛抽样,获得目标场地各点土体参数的样本,且该样本服从所获取的概率分布函数；S4、基于目标场地各点土体参数的样本,通过映射函数计算出评估钻孔布置方案的鲁棒性的表征参数。</td>   <td>G06Q10/04;G06Q50/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘玉葆;                   滕伟       </td>   <td>中山大学</td>   <td>一种基于门控图卷积网络的商品推荐方法及系统、存储介质</td>   <td>广东省</td>   <td>CN111080400A</td>   <td>2020-04-28</td>   <td>本发明涉及一种基于门控图卷积网络的商品推荐方法,包括：将会话序列建模为无向图；无向图中,一个顶点代表一个商品,每条边代表用户在会话的连续两次点击中点击了边两端的商品,根据每条边在会话中出现的次数赋予每条边相应次数的权重；将会话序列中所有会话中的商品初始化到一个统一的嵌入空间中,得到每个会话中的商品的嵌入表示,通过图卷积网络和门控循环单元学习会话中商品的嵌入表示；根据学习到的会话中商品的嵌入表示,对会话的嵌入表示进行学习；根据得到的所有商品的嵌入表示,以及每个会话的嵌入表示,将之进行相乘,然后通过softmax函数进行归一化处理,得到针对每个会话所有商品的推荐分数,根据所述推荐分数进行商品推荐。</td>   <td>1.一种基于门控图卷积网络的商品推荐方法,其特征在于：包括：将会话序列建模为无向图；无向图中,一个顶点代表一个商品,每条边代表用户在会话的连续两次点击中点击了边两端的商品,根据每条边在会话中出现的次数赋予每条边相应次数的权重；将会话序列中所有会话中的商品初始化到一个统一的嵌入空间中,得到每个会话中的商品的嵌入表示,通过图卷积网络和门控循环单元学习会话中商品的嵌入表示；根据学习到的会话中商品的嵌入表示,对会话的嵌入表示进行学习；根据得到的所有商品的嵌入表示,以及每个会话的嵌入表示,将之进行相乘,然后通过softmax函数进行归一化处理,得到针对每个会话所有商品的推荐分数,根据所述推荐分数进行商品推荐。</td>   <td>G06Q30/06;G06N3/08;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡延庆;              左攀星;              黄怡文;              袁悠悠;                   孟凡辉       </td>   <td>中山大学</td>   <td>基于信息非均匀传播特征的社交媒体关键用户识别方法</td>   <td>广东省</td>   <td>CN111080462A</td>   <td>2020-04-28</td>   <td>本发明公开了基于信息非均匀传播特征的社交媒体关键用户识别方法,该关键用户识别方法为基于模拟传播的方法,并采用非均匀SIR模型作为信息传播模型的关键用户识别方法；非均匀SIR模型基于信息的非均匀传播特征对标准SIR模型进行改进得到,其中非均匀SIR模型中每条边的感染概率是不同的。本发明首先针对标准SIR模型中假设任两者之间的感染概率是相同的,不考虑邻居的异质性的不足之处进行改进,使其符合信息非均匀传播的特性,从而对信息传播能力进行正确估计,并在此基础之上结合基于渗流的贪婪算法,提出了一种基于信息非均匀传播特征的社交媒体关键用户识别方法,使得依赖于信息传播模型的关键用户识别方法的性能得到优化。</td>   <td>1.基于信息非均匀传播特征的社交媒体关键用户识别方法,其特征在于,所述关键用户识别方法为基于模拟传播的方法,并采用非均匀SIR模型作为信息传播模型的关键用户识别方法；所述非均匀SIR模型基于信息的非均匀传播对标准SIR模型进行改进得到,所述非均匀SIR模型中每条边的感染概率不同。</td>   <td>G06Q50/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         梁耀华;                   方艳梅       </td>   <td>中山大学</td>   <td>一种基于卷积神经网络的重采样图像检测方法</td>   <td>广东省</td>   <td>CN111080587A</td>   <td>2020-04-28</td>   <td>本发明公开了一种基于卷积神经网络的重采样图像检测方法,包括以下步骤：将检测图像按照α进行分割,得到若干个子图像；将全部的子图像的颜色通道进行归一化处理；根据检测图像构建重采样数据库；重采样数据库通过卷积神经网络进行训练优化；归一化后子图像根据优化后的重采样数据库进行筛选；筛选后的子图像通过阈值法进行判断,若不小于阈值,则认定检测图像存在重采样操作；若小于阈值,则认定检测图像不存在重采样操作。本发明通过卷积网络进行特征提取,使用残差的思想络进行设计整体网络结构,摒弃了传统人工设置初始化参数,通过动量的技术方法优化卷积神经网络中的参数,达到全局最优的检测效果,防止卷积神经网络出现局部最优的情况。</td>   <td>1.一种基于卷积神经网络的重采样图像检测方法,其特征在于,包括以下步骤：将检测图像按照α进行分割,得到若干个子图像,所述的α是人为预设值；将全部的子图像的颜色通道进行归一化处理；根据检测图像构建重采样数据库；重采样数据库通过卷积神经网络进行训练优化；归一化后子图像根据优化后的重采样数据库进行筛选；筛选后的子图像通过阈值法进行判断,若不小于阈值,则认定检测图像存在重采样操作；若小于阈值,则认定检测图像不存在重采样操作。</td>   <td>G06T7/00;G06T7/11;G06T7/90;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         高成英;                   刘颀       </td>   <td>中山大学</td>   <td>一种基于显著性检测的颜色迁移方法及系统</td>   <td>广东省</td>   <td>CN111080722A</td>   <td>2020-04-28</td>   <td>本发明公开了一种基于显著性检测的颜色迁移方法及系统,包括以下步骤：区分出输入图像的前景和背景；计算出前景和背景的主题颜色；将前景主题颜色和背景主题颜色进行合并；将输入图像每个像素重新划分进最终主题色谱；根据用户需求对输入图像进行重新着色。通过先将输入图像的前景和背景区分开,再分别提取前景和背景的主题颜色,这使得主题颜色提取能更加准确,更加适用于布料图片；同时,通过根据用户需求,灵活地调整主题颜色,减少生产布料所需的成本,帮助用户高效地找到最满意的配色方案；此外,本方法不仅能够将目标颜色准确应用到特定的图像区域,还能够高度保留布料纹理特征,且算法的计算效率较高,适用于工业场景。</td>   <td>1.一种基于显著性检测的颜色迁移方法,其特征在于,具体包括以下步骤：步骤S1,输入图像,并通过显著性检测区分出输入图像的前景和背景；步骤S2,基于颜色空间计算出前景和背景的主题颜色；步骤S3,将前景主题颜色和背景主题颜色进行合并,得到最终主题色谱；步骤S4,依据色差将输入图像每个像素重新划分进最终主题色谱；步骤S5,根据用户需求修改最终主题色谱,得到修改后的主题色谱,并依据修改后的主题色谱对输入图像进行重新着色。</td>   <td>G06T7/90;G06K9/46</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李冠彬;              刘劲;                   林倞       </td>   <td>中山大学</td>   <td>基于深度学习的弱监督视频时序动作定位的方法及系统</td>   <td>广东省</td>   <td>CN111079646A</td>   <td>2020-04-28</td>   <td>本发明公开了一种基于深度学习的弱监督视频时序动作定位方法及系统,该方法包括：S1,提取视频中当前帧及前一帧,利用光流估算网络提取光流,并连同等间隔对视频采样的帧输入双流动作识别网络提取视频特征；S2,将视频特征进行语义一致性建模,获得嵌入特征；S3,训练分类模块将嵌入特征映射到类激活序列；S4,采用注意力模块更新视频特征；S5,将更新后的视频特征作为下一次循环的输入,重复S2-S4直到停止；S6,将每次循环产生的类激活序列融合,计算估计的动作类别与真实类别标签的分类损失；S7,将每次循环的嵌入特征融合计算动作特征间的相似性损失；S8,根据分类损失及相似性损失得到目标损失,更新系统模型参数。</td>   <td>1.一种基于深度学习的弱监督视频时序动作定位方法,包括如下步骤：步骤S1,提取视频的当前帧以及其前一帧,利用光流估算网络提取光流,并连同等间隔对视频采样的帧输入预训练的双流动作识别网络,提取视频特征；步骤S2,将提取的视频特征通过循环神经网络进行语义一致性建模,获得所述视频特征的嵌入表示；步骤S3,训练分类模块将步骤S2获得的嵌入特征映射到类激活序列；步骤S4,采用基于嵌入特征的注意力模块根据步骤S2获得的嵌入特征得到视频时间维度的注意力分布,并使用所述注意力分布更新视频特征；步骤S5,将更新后的视频特征作为下一次循环的输入,重复步骤S2-S4的训练过程,直到符合停止条件；步骤S6,将每次循环产生的类激活序列进行融合,进而解析生成时序动作定位结果和估计的动作类别,计算估计的动作类别与真实动作类别标签的分类损失；步骤S7,将每次循环的嵌入特征进行融合,计算视频动作特征间的相似性损失；步骤S8,将分类损失及相似性损失按权相加,得到目标损失,更新系统的模型参数。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         余志;              黄柳红;                   李熙莹       </td>   <td>中山大学</td>   <td>一种车辆所在车道快速划分方法</td>   <td>广东省</td>   <td>CN111079668A</td>   <td>2020-04-28</td>   <td>本发明公开一种车辆所在车道快速划分方法,适用于直线车道区域,包括以下步骤：根据图像中的直线车道区域,得到各个车道线与各个车辆关键点的交点差值；计算相邻车道线之间的交点差值的乘积；根据交点差值的乘积判断各个车辆的所属车道。本发明通过矩阵运算,获取所有车辆所在车道信息的方法,仅通过三次矩阵运算,便可获取道路面内所有车辆的所在车道信息,计算量不大,整体运算速度快。</td>   <td>1.一种车辆所在车道快速划分方法,适用于直线车道区域,其特征在于,包括以下步骤：根据图像中的直线车道区域,得到各个车道线与各个车辆关键点的交点差值；计算相邻车道线之间的交点差值的乘积；根据交点差值的乘积判断各个车辆的所属车道。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;                   袁子逸       </td>   <td>中山大学</td>   <td>基于增广网络的无监督行人重识别方法</td>   <td>广东省</td>   <td>CN111062329A</td>   <td>2020-04-24</td>   <td>本发明提供一种基于增广网络的无监督行人重识别方法,该方法以原始数据库中的行人图像数据为基础,进行多种形式的数据增广,将增广后的图像数据视作基础数据的同一标签数据通入参数不共享的网络分别提取特征,帮助网络进行训练。本发明的方法主要考虑如何在数据集不够丰富的情况下对无法直接作为输入的无标签数据进行利用,可以直接使用这种方法得到的主网络模型直接在测试集上进行特征提取后用于测试；也可以用这种方式先使用无标签的数据进行多个增广网络与主网络的预训练,再使用有标签的数据对主网络参数进行微调,从而有效利用无标签信息并提升行人重识别的准确率。</td>   <td>1.一种基于增广网络的无监督行人重识别方法,其特征在于,包括以下步骤：S1：对无标签的原始行人图像数据集D0进行增广操作,所述增广操作包括图像缩放、随机裁剪、随机擦除、加噪和高斯模糊的一种或多种,得到M个新的增广数据集D1～DM,M为正整数；S2：将原始行人图像数据集D0中的原始图像数据通入一个卷积神经网络作为主网络N0进行前向传播提取得到特征F0；S3：将M个增广数据集D1～DN中对应的增广图像数据分别输入M个参数不共享的卷积神经网络作为增广网络N1～NM进行前向传播提取得到特征F1～FM；S4：从原始行人图像数据集D0中随机选取一张图像Inegative作为负样本,通入主网络N0前向传播提取得到特征Fnegative；S5：用输出特征F0分别与输出特征F1～FM计算欧式距离,得到M个损失值L1～LM；S6：用输出特征Fnegative分别与输出特征F0～FM计算欧氏距离,得到M+1个损失值L0nagetive～LMnegative；S7：将S5中得到的M个损失值L1～LM分别与S6中得到的M个损失值L1negative～LMnegative相减后得到的结果作为损失对增广网络N1～NM进行后向传播计算梯度更新增广网络参数；S8：将S5中得到的M个损失值L1～LM进行求和与S6中得到的损失值L0negative～LMnegative求和的结果相减,得到总损失值L0；S9：将S8中得到的总损失值L0作为损失对主网络N0进行后向传播计算梯度更新主网络参数；S10：重复S2～S9的操作,直到主网络与增广网络收敛；S11：将主网络模型作为输出。</td>   <td>G06K9/00;G06K9/46;G06N3/04;G06N3/08;G06T3/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         齐志新;              沈雪娇;                   张慧       </td>   <td>中山大学</td>   <td>基于Landsat时间序列遥感影像的城市更新区域监测方法</td>   <td>广东省</td>   <td>CN111062368A</td>   <td>2020-04-24</td>   <td>本发明公开了基于Landsat时间序列遥感影像的城市更新监测方法,包括以下步骤：利用研究区t<Sub>1</Sub>和t<Sub>2</Sub>连续两个年份的Landsat卫星影像计算土地开发指数,利用t<Sub>1</Sub>年份的Landsat卫星影像计算归一化植被指数、归一化水体指数；基于土地开发指数,通过阈值法提取出t<Sub>1</Sub>和t<Sub>2</Sub>年份间的土地开发区域；基于植被指数和水体指数,通过阈值法提取t<Sub>1</Sub>年份的植被和水体区域；对提取的土地开发区域、植被区域、水体区域进行叠加分析,从土地开发区域中去除由水体和植被区域转变而来的误差,从而提取出t<Sub>1</Sub>到t<Sub>2</Sub>年份的城市更新区域,再通过滤波降噪算法去除/降低噪声,得到t<Sub>1</Sub>到t<Sub>2</Sub>年份最终的城市更新区域。利用本方法,可以实现1984年至今任意年份、任意区域的城市更新范围提取。</td>   <td>1.基于Landsat时间序列遥感影像的城市更新监测方法,其特征在于,包括以下步骤：获取t<Sub>2</Sub>年份和上一年份t<Sub>1</Sub>年份的Landsat卫星遥感影像；通过t<Sub>1</Sub>和t<Sub>2</Sub>两个年份的Landsat卫星影像计算土地开发指数LDI,通过t<Sub>1</Sub>年份的Landsat卫星影像计算植被指数NDVI、水体指数NDWI；根据土地开发指数LDI通过阈值法提取t<Sub>2</Sub>年份基于t<Sub>1</Sub>年份的土地开发区域；根据植被指数NDVI和水体指数NDWI通过阈值法提取t<Sub>1</Sub>年份Landsat卫星影像上的植被和水体区域；对土地开发指数LDI、植被指数NDVI、水体指数NDWI进行叠加分析,从土地开发区域中去除由水体和植被转变而来的区域,即城市扩张区域,保留土地开发区域中由建筑物转变而来的区域,从而筛选出土地开发区域中全部的城市更新区域,即为t<Sub>1</Sub>到t<Sub>2</Sub>年份的城市更新区域；针对提取的城市更新区域通过滤波降噪算法去除/降低噪声、保持边界平滑,得到t<Sub>1</Sub>到t<Sub>2</Sub>年份的最终城市更新区域。</td>   <td>G06K9/00;G06T5/00;G06T7/136;G06T7/187</td>  </tr>        <tr>   <td>中国专利</td>   <td>         盛紫琦;                   卓汉逵       </td>   <td>中山大学</td>   <td>一种基于AIGAN的对原始观测数据的动作规划方法</td>   <td>广东省</td>   <td>CN111062621A</td>   <td>2020-04-24</td>   <td>本发明涉及一种基于AIGAN的对原始观测数据的动作规划方法,包括以下步骤,步骤一：收集观测数据；步骤二：将观测数据放入训练网络模块；步骤三：给定高维初始观测值和高维目标观测值,使用训练网络模块,将其分别转化成低维初始状态值和低维目标状态值；步骤四：根据规划策略,在低维状态解空间中进行规划,求得从低维初始状态值到低维目标状态值的动作状态序列；步骤五：将得到的动作状态序列中的低维状态向量,转化为高维的观测值并得到高维的观测动作序列。先通过深度学习框架学习所需模型,再利用这些学习到的模型来做规划,采用无监督学习方式,高效学习数据总的规律,能够解决现实问题中复杂问题进行建模困难,难以进行动作规划的问题。</td>   <td>1.一种基于AIGAN的对原始观测数据的动作规划方法,其特征在于,包括如下步骤：步骤一：收集数据集的观测数据；步骤二：将数据集的观测数据放入到基于AIGAN框架的训练网络模块,获得状态表示模型Q、启发式模型F、状态转移模型T、生成器G和判别器D；步骤三：给定高维初始观测值和高维目标观测值,使用状态表示模型Q,将其分别转化成低维初始状态值和低维目标状态值；步骤四：规划策略中的目标函数使用启发式模型F和状态转移模型T,在低维状态解空间中进行规划,求得从低维初始状态值到低维目标状态值的动作状态序列；步骤五：将得到的动作状态序列中的全部低维状态向量,使用生成器G转化为高维的观测值,最终得到高维的观测动作序列。</td>   <td>G06Q10/06;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑子彬;              李威琪;                   周晓聪       </td>   <td>中山大学</td>   <td>一种基于注意力机制的推荐系统召回方法</td>   <td>广东省</td>   <td>CN111062775A</td>   <td>2020-04-24</td>   <td>本发明公开一种基于注意力机制的推荐系统召回方法,包括：提取训练样本中的用户特征和商品特征,将用户特征转化为用户嵌入向量,将商品特征转化为商品嵌入向量；将用户嵌入向量和商品嵌入向量输入注意力机制模型训练,通过模型中注意力网络学习每个特征的权重,依据权重对所有特征的嵌入向量做加权求和,得到用户表征向量和商品表征向量；计算用户表征向量和商品表征向量的内积,得到训练样本的用户购买商品意愿匹配度,建立用户购买商品意愿匹配度的交叉熵损失函数,计算最小化的交叉熵损失函数,收敛注意力机制模型；将待测样本输入收敛后的注意力机制模型,获取待测样本的用户购买商品意愿匹配度,选择用户购买商品意愿匹配度在预置区间的商品作为召回结果进行推荐。本发明增强了泛化性,大量简化了召回推荐的计算量。</td>   <td>1.一种基于注意力机制的推荐系统召回方法,其特征在于,包括如下步骤：S10提取训练样本中的用户特征和商品特征,将用户特征转化为用户嵌入向量,将商品特征转化为商品嵌入向量；S20将用户嵌入向量和商品嵌入向量输入注意力机制模型训练,通过模型中注意力网络学习每个特征的权重,依据权重对所有特征的嵌入向量做加权求和,得到用户表征向量和商品表征向量；计算用户表征向量和商品表征向量的内积,得到训练样本的用户购买商品意愿匹配度,建立用户购买商品意愿匹配度的交叉熵损失函数,计算最小化的交叉熵损失函数,收敛注意力机制模型；S30将待测样本输入收敛后的注意力机制模型,获取待测样本的用户购买商品意愿匹配度,选择用户购买商品意愿匹配度在预置区间的商品作为召回结果进行推荐。</td>   <td>G06Q30/06;G06F16/9535</td>  </tr>        <tr>   <td>中国专利</td>   <td>         梁小丹;              谢震宇;                   董浩业       </td>   <td>中山大学</td>   <td>一种可保留示例衣服细节的虚拟试穿方法及系统</td>   <td>广东省</td>   <td>CN111062777A</td>   <td>2020-04-24</td>   <td>本发明公开了一种可保留示例衣服细节的虚拟试穿方法及系统,该方法包括：步骤S1,对于一张人体图像,基于与衣服无关的人体特征表示方法,获得与衣服无关的人体特征图p；步骤S2,分别提取人体特征图p和示例衣服图c的高层特征,计算两者之间的相关性,得到代表人体特征和衣服特征相关性的张量,并基于回归网络以及薄板样条插值模块,获得变形后的衣服图<Image he="76" wi="70" file="DDA0002311286920000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>步骤S3,将步骤S1获得的人体特征图p和由步骤S2得到的变形衣服图<Image he="67" wi="44" file="DDA0002311286920000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>拼接作为深度学习UNet网络的输入,获得初步合成的试穿结果I<Sub>r</Sub>以及掩模M；步骤S4,由掩模M将初步合成的试穿结果I<Sub>r</Sub>和形变衣服图<Image he="71" wi="54" file="DDA0002311286920000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>融合在一起,得到最终的试穿结果I<Sub>o</Sub>。</td>   <td>1.一种可保留示例衣服细节的虚拟试穿方法,包括如下步骤：步骤S1,对于一张人体图像,基于与衣服无关的人体特征表示方法,获得与衣服无关的人体特征图p；步骤S2,分别提取人体特征图p和示例衣服图c的高层特征,并计算两个特征图之间的相关性,得到代表人体特征和衣服特征相关性的张量,并基于回归网络以及薄板样条插值模块,获得变形后的衣服图<Image id="icf0001" he="63" wi="60" file="FDA0002311286890000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>步骤S3,将步骤S1获得的人体特征图p和由步骤S2得到的变形衣服<Image id="icf0002" he="60" wi="43" file="FDA0002311286890000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>拼接起来作为深度学习UNet网络的输入,获得初步合成的试穿结果I<Sub>r</Sub>以及用于融合的掩模M；步骤S4,由掩模M将初步合成的试穿结果I<Sub>r</Sub>和形变衣服图<Image id="icf0003" he="57" wi="43" file="FDA0002311286890000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>融合在一起,得到最终的试穿结果I<Sub>o</Sub>。</td>   <td>G06Q30/06;G06T19/00;G06T7/11</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              赵兰杰;                   谢晓华       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于偏微分方程学习的本征图像分解方法</td>   <td>广东省</td>   <td>CN107133953B</td>   <td>2020-04-24</td>   <td>本发明提供一种基于偏微分方程学习的本征图像分解方法,本发明在处理图像本征成分估计上不依赖于确定的先验约束,且采用数据驱动的方式构建偏微分方程；利用共轭梯度法来决定搜索方向,相对于最速下降法和牛顿法,该方法将共轭性与最速下降法相结合；利用已知点处的梯度构造一组共轭方向,并沿着这组方向进行搜索,求出目标函数的极小点,来确定最佳的搜索方向；该方法可有效实现不同光照条件的图像进行本征分解得到该图像的反射成分和阴影成分。</td>   <td>1.一种基于偏微分方程学习的本征图像分解方法,其特征在于,包括以下步骤：S1：输入训练数据对；S2：初始化控制函数,由于目标函数是非凸的,最小化过程的收敛朝向取决于初始化的局部最小值,当初始化过程不收敛的时候转向步骤S3,执行循环；否则转向步骤S8；S3：求解具有PDE约束的最优控制方程；S4：求解伴随函数取特定值时的伴随方程；S5：利用下式计算j＝0,1,...,16时的平移旋转不变量的导数：          <Image id="icf0001" he="217" wi="700" file="FDA0002301030580000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,J是平移旋转不变量,j是平移旋转不变量的个数,a<Sub>j</Sub>,b<Sub>j</Sub>是控制函数,λ<Sub>j</Sub>和u<Sub>j</Sub>是正的加权参数,<Image id="icf0002" he="55" wi="67" file="FDA0002301030580000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和φ<Sub>m</Sub>是伴随函数,u是输出图像,Ω是输入图像所占据的矩形区域,f<Sub>Ω</Sub>是Ω的初始函数,m＝1,2,...,M,M为输入的数据样本对数,inv(u,v)表示对矩阵(u,v)求逆,v是指示函数,引入指示函数的目的是收集图像中的大规模信息,以便指导u的演变；S6：使用共轭梯度法来决定搜索方向；S7：沿着搜索方向执行黄金分割搜索,并不断更新系统函数,进行下一个循环,直到j＝16,进行训练；S8：终止循环,输出系统函数；S9：准备应用数据,数据图片特点是背景黑色,目标物单一且突出；S10：利用得到的系统函数,对所给数据进行本征分解应用,得到图像的反射成分和阴影成分；所述步骤S2中通过求解(1)式对控制函数a<Sub>j</Sub>(t),t＝0,Δt,···,1-Δt进行初始化,此时固定b<Sub>j</Sub>(t),j＝0,1,···,16,F·a(t)＝d     (1)          <Image id="icf0003" he="129" wi="700" file="FDA0002301030580000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0004" he="142" wi="579" file="FDA0002301030580000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,a<Sub>j</Sub>(t)和b<Sub>j</Sub>(t)是控制函数,a(t)＝{a<Sub>j</Sub>(t)}和b(t)＝{b<Sub>j</Sub>(t)}是定义在Q上的函数集合,分别用来控制u和v的演变,f<Sub>u</Sub>和f<Sub>v</Sub>分别是u和v的初始函数；所述步骤S3中通过引入(2)式的伴随方程,计算第j个平移旋转不变量的加托导数,因此局部最优值<Image id="icf0005" he="84" wi="198" file="FDA0002301030580000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0006" he="80" wi="219" file="FDA0002301030580000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>通过基于梯度的算法计算得到,对于u<Sub>m</Sub>和v<Sub>m</Sub>,当m＝1,2,…,M时解(2)式：          <Image id="icf0007" he="364" wi="700" file="FDA0002301030580000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,T是PDE系统完成视觉信息处理并输出结果的时间,Q为Ω×(0,T),Γ为<Image id="icf0008" he="68" wi="259" file="FDA0002301030580000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image></td>   <td>G06T7/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              吴永波;              李昊曦;                   杜灵双       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种跨年龄人脸识别方法</td>   <td>广东省</td>   <td>CN106650650B</td>   <td>2020-04-24</td>   <td>本发明提供一种跨年龄人脸识别方法,该方法通过大量包含四个年龄段的人脸图像训练得到一个由两大模块(最大熵特征描述模块和老化感知去噪自动编码模块)组成的跨年龄人脸识别系统,实现对任意两幅不同年龄人脸图像的识别。最大熵特征描述模块利用决策树的最大熵分裂来实现包含最大信息量的编码分配,老化感知去噪自动编码模块将一个任意年龄段的特征描述符重构成四个不同年龄段的特征描述符,综合这些描述符获得一个消除老化影响的人脸综合特征向量,最后计算不同人脸的综合特征向量的余弦距离实现人脸识别。本发明能够很好地减少一些传统描述符的信息丢失问题,并且消除了跨年龄人脸识别中老化因素的影响,在跨年龄人脸识别问题中有很好的表现。</td>   <td>1.一种跨年龄人脸识别方法,其特征在于,包括以下步骤：S1：对要识别的人脸图像进行密集采样,即将人脸图像划分为多个互相重叠的块,对每一块进行像素矢量的提取,划分时块的重叠半径采取多个值来保留人脸的局部信息；S2：对于已提取的像素矢量,建立一棵决策树,将树的根节点概率值设置为1,采用最大熵的原则递归扩展树,最后为树的每个叶子节点分配一个编码,其中,每个叶子节点代表了一个局部特征；S3：对每一幅人脸图像,将获取的最大熵特征描述编码串联成一个特征向量,对该特征向量重新进行分割,采用主成分分析方法对特征向量进行降维,获得的特征向量v作为老化感知去噪自动编码器的输入；S4：用老化感知去噪自动编码器对特征向量v进行编码,生成4个年龄段的人脸特征向量v'<Sub>i</Sub>,i＝1,...,4,其中4个年龄段包括：幼年,青年,成年,老年；S5：将两幅人脸合成的特征向量按照年龄段串联成一个长向量,同时原始人脸的特征向量也合并入该向量,通过计算两向量的余弦距离来判断两幅人脸是否来自同一人；S6：模型训练时,提取同一个人的四个年龄段的人脸最大熵特征,将特征向量v加入噪声后映射到隐含层得到一个有损压缩码h,然后用h来预测四个年龄段的特征向量v'<Sub>i</Sub>,通过最小化损失函数得到自动编码器,通过多次上述的映射与重构过程生成多层老化感知去噪自动编码器,在构造多层去噪自动编码器时,需要用严格玻尔兹曼机以非监督的方式逐层地进行预训练。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王军;              谢启超;                   陈谋奇       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>基于BP神经网络的鱼眼镜头拍摄图像畸变矫正方法</td>   <td>广东省</td>   <td>CN106952236B</td>   <td>2020-04-24</td>   <td>本发明提供的方法应用BP神经网络来对鱼眼镜头拍摄的图像进行畸变矫正,矫正畸变的效率与现有相比得到了提高。并且神经网络具有很强的自学习性、自组织性、高度非线性和鲁棒性等特点使得其在解决非线性拟合等问题上有着独特的优势,能够解决多层次的复杂问题。本发明采用神经网络解决图像畸变矫正,打破传统的图像畸变矫正技术的束缚,在非线性畸变矫正方面正有着无可替代的优越性。</td>   <td>1.一种基于BP神经网络的鱼眼镜头拍摄图像畸变矫正方法,其特征在于：包括以下步骤；S1.设纸张A上均匀分布有m行、n列的特征点,使用鱼眼镜头对纸张A进行拍摄,获得拍摄的图像；S2.对图像进行预处理；S3.对经过预处理的图像进行特征点的提取；S4.提取的特征点中,水平距离最大的相邻的两个特征点间的水平距离x为每列特征点间的距离,垂直距离最大的相邻的两个特征点间的垂直距离y为每行特征点间的距离；S5.利用x、y、m、n构建起理想的特征点分布图；S6将提取的图像特征点与理想的特征点分布图中的特征点按照特征点的排列顺序进行匹配；S7.得到匹配结果后,将提取的图像特征点和理想的特征点分布图中的特征点作为BP神经网络的输入和输出,得到神经网络的网络层间加权系数wki和wij；S8.将拍摄的图像中每个像素点作为BP神经网络的输入,即可得到其矫正后的坐标,像素点的像素值保持为拍摄的图像中该像素点的像素值；通过以上操作完成拍摄图像的畸变矫正；所述图像进行预处理具体包括以下依次执行的内容：S11.将图像转换为灰度图像并反向处理；S12.对图像进行二值化处理；S13.对图像进行滤波处理；S14.对图像进行腐蚀处理；S15.对图像进行均值滤波处理；S16.对图像进行二值化处理。</td>   <td>G06T5/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘玉葆;                   黄荣洲       </td>   <td>中山大学</td>   <td>交通特征预测方法、系统及存储介质</td>   <td>广东省</td>   <td>CN111047078A</td>   <td>2020-04-21</td>   <td>本发明涉及一种交通特征预测方法,基于深度学习模型GA-GCN实现,包括以下内容：获取历史交通特征数据集；预处理所述历史交通特征数据集；将所述历史交通特征数据集按固定时间间隔进行划分；使用所述划分后的每份历史交通特征数据集对深度学习模型GA-GCN进行训练；结束训练,使用训练好的深度学习模型GA-GCN对测试集中的交通特征进行预测,得到预测结果。</td>   <td>1.交通特征预测方法,其特征在于：基于深度学习模型GA-GCN实现,包括以下内容：获取历史交通特征数据集；预处理所述历史交通特征数据集；将所述历史交通特征数据集按固定时间间隔进行划分；使用所述划分后的每份历史交通特征数据集对深度学习模型GA-GCN进行训练；结束训练,使用训练好的深度学习模型GA-GCN对测试集中的交通特征进行预测,得到预测结果。</td>   <td>G06Q10/04;G06Q50/26;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张宏伟;              张小虎;                   杨夏       </td>   <td>中山大学</td>   <td>一种平滑约束无迹卡尔曼滤波方法及目标跟踪方法</td>   <td>广东省</td>   <td>CN111047627A</td>   <td>2020-04-21</td>   <td>本发明提供了一种平滑约束无迹卡尔曼滤波方法,包括如下步骤：步骤1,根据无迹变换获取当前目标观测时刻目标状态的原始先验概率密度函数；步骤2,通过数值期望计算原始先验概率的均值和方差；步骤3,引入噪声约束信息,通过计算近似可行域的中心,以获取修正先验概率密度；步骤4,通过后验迭代优化寻求满足约束条件的高斯分布均值和方差,产生满足约束条件的新的高斯西格玛点；步骤5,对高斯西格玛点进行加权计算,完成滤波过程。本发明所述的一种平滑约束无迹卡尔曼滤波方法在准确性和鲁棒性方面具有优势,同时,其实时性方面优于粒子滤波算法。相应地,本发明还提供一种目标跟踪方法。</td>   <td>1.一种平滑约束无迹卡尔曼滤波方法,其特征在于,包括如下步骤：步骤1,根据无迹变换获取当前目标观测时刻目标状态的原始先验概率密度函数；步骤2,通过数值期望计算原始先验概率的均值和方差；步骤3,引入噪声约束信息,通过计算近似可行域的中心,以获取修正先验概率密度；步骤4,通过后验迭代优化寻求满足约束条件的高斯分布均值和方差,产生满足约束条件的新的高斯西格玛点；步骤5,对高斯西格玛点进行加权计算,完成滤波过程。</td>   <td>G06T7/277</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              王弘焌;              王广润;                   李冠彬       </td>   <td>中山大学</td>   <td>一种基于类激活映射图引导的正则化方法及系统</td>   <td>广东省</td>   <td>CN111027634A</td>   <td>2020-04-17</td>   <td>本发明公开了一种基于类激活映射图引导的正则化方法及系统,该方法包括：S1,利用深度神经网络的全局池化层及全连接层参数产生基于标签类别的通道权重因子及类激活映射图；S2,将生成的通道权重因子及类激活映射图分别根据其对神经网络中各层特征图的所有通道及空间区域的贡献度排序；S3,根据步骤S2得到特征通道集以及特征点集,进而得到基于通道权重因子和类激活映射图的二元掩模图M<Sup>(1)</Sup>和M<Sup>(2)</Sup>；S4,生成基于伯努利分布的随机种子二元图M<Sup>(3)</Sup>,与M<Sup>(1)</Sup>和M<Sup>(2)</Sup>进行逻辑运算得到最终的二元掩模图M,并由此获得正则化掩模图M<Sup>l</Sup>；S5,多次迭代式地进行S1-S4的训练过程,完成正则化的优化过程。</td>   <td>1.一种基于类激活映射图引导的正则化方法,包括如下步骤：步骤S1,利用深度神经网络的全局池化层及全连接层参数产生基于标签类别k′的通道权重因子α<Sup>k′</Sup>及类激活映射图J<Sup>k′</Sup>；步骤S2,将步骤S1产生的基于标签类别k′的通道权重因子α<Sup>k′</Sup>及类激活映射图J<Sup>k′</Sup>,通过生成的通道权重因子及类激活映射图分别对所述深度神经网络中各层不同分辨率的特征图的所有通道及空间区域的贡献度从大到小进行排序；步骤S3,从所有通道中抽取前n个重要的特征通道,得到特征通道集<Image id="icf0001" he="69" wi="91" file="FDA0002320223500000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>并对类激活映射图的所有空间区域也选定前n′个重要的特征点,得到特征点集<Image id="icf0002" he="72" wi="96" file="FDA0002320223500000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>并根据上述集合分别得到两张基于通道权重因子和基于类激活映射图的二元掩模图M<Sup>(1)</Sup>和M<Sup>(2)</Sup>；步骤S4,根据预指定的保留率参数γ,生成基于伯努利分布的随机种子二元图M<Sup>(3)</Sup>,对其自身进行逻辑运算后,与步骤S3中生成的M<Sup>(1)</Sup>和M<Sup>(2)</Sup>一同进行逻辑运算,得到最终的二元掩模图M,最后对二元掩模图M做归一化计算,得到当前迭代时刻及对应网络层l的正则化掩模图M<Sup>l</Sup>；步骤S5,多次迭代式地进行步骤S1-S4的训练过程,最终完成正则化的优化过程。</td>   <td>G06K9/62;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王国利;              谈志超;              许沥文;                   郭雪梅       </td>   <td>中山大学</td>   <td>基于多传感网络的室内活动检测识别方法及系统</td>   <td>广东省</td>   <td>CN106815603B</td>   <td>2020-04-14</td>   <td>本发明涉及一种基于多传感网络的室内活动检测识别方法及系统,其中,该方法主要通过所有触发式传感器在目标室内环境中的位置信息和触发信息,在进行活动类型识别前对KNN最近邻算法训练进行处理并得到识别库,在实际应用中再根据触发信息、异构信息素残留率掩膜和单帧图片变化处理得到相应的信息素图矩阵,最终基于识别库实现对人体活动类型的识别。由此本发明通过在室内环境中布置多个触发式传感器,记录触发信息和每个触发式传感器的位置信息,大大提高了活动识别结果的准确性和可靠性。</td>   <td>1.一种基于多传感网络的室内活动检测识别方法,其特征在于：包括以下步骤：记录所有触发式传感器在目标室内环境中的位置信息；记录所有触发式传感器的触发信息；根据所有触发式传感器的位置信息生成相应的二维平面图；根据触发时间顺序依序读取触发信息,并根据当前读取到的触发信息所对应的触发式传感器的位置信息在所述二维平面图上作出触发时刻t对应的单帧图片变化Δs(t)；根据单帧图片变化Δs(t)和高斯卷积核h(t)生成触发时刻t的子信息素矩阵s(t),<Image id="icf0001" he="63" wi="392" file="FDA0002205770580000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>根据当前读取到的触发信息所对应的触发式传感器的位置信息生成基于欧氏距离的异构信息素残留率掩膜ρ(t)；根据子信息素矩阵s(t)和异构信息素残留率掩膜ρ(t)生成触发时刻t的信息素图矩阵S(t),S(t)＝ρ(t)*S(t-1)+s(t)；通过十折交叉验证法,对KNN最近邻算法进行训练,得到KNN识别样本和相应的活动类型识别库；将信息素图矩阵S(t)输入到KNN识别样本中,计算得到相应的识别数据,并结合所述活动类型识别库实现对当前活动类型的识别,得到识别结果。</td>   <td>G06K9/62;H04W4/02;H04W4/33</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;                   洪发挺       </td>   <td>中山大学</td>   <td>结合深度学习与关系建模的图片/视频重要人物检测方法</td>   <td>广东省</td>   <td>CN111008558A</td>   <td>2020-04-14</td>   <td>本发明公开了一种结合深度学习与关系建模的图片/视频重要人物检测方法,包括下述步骤：S1、对图片/视频中的人像的外表信息和几何信息进行特征提取,并融合成一个表征高层次语义的个人特征；S2、通过挖掘场景中人与人之间、人与场景之间的联系,计算出单独依靠个人特征无法表达或者无法高度表达的关系特征；S3、进行重要性分类,通过对在关系计算模型中提取的每个人像的最终特征表达进行重要或不重要的二分类,将每个人像被分为重要这个类别的概率作为重要性得分,得分最高的人像就是关系计算模型认定的重要人物。通过本发明能够通过学习,自主地去构建图片/视频中人物间的关系以及人物与图片中事件的关系,并自动推断出人物的重要程度。</td>   <td>1.一种结合深度学习与关系建模的图片/视频重要人物检测方法,其特征在于,包括下述步骤：S1、对图片/视频中的人像的外表信息和几何信息进行特征提取,并将人像的外表信息和几何信息进行融合成一个表征高层次语义的个人特征,同时提取整个图片/视频的信息作为全局特征；S2、通过挖掘场景中人与人之间、人与场景之间的联系,计算出单独依靠个人特征无法表达或者无法高度表达的关系特征,再将关系特征rfeat与个人信息pfeat融合生成能高度表达个人在场景中重要性的重要性特征,所述关系特征rfeat包含人与人、人与场景之间关系的信息；S3、进行重要性分类,通过对在关系计算模型中提取的每个人像的最终特征表达进行重要或不重要的二分类,将每个人像被分为重要这个类别的概率作为重要性得分,得分最高的人像就是关系计算模型认定的重要人物。</td>   <td>G06K9/00;G06K9/46;G06K9/62;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王资;                   朝红阳       </td>   <td>中山大学</td>   <td>基于DSP芯片与量化模型的三维物体识别方法</td>   <td>广东省</td>   <td>CN110991229A</td>   <td>2020-04-10</td>   <td>本发明涉及一种基于DSP芯片与量化模型的三维物体识别方法。包括三维数据采集器、三维数据特征提取器以及特征解码器,三维数据采集器为RGB-D摄像头,拍摄后得到画面中物体的深度信息,最终合成为点云数据；将该点云数据输入到三维数据特征提取器中,特征提取器中的量化参数模型存储模块用于存量化模型的储参数,利用DSP并行计算加速模块加速特性,快速完成深度神经网络中卷积、池化、残差操作,得到输入数据的特征；特征解码器根据模型训练时对特征加密的方式反向解码,得到需要的特征格式。发明中的特征提取器可以提取三维数据的特征,并且可以通过数据结构优化和硬件加速的方法加速特征提取的速度。</td>   <td>1.一种基于DSP芯片与量化模型的三维物体识别方法,其特征在于,包括三维数据采集器、三维数据特征提取器以及特征解码器,所述的三维数据特征提取器包括量化参数模型存储模块和DSP并行计算加速模块；所述的三维数据采集器为RGB-D摄像头,拍摄后得到画面中物体的深度信息,最终合成为点云数据；将该点云数据输入到三维数据特征提取器中,特征提取器中的量化参数模型存储模块用于存量化模型的储参数,利用DSP并行计算加速模块加速特性,快速完成深度神经网络中卷积、池化、残差操作,最后得到输入数据的特征；特征解码器根据模型训练时对特征加密的方式反向解码,得到需要的特征格式。</td>   <td>G06K9/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;                   包笛       </td>   <td>中山大学</td>   <td>一种基于多尺度透视感知型网络的人群计数方法</td>   <td>广东省</td>   <td>CN110991317A</td>   <td>2020-04-10</td>   <td>本发明涉及图像处理和计算机视觉技术,为基于多尺度透视感知型网络的人群计数方法,包括步骤：根据原图像生成密度图和透视图,形成密度图和透视图数据集；构建神经网络模型,该神经网络模型分为主干网络和透视图分支网络；基于密度图和透视图数据集,训练神经网络模型,并利用训练好的神经网络模型输出密度图,得到人群计数结果。本发明的神经网络在每列网络使用不同扩张率的空洞卷积使得网络进行多尺度人头的特征提取,获得人群密度计数所需的密度图,对人群密度进行准确计算。</td>   <td>1.一种基于多尺度透视感知型网络的人群计数方法,其特征在于,包括以下步骤：S1、数据预处理,根据原图像生成密度图和透视图,形成密度图和透视图数据集；S2、构建神经网络模型,该神经网络模型分为主干网络和透视图分支网络；S3、基于密度图和透视图数据集,训练神经网络模型,并利用训练好的神经网络模型输出密度图,得到人群计数结果；所述主干网络包括至少三列利用空洞卷积进行计算的卷积神经网络,在每列卷积神经网络使用不同扩张率的空洞卷积使得神经网络进行多尺度人头的特征提取,获得人群密度计数所需的密度图。</td>   <td>G06K9/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         曾坤;              舒丁飞;              周凡;                   林格       </td>   <td>中山大学</td>   <td>一种基于多类型医学数据的pCR预测方法</td>   <td>广东省</td>   <td>CN110991535A</td>   <td>2020-04-10</td>   <td>本发明公开了一种基于多类型医学数据的pCR预测方法。本发明从医疗部门获取临床数据、CT诊断报告和肠镜图像；使用临床数据来训练SVM,使用CT诊断报告来迁移学习训练BERT,使用肠镜图像来迁移学习训练Faster-RCNN模型；将待预测患者的临床数据、CT诊断报告和肠镜图像输入到训练好的三个模型中得到预测的pCR概率p<Sub>1</Sub>、p<Sub>2</Sub>和p<Sub>3</Sub>,并融合得到最终预测的pCR概率p,若p大于设定的阈值T,则预测患者为pCR。本发明使用Faster-RCNN网络,能自动生成肿瘤ROI,整个过程无需人工干预,提高预测的效率；使用神经网络进行表征学习,而不用人工设定和选择特征,提高了预测的准度和效率；结合使用患者的临床数据和CT诊断报告进行pCR的预测,提高了预测的准度。</td>   <td>1.一种基于多类型医学数据的pCR预测方法,其特征在于,所述方法包括：从医疗部门获取临床数据、CT诊断报告和肠镜图像,对这三种医学数据进行预处理,输出归一化后的临床数据的特征、定长向量表示的CT诊断报告,以及固定尺寸归一化后的肠镜图像,并分为三种医学数据各自的训练集和验证集；使用得到的归一化后的临床数据的特征的训练集来训练SVM,使用得到的定长向量表示的CT诊断报告的训练集来迁移学习训练BERT模型,使用得到的固定尺寸归一化后的肠镜图像的训练集来迁移学习训练Faster-RCNN模型；将待预测患者的临床数据输入到训练好的SVM中得到预测的pCR概率p<Sub>1</Sub>,将待预测患者的CT诊断报告输入到训练好的BERT中得到预测的pCR概率p<Sub>2</Sub>,将待预测患者的肠镜图像输入到训练好的Faster-RCNN网络中得到预测的pCR概率p<Sub>3</Sub>,之后对p<Sub>1</Sub>、p<Sub>2</Sub>和p<Sub>3</Sub>进行融合得到最终预测的pCR概率p,若p大于设定的阈值T,则预测患者为pCR,否则是非pCR。</td>   <td>G06K9/62;G06N3/04;G16H30/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>              戴小款       </td>   <td>中山大学</td>   <td>一种采用混合搜索策略的迭代局部搜索多目标算法</td>   <td>广东省</td>   <td>CN110991752A</td>   <td>2020-04-10</td>   <td>本发明提供一种采用混合搜索策略的迭代局部搜索多目标算法,将小邻域搜索策略和大邻域搜索策略相结合的方式对可行解进行搜索。该算法的基本思想是从一个初始解开始,对该初始解进行邻域操作来搜索其邻域解,然后从搜索得来的邻域解按照一定的策略选择当前最合适的解,之后对新得到的解进行抖动操作,重复迭代上述过程不断对这个解进行优化,直至满足终止条件。</td>   <td>1.一种采用混合搜索策略的迭代局部搜索多目标算法,其特征在于,包括以下步骤：S1：输入当前解的规模N、存档A、权重向量个数N<Sub>λ</Sub>和最大迭代次数It<Sub>max</Sub>,初始化当前解集C,并用当前解集C更新用于存放帕累托非占优解的存档A,其中,N表示初始化解集的个数,A表示用于存放算法过程中产生的帕累托非占优解；S2：生成N<Sub>λ</Sub>个均匀分布的权重向量<Image id="icf0001" he="78" wi="326" file="FDA0002307258490000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中<Image id="icf0002" he="72" wi="398" file="FDA0002307258490000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>在当前解的每次迭代搜索中,算法都会从<Image id="icf0003" he="72" wi="305" file="FDA0002307258490000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>中随机选择一个权重向量λ<Sup>j</Sup>,然后算法根据当前的权重向量λ<Sup>j</Sup>从N个初始解中选择目标值加权和f<Sup>ws</Sup>(x|λ<Sup>j</Sup>)最小的解x；S3：以预先设为的概率p随机选择小邻域搜索方式或者大邻域搜索方式中的其中一种对x进行局部搜索,得到一个局部最优解x′；S4：如果x′为可行解并且可以插入存档A,用x′代替x并且重置迭代次数；反之,则不做任何操作；然后,用解x代替x′；S5：对当前解x进行抖动操作即跳出局部最优并且用抖动之后的解代替当前解x,重复进行局部搜索过程,直至当前迭代次数i超过预先设定的最大局部搜索次数MaxIt；S6：当前解x经过最大局部搜索次搜索后,用x′更新当前解集C：如果解x′的目标值加权和f<Sup>ws</Sup>(x|λ<Sup>j</Sup>)优于最初从N个初始解中选择的x,则用x′替换当前解集C中的解x；重复上述过程直至迭代It<Sub>max</Sub>次,最终返回存档A作为最终结果。</td>   <td>G06Q10/04;G06Q10/06;G06Q10/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         吴迪;                   陈雅迪       </td>   <td>中山大学</td>   <td>一种结合时间因子的社交网络高影响力用户识别方法</td>   <td>广东省</td>   <td>CN110992195A</td>   <td>2020-04-10</td>   <td>本发明提供一种结合时间因子的社交网络高影响力用户识别方法,包括以下步骤：S1.通过用户的社交关系和互动行为抽象出社交网络图模型；S2.根据抽象出的社交网络图模型,对社交网络图进行分解识别,识别出社交网络图的关键节点(k,h)-核,其中(k,h)-核中的每个节点都有大于等于k个邻居,同时与每个邻居之间都有大于等于h条边；S3.动态识别出S2所得的关键节点并做最小调整。本发明充分考虑了用户之间社交关系的强弱对用户影响力的作用,同时结合时间信息,可以挖掘出指定时间范围内的高影响力用户,也可用于用户影响力变化的研究。</td>   <td>1.一种结合时间因子的社交网络高影响力用户识别方法,其特征在于,包括以下步骤：S1.通过用户的社交关系和互动行为抽象出社交网络图模型；S2.根据抽象出的社交网络图模型,对社交网络图进行分解识别,识别出社交网络图的关键节点(k,h)-核,其中(k,h)-核中的每个节点都有大于等于k个邻居,同时与每个邻居之间都有大于等于h条边；S3.动态识别出S2所得的关键节点并做最小调整。</td>   <td>G06Q50/00;G06F16/906</td>  </tr>        <tr>   <td>中国专利</td>   <td>         严晓威;              吴发明;              马锦华;                   李沁航       </td>   <td>中山大学</td>   <td>基于深度学习和多目标跟踪技术的包裹计数方法及系统</td>   <td>广东省</td>   <td>CN110992305A</td>   <td>2020-04-10</td>   <td>本发明公开了一种基于深度学习和多目标跟踪技术的包裹计数方法及系统,首先获取物流的包裹数据并进行预处理；通过训练集对预设的基于深度学习的包裹检测模型进行训练,通过验证集对训练后的包裹检测模型进行测试和参数调整；得到最终的包裹检测模型；获取实时的物流包裹视频并对其进行关键帧采样,输入最终的包裹检测模型进行检测从而获得每个关键帧的包裹位置信息；根据关键帧的包裹位置信息,采用多目标跟踪算法对包裹进行计数。本发明基于现代计算机视觉技术和多目标跟踪计数进行物流包裹的自动计数,通过对物流的包裹视觉和位置信息进行挖掘,即使实时获取的包裹图片中含有密集且不规则摆放包裹时,亦能准确对包裹进行准确计数。</td>   <td>1.基于深度学习和多目标跟踪技术的包裹计数方法,其特征在于,包括以下步骤：S1.获取物流的包裹数据并进行预处理；其中物流的包裹数据划分为训练集、验证集；S2.通过所述训练集对预设的基于深度学习的包裹检测模型进行训练,通过所述验证集对训练后的包裹检测模型进行测试和参数调整；得到最终的包裹检测模型；S3.获取实时的物流包裹视频并对其进行关键帧采样,输入所述最终的包裹检测模型进行检测从而获得每个关键帧的包裹位置信息；S4.根据关键帧的包裹位置信息,采用多目标跟踪算法对包裹进行计数。</td>   <td>G06T7/00;G06T7/246</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈定安;              刘智泉;                   招倩莹       </td>   <td>中山大学</td>   <td>基于单一图像的国旗检测方法</td>   <td>广东省</td>   <td>CN110992308A</td>   <td>2020-04-10</td>   <td>本发明提供的基于单一图像的国旗检测方法,包括：构建五角星几何模型；从拍摄的图像中提取每个五角星的边缘点；从大五角星的边缘点中提取出五角星的五个顶点；计算大星五个顶点的物体空间坐标,校正相机姿态及畸变；将每个五角星的顶点进行坐标转换,计算每个五角星的中心和角度；根据计算出的五角星的中心和角度与国家国旗规格对比,判断国旗是否符合规格。本发明提供的基于单一图像的国旗检测方法,通过基于五边形几何模型构建出五角星几何模型,将从普通数码照相机拍摄到的单一图像进行五角星提取并还原实物比例,利用构建的五角星形状的二维几何模型准去地判别国旗是否含有不符合规格的五角星。</td>   <td>1.基于单一图像的国旗检测方法,其特征在于：包括以下步骤：S1：基于五边形几何模型构建五角星几何模型；S2：运用边缘检测算法从拍摄的图像中提取每个五角星的边缘点；S3：利用直线提取的方式,从大五角星边缘点中计算出大五角星的五个顶点；S4：在仿真平台上确定一坐标点,根据正五角星二维集合模型得到大星五个顶点的物体空间坐标,校正照相机拍摄国旗时的姿态；S5：将每个五角星的顶点从照相机的影像坐标转换到物体空间坐标,计算每个五角星的中心和角度；S6：根据计算出的五角星的中心和角度与国家国旗规格对比,判断国旗是否符合规格。</td>   <td>G06T7/00;G06T7/60</td>  </tr>        <tr>   <td>中国专利</td>   <td>         单云霄;              陈自博;              崔良语;                   黄凯       </td>   <td>中山大学</td>   <td>一种实时水面船只视觉跟踪系统及其方法</td>   <td>广东省</td>   <td>CN110992403A</td>   <td>2020-04-10</td>   <td>本发明涉及一种实时水面船只视觉跟踪系统及其方法,该系统包括用于获取图像的图像获取模块、卡尔曼滤波模块和自适应模块,所述卡尔曼滤波模块包括卡尔曼滤波更新模块和卡尔曼滤波预测模块；自适应模块根据图像的误差值将图像决定调用卡尔曼滤波预测模块或卡尔曼滤波更新模块进行不同的追踪流程。本发明提出的自适应模块,通过预测帧间误差以及误差的变化趋势,动态的调用卡尔曼滤波的预测与更新模块,相较于现有方法面对不同的应用场景拥有更强大的适应能力。</td>   <td>1.一种实时水面船只视觉跟踪系统,包括用于获取图像的图像获取模块,其特征在于,还包括卡尔曼滤波模块和自适应模块,所述卡尔曼滤波模块包括卡尔曼滤波更新模块和卡尔曼滤波预测模块；所述自适应模块预设有误差协方差矩阵阈值和梯度阀值,自适应模块将误差协方差矩阵阈值和梯度阀值与图像的误差值进行比较,并将根据比较结果选择卡尔曼滤波预测模块或卡尔曼滤波更新模块对图像进行进一步的处理。</td>   <td>G06T7/246;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         高成英;                   罗幸荣       </td>   <td>中山大学</td>   <td>一种纤维级织物实时渲染方法、系统及终端</td>   <td>广东省</td>   <td>CN110969692A</td>   <td>2020-04-07</td>   <td>本申请公开了一种纤维级织物实时渲染方法、系统及终端,对纤维级织物的纱线控制点进行曲线插值和预计算；对曲线插值和预计算后的所述纱线控制点进行遍历并渲染得到深度贴图；生成所述纤维级织物的核心纤维与常规纤维；对所述核心纤维与常规纤维进行遮挡剔除；对所述深度贴图中的纤维级织物进行阴影和光照计算获得渲染结果。对纤维级织物的纱线控制点进行预计算,由前往后对纱线控制点进行遍历并渲染获得初步的深度贴图,然后进行核心纤维和常规纤维生成,最后进行遮挡剔除和亮度处理,进而保证了在不丢失外观细节的前提下进一步提升了渲染效率。</td>   <td>1.一种纤维级织物实时渲染方法,其特征在于,所述方法包括：对纤维级织物的纱线控制点进行曲线插值和预计算；对曲线插值和预计算后的所述纱线控制点进行遍历并渲染得到深度贴图；生成所述纤维级织物的核心纤维与常规纤维；对所述核心纤维与常规纤维进行遮挡剔除；对所述深度贴图中的纤维级织物进行阴影和光照计算获得渲染结果。</td>   <td>G06T15/40;G06T15/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         印鉴;                   周晨星       </td>   <td>中山大学</td>   <td>一种基于生成语义分割图的文本改写图片方法</td>   <td>广东省</td>   <td>CN110956579A</td>   <td>2020-04-03</td>   <td>本发明提供一种基于生成语义分割图的文本改写图片方法,该方法通过文字描述去修改图片中人物的衣服。不同于以往直接生成修改图片的方法,该方法首先通过双向的LSTM对文本进行编码来获取文本的语义特征,接着通过一个现有的语义分割模型获取原图片的语义分割图,然后将该语义分割图和文本编码进行拼接放入resnet网络中去学习文本编码和原语义分割图的联合表示,从而生成出修改图片的语义分割图,最后再将该生成的语义分割图和原文本编码再次拼接放入另外一个resnet网络中去学习文本编码和生成的语义分割图之间的关系表示生成出最终修改完成的图片。</td>   <td>1.一种基于生成语义分割图的文本改写图片方法,其特征在于,包括以下步骤：S1：建立生成输入图片的语义分割图模型G,语义分割图的特征抽取器T以及生成文本语义信息的双向编码器LSTM网络；S2：构建resnet1网络,将S1中生成的语义分割特征和文本语义特征输入该网络中通过GAN训练方法生成修改图片的语义分割图P；S3：构建resnet2网络,将S2中生成的语义分割图P和S1中生成的文本语义特征输入该网络中通过GAN训练方法生成修改图片。</td>   <td>G06T3/00;G06T7/11;G06T5/50;G06F40/211;G06F40/279</td>  </tr>        <tr>   <td>中国专利</td>   <td>         韦艳宏;              钟霞丽;              柯炜健;                   王璨       </td>   <td>中山大学</td>   <td>基于高内涵成像系统的芳基磷酸酯类阻燃剂血管毒性的评价方法</td>   <td>广东省</td>   <td>CN110956625A</td>   <td>2020-04-03</td>   <td>本发明公开了基于高内涵成像系统的芳基磷酸酯类阻燃剂血管毒性的评价方法,包括以下步骤：1)获取芳基磷酸酯类阻燃剂处理的斑马鱼胚胎/幼鱼血管荧光图像；2)血管图像分割识别与分析；3)获得评价和筛选结果。通过采集特异性血管荧光标记的转基因斑马鱼胚胎/幼鱼的血管图像,分析多血管参数实现对芳基磷酸酯类阻燃剂血管发育毒性评价和筛选。该方法配置包括：硬件系统负责血管图像数据的获取,由高内涵成像系统及配套琼脂糖包被的96孔玻底板组成；软件系统用于目标血管的半自动识别以及多参数输出分析,涉及Photoshop、Matlab以及GraphPad Prism等软件。本发明具有高通量的特征,结果稳定,能快速准确对芳基磷酸酯类阻燃剂进行多参数指标的血管发育毒性评价与筛选。</td>   <td>1.基于高内涵成像系统的芳基磷酸酯类阻燃剂血管毒性的评价方法,其特征在于,包括以下步骤：1)获取芳基磷酸酯类阻燃剂处理的斑马鱼胚胎/幼鱼血管荧光图像；2)血管图像分割识别与分析；3)获得评价和筛选结果。</td>   <td>G06T7/00;G06T7/11;G06T7/62;G01N21/64</td>  </tr>        <tr>   <td>中国专利</td>   <td>         朱艺;                   衣杨       </td>   <td>中山大学</td>   <td>一种基于深度学习的人体行为识别方法</td>   <td>广东省</td>   <td>CN110956085A</td>   <td>2020-04-03</td>   <td>本发明提供一种基于深度学习的人体行为识别方法,该方法包括基于3D ResNet的改进模型M,是一种基于深度学习的视频特征提取模型；对输入数据预处理的方法,利用TSN方法对输入的数据进行预处理；提供了一种应用实现的方法,包括如何将一个视频的时间特征以及空间特征提取出来,并使用这些特征识别视频中的人体行为。当用户使用视频作为输入时,可以根据输入视频,识别出视频中的行为。</td>   <td>1.一种基于深度学习的人体行为识别方法,其特征在于,包括以下步骤S1：对输入视频片段进行预处理；S2：构建基于ResNet3D的改进网络模型M；S3：模型训练与测试；S4：建立用于提供后台接口的进程,提供识别入口及预测反馈；其中ResNet3D是基于深度学习的视频特征提取模型。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              卓嘉璇;                   陈培佳       </td>   <td>中山大学</td>   <td>一种基于教师学生学习框架的遮挡行人再标识方法</td>   <td>广东省</td>   <td>CN110956158A</td>   <td>2020-04-03</td>   <td>本发明公开了一种基于教师学生学习框架的遮挡行人再标识方法,包括步骤：首先训练教师网络,利用已有的大规模完整行人数据来模拟遮挡行人再标识的训练过程,该过程的实现由带有跨域模拟器的联合显著检测网络来完成,这个过程为教师教学过程；然后再将教师网络传递给学生网络,让学生网络利用教师网络的模型在真实的小规模遮挡行人数据上继续训练,这个过程为学生实践过程；最后,通过上述教师教学和学生实践过程,训练得到一个既有行人判别性又有遮挡鲁棒性的模型,可用于遮挡行人再标识。本发明能较大幅度地提高现有的遮挡行人再标识任务的效果性能,具有广泛的应用价值。</td>   <td>1.一种基于教师学生学习框架的遮挡行人再标识方法,其特征在于,包括步骤：首先,训练教师网络,利用已有的大规模完整行人数据来模拟遮挡行人再标识的训练过程,该过程的实现由带有跨域模拟器的联合显著检测网络来完成,这个过程为教师教学过程；然后,再将教师网络传递给学生网络,让学生网络利用教师网络的模型在真实的小规模遮挡行人数据上继续训练,这个过程为学生实践过程；最后,通过上述教师教学和学生实践过程,训练得到一个既有行人判别性又有遮挡鲁棒性的模型,用于遮挡行人再标识。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         侯舟帆;              陈龙;                   张亚琛       </td>   <td>中山大学</td>   <td>基于轻量级网络的实时目标检测与语义分割的多任务学习方法</td>   <td>广东省</td>   <td>CN110941995A</td>   <td>2020-03-31</td>   <td>本发明涉及一种基于轻量级网络的实时目标检测与语义分割的多任务学习方法。包括特征提取模块、语义分割模块、目标检测模块以及多尺度感受野模块；特征提取模块选择轻量级卷积神经网络MobileNet,通过MobileNet网络提取特征,送入语义分割模块去完成道路可行驶区域与可选择行驶区域的分割问题,同时将特征送入目标检测模块去完成道路场景下出现的物体检测；通过多尺度感受野模块增加特征图的感受域,用不同尺度的卷积解决多尺度难题,最终损失函数通过语义分割模块的损失函数与目标检测模块的损失函数进行加权求和,对总模块进行优化。本发明提供的方法相比现有技术做到了更快速,更准确地完成道路物体检测以及道路行驶区域分割这两种常见的无人驾驶感知任务。</td>   <td>1.一种基于轻量级网络的实时目标检测与语义分割的多任务学习方法,其特征在于,包括特征提取模块、语义分割模块、目标检测模块以及多尺度感受野模块；所述的特征提取模块选择轻量级卷积神经网络MobileNet,通过MobileNet网络提取特征,送入上层的语义分割模块去完成道路可行驶区域与可选择行驶区域的分割问题,同时将特征送入下层的目标检测模块去完成道路场景下出现的物体检测；通过多尺度感受野模块增加特征图的感受域,用不同尺度的卷积解决多尺度难题,最终损失函数通过语义分割模块的损失函数与目标检测模块的损失函数进行加权求和,对总模块进行优化。</td>   <td>G06K9/00;G06K9/34;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         印鉴;              陈智聪;                   陈殷齐       </td>   <td>中山大学</td>   <td>一种基于旋转不变感知哈希的无人机航拍视频帧定位方法</td>   <td>广东省</td>   <td>CN110942002A</td>   <td>2020-03-31</td>   <td>本发明提供一种基于旋转不变感知哈希的无人机航拍视频帧定位方法,该方法有一定的平移不变形(pooling原理)；编码时使用差分编码,使得光照天气条件无关；再加上起点无关的圆周编码顺序使具有旋转不变性；圆周编码顺序时不编码边角部分,因为边角部分会因为视角不同差异大,从而排除了旋转时边角的其他景物不同影响。通过在真实无人机视频上的实验表明,本发明对比之前的感知哈希方法,成功解决了传统感知哈希不具有旋转不变的特性。</td>   <td>1.一种基于旋转不变感知哈希的无人机航拍视频帧定位方法,其特征在于,包括以下步骤：S1：对需要定位的视频帧-参考帧和需要检索的视频帧-目标帧进行旋转不变的哈希值计算；S2：将参考帧和目标帧与另一个时间的无人机视频所有帧对比哈希值差异找到差异最小的一帧。</td>   <td>G06K9/00;G06T7/80;G06T9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄以华;                   张睿       </td>   <td>中山大学</td>   <td>语音唇形拟合方法、系统及存储介质</td>   <td>广东省</td>   <td>CN110942502A</td>   <td>2020-03-31</td>   <td>本发明涉及一种语音唇形拟合方法,包括以下内容：采集目标人物视频数据集的图像数据和语音数据；提取所述图像数据中目标人物的唇形特征向量；提取所述语音数据中目标人物的语音特征向量；以语音特征向量为输入,唇形特征向量为输出,训练多尺度融合卷积神经网络；向多尺度融合卷积神经网络输入目标人物待拟合的语音特征向量,多尺度融合卷积神经网络生成拟合的唇形特征向量并进行输出,基于所述唇形特征向量对唇形进行拟合。</td>   <td>1.语音唇形拟合方法,其特征在于：包括以下内容：采集目标人物视频数据集的图像数据和语音数据；提取所述图像数据中目标人物的唇形特征向量；提取所述语音数据中目标人物的语音特征向量；以语音特征向量为输入,唇形特征向量为输出,训练多尺度融合卷积神经网络；向多尺度融合卷积神经网络输入目标人物待拟合的语音特征向量,多尺度融合卷积神经网络生成拟合的唇形特征向量并进行输出,基于所述唇形特征向量对唇形进行拟合。</td>   <td>G06T13/20;G06T13/40;G10L15/16;G10L15/25</td>  </tr>        <tr>   <td>中国专利</td>   <td>              李钊锋       </td>   <td>中山大学南方学院</td>   <td>大学生助学贷款还款管理系统</td>   <td>广东省</td>   <td>CN110930239A</td>   <td>2020-03-27</td>   <td>本发明公开大学生助学贷款还款管理系统,包括：学生通过学生客户端查看助学贷款信息、还款计划和今年还款情况,辅导员通过辅导员客户端查看所负责学生的贷款明细和今年还款情况,学生工作部通过学生工作部客户端上传全校学生的贷款数据、查看全校学生的贷款明细和今年还款情况。这样就能有助于大学生理清自己的助学贷款还款计划,同时有助于辅导员和学生工作部实时跟踪学生还款情况,及时对即将还款逾期的大学生进行个别关注,大大提高工作效率,帮助更好地做好贷后管理工作。</td>   <td>1.一种大学生助学贷款还款管理系统,其特征在于,该大学生助学贷款还款管理系统包括：学生客户端、辅导员客户端、学生工作部客户端和云服务器；所述学生客户端为安装助学贷款还款管理系统应用程序的智能手机,通过学生账号登录,所述学生账号为学号,初始登录密码为身份证号后6位数字；所述辅导员客户端为安装所述助学贷款还款管理系统应用程序的电脑,通过辅导员账号登录,所述辅导员账号为工号,初始登录密码为身份证号后6位数字；所述学生工作部客户端为安装所述助学贷款还款管理系统应用程序的电脑,通过学生工作部账号登录,所述学生工作部账号为“admin”,初始登录密码为“123456”；所述学生客户端、所述辅导员客户端、所述学生工作部客户端分别与所述云服务器连接。</td>   <td>G06Q40/02</td>  </tr>        <tr>   <td>中国专利</td>   <td>         吴迪;                   方静如       </td>   <td>中山大学</td>   <td>一种基于认知诊断的实践效果评估及学习路径推荐系统和方法</td>   <td>广东省</td>   <td>CN110930274A</td>   <td>2020-03-27</td>   <td>本发明提供一种基于认知诊断的实践效果评估及学习路径推荐方法,包括拓展特征预处理模型,深度知识追踪模型,知识网络构建模型和基于认知能力的路径推荐算法。拓展特征预处理模型根据学习者测验过程中的技能属性进行认知能力的初次评估,将个性化差异信息引入诊断模型。深度知识追踪模型根据测验序列与隐式编码的异构特征预测学习者的知识掌握能力,作为学习引导的基础。习题与知识网络构建模型提供了科学思维的全局导图,结合认知诊断为学习者推荐学习路径不但考虑到学习过程中的认知能力差异,而且遵循知识结构的逻辑。</td>   <td>1.一种基于认知诊断的实践效果评估及学习路径推荐系统,其特征在于,包括拓展特征预处理模型、深度学习追踪模型、知识网络构建模型和路径推荐模型；所述的拓展特征预处理模型采用树模型根据学习者的历史交互记录预测学习者在异构特征条件下的习题回答情况,得到学习者的认知能力的初步预测,并将预测结果与原始的习题答案序列联合作为深度学习追踪模型的输入；所述的深度学习追踪模型根据拓展特征预处理模型输出的信息输入至循环神经网络学习学习者的知识状态,经过神经网络的sigmoid激活函数将隐藏单元h<Sub>t</Sub>传递至全连接层获得输出y<Sub>t</Sub>,其表现出学习者对知识概念的认知能力,得到学习者的认知能力诊断；所述的知识网络构建模型根据学习者的历史交互记录构建知识网络,其中,自动发掘习题所对应知识概念是通过利用深度知识追踪模型探索习题间作答正确概率关系以及习题的题面经向量化后聚类两方面互相参照而形成；知识概念自身关联关系则根据总体习题得分率并适当参照学习者测验顺序和教材结构进行构建；所述的路径推荐模型根据上述模型所得的学习者的认知能力的初步预测、学习者的认知能力诊断和知识网络确定学习路径的最终元素,生成个性化学习路径。</td>   <td>G06Q50/20;G06F16/9535;G06F16/901;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王国利;              方媛;              郭雪梅;                   戴宪华       </td>   <td>中山大学</td>   <td>一种低剂量CT图像修复去噪方法</td>   <td>广东省</td>   <td>CN110930318A</td>   <td>2020-03-27</td>   <td>本发明公开了一种低剂量CT图像修复去噪的方法,包括：模拟生成训练所需的低剂量CT图像：将高剂量CT图像做扇形射束投影变换得到投影域的投影数据,将得到的投影矩阵进行指数运算后加入泊松噪声之后取对数,通过MATLAB自带的反投影函数将模拟的投影数据转回图像域得到模拟的低剂量CT图像。本发明所公开的低剂量CT图像修复去噪的方法高效地实现了低剂量CT图像到高剂量CT图像的转化,有效地恢复了CT图像上的细节部分,同时可以降低网络的复杂度,加快网络训练并提高重构效率,可在不影响医生诊断的情况下有效的降低CT技术对于病人带来的伤害。</td>   <td>1.一种低剂量CT图像修复去噪方法,其特征在于,包括以下步骤：S1：模拟生成训练所需的低剂量CT图像；S2：对S1中得到的低剂量CT图像进行兴趣区域的提取和分割；S3：将S2中得到的图像输入将训练好的模型得到修复好的CT图像。</td>   <td>G06T5/00;G06K9/32;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王国利;              董金凤;                   郭雪梅       </td>   <td>中山大学</td>   <td>一种基于群等变神经网络和条件概率场的病理切片识别方法</td>   <td>广东省</td>   <td>CN110930369A</td>   <td>2020-03-27</td>   <td>本发明提供一种基于群等变神经网络和条件概率场的病理切片识别方法,该方法使用群等变卷积核代替传统卷积核,提高了系统对于输入切片识别的鲁棒性,对于不同角度的输入具有更加优秀的一致性,在以上改变的基础上,本专利系统还使用了条件随机场算法,减少了预测结果中的噪声,提高了预测的准确率。</td>   <td>1.一种基于群等变神经网络和条件概率场的病理切片识别方法,其特征在于,包括以下步骤：S1：收集病理切片数据,并将获取的数据分成训练集和测试集；S2：对训练集数据进行人工标注,之后进行分割；S3：构建基于群等变换卷积和条件随机场的端到端的深度学习神经网络,并将S2得到的数据输入到该深度学习神经网络中进行训练；S4：将测试集数据输入到训练好的深度学习神经网络中进行测试。</td>   <td>G06T7/00;G06T7/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         钟鸣;              林凯荣;              陈晓宏;              江涛;                   王娇       </td>   <td>中山大学</td>   <td>一种基于信息扩散的小流域山洪灾害风险分析方法</td>   <td>广东省</td>   <td>CN108269016B</td>   <td>2020-03-27</td>   <td>本发明涉及一种基于信息扩散的小流域山洪灾害风险分析方法,其引入信息扩散理论,以有限的历史山洪资料为依据,将不完备信息条件下具有单值观测值的样本进行集值化,转变为具有模糊不确定性的模糊集值样本,建立山洪灾害信息矩阵,并引入皮尔逊三型曲线、超越概率及置信度的计算,从而实现通过有限的知识快速地进行山洪灾害风险定量化评价,使结果更加接近实际情况。本专利能够提高山洪灾害风险分析结果的客观性和科学性。</td>   <td>1.一种基于信息扩散的小流域山洪灾害风险分析方法,其特征在于：包括以下步骤：(1)资料搜集的步骤：收集小流域在历史上经受山洪灾害时的环境数据；(2)风险因子分析与选取的步骤：通过对收集的数据的分析,选取若干个可能诱发山洪灾害的环境影响因素作为风险因子；(3)利用信息扩散构建山洪灾害信息矩阵的步骤：根据信息扩散理论,利用正态信息扩散函数,构造研究区域山洪灾害信息矩阵,具体步骤如下：1)确定样本集的步骤：将所选取风险因子的在收集的环境数据中的实测数据作为观测样本点,用x表示,假设x数量为n,则样本集X如式①所示,X＝{x<Sub>1</Sub>,x<Sub>2</Sub>,x<Sub>3</Sub>,...,x<Sub>n-1</Sub>,x<Sub>n</Sub>}   ①2)确定论域的步骤：根据X的最大值b和最小值a,选取适当的监控点数m,取得监测点集合U如式②所示,          <Image id="icf0001" he="62" wi="700" file="FDA0002240468480000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0002" he="95" wi="538" file="FDA0002240468480000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中x<Sub>i</Sub>为观测样本集X中第i个样本点,1≤i≤n3)构建山洪灾害信息矩阵的步骤：由于山洪灾害中水文过程的随机性比较符合正态分布,所以选取正态分布模型作为扩散模型,通过正态信息扩散函数③,样本集X中的每一个观测样本点x<Sub>i</Sub>都将其所携带的信息扩散给监测点集合U中的所有点,          <Image id="icf0003" he="152" wi="700" file="FDA0002240468480000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,u<Sub>j</Sub>为监测点集合U中的第j个元素,1≤j≤m；μ<Sub>ij</Sub>表示x<Sub>i</Sub>扩散至u<Sub>j</Sub>的信息；h是扩散系数,根据样本X中的最大值b和最小值a及样本X中x的数量n来确定,如公式④所示：          <Image id="icf0004" he="428" wi="700" file="FDA0002240468480000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        则,X在二维空间X×U上的山洪灾害信息矩阵Q,如式⑤所示：          <Image id="icf0005" he="310" wi="550" file="FDA0002240468480000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,Q<Sub>ij</Sub>为山洪灾害信息矩阵Q中第i行第j列的元素(4)对矩阵归一化处理的步骤：对山洪灾害信息矩阵Q的各行进行归一化处理：          <Image id="icf0006" he="88" wi="415" file="FDA0002240468480000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        将各行分别除以所对应的C<Sub>i</Sub>的值,使得各行累加之和分别等于1,生成新的矩阵P,其中,p<Sub>ij</Sub>为矩阵P中第i行第j列的元素,如式⑦所示：          <Image id="icf0007" he="133" wi="290" file="FDA0002240468480000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        (5)计算风险概率的步骤：将各个监控点由不同实测数据分配得到的信息叠加,得到各个监测点所分配到的信息总和q<Sub>j</Sub>,如式⑧所示          <Image id="icf0008" he="86" wi="320" file="FDA0002240468480000025.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        再将监测点累加得到的信息除以q<Sub>j</Sub>叠加得到的样本总和数,得到各监控点的风险概率值w<Sub>j</Sub>,如式⑨所示：          <Image id="icf0009" he="151" wi="337" file="FDA0002240468480000026.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        (6)计算风险超越概率的步骤：完成风险评估要计算超过u<Sub>j</Sub>值的概率总和,即形成超越概率R<Sub>k</Sub>,超越概率值表明不同区域面临不同程度山洪风险的差别,如式⑩所示：          <Image id="icf0010" he="93" wi="341" file="FDA0002240468480000031.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        (7)计算超越概率分布区间的步骤：计算超越概率后,对不同置信水平下的概率风险做一个范围的估计,假设置信水平为1-α,将R<Sub>k</Sub>重新从小到大排序,得到新的R<Sub>k</Sub>后,计算1-α置信度下的置信区间[R<Sub>k1</Sub>,R<Sub>k2</Sub>],其中,          <Image id="icf0011" he="89" wi="321" file="FDA0002240468480000032.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0012" he="91" wi="326" file="FDA0002240468480000033.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,z＝round(M(α/2)),round为四舍五入函数,M为监控点数；(8)计算风险因子阈值的步骤：通过皮尔逊三型曲线拟合水文关系曲线,根据历史数据得到灾害发生频率,结合具体情况确定洪水频率,通过拟合曲线获取风险因子阈值；(9)确定山洪灾害风险值的步骤：将计算得到的风险因子阈值代入超越概率与其分布区间,得到该值下风险值以及风险区间；将给定洪水频率对应的风险因子的实测数据代入超越概率分布中,计算超越概率值以及分布区间下的取值范围,即流域发生大于或等于某量级山洪灾害的风险；所述环境数据包括地形地貌数据、土壤植被数据、气象水文数据、人类活动数据；所述可能诱发山洪灾害的环境影响因素包括降雨因素、土壤因素、地形地貌因素、河段信息因素、人类活动因素、气象水文因素。</td>   <td>G06Q10/06;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              刘颜;              陈荣军;              李智文;              朱雄泳;              黄登;              邹兵兵;              嵇志辉;                   谢舜道       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学;中山大学花都产业科技研究院</td>   <td>基于低秩矩阵恢复的多曝光图像去伪影融合方法</td>   <td>广东省</td>   <td>CN106373105B</td>   <td>2020-03-24</td>   <td>本发明公开了一种基于低秩矩阵恢复的多曝光图像去伪影融合方法。首先,归一化输入多曝光图像序列；接着,使用相机响应函数对归一化后的图像序列进行辐射校准；然后向量化多曝光图像序列构成低秩矩阵恢复的数据矩阵；使用改进的低秩矩阵恢复算法得到低秩矩阵；从低秩矩阵数据中恢复目标的高动态范围(High dynamic range,HDR)图像。本发明利用低秩矩阵恢复的最新研究成果,能够得到有效去除融合后的HDR图像中的伪影和模糊问题。</td>   <td>1.一种基于低秩矩阵恢复的多曝光图像去伪影融合方法,其特征在于,包括：a)对输入多曝光图像序列进行归一化处理；b)对归一化处理后的多曝光图像序列使用相机响应函数实现辐射校准；c)向量化多曝光图像序列中每一幅辐射校正的图像作为数据矩阵的列向量；d)使用低秩矩阵恢复算法求解数据矩阵的低秩矩阵；e)由低秩矩阵重构出高动态范围图像；所述步骤d)的具体实现过程为：d1)对数据矩阵D进行低秩矩阵恢复算法,求解得到低秩矩阵L：          <Image id="icf0001" he="52" wi="700" file="FDA0002258780310000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,参数为优化方程的惩罚因子λ＞0,R为目标低秩矩阵L的秩,E表示稀疏噪声矩阵；上式为多曝光图像去伪影融合问题,利用凸松弛将求秩函数转换为截断核函数,l<Sub>0</Sub>范数转换为l<Sub>1</Sub>范数求得最优解：          <Image id="icf0002" he="53" wi="700" file="FDA0002258780310000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,截断核函数为<Image id="icf0003" he="96" wi="536" file="FDA0002258780310000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>σ<Sub>i</Sub>为矩阵L的奇异值；对于多曝光图像去伪影融合方法令R＝1；d2)采用增广拉格朗日乘子法求解该问题,定义拉格朗日方程为<Image id="icf0004" he="80" wi="317" file="FDA0002258780310000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>          <Image id="icf0005" he="98" wi="700" file="FDA0002258780310000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,μ＞0是拉格朗日乘子法的惩罚因子,Y是拉格朗日乘子矩阵,&lt;A,B&gt;＝trace(A<Sup>T</Sup>B)为矩阵内积,<Image id="icf0006" he="143" wi="569" file="FDA0002258780310000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为Frobenius范数；则优化方程转化为无约束问题<Image id="icf0007" he="79" wi="401" file="FDA0002258780310000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>求解方法如下：d3)令迭代次数定义为k；迭代开始时k＝0,初始化各个矩阵为：Y<Sub>0</Sub>＝D/max(||D||<Sub>2</Sub>,||D||<Sub>∞</Sub>)L<Sub>0</Sub>＝0E<Sub>0</Sub>＝0          <Image id="icf0008" he="51" wi="700" file="FDA0002258780310000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,M为图像像素总数,Y<Sub>0</Sub>的初始化可以使得目标函数值&lt;D,Y<Sub>0</Sub>&gt;适当的大；||D||<Sub>2</Sub>＝max(σ<Sub>i</Sub>|i＝1,...,rank(D)),<Image id="icf0009" he="163" wi="493" file="FDA0002258780310000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>d4)更新低秩矩阵L<Sub>k+1</Sub>和稀疏噪声矩阵E<Sub>k+1</Sub>：          <Image id="icf0010" he="57" wi="700" file="FDA0002258780310000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        d5)更新拉格朗日乘子矩阵Y<Sub>k+1</Sub>和拉格朗日乘子法的惩罚因子μ<Sub>k+1</Sub>：Y<Sub>k+1</Sub>＝Y<Sub>k</Sub>+μ<Sub>k</Sub>(D-L<Sub>k+1</Sub>-E<Sub>k+1</Sub>)μ<Sub>k+1</Sub>＝min(ρμ<Sub>k</Sub>,μ<Sub>max</Sub>)         (10)其中,ρ为迭代收敛因子,控制收敛速度和收敛误差；μ<Sub>max</Sub>确保μ<Sub>k</Sub>有界,且令μ<Sub>max</Sub>＝10<Sup>7</Sup>μ<Sub>0</Sub>；增广拉格朗日乘子法求解的好处是如果{μ<Sub>k</Sub>}是递增序列且有界该求解方法线性收敛,如果{μ<Sub>k</Sub>}无界该求解方法超线性收敛；这里令ρ＝1.5,可以保证收敛速度和收敛误差得到平衡；d6)定义迭代终止条件为：          <Image id="icf0011" he="86" wi="700" file="FDA0002258780310000025.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,∈为误差阈值；为了得到较精准的收敛结果,令∈＝10<Sup>-5</Sup>；令最大迭代次数为k<Sub>max</Sub>＝500,如果满足迭代条件或者到达最大迭代次数则进入步骤e)；否则自增迭代步数k,回到步骤d4)继续迭代。</td>   <td>G06T5/00;G06T5/50;G06T7/80</td>  </tr>        <tr>   <td>中国专利</td>   <td>              李钊锋       </td>   <td>中山大学南方学院</td>   <td>大学生放假前后去向跟踪系统</td>   <td>广东省</td>   <td>CN110910287A</td>   <td>2020-03-24</td>   <td>本发明公开大学生放假前后去向跟踪系统,包括：学生通过学生客户端填写假期去向情况和假后报到,辅导员通过辅导员客户端查看所负责学生的假期去向情况和报到情况,学生工作部通过学生工作部客户端上传和修改学生信息、设置具体假期、查看所有学生的假期去向和报到情况。这样就能实现提高辅导员对于学生假期去向的统计工作效率,便于辅导员详细了解学生的具体去向,做好学生的假期安全保障工作。</td>   <td>1.一种大学生放假前后去向跟踪系统,其特征在于,该大学生放假前后去向跟踪系统包括：学生客户端、辅导员客户端、学生工作部客户端和云服务器；所述学生客户端为安装放假前后去向跟踪系统应用程序的智能手机,通过学生账号登录,所述辅导员客户端为安装所述放假前后去向跟踪系统应用程序的电脑,通过辅导员账号登录,所述学生工作部客户端为安装所述放假前后去向跟踪系统应用程序的电脑,通过学生工作部账号登录；所述学生客户端、所述辅导员客户端、所述学生工作部客户端分别与所述云服务器连接。</td>   <td>G06Q50/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         罗锦兴;              何朗;                   谢晓华       </td>   <td>中山大学</td>   <td>一种三维脉波影像去噪方法、系统及存储介质</td>   <td>广东省</td>   <td>CN110910321A</td>   <td>2020-03-24</td>   <td>本发明公开了一种三维脉波影像去噪方法、系统及存储介质,所述方法包括：获取三维脉波影像数据；在某个心跳周期内三维脉波影像的帧数小于阈值时,将每帧图像插值,使所有心跳周期内三维脉波影像的帧数大于或等于阈值；将插值处理后的三维脉波影像数据进行分解,得到一个低秩矩阵；根据所述低秩矩阵,建立三维脉波影像曲面,输出去噪结果。本发明能够弥补三维脉波影像的图像去噪的缺失,改善去噪的效果。</td>   <td>1.一种三维脉波影像去噪方法,其特征在于,包括：获取三维脉波影像数据；在某个心跳周期内三维脉波影像的帧数小于阈值时,将每帧图像插值,使所有心跳周期内三维脉波影像的帧数大于或等于阈值；将插值处理后的三维脉波影像数据进行分解,得到一个低秩矩阵；根据所述低秩矩阵,建立三维脉波影像曲面,输出去噪结果。</td>   <td>G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         朱亚琛;              陈龙;                   刘聪       </td>   <td>中山大学</td>   <td>一种基于图描述子的激光SLAM回环检测系统及方法</td>   <td>广东省</td>   <td>CN110910389A</td>   <td>2020-03-24</td>   <td>本发明涉及一种基于图描述子的激光SLAM回环检测系统及方法。包括语义分割模块,全图描述子提取模块,全图描述子匹配模块,顶点描述子提取模块,顶点描述子匹配模块和几何一致性验证模块。本发明利用了从点云数据中提取的语义信息形成了全图描述子和顶点描述子两种图描述子来进行点云帧和语义物体的表征。相比于传统算法从像素级别提取描述子,本发明可以避免繁重的法向量计算任务和避免视角大幅变化带来的问题,可以更加快速和鲁棒地检测回环。本发明先通过全图描述子粗略地筛选出潜在的回环候选帧,接着用顶点描述子更加精细地匹配查询帧与回环候选帧的细节信息,是一个由粗到精的查找过程,为方法的实时性提供了理论保障。</td>   <td>1.一种基于图描述子的激光SLAM回环检测系统,其特征在于,包括：语义分割模块：用于感知外部环境,提取无序点云数据中的语义物体信息,得到物体的预测置信度和其质心三维空间坐标并输出给全图描述子提取模块和顶点描述子提取模块；全图描述子提取模块：用于以点云数据中物体的质心为顶点,顶点与顶点之间的欧几里得距离为边构成一个完全图,将图中所有边按长度存放到一维计数向量中,该向量即全图描述子,后将全图描述子输出到全图描述子匹配模块；全图描述子匹配模块：用于利用KD树这一数据结构加速查找查询帧与历史所有帧欧氏距离最近的n个帧,并把这n个帧都记为回环候选帧,后把回环候选帧输出给顶点描述子提取模块；顶点描述子提取模块：用于对查询帧和所有回环候选帧中的每一个顶点,将与该顶点相连的边按长度存放到一维计数向量中,该向量即为对应顶点的顶点描述子,后输出到顶点描述子匹配模块；顶点描述子匹配模块：利用欧氏距离度量查询帧与回环候选帧中有相似预测置信度的语义物体的顶点描述子,得出查询帧与每一回环候选帧中顶点的一一对应关系；并利用基于RANSAC的几何一致性验证方法得出查询帧与所有回环候选帧的位姿变换关系和匹配误差,并依据这个匹配误差判断是否存在回环。</td>   <td>G06T7/10;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         高燕;              武志毅;              李文龙;                   周羿彬       </td>   <td>中山大学</td>   <td>基于Revit软件的地质环境载体断层模型的构建方法及装置</td>   <td>广东省</td>   <td>CN110910499A</td>   <td>2020-03-24</td>   <td>本发明涉及一种基于Revit软件的地质环境载体断层模型的构建方法及装置,其中方法包括：依据断层发育情况将区域分为多个单位,并对各单位分别进行钻孔族及地层建模；基于Kriging插值法得到每个地层的内插点；基于Delaunay三角插值法进行地质环境载体地层的建模；载体断层平面模型构建：基于边界虚拟钻孔法和三维空间平面拟合构建地质环境载体断层平面模型构建；载体断层曲面模型构建：基于边界虚拟钻孔法、Kriging插值法和Delaunay三角插值法构建地质环境载体断层曲面模型。与现有技术相比,本发明解决了Revit软件中没有三维地质环境载体建模模块的问题。</td>   <td>1.一种基于Revit软件的地质环境载体断层模型的构建方法,其特征在于,包括：依据断层发育情况将区域分为多个单位,并对各单位分别进行钻孔族及地层建模；基于Kriging插值法得到每个地层的内插点；基于Delaunay三角插值法进行地质环境载体地层的建模；载体断层平面模型构建：基于边界虚拟钻孔法和三维空间平面拟合构建地质环境载体断层平面模型构建；载体断层曲面模型构建：基于边界虚拟钻孔法、Kriging插值法和Delaunay三角插值法构建地质环境载体断层曲面模型。</td>   <td>G06T17/05</td>  </tr>        <tr>   <td>中国专利</td>   <td>         彭虎;              陈翔;                   陈晓春       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;深圳清华大学研究院;中山大学</td>   <td>一种交叉纤维特征点匹配方法</td>   <td>广东省</td>   <td>CN107016394B</td>   <td>2020-03-24</td>   <td>本发明公开一种交叉纤维特征点匹配方法,该方法主要流程为：生成纤维原始图像进行预处理操作,提取边缘纤维轮廓线,计算边缘纤维轮廓线上各点的曲率值并判断纤维轮廓线角点,构建动态支撑域删除伪角点,使用两点法确定凹点并标注,计算各个凹点的左右斜率和各个凹点与凹点之间的斜率并进行比较查找匹配点。本发明克服现有技术所述的传统人工识别纤维技术的缺陷,通过交叉重叠纤维特征点的匹配检测纤维成分含量及参数测量,不受人为主观情绪影响,操作简单,智能化程度高。</td>   <td>1.一种交叉纤维特征点匹配方法,其特征在于,包括步骤：S1：生成纤维原始图像,读入并将其二值化,将二值化后纤维灰度图像经开、闭运算、图像填充和增强预处理操作；S2：提取经预处理操作后的图像纤维边缘并对其进行平滑处理,抽取图像纤维轮廓线；S3:计算图像纤维轮廓线上各像素点的曲率值,通过与预设的第一阈值比较来确定角点；S4：在确定角点的基础上使用两点法确定凹点,把同一交叉区域内的凹点归类为同一集合,并对该集合的所有凹点进行标注；S5：在同一集合内,计算两相邻凹点P<Sub>i</Sub>和P<Sub>j</Sub>之间的斜率K<Sub>ij</Sub>,其中i≠j,在纤维轮廓线上计算凹点P<Sub>i</Sub>与距离其左边n个像素的点P<Sub>L</Sub>斜率K<Sub>iL</Sub>,记为凹点P<Sub>i</Sub>左斜率；在纤维轮廓线上计算凹点P<Sub>i</Sub>与距离其右边n个像素的点P<Sub>R</Sub>斜率K<Sub>iR</Sub>,记为凹点P<Sub>i</Sub>右斜率；同时记凹点P<Sub>j</Sub>与点P<Sub>L</Sub>之间斜率为K<Sub>jL</Sub>；记凹点P<Sub>j</Sub>与点P<Sub>R</Sub>之间斜率为K<Sub>jR</Sub>；S6：若凹点P<Sub>i</Sub>相邻的凹点P<Sub>j</Sub>斜率K<Sub>ij</Sub>与P<Sub>i</Sub>的左斜率K<Sub>iL</Sub>相减差值h<Sub>1</Sub>在预设的第二阈值范围之内；若斜率K<Sub>jL</Sub>与两相邻凹点P<Sub>i</Sub>和P<Sub>j</Sub>之间的斜率K<Sub>ij</Sub>相减差值h<Sub>2</Sub>在预设的第三阈值范围之内；取最小的h<Sub>1</Sub>和h<Sub>2</Sub>,则该凹点判为凹点P<Sub>i</Sub>左匹配点,即          <Image id="icf0001" he="139" wi="700" file="FDA0002206799680000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        S7：若凹点P<Sub>i</Sub>相邻的凹点P<Sub>j</Sub>斜率K<Sub>ij</Sub>与P<Sub>i</Sub>的右斜率K<Sub>iR</Sub>相减差值h<Sub>3</Sub>在预设的第二阈值范围之内；若斜率K<Sub>jR</Sub>与两相邻凹点P<Sub>i</Sub>和P<Sub>j</Sub>之间的斜率K<Sub>ij</Sub>相减差值h<Sub>4</Sub>在预设的第三阈值范围之内；取最小的h<Sub>3</Sub>和h<Sub>4</Sub>,则该凹点判为凹点P<Sub>i</Sub>右匹配点,即          <Image id="icf0002" he="137" wi="700" file="FDA0002206799680000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image></td>   <td>G06K9/46</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陶雪锋;              蔡俊浩;                   成慧       </td>   <td>中山大学</td>   <td>一种基于协同注意力机制的机械臂指定物体抓取方法</td>   <td>广东省</td>   <td>CN110889460A</td>   <td>2020-03-17</td>   <td>本发明涉及一种基于协同注意力机制的机械臂指定物体抓取方法,包括以下步骤：步骤一：将目标物体放入工作空间中,通过拍摄获得检索数据；将目标物体和其他物体一起放入工作空间中,通过拍摄获得工作空间数据；步骤二：将检索数据和工作空间数据输入深度神经网络,进行深度神经网络训练,深度神经网络为带约束的协同注意力网络；步骤三：将完成训练的深度神经网络模型应用于真实的物体抓取环境中。通过同时输入检索图片和工作空间图片,方法能够实时,有效,快速地根据检索图片信息从工作空间图片中输出目标物体的抓取功能图。即使是全新的,模型未见过的物体,模型同样能够做到输出正确的物体的抓取功能图,并且具有较高的抓取准确率。</td>   <td>1.一种基于协同注意力机制的机械臂指定物体抓取方法,其特征在于,包括以下步骤：步骤一：将目标物体放入工作空间中,通过拍摄获得检索数据,包括检索图片和生成目标物体的掩膜；将目标物体和其他物体一起放入工作空间中,通过拍摄获得工作空间数据,包括工作空间图片、目标物体掩膜和其他物体的掩膜；步骤二：将检索数据和工作空间数据输入深度神经网络,进行深度神经网络训练,深度神经网络为带约束的协同注意力网络；步骤三：将完成训练的深度神经网络模型应用于真实的物体抓取环境中。</td>   <td>G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              陈志宇;                   袁陶希       </td>   <td>中山大学</td>   <td>一种基于特征点及邻域特征匹配的掌纹认证方法</td>   <td>广东省</td>   <td>CN106951874B</td>   <td>2020-03-13</td>   <td>本发明以掌纹验证为应用背景,提出一种基于特征点和邻域特征匹配的掌纹认证方法,通过对注册样本和问询样本的SIFT特征点与其包含的邻域特征进行结合,提高特征点匹配的可靠性。同时进一步针对掌纹的非线性形变,将其分解为若干个局部线性变换,解决了现有方法在所采集掌纹存在形变时认证准确率下降的问题,提高掌纹认证的准确率。</td>   <td>1.一种基于特征点及邻域特征匹配的掌纹认证方法,其特征在于：包括以下步骤：S1.对于问询样本I<Sub>2</Sub>和注册样本I<Sub>1</Sub>,分别使用尺度不变特征变换方法对其进行尺度不变特征变换SIFT特征点的检测；S2.得到问询样本I<Sub>2</Sub>和注册样本I<Sub>1</Sub>的SIFT特征点后,将问询样本I<Sub>2</Sub>的SIFT特征点与注册样本I<Sub>1</Sub>的SIFT特征点进行SIFT特征点对的匹配,得到若干对SIFT特征点对,并将其存储在匹配点集合S中；S3.对于匹配点集合S中的每对SIFT特征点对,通过步骤S31～S331的方法分别计算其包括的两个SIFT特征点的邻域特征：S31.设其中任一SIFT特征点为X,遍历其周围的SIFT特征点,选取与SIFT特征点X欧氏距离最近的n个SIFT特征点作为近邻点；其中n为正整数；S32.以欧氏距离最近的SIFT特征点为起点,按照顺时针方向排列这n个近邻点；S33.基于排列好的n个近邻点依次提取其几何特征描述,如步骤S331所示：S331.设当前近邻点为A,顺时针顺序的下一个近邻点为B,提取当前近邻点A的以下几何特征描述：1)相对距离：分别计算SIFT特征点X和近邻点A、近邻点B的距离,其比值为相对距离R<Sub>1</Sub>,<Image id="icf0001" he="146" wi="223" file="FDA0002309700070000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>2)相对方向：SIFT特征点X中包含特征点X的主方向信息,设SIFT特征点X的主方向和近邻点A、近邻点B的主方向之间的夹角分别为<Image id="icf0002" he="46" wi="152" file="FDA0002309700070000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其比值定义为相对方向R<Sub>2</Sub>：<Image id="icf0003" he="171" wi="234" file="FDA0002309700070000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>3)相对夹角：SIFT特征点X和近邻点A、近邻点B形成三角形ΔAXB,其中∠XBA和∠XAB的比值定义为相对夹角R<Sub>3</Sub>：<Image id="icf0004" he="148" wi="304" file="FDA0002309700070000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>S34.经过步骤S33后得到n组几何特征描述,在所述n组几何特征描述中,每组几何特征描述使用三元组r<Sub>i</Sub>＝(R<Sub>1</Sub>,R<Sub>2</Sub>,R<Sub>3</Sub>)表示,i＝1,2,…,n,则SIFT特征点X的邻域特征以3n维几何特征描述向量进行表示；S4.对于每对SIFT特征点对,计算得到SIFT特征点对的两个SIFT特征点的邻域特征后,计算两个SIFT特征点的基于邻域特征的距离：          <Image id="icf0005" he="197" wi="606" file="FDA0002309700070000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中R<Sub>ij1</Sub>和R<Sub>ij2</Sub>分别表示两个SIFT特征点的第i个近邻点的第j个几何特征描述；S5.当R＜t时,在匹配点集合S中保留该SIFT特征点对,否则在匹配点集合S中删除这对SIFT特征点对,其中t为设定的阈值；S6.计算匹配点集合S中每对SIFT特征点对的相对偏角；S7.对于匹配点集合S中的任意两对SIFT特征点对,它们的距离Q定义为：          <Image id="icf0006" he="152" wi="331" file="FDA0002309700070000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中Δθ<Sub>a</Sub>与Δθ<Sub>b</Sub>分别表示两对SIFT特征点对的相对偏角；若Q＜τ即Q小于设定的阈值,则认为这两对SIFT特征点对属于同一线性变换模型；S8.基于步骤S7将匹配点集合S划分为M个集合S<Sub>1</Sub>,S<Sub>2</Sub>,...,S<Sub>M</Sub>；S9.对于集合S<Sub>m</Sub>,其对应的线性变换模型用相对偏角<Image id="icf0007" he="78" wi="59" file="FDA0002309700070000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和相对位移<Image id="icf0008" he="83" wi="66" file="FDA0002309700070000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>刻画,其中m＝{1,2...M},<Image id="icf0009" he="73" wi="59" file="FDA0002309700070000025.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0010" he="80" wi="71" file="FDA0002309700070000026.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>的计算方式如下：          <Image id="icf0011" he="238" wi="305" file="FDA0002309700070000027.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0012" he="92" wi="599" file="FDA0002309700070000028.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中q表示集合S<Sub>m</Sub>中包含的SIFT特征点对的数量,Δθ<Sub>i</Sub>表示第i对SIFT特征点对的相对偏角,<Image id="icf0013" he="72" wi="205" file="FDA0002309700070000029.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0014" he="71" wi="212" file="FDA00023097000700000210.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>分别表示在注册样本I<Sub>1</Sub>和问询样本I<Sub>2</Sub>中,集合S<Sub>m</Sub>包含的SIFT特征点的质心坐标；S10.依据<Image id="icf0015" he="73" wi="57" file="FDA00023097000700000211.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0016" he="81" wi="69" file="FDA00023097000700000212.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>对问询样本进行矫正,得到问询图像I'<Sub>m2</Sub>,具体过程如下：1).将问询样本I<Sub>2</Sub>以<Image id="icf0017" he="71" wi="210" file="FDA00023097000700000213.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为中心,逆时针旋转角度<Image id="icf0018" he="73" wi="85" file="FDA00023097000700000214.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>2).将1)中旋转后的问询样本平移<Image id="icf0019" he="79" wi="98" file="FDA00023097000700000215.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>得到矫正的问询图像I'<Sub>m2</Sub>；S11.将注册样本I<Sub>1</Sub>和问询图像I'<Sub>m2</Sub>划分成n×n个方块单元格；S12.将包含有SIFT特征点的单元格作为种子单元格,然后将注册样本I<Sub>1</Sub>和问询图像I'<Sub>m2</Sub>中的种子单元格分别添加进像素区域集合R<Sub>m</Sub>和R'<Sub>m</Sub>；S13.将注册样本I<Sub>1</Sub>和问询图像I'<Sub>m2</Sub>中的种子单元格的8邻域单元格分别添加进像素区域集合R<Sub>m</Sub>和R'<Sub>m</Sub>中；S14.对像素区域集合R<Sub>m</Sub>和R'<Sub>m</Sub>进行以单元格为单位的膨胀和腐蚀的操作,直至R<Sub>m</Sub>和R'<Sub>m</Sub>形成完整的封闭区域,得到一组对应的匹配区域；S15.对M个集合分别进行步骤S9～S14的处理,共得到M组对应的匹配区域；S16.对M组对应的匹配区域进行像素级的特征提取,然后基于提取的特征分别计算每组对应的匹配区域之间的距离；进一步统计M组对应的匹配区域之间的距离之和；S17.判断距离之和是否小于所设定的阈值,若是则认定问询样本I<Sub>2</Sub>和注册样本I<Sub>1</Sub>匹配,否则则认定问询样本I<Sub>2</Sub>和注册样本I<Sub>1</Sub>不匹配。</td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              何娜;              陈佳捷;              罗子泉;                   朱睿       </td>   <td>中山大学;佛山市新东方电子技术工程有限公司</td>   <td>一种基于方向梯度特征学习的目标检测方法</td>   <td>广东省</td>   <td>CN107145894B</td>   <td>2020-03-13</td>   <td>一种基于方向梯度特征学习的目标检测方法。本发明提供的方法以图像块中不同大小矩形区域内梯度大小之和为特征,基于boosting进行筛选,产生图像块的方向梯度特征,可替代手工设计的HOG特征。对不同目标的检测结果也表明,本发明所提出的方向梯度特征相对于传统的HOG特征能获得更好的检测效果。</td>   <td>1.一种基于方向梯度特征学习的目标检测方法,其特征在于：包括以下步骤：(1).方向梯度特征学习S1.采集N<Sub>+</Sub>个只含目标的图像区域和N<Sub>-</Sub>个不含目标的图像区域构成图像区域训练集X,在图像区域上每隔s个像素定义一个大小为w×h像素的图像块；S2.对于训练集X中的每个图像区域,计算其每个像素点的梯度大小及方向；S3.将梯度方向分成l个连续的方向区间,然后为每个图像区域生成l幅方向梯度图,其具体过程如下：S31.对于方向区间g的方向梯度图,若图像区域某个位置像素点的梯度方向在方向区间g内,则该方向梯度图在相同位置的取值设为步骤S2计算得到的像素点的梯度大小,否则将该方向梯度图在此处的取值置为0；S32.通过对每个像素点进行步骤S31的操作得到训练集X中每个图像区域在方向区间g的方向梯度图；S33.通过对每个方向区间进行步骤S31、S32的操作得到训练集X中每个图像区域的l幅方向梯度图；S4.对图像区域的每幅方向梯度图,基于S1中所述分块方式得到对应的方向梯度图像块集合；S5.对于每个方向区间的方向梯度图像块集合,执行以下操作：S51.从方向区间g的方向梯度图像块集合中选出若干方向梯度图像块形成方向梯度特征学习的训练集；S52.设方向梯度图像块训练集为<Image id="icf0001" he="94" wi="341" file="FDA0002309693230000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中N为训练集P中正样本、负样本的总数,x<Sub>i</Sub>表示第i个方向梯度图像块,y<Sub>i</Sub>表示第i个方向梯度图像块的类标；当x<Sub>i</Sub>属于正样本时,y<Sub>i</Sub>＝1；当x<Sub>i</Sub>属于负样本时,y<Sub>i</Sub>＝-1；设x<Sub>i</Sub>上包括有若干不同位置与大小的矩形区域,这些矩形区域的特征用矩形区域内的梯度大小之和来表示；令<Image id="icf0002" he="60" wi="57" file="FDA0002309693230000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示第i个方向梯度图像块中第m个矩形区域的特征,m＝1:M,M表示x<Sub>i</Sub>中所有的矩形区域的总数,对应的训练集P中每个方向梯度图像块都会得到M个矩形区域；S53.初始化训练集P中所有样本的权重：w<Sub>1,i</Sub>＝1/N,i＝1,...,N；S54.初始化迭代参数t＝1；S55.为方向梯度图像块中的各矩形区域训练弱分类器；S56.计算每个弱分类器在训练集P中所有样本上的加权总误差,挑选出使加权总误差最小的弱分类器<Image id="icf0003" he="82" wi="73" file="FDA0002309693230000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>          <Image id="icf0004" he="90" wi="384" file="FDA0002309693230000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0005" he="134" wi="525" file="FDA0002309693230000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中h<Sub>m</Sub>表示基于训练集P中方向梯度图像块的第m个矩形区域训练得到的弱分类器,H表示所有矩形区域弱分类器的集合；δ(·)为指示函数,当其参数为真时函数值为1,否则函数值为0；S57.计算<Image id="icf0006" he="84" wi="41" file="FDA0002309693230000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>的权重α<Sub>t</Sub>：<Image id="icf0007" he="85" wi="435" file="FDA0002309693230000025.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中<Image id="icf0008" he="71" wi="45" file="FDA0002309693230000026.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示最小的加权总误差；S58.更新训练集P中所有样本的权重：          <Image id="icf0009" he="109" wi="688" file="FDA0002309693230000027.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0010" he="136" wi="608" file="FDA0002309693230000028.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        S59.令t＝t+1然后重复执行步骤S55～S58,直至t&gt;r；S510.执行完毕步骤S59后将训练过程中选中的前r个弱分类器对应的矩形区域在方向梯度图像块中的位置进行输出；S511.方向区间g的方向梯度图像块集合中的各个方向梯度图像块按照步骤S510输出的位置信息提取相应方向梯度图像块中的r个矩形区域；S512.对各个方向区间的方向梯度图像块集合进行步骤S51～S511的操作,此时图像区域中各个图像块对应的l个方向区间的方向梯度图像块都分别提取有r个矩形区域,计算每个矩形区域内的梯度大小之和,最后图像块就可以用一个lr维的方向梯度特征向量表示；(2).训练级联目标检测器S6.设定全局的假阳率F<Sub>t</Sub>及最小真阳率d<Sub>min</Sub>,以及初始化级联次数j＝1,初始化全局假阳率F<Sub>j</Sub>＝1.0,全局真阳率D<Sub>j</Sub>＝1.0；S7.基于(1)中提取的方向梯度特征向量,为图像区域中的每个图像块训练弱分类器,以AUC为收敛准则进行若干次boosting迭代,每次迭代挑选出一个最优的弱分类器；S8.采用Gentle Adaboost整合步骤S7选中的所有弱分类器得到强分类器H<Sup>j</Sup>(x)；S9.利用H<Sup>j</Sup>(x)预测训练集X中所有图像区域的得分,并生成ROC曲线；在ROC曲线上查找使d<Sub>j</Sub>＝d<Sub>min</Sub>的点(d<Sub>j</Sub>,f<Sub>j</Sub>),其中d<Sub>j</Sub>表示真阳率,f<Sub>j</Sub>表示假阳率；S10.令j＝j+1,然后更新F<Sub>j</Sub>、D<Sub>j</Sub>,F<Sub>j+1</Sub>＝F<Sub>j</Sub>×f<Sub>j</Sub>,D<Sub>j+1</Sub>＝D<Sub>j</Sub>×d<Sub>j</Sub>；S11.当F<Sub>j</Sub>&gt;F<Sub>t</Sub>时,重复执行步骤S7-S11；F<Sub>j</Sub>小于或等于F<Sub>t</Sub>时,输出级联目标检测器；(3).目标检测S12.使用多个窗口扫描可能包含目标的待检测图像,提取每个扫描窗口的方向梯度特征；S13.采用训练好的级联目标检测器对扫描窗口进行二分类检测,输出检测到的结果。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         彭枫;              陈艺芳;                   康显桂       </td>   <td>中山大学</td>   <td>基于循环卷积神经网络的视频人脸篡改检测方法及系统</td>   <td>广东省</td>   <td>CN110880172A</td>   <td>2020-03-13</td>   <td>本发明公开了一种基于循环卷积神经网络的视频人脸篡改检测方法及系统,其方法步骤包括：将视频分为帧,并从每一帧中提取出一张人脸图像,再把所有人脸图像数据分为训练集、验证集和测试集；用训练集和验证集的人脸图像数据,使用梯度下降法训练一个用于二分类的卷积神经网络模型；用训练好的卷积神经网络模型提取一个序列的人脸图像的特征,再输入到循环神经网络中,用梯度下降法训练循环神经网络的参数；使用测试集的人脸图像数据对训练好的循环卷积神经网络模型进行测评；使用预测结果来标记帧,并合成预测视频。其系统包括：视频处理模块、预测模块和视频生成模块；本发明能自动地预测视频中的人脸是否经过篡改并做标记,有广阔的使用前景。</td>   <td>1.一种基于循环卷积神经网络的视频人脸篡改检测方法,其特征在于,包括以下步骤：S1：把原始视频和篡改视频分成帧,并从每一帧中获取一张人脸图像,再把所有人脸图像分为训练集、验证集和测试集；S2：先用训练集和验证集的人脸图像数据,通过梯度下降算法训练一个二分类的卷积神经网络模型,并保存最优网络参数；S3：用训练好的卷积神经网络模型提取一个序列的人脸图像的特征,再输入到循环神经网络中,用梯度下降算法训练循环神经网络的参数,得到一个二分类的循环卷积神经网络模型,并保存最优网络参数；S4：使用测试集的人脸图像数据对训练好的循环卷积神经网络进行测试,评估循环卷积神经网络模型的性能；S5：通过测试后的循环卷积神经网络模型来预测人脸图像的真假并合成预测之后的视频。</td>   <td>G06T7/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨夏;              郭贵松;              宁丞浩;              甘叔玮;              叶雪辀;                   张小虎       </td>   <td>中山大学</td>   <td>长度约束下的弹目标高精度姿态测量方法</td>   <td>广东省</td>   <td>CN110866954A</td>   <td>2020-03-06</td>   <td>本发明公开的一种长度约束下的弹目标高精度姿态测量方法,通过搭建双目立体视觉交汇测量系统,将两个摄像机置于弹尾后侧的两边,标定得到的左、右相机的内外参数以及左、右摄像机之间的平移以及旋转关系。利用双目立体视觉交汇测量系统对着靶过程中的导弹目标进行拍摄,获得导弹目标着靶时的图像,获取导弹目标的世界坐标。用导弹目标其导弹头至导弹尾部的已知长度,弥补图像测量系统沿弹体方向精度不足的问题,得到三维高精度测量结果,从而得到高精度弹体姿态。</td>   <td>1.一种长度约束下的弹目标高精度姿态测量方法,其特征在于,包括如下步骤：第一步,搭建双目立体视觉交汇测量系统；双目立体视觉交汇测量系统包括两台焦距相同的CCD摄像机；两台CCD摄像机左右对称设置在靶子的两侧,导弹目标从左、右摄像机之间进入到两相机视场区域,左、右摄像机用于记录导弹进入两相机视场区域到导弹目标到达靶子的过程的图像；第二步,通过对双目立体视觉交汇测量系统进行标定,得到双目立体视觉交汇测量系统中左、右摄像机的内外参数；第三步,利用双目立体视觉交汇测量系统对着靶过程中的导弹目标进行拍摄,获得导弹目标着靶时的图像；第四步,获取导弹目标的世界坐标；导弹目标的世界坐标包括导弹头p<Sub>head</Sub>坐标(x<Sub>head</Sub>,y<Sub>head</Sub>,z<Sub>head</Sub>)以及尾部p<Sub>tail</Sub>坐标为(x<Sub>tail</Sub>,y<Sub>tail</Sub>,z<Sub>tail</Sub>)；结合第二步中标定得到的左、右相机的内外参数,使用三角测量法计算导弹头p<Sub>head</Sub>坐标(x<Sub>head</Sub>,y<Sub>head</Sub>,z<Sub>head</Sub>)以及尾部p<Sub>tail</Sub>坐标(x<Sub>tail</Sub>,y<Sub>tail</Sub>,z<Sub>tail</Sub>)；第五步,校正定位误差；导弹目标其导弹头至导弹尾部的长度L已知,使用L对导弹头坐标p<Sub>head</Sub>以及导弹尾部坐标p<Sub>tail</Sub>进行校正,得到校正后的导弹头坐标p′<Sub>head</Sub>以及导弹尾部坐标′p<Sub>tail</Sub>；第六步,根据校正后的导弹头坐标p′<Sub>head</Sub>以及导弹尾部坐标p′<Sub>tail</Sub>,计算导弹目标的姿态。</td>   <td>G06T7/73;G06T7/80</td>  </tr> </table></body></html>