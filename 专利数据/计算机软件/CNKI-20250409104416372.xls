<html xmlns:o='urn:schemas-microsoft-com:office:office' xmlns:x='urn:schemas-microsoft-com:office:excel' xmlns='http://www.w3.org/TR/REC-html40'><head><style>table td,th{vnd.ms-excel.numberformat:@;text-align: center;} table th{color:red}</style><title></title><meta http-equiv='Content-Type' content="text/html; charset=utf-8"></head><body><table cellspacing='0' border='1'>     <tr>   <td style='vnd.ms-excel.numberformat:@'>SrcDatabase-来源库</td>   <td style='vnd.ms-excel.numberformat:@'>Author-作者</td>   <td style='vnd.ms-excel.numberformat:@'>Applicant-申请人</td>   <td style='vnd.ms-excel.numberformat:@'>Title-题名</td>   <td style='vnd.ms-excel.numberformat:@'>CountryName-国省名称</td>   <td style='vnd.ms-excel.numberformat:@'>PubNo-公开号</td>   <td style='vnd.ms-excel.numberformat:@'>PubTime-公开日期</td>   <td style='vnd.ms-excel.numberformat:@'>Summary-摘要</td>   <td style='vnd.ms-excel.numberformat:@'>Claims-主权项</td>   <td style='vnd.ms-excel.numberformat:@'>CLC-中图分类号</td>  </tr>  <tr>   <td>中国专利</td>   <td>         郭雪梅;              张千瑶;                   王国利       </td>   <td>中山大学</td>   <td>一种医学图像任意倍数放大的超分辨处理方法</td>   <td>广东省</td>   <td>CN110866870A</td>   <td>2020-03-06</td>   <td>本发明提供一种医学图像任意倍数放大的超分辨处理方法,该方法采用了梯度算子和形态学操作来检测组织区域,梯度算子可以检测出灰度值变化较大的位置,而形态学中的开操作可以去除细小的连接和孤立点,减小噪声和气泡的影响,膨胀操作会粗化物体。此方法提高了组织区域检测的速度,实现了快速定位,同时保证了检测的精度。</td>   <td>1.一种医学图像任意倍数放大的超分辨处理方法,其特征在于,包括：图像的预处理：预读取WSI图像第6层的信息,应用梯度算子、形态学操作等算法检测出组织区域；将第6层检测出的组织区域映射到分辨率最高的第0层,在第0层的组织区域内使用滑动窗口进行图像块的提取,然后判断图像块是否包含组织区域来区分有信息的图像块和无信息的图像块；图像块的任意倍数放大：超分辨处理主要包括特征学习和元放大两个部分,其中特征学习主要是利用卷积神经网络和残差密集块对低分辨率图像进行特征提取,元放大部分是对提取到的特征相乘得到高分辨率图像。</td>   <td>G06T3/40;G06N3/04;G16H30/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         韩佳琪;                   卓汉逵       </td>   <td>中山大学</td>   <td>基于强化学习和迁移学习的图像语义分割模型及建模方法</td>   <td>广东省</td>   <td>CN110866922A</td>   <td>2020-03-06</td>   <td>本发明涉及图像的语义分割技术领域,更具体地,涉及一种基于强化学习和迁移学习的图像语义分割模型及建模方法,包括顺次通信连接的用于对原始图像进行预处理的预处理模块、用于强化学习中的环境部分的感知模块、用于强化学习中的智能体部分的像素类别决策模块。本发明将图像语义分割看成一种序列决策的过程,顺序地决定各像素的类别而不是一次性生成整张图像的分割结果,能够有效利用像素与像素之间的关系,减少训练时间,提高图像分割效果的精确性。</td>   <td>1.一种基于强化学习和迁移学习的图像语义分割模型,其特征在于,包括：预处理模块,用于对原始图像进行预处理；感知模块,用于强化学习中的环境部分；像素类别决策模块,用于强化学习中的智能体部分；所述预处理模块、感知模块、像素类别决策模块顺次通信连接。</td>   <td>G06T7/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         印鉴;                   蓝海珊       </td>   <td>中山大学</td>   <td>一种基于数据增强的深度哈希行人重识别方法</td>   <td>广东省</td>   <td>CN110852152A</td>   <td>2020-02-28</td>   <td>本发明提供一种基于数据增强的深度哈希行人重识别方法,首先利用K-means聚类方法对原始数据集图片进行聚类,然后分别将得到的n组聚类图片通过深度卷积生成对抗网络生成n组无标签行人图片进行数据增强,这样每组生成的图片特征上有更多地相似性。然后将原始数据集图片和生成图片一起放入深度哈希卷积神经网络中训练,用三元组损失分别拉近原始图片和生成图片中相同类行人的距离和拉开不同类行人的距离,对于生成图片,本发明还提出用均值标签平滑损失,让增强的这部分数据达到比较好的正则化效果,最后测试时将提取到的实数值特征映射为01向量,通过计算两个01向量之间的汉明距离判定行人图片的命中率,提高检索速率。</td>   <td>1.一种基于数据增强的深度哈希行人重识别方法,其特征在于,包括以下步骤：S1：对原始数据集图片进行K-means聚类,建立深度卷积生成对抗网络分别对n组聚类图片生成n组无标签行人图片；S2：建立深度哈希卷积神经网络,建立三元组损失和均值标签平滑损失函数；S3：将原始图片和生成图片一起送入网络中训练,并在测试集上进行测试。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苏卓;              孙峰;                   周凡       </td>   <td>中山大学</td>   <td>一种基于神经网络的二维虚拟试衣方法</td>   <td>广东省</td>   <td>CN110852941A</td>   <td>2020-02-28</td>   <td>本发明公开了一种基于神经网络的二维虚拟试衣方法。本发明输入原始人物图和目标服装图；提取人体外形图、人体关节点图和人体解析图；通过编解码器网络生成目标人体解析图；卷积神经网络生成变形服装图；编解码器网络生成粗略结果图和服装掩码图；粗略结果图与变形服装图通过服装掩码图进行重组生成最终效果图。本发明应用了二维图片深度学习算法,相比于昂贵的三维硬件采集设备和计算量较大的三维计算,成本低、效率高；使用神经网络方法,通过编码解码结构生成目标人体解析图,能最大限度指导神经网络保留原始图片中人的各部位特征,使用卷积神经网络对目标服装图进行服装变形,能够最大限度地保留目标服装图的纹理信息。</td>   <td>1.一种基于神经网络的二维虚拟试衣方法,其特征在于,所述方法包括：步骤一,从服装数据集筛选并输入原始人物图和目标服装图,并处理成统一尺寸；步骤二,对所述处理成统一尺寸的原始人物图进行进一步处理生成人体外形图、人体关节点图和人体解析图作为人体部分特征；步骤三,使用所述人体部分特征和步骤一得到的统一尺寸的目标服装图,使用卷积神经网络D中的编码解码结构输出目标人体解析图；步骤四,使用神经网络E对步骤一得到的统一尺寸的目标服装图按照所述目标人体解析图中的服装层进行变形,生成变形服装图；步骤五,使用所述变形服装图、目标人体解析图和步骤二得到的人体外形图、人体关节点图和人体解析图通过神经网络F中的编码解码结构进行训练,生成最终的效果图；步骤六,将步骤一中的目标服装图改为与原始人物图所穿着衣服不同的服装图,重复步骤二、三、四、五的过程,其中步骤三、四、五中的神经网络D、E、F不需要再训练,直接输出结果应用到之后的步骤。经过步骤五输出的最终效果图即为原始人物图中的人物换装之后的图片。</td>   <td>G06T3/00;G06T5/50;G06T7/11;G06T7/40;G06Q30/06;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张榕瑜;                   倪江群       </td>   <td>中山大学</td>   <td>一种采用密集结构卷积网络的图像篡改检测和定位方法</td>   <td>广东省</td>   <td>CN110852316A</td>   <td>2020-02-28</td>   <td>本发明提供的一种采用密集结构卷积网络的图像篡改检测和定位方法,包括输入待测图像,对待测进行空间富集SRM卷积进行预处理,得到预处理后的图像；构建密集连接卷积网络对预处理后的图像进行篡改图像特征提取,得到待测图像的二分类信息,完成对图像篡改的检测；构建与连接卷积网络结构对称的反卷积网络,将二分类信息作为输入；根据得到的图像篡改区域,由反卷积网络完成定位后的图像。本发明所提供的方法,将深度学习技术应用到图像篡改检测与定位中,适用于多种篡改手段,具有好的鲁棒性和实用性；提供了检测和定位的统一框架,不仅能够多图像是否经过篡改做出预测,还能对篡改区域进行预测,给出逐像素的精确标注,得到细致的物体轮廓边界。</td>   <td>1.一种采用密集结构卷积网络的图像篡改检测和定位方法,其特征在于,包括以下步骤：S1：输入待测图像,对待测进行空间富集SRM卷积进行预处理,得到预处理后的图像；S2：构建密集连接卷积网络对预处理后的图像进行篡改图像特征提取,得到待测图像的二分类信息,完成对图像篡改的检测；S3：构建与连接卷积网络结构对称的反卷积网络,将待测图像的二分类信息作为输入,定位图像篡改区域；S4：根据得到的图像篡改区域,由反卷积网络完成定位后的图像,完成对图像篡改的定位。</td>   <td>G06K9/20;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈伟威;                   潘嵘       </td>   <td>中山大学</td>   <td>一种基于多智能体强化学习的合作型智能体的学习方法</td>   <td>广东省</td>   <td>CN110852448A</td>   <td>2020-02-28</td>   <td>本发明涉及一种基于多智能体强化学习的合作型智能体的学习方法,步骤一：重置多个目标环境；步骤二：初始化策略网络π<Sub>θ</Sub>的模型参数θ<Sub>π</Sub>和全局信息预测网络f<Sub>θ</Sub>的模型参数θ<Sub>f</Sub>；步骤三：在环境中对多环境中的多智能体以当前策略π进行采样；每一步中,环境中的多个智能体共享同一状态,针对每个智能体对状态提取特征后作为模型输入的数据；步骤四：对模型参数θ<Sub>π</Sub>和θ<Sub>f</Sub>进行更新；步骤五：直至模型收敛或达到最大步数。本发明在智能体处于合作关系的环境下更好地利用了全局特征信息,通过局部信息预测全局信息的模型令每个智能体学会感知局部信息与全局信息的联系,更好地协作；使得不同智能体得以直接共享模型参数,简化模型复杂度,提高效率。</td>   <td>1.一种基于多智能体强化学习的合作型智能体的学习方法,其特征在于,包括如下步骤：步骤一：重置多个目标环境；步骤二：初始化策略网络π<Sub>θ</Sub>的模型参数θ<Sub>π</Sub>和全局信息预测网络f<Sub>θ</Sub>的模型参数θ<Sub>f</Sub>；步骤三：以固定步数在环境中对多环境中的多智能体以当前策略π进行采样；每一步中,同一个环境e<Sub>i</Sub>中的多个智能体共享同一状态s<Sub>i,t</Sub>,针对该状态提取全局特征s<Sub>i,t,global</Sub>,并针对每个智能体对状态s<Sub>i,t</Sub>提取局部特征s<Sub>i,t,local</Sub>,两者合并得智能体特征s<Sub>i,t,comb</Sub>后作为策略网络模型输入的数据；步骤四：使用近端策略优化算法对策略网络π<Sub>θ</Sub>的模型参数θ<Sub>π</Sub>进行更新,算法的目标函数如下：          <Image id="icf0001" he="106" wi="700" file="RE-FDA0002307980970000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,s<Sub>i,t</Sub>为环境e<Sub>i</Sub>中时刻t的状态,a<Sub>i,t</Sub>为智能体在环境e<Sub>i</Sub>中时刻t选择的动作,θ为当前模型的参数,θ′为采集数据的模型参数,A为优势函数；同时,更新全局信息预测网络f<Sub>θ</Sub>的模型参数θ<Sub>f</Sub>；步骤五：重复步骤三和步骤四,直至模型收敛或达到最大步数。</td>   <td>G06N20/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         叶永杰;                   吴维刚       </td>   <td>中山大学</td>   <td>一种区块链支付通道网络的交易路径短路方法</td>   <td>广东省</td>   <td>CN110852485A</td>   <td>2020-02-28</td>   <td>本发明提供的一种区块链支付通道网络的交易路径短路方法,通过设置支付通道集线器以连接多个支付通道,从而进行不同支付通道间的资金交易,实现交易路径的短路。本发明提供的一种区块链支付通道网络的交易路径短路方法,充分利用了支付网络高灵活性的特点,通过在网络上设置支付通道集线器,实现了对不同支付通道的短路,从而大幅度缩短非直连节点之间交易的路径长度,提高了支付网络中的交易效率。</td>   <td>1.一种区块链支付通道网络的交易路径短路方法,其特征在于：通过设置支付通道集线器以连接多个支付通道,从而进行不同支付通道间的资金交易,实现交易路径的短路。</td>   <td>G06Q10/04;G06Q20/38;G06Q20/42;G06Q40/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>              蔡昆京       </td>   <td>中山大学</td>   <td>一种基于人体头部特征的考场偷窥作弊检测系统</td>   <td>广东省</td>   <td>CN110837784A</td>   <td>2020-02-25</td>   <td>本发明公开了一种基于人体头部特征的考场偷窥作弊检测系统,包括RGB-D数据采集模块,采集关于考生的RGB彩色视频和深度信息数据；头部特征提取模块,由头部位置轨迹计算单元、头部姿态估计单元、眼神方向估计单元以及人脸识别单元组成,根据RGB-D视频数据分析头部位置轨迹、头部姿态、眼神注视方向和人脸身份等各种人体头部特征；作弊行为判定分类模块,对提取的头部特征按照多条规则分别判断是否作弊,再加权综合每条规则分类的结果得到最终是否偷窥作弊的结论。</td>   <td>1.一种基于人体头部特征的考场偷窥作弊检测系统,其特征在于,包括：RGB-D数据采集模块,用于实时记录考场中监考人员及考生的RGB彩色视频和深度信息数据；头部特征提取模块,用于逐帧对采集到的RGB-D视频数据进行分析,获取头部位置、头部姿态、头部运动轨迹、人脸身份以及眼神注视方向的等各种人体头部特征；作弊行为判定分类模块,用于对RGB-D视频数据提取的特征按照若干条规则进行判断分类,再综合每条规则判断的结果给出最后是否偷窥作弊的结论。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              袁帅;              何娜;                   陈至宇       </td>   <td>中山大学</td>   <td>一种鲁棒的车牌、车标识别方法</td>   <td>广东省</td>   <td>CN106650731B</td>   <td>2020-02-21</td>   <td>本发明公开一种鲁棒的车牌、车标识别方法。采用基于Adaboost检测、SVM筛选以及纹理分析去边的方法进行车牌精确检测,以有效应对不同场景、光照、视角、分辨率等,检测出的车牌区域只包含较少的背景。以最大稳定极值区域检测为主、滑动窗检测为辅,配合能量优化进行车牌识别,不仅能够有效地检测出传统的基于字符分割方法难以处理的车牌污损、分辨率不足等情况下的字符,而且使得字符检测与识别同步进行,打破传统的“先分割再识别”的模式。采用基于置信度加权的特征编码表达车标样本,利用基于组稀疏的判别性字典对模型识别编码的车标特征,对包含复杂背景的车标样本具有较强的鲁棒性。</td>   <td>1.一种鲁棒的车牌识别方法,其特征在于,包括车牌检测阶段和车牌识别阶段,在车牌检测阶段实现车牌区域的定位,基于定位后的车牌区域再进行车牌识别,其中车牌识别阶段具体实现过程为：11)对定位后车牌区域进行灰度化；12)利用最大稳定极值区域MSER检测方法在灰度化的车牌区域中寻找最大稳定极值区域,并对其检测结果进行候选字符窗口的排序,具体是根据每个候选字符窗口左上角起点的横坐标进行排序；13)对MSER的检测结果进行非字符窗口的初步滤除；14)记录并保存初步过滤后的每个候选字符窗口的置信度、中心点坐标以及宽和高,进而得到车牌的字符宽、高均值；15)利用保存的候选字符窗口的中心点坐标以及车牌的字符宽、高的均值进行字符窗口恢复；16)利用基于滑动窗的检测方法和车牌字符宽、高的均值,进一步检测基于MSER的方法可能漏检的字符；17)利用能量优化的方法对候选字符窗口进行识别,输出最终车牌识别的结果；能量优化方程如下式所示：          <Image id="icf0001" he="124" wi="662" file="FDA0002202561970000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,x＝{x<Sub>i</Sub>|i＝1,2,...,n}为窗口类标的集合,x<Sub>i</Sub>表示第i个窗口的类标,E<Sub>i</Sub>(x<Sub>i</Sub>)＝1-p(x<Sub>i</Sub>|c<Sub>i</Sub>)为一元项,表示自能量,其中c<Sub>i</Sub>表示第i个窗口,p(x<Sub>i</Sub>|c<Sub>i</Sub>)为窗口c<Sub>i</Sub>属于x<Sub>i</Sub>类的概率；E<Sub>ij</Sub>(x<Sub>i</Sub>,x<Sub>j</Sub>)为二元项,表示互能量,当x<Sub>i</Sub>与x<Sub>j</Sub>均为背景类时,E<Sub>ij</Sub>(x<Sub>i</Sub>,x<Sub>j</Sub>)＝0,否则E<Sub>ij</Sub>(x<Sub>i</Sub>,x<Sub>j</Sub>)＝λexp(-[100-Overlap(x<Sub>i</Sub>,x<Sub>j</Sub>)]<Sup>2</Sup>),其中Overlap(x<Sub>i</Sub>,x<Sub>j</Sub>)表示窗口之间重叠部分占窗口总面积的百分数,λ为调整互能量权重的参数；ε表示互有交集的窗口对的集合。</td>   <td>G06K9/32;G06K9/34;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苏卓;              曾碧怡;              李波;              冷成财;                   颜吉超       </td>   <td>中山大学;南昌航空大学</td>   <td>一种基于双边核回归的相对约减纹理分解方法及其装置</td>   <td>广东省</td>   <td>CN106683129B</td>   <td>2020-02-21</td>   <td>本发明实施例公开了一种基于双边核回归的相对约减纹理分解方法及其装置,其中,该方法包括：输入所要处理的输入图像；对所述输入图像的每一个像素计算相对约减率；根据软阈值构造结构核描述子；将所述结构核描述子作为联合双边滤波的引导图像与双边核进行回归融合,获得滤波结果。在本发明实施例中,采用基于局部全变分的结构核描述子并结合核回归模型来进行构建,采用相对减少的纹理分解来构造结构核描述子,将该描述子与双边核回归融合来获得期望的结构感知滤波输出,能够弥补当前边缘感知滤波在提取图像结构时所出现的结构和纹理分解不完全的缺陷。</td>   <td>1.一种基于双边核回归的相对约减纹理分解方法,其特征在于,所述方法包括：输入所要处理的输入图像；对所述输入图像的每一个像素计算相对约减率；根据所述相对约减率计算获得软阈值：          <Image id="icf0001" he="196" wi="700" file="FDA0002236949400000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,α和β是经验阈值,k为相对约减率；根据所述软阈值构造结构核描述子：          <Image id="icf0002" he="74" wi="700" file="FDA0002236949400000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,<Image id="icf0003" he="55" wi="56" file="FDA0002236949400000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示空间卷积运算,I表示输入图像,G<Sub>σ</Sub>是方差为σ的高斯核；将所述结构核描述子作为联合双边滤波的引导图像与双边核进行回归融合,获得滤波结果。</td>   <td>G06T7/49</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘宁;                   陈浩基       </td>   <td>中山大学</td>   <td>一种自主更新的室内定位方法</td>   <td>广东省</td>   <td>CN110826448A</td>   <td>2020-02-21</td>   <td>本发明公开了一种自主更新的室内定位方法，包括以下步骤：构建特征提取网络；构建隐马尔可夫模型；构建新地标检测网络；计算新地标的二维平面坐标；将计算出的新地标二维平面坐标写入数据库；依据新地标的ROI和类别与未改变地标的ROI和类别作为训练数据，得到新的特征提取网络。通过结合隐马尔可夫模型和运动恢复结构技术，对特征提取网络的输出进行判断，能够进一步减少特征提取网络的分类误差，有效降低了误判率；同时，通过构建新地标检测网络，能够确定三维点云的水平面和新地标的相对位置，进而将新地标的三维坐标映射回平面地图空间，完成室内地图更新，并不需要耗费大量人力去更新维护室内平面地图，大大减少了视频室内定位的后期维护成本。</td>   <td>1.一种自主更新的室内定位方法，其特征在于，基于运动恢复结构和隐马尔可夫模型，具体包括以下步骤：步骤S1，构建特征提取网络，所述特征提取网络以拍摄的视频帧为输入，各视频帧对应的地标ROI和类别为输出；步骤S2，构建隐马尔可夫模型，对所述特征提取网络输出的地标ROI和类别进行判断，判断是否有新地标，若有，则执行步骤S3，若没有，则停止更新；步骤S3，构建新地标检测网络，所述新地标检测网络以拍摄的视频帧为输入，输出各视频帧对应的新地标ROI和类别与未改变地标的ROI和类别；步骤S4，依据未改变的地标ROI和类别与新地标ROI和类别，计算出新地标的二维平面坐标；步骤S5，将计算出的新地标二维平面坐标写入数据库，同时删除发生改变的地标的相应数据；步骤S6，依据新地标的ROI和类别与未改变地标的ROI和类别作为训练数据，得到新的特征提取网络，完成更新。</td>   <td>G06K9/00;G06K9/32;G06K9/62;G06F16/75;G06F16/787</td>  </tr>        <tr>   <td>中国专利</td>   <td>         印鉴;              陈波;                   朱怀杰       </td>   <td>中山大学</td>   <td>一种基于会面点的最优组次序路径查询方法</td>   <td>广东省</td>   <td>CN110826761A</td>   <td>2020-02-21</td>   <td>本发明公开了一种基于会面点的最优组次序路径查询方法，属于路径规划技术领域。本发明能高效处理多用户查询场景下基于会面点的最优组次序路径查询，提出了OGSRM算法框架，该框架能够求解基于会面点的最优组次序路径也能快速的求解相应的高质量近似解，另外该框架具有很高的灵活性能使用不同的候选点启发式式函数、最优次序路径算法和近似最优次序路径算法。</td>   <td>1.一种基于会面点的最优组次序路径查询方法，其特征在于，包括以下步骤：S1：初始化OGSRM算法框架参数λ，μ，h，ALGO-OSR和ALGO-AOSR；S2：初始化最优组次序路径变量routes为空；S3：使用h计算所有会面候选点的启发式函数值并将候选点按其启发式函数值升序排列；S4：按S3产生的候选点排列顺序依次检查每个候选点d，如果d的启发式函数值的λ倍大于或等于当前routes中路径的最大值或者所有候选点检查完毕，转到S6，否则执行S5后继续执行S4；S5：前μ×100％用户使用ALGO-OSR算法查询，剩余用户使用ALGO-AOSR算法查询，如果查询得到结果中用户组的路径中的最大值比routes中路径最大值小，更新routes为当前查询结果；S6：返回最优组次序路径routes。</td>   <td>G06Q10/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈龙;              谢玉婷;                   张亚琛       </td>   <td>中山大学</td>   <td>面向动态环境的语义分割与视觉SLAM紧耦合方法</td>   <td>广东省</td>   <td>CN110827305A</td>   <td>2020-02-21</td>   <td>本发明属于机器人、计算机视觉、人工智能技术领域，更具体地，涉及一种面向动态环境的语义分割与视觉SLAM紧耦合方法。包括：S1.经过语义分割模块输出图像的像素级语义分割结果和深度恢复结果；然后原图像和每个像素点的语义标签以及对应深度图传递给视觉SLAM模块；S2.视觉SLAM模块利用这些信息获知新图像与序列中历史图像的数据关联状况，并将这个信息反馈回语义分割模块；S3.语义分割模块利用历史图像的分割结果及历史图像与新图像帧之间的数据关联状况，优化新图像帧的语义分割结果；S4.优化后的语义分割结果再一次传送回视觉SLAM模块，得到最终精细化的三维重建结果。本发明有效提升了语义分割的效果，从而进一步提高依赖于语义分割结果的SLAM性能。</td>   <td>1.一种面向动态环境的语义分割与视觉SLAM紧耦合方法，其特征在于，包括以下步骤：S1.经过语义分割模块输出图像的像素级语义分割结果和深度恢复结果；然后原图像和每个像素点的语义标签以及对应深度图传递给视觉SLAM模块；S2.视觉SLAM模块利用这些信息获知新图像与序列中历史图像的数据关联状况，并将这个信息反馈回语义分割模块；S3.语义分割模块利用历史图像的分割结果及历史图像与新图像帧之间的数据关联状况，优化新图像帧的语义分割结果；S4.优化后的语义分割结果再一次传送回视觉SLAM模块，得到最终精细化的三维重建结果。</td>   <td>G06T7/12;G06T17/05;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         巫海维;                   张东       </td>   <td>中山大学</td>   <td>一种基于深度学习的鲸鱼活动音频分类方法</td>   <td>广东省</td>   <td>CN110827837A</td>   <td>2020-02-21</td>   <td>本发明涉及一种基于深度学习的鲸鱼活动音频分类方法。包括，1.采集水底语音数据；2.对采集的语音数据进行数据扩充；3.对训练数据进行声学特征提取，将一维的语音序列转换成二维的声学特征序列；4.利用声学特征，分别训练两组神经网络模型：基于帧的神经网络系统和基于语音片段的卷积神经网络系统；5.训练完基于帧的神经网络系统之后，提取基于帧的得分输出并做平均值处理，得到得分A；对于基于语音片段的卷积神经网络系统，利用该模型提取深度特征，用深度特征训练后端分类器，由后端分类器输出得分B；6.将得分A与得分B进行融合，得出最终的判断结果。本发明使用深度学习算法，能够得到更高的识别准确率，在具体应用中能够更加鲁棒，稳定。</td>   <td>1.一种基于深度学习的鲸鱼活动音频分类方法，其特征在于，包括以下步骤：S1.采集水底语音数据；S2.对采集的语音数据进行数据扩充，增加训练数据量；S3.对扩充后的训练数据进行声学特征提取，将一维的语音序列转换成二维的声学特征序列；S4.利用S3步骤的声学特征，分别训练两组神经网络模型，分别是基于帧的神经网络系统和基于语音片段的卷积神经网络系统；S5.训练完基于帧的神经网络系统之后，判断出音频信号是否为鲸鱼叫声，提取基于帧的得分输出并做平均值处理，得到一组得分A；对于基于语音片段的卷积神经网络系统，训练卷积神经网络之后，利用该模型提取深度特征，用深度特征训练后端分类器，判断出音频信号是否为鲸鱼叫声，由后端分类器输出得分B；S6.将得分A与得分B进行融合，得出最终的判断结果。</td>   <td>G10L17/26;G10L25/18;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         檀祖冰;              张彧;                   陈龙       </td>   <td>中山大学</td>   <td>一种视觉机器人的地图表示系统及其构建方法</td>   <td>广东省</td>   <td>CN110807782A</td>   <td>2020-02-18</td>   <td>本发明属于移动机器人环境表示、规划和定位领域,更具体地,涉及一种视觉机器人的地图表示系统及其构建方法。该地图表示系统由多信息体素层、地图元素层以及拓扑层的数据结构组成,分别涵盖了空间信息、场景实例、连通性三方面特征。该地图由语义信息提取模块、几何信息提取模块、场景语义提取模块、多信息体素整合模块、提取空间拓扑模块和拓扑整合模块六个模块共同完成构建。本发明仅基于视觉传感器,具有构建流程清晰,信息全面,层次关系紧密、易于可视化的优点,适用于移动机器人室内外场景的规划、定位与导航等工作。</td>   <td>1.一种视觉机器人的地图表示系统,其特征在于,包括语义信息提取模块、几何信息提取模块、场景语义提取模块、多信息体素整合模块、提取空间拓扑模块、拓扑整合模块、以及地图元素层、多信息体素层和拓扑图层；其中,所述的语义信息提取模块用于利用视觉传感器在需要构造地图的环境中采集图像,并进行图像语义信息提取,得到图像分割结果；接着进行其余特定语义提取,得到特定的语义信息,特定的语义信息部分的结果最终会存储在地图元素层中；所述的几何信息提取模块用于使用视觉传感器在需要构造地图的环境中采集图像,经过计算得到深度图、顶点集合和顶点对应的法向量,接着基于深度图由几种关于距离与法线的几何特征分割深度图；所述的场景语义提取模块用于利用视觉传感器在需要构造地图的环境中采集图像,并进行场景语义提取,得到场景的分类信息,场景的分类信息是人为定义的、具有一定的标识度的场景称号；所述的多信息体素整合模块,用于将语义信息提取模块得到的图像分割结果和几何信息提取模块得到的深度分割结果进行融合,得到三维语义部件,该部件描述了场景中某一个由人类预定的有一定标识度和对规划、定位来说意义的物体；接着,结合视觉SLAM方法得到的相机位姿,将使用三维语义部件与几何信息提取模块得到的顶点集合计算得到的多信息体素,更新到地图的多信息体素层中；所述的提取空间拓扑模块用于基于所述的多信息体素整合模块得到的多信息体素层,提取出包含场景语义的空间凸包及表达其连接关系的拓扑信息；所述拓扑整合模块用于基于空间的相邻关系,关联所述的多信息体素整合模块中三维空间部件和所述的提取空间拓扑模块得到的三维空间凸包,计算空间节点到部件节点的拓扑信息,并将此信息与空间节点到空间节点的拓扑信息合并,得到完整的拓扑图层；所述的地图元素层、多信息体素层和拓扑图层共同构成地图,拓扑图将地图元素层中的空间凸包和三维部件信息联系起来。</td>   <td>G06T7/11;G06T7/50;G06T7/70;G06T7/80;G06T11/60;G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         徐韩悦;                   陈龙       </td>   <td>中山大学</td>   <td>基于点线特征和深度滤波器的轻量级单目视觉定位方法</td>   <td>广东省</td>   <td>CN110807809A</td>   <td>2020-02-18</td>   <td>本发明涉及一种基于点线特征和深度滤波器的轻量级单目视觉定位方法。包括：采集标定图像信息对相机进行标定,得到相机的畸变参数和内参矩阵；采集图像数据,进行畸变校正,得到校正后的图像数据；检测校正后图像的特征点；选取关键帧初始化系统；提取图像线段特征和特征点,跟踪每一帧得到图像的位姿；三维特征加入时使用CNN初始化深度滤波器；对种子进行迭代更新直到收敛。本发明适用于计算性能受限的设备,可用于纹理比较缺乏的场景,不计算、匹配描述子,具有更高的运行速度；加入了线段特征,能够得到更多的特征点；使用了CNN估计的深度值初始化深度滤波器,大大降低迭代的次数。</td>   <td>1.一种基于点线特征和深度滤波器的轻量级单目视觉定位方法,其特征在于,包括以下步骤：S1.使用待标定单目相机采集含有标定板信息的图像数据,使用标定得到相机模型的内参和畸变参数；S2.将载有单目相机的设备放置至目标环境中,记录传感器数据,对采集到的每一帧图片进行畸变校正,并计算ORB特征和LSD线段特征；S3.跟踪步骤S2得到的特征点,确定第一个关键帧和第二个关键帧,计算单应矩阵和基础矩阵,并确定最佳的参数,作为单目初始化全局的尺度；S4.将采集线段特征得到的线段进行等距采样得到新的点,对所有ORB特征点和LSD特征上的点,确定一块特征块,使用直接法对所有特征块的位姿参数迭代,通过最小化特征块灰度差值,得到相机的初始位姿；利用得到的初始位姿和当前帧具有共视关系的关键帧,计算仿射变换并最小化光度误差,来对特征块的位置进行优化；最后利用得到的位姿和特征位置计算重投影误差,通过高斯牛顿法优化位姿和三维点的位置；S5.计算当前帧与所有关联关键帧平移量,如果平移足够大则确定当前帧是关键帧；S6.对于关键帧,使用CNN计算特征点的深度值初始化深度滤波器；对于普通帧,融合观测数据和预测数据,得到新的关于深度估计的高斯分布。</td>   <td>G06T7/73;G06T7/80;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赵一天;              尚巧玲;              郝华颖;              李飞;              张秀兰;                   刘江       </td>   <td>中国科学院宁波工业技术研究院慈溪生物医学工程研究所;中国科学院宁波材料技术与工程研究所;中山大学中山眼科中心</td>   <td>一种基于局部相位张量算法的AS-OCT图像的虹膜分割方法</td>   <td>浙江省</td>   <td>CN110796086A</td>   <td>2020-02-14</td>   <td>本发明属于图像分割技术领域,具体涉及一种基于局部相位张量算法的AS-OCT图像的虹膜分割方法。包括：基于局部相位张量算法对AS-OCT图像的ROI区域图像进行曲线结构增强处理；并且从经过所述曲线结构增强处理后的图像分割出虹膜图像。局部相位张量可以用于区分图像中固有的特征,并且这些特征不会跟随照明的变化而变化。在基于局部相位张量增强曲线结构的步骤中,加入多个方向检测的约束来捕获不同方向上的曲线结构,并且在不同尺度下融合增强捕获的曲线结构,能够获得一个合理不变的张量,从而获得准确的虹膜边界轮廓。</td>   <td>1.一种基于局部相位张量算法的AS-OCT图像的虹膜分割方法,其特征在于,包括：基于局部相位张量算法AS-OCT图像的ROI区域图像进行曲线结构增强处理；并且从经过所述曲线结构增强处理后的图像分割出虹膜图像。</td>   <td>G06K9/00;G06K9/32;G06T7/10;G06T7/136</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;                   余嘉豪       </td>   <td>中山大学</td>   <td>一种基于情感显著性区域检测的图片情感识别方法</td>   <td>广东省</td>   <td>CN110796150A</td>   <td>2020-02-14</td>   <td>本发明公开了一种基于情感显著性区域检测的图片情感识别方法,包括下述步骤：S1、构建一个双任务深度网络模型；S2、将同一张图片缩放成两张尺寸大小不同的图片,并将这两张图片分别输入到结构相同的深度网络；S3、两个深度网络的输出都是图片提取到的有用特征,然后这两部分图片特征将会进行合并,衍生出两个任务分支；S4、一个分支负责利用这些图片特征来推测出这张图片中对观看者情感有决定性影响的情感显著性区域,另一个分支则根据前一个任务分支推测出的情感显著区域来进一步提取图片特征和进行情感分类。本发明创新的将情感显著性区域检测引入到图片情感识别问题中,对图片的情感识别分类效果更好,准确率更高。</td>   <td>1.一种基于情感显著性区域检测的图片情感识别方法,其特征在于,包括下述步骤：S1、构建一个双任务深度网络模型,所述双任务深度网络模型包括第一深度网络和第二深度网络,所述第一深度网络负责情感显著性区域检测,所述第二深度网络根据检测到的情感显著性区域来提取图片的特征；S2、将同一张图片缩放成两张尺寸大小不同的图片,并将这两张图片分别输入到结构相同的第一深度网络和第二深度网络中,经过第一深度网络和第二深度网络的计算后得到两个输出；S3、两个深度网络的输出都是图片提取到的有用特征,然后这两部分图片特征将会进行合并,衍生出两个任务分支,即情感显著性区域检测任务分支和图片情感分类分支；S4、所述情感显著性区域检测任务分支负责利用这些图片特征来推测出这张图片中对观看者情感有决定性影响的情感显著性区域,所述图片情感分类分支则根据前一个任务分支推测出的情感显著区域来进一步提取图片特征和进行情感分类。</td>   <td>G06K9/46;G06K9/62;G06F16/55;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林宇烽;                   卓汉逵       </td>   <td>中山大学</td>   <td>一种广告转化率预估模型及其训练方法</td>   <td>广东省</td>   <td>CN110796499A</td>   <td>2020-02-14</td>   <td>本发明涉及一种广告转化率预估模型及其训练方法,预估模型中包括编码网络、预测网络和损失函数计算模块,损失函数计算模块根据预测网络得到的部分预测结果和广告主标签数据计算出具体的损失函数值并加入差分隐私机制；预测网络根据损失函数值更新自身网络。通过训练方法对预估模型进行训练。本发明通过部分预测结果结合广告主的标签数据计算损失函数值来反馈预测网络,广告转化率预测模型可以利用广告主的标签数据,同时也保证了数据隐私,提高预估广告转化率的准确性。</td>   <td>1.一种广告转化率预估模型,包括编码网络和预测网络,所述预测网络输入编码网络得到的整体特征向量后输出预测结果,其特征在于,还包括损失函数计算模块；所述损失函数计算模块根据预测网络得到的部分预测结果和广告主标签数据计算出具体的损失函数值；所述预测网络根据所述损失函数值更新自身网络；损失函数值的具体计算公式为：loss＝-(y*log(p)+(1-y)*log(1-p))其中,p为预测结果的概率值；y为其对应的转化标签值。</td>   <td>G06Q30/02;G06Q10/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李冠彬;              曾怡瑞;              李灏峰;                   林倞       </td>   <td>中山大学</td>   <td>一种基于在线迭代生成器的对抗防御方法及系统</td>   <td>广东省</td>   <td>CN110796608A</td>   <td>2020-02-14</td>   <td>本发明公开了一种基于在线迭代生成器的对抗防御方法及系统,该方法包括如下步骤：步骤S1,随机初始化生成器网络F的参数θ,并用0初始化与输入图像相同大小的合成图像；步骤S2,给定可能是对抗样本的输入图像,将其定义为参考图像I<Sub>z</Sub>,将其输入至生成器网络模块,生成合成图像,并交替迭代更新网络参数和合成图像,最终获得去除对抗噪声且与原输入图像语义相同的合成图像,直到符合停止的条件,本发明可在有效地去除对抗噪声的同时,合成与输入图像具有相同语义图像以代替原有的输入图像。</td>   <td>1.一种基于在线迭代生成器的对抗防御方法,包括如下步骤：步骤S1,随机初始化生成器网络F的参数θ,并用0初始化与输入图像相同大小的合成图像；步骤S2,给定可能是对抗样本的输入图像,将其定义为参考图像I<Sub>z</Sub>,将其输入至包含所述生成器网络F的在线迭代生成器模块,生成合成图像,并交替迭代更新网络参数和合成图像,最终获得去除对抗噪声且与原输入图像语义相同的合成图像。</td>   <td>G06T5/00;G06K9/62;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;              余浩强;              吴箫;                   林鹏       </td>   <td>中山大学;广东海启星海洋科技有限公司</td>   <td>一种基于IoU的水下多目标跟踪方法</td>   <td>广东省</td>   <td>CN110796678A</td>   <td>2020-02-14</td>   <td>本发明涉及计算机视觉和水下目标跟踪技术,为基于IoU的水下多目标跟踪方法,获取水下感兴趣目标的数据集；训练并获得收敛的目标检测模型；对输入的当前图像帧进行图像增强；使用训练好的目标检测模型对增强后的图像帧进行目标检测,得到该帧的所有检测框信息和检测得分；通过场景拥挤检测算法,自适应调整目标检测的得分阈值,筛选部分检测目标,对检测目标集合划分为高、低得分检测目标集合；计算各检测目标与各运动轨迹之间的IoU得分,对检测目标与运动轨迹进行分集匹配；根据匹配结果对所有检测目标与运动轨迹进行状态转移处理,以保持检测目标ID的一致性。本发明可自适应调整检测目标的得分阈值,能有效地提高水下多目标跟踪的实时性。</td>   <td>1.一种基于IoU的水下多目标跟踪方法,其特征在于,包括以下步骤：S1、获取水下感兴趣目标的数据集,进行图像预处理、图像增强；S2、将数据集划分为训练集、测试集与验证集,训练并获得收敛的目标检测模型；S3、对输入的当前图像帧进行图像增强；S4、使用训练好的目标检测模型对S3进行增强后的图像帧进行目标检测,得到该帧的所有检测框信息和该帧的检测得分；S5、通过场景拥挤检测算法,自适应调整目标检测的得分阈值,筛选部分检测目标,同时根据检测得分对检测目标集合划分为高、低得分检测目标集合；S6、计算各检测目标与各运动轨迹之间的IoU得分；S7、根据IoU得分,对检测目标与运动轨迹进行分集匹配；S8、根据步骤S7的匹配结果对所有检测目标与运动轨迹进行状态转移处理,以保持检测目标ID的一致性。</td>   <td>G06T7/20;G06K9/62;G01C11/36</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张宏伟;              张小虎;                   杨夏       </td>   <td>中山大学</td>   <td>一种基于信息论流形的人脸图像识别方法</td>   <td>广东省</td>   <td>CN110781802A</td>   <td>2020-02-11</td>   <td>本发明提供了一种基于信息论流形的人脸图像识别方法,包括如下步骤：步骤1,通过Gabor滤波器对人脸二维图像进行处理,使用多个中心尺度和多个方向组合,提取人脸二维图像的纹理结构特征；步骤2,使用不确定度计算人脸二维图像经过多尺度变换后各个尺度特征结果的权值；步骤3,对高维张量空间数据进行降维,得到低维数据并提取人脸二维图像数据的特征；步骤4,根据已提取到的人脸二维图像数据,利用线性判别分析和最近邻算法识别人脸二维图像；本发明相对于一些基础的人脸二维图像特征提取算法而言,具有较好的识别率。</td>   <td>1.一种基于信息论流形的人脸图像识别方法,包括如下步骤：步骤1,通过Gabor滤波器对人脸二维图像进行处理,使用多个中心尺度和多个方向组合,提取人脸二维图像的纹理结构特征；步骤2,使用不确定度计算人脸二维图像经过多尺度变换后各个尺度特征结果的权值；步骤3,对高维张量空间数据进行降维,得到低维数据并提取人脸二维图像数据的特征；步骤4,根据已提取到的人脸二维图像数据,利用线性判别分析和最近邻算法识别人脸二维图像。</td>   <td>G06K9/00;G06K9/34;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张宏伟;              张小虎;                   杨夏       </td>   <td>中山大学</td>   <td>一种基于扩展卡尔曼滤波器的人体姿态识别方法</td>   <td>广东省</td>   <td>CN110781803A</td>   <td>2020-02-11</td>   <td>本发明提供了一种基于扩展卡尔曼滤波器的人体姿态识别方法,其包括获取一张人体姿态背景图像并对获取的人体姿态背景图像进行数据解码,利用有限状态机方法进行人体姿态背景图像数据的提取,得到人体姿态背景图像数据；按照一定周期连续采集人体姿态图像并对人体姿态图像进行数据解码,利用有限状态机方法进行人体姿态图像数据的提取,得到人体姿态图像数据；提取人体姿态背景图像数据的亮度Y和色度C<Sub>b</Sub>以及人体姿态图像数据的亮度Y<Sub>S</Sub>和色度C<Sub>bs</Sub>,得到前景图像为|C<Sub>b</Sub>-C<Sub>bs</Sub>|,对前景图像进行二值化,得到二值化处理后的人体姿态图像；对二值化处理后的人体姿态图像通过扩展卡尔曼滤波器进行姿态解算,识别人体姿态。本发明能够人体运动过程中的姿态。</td>   <td>1.一种基于扩展卡尔曼滤波器的人体姿态识别方法,其特征在于,包括以下步骤：获取一张人体姿态背景图像并对获取的人体姿态背景图像进行数据解码,利用有限状态机方法进行人体姿态背景图像数据的提取,得到人体姿态背景图像数据；按照一定周期连续采集人体姿态图像并对人体姿态图像进行数据解码,利用有限状态机方法进行人体姿态图像数据的提取,得到人体姿态图像数据；提取人体姿态背景图像数据的亮度Y和色度C<Sub>b</Sub>以及人体姿态图像数据的亮度Y<Sub>S</Sub>和色度C<Sub>bs</Sub>,得到前景图像为|C<Sub>b</Sub>-C<Sub>bs</Sub>|,对前景图像进行二值化,得到二值化处理后的人体姿态图像；对二值化处理后的人体姿态图像通过扩展卡尔曼滤波器进行姿态解算,识别人体姿态。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         孔繁校;                   陈龙       </td>   <td>中山大学</td>   <td>一种基于激光雷达与扇状空间分割的路沿检测系统及其方法</td>   <td>广东省</td>   <td>CN110781827A</td>   <td>2020-02-11</td>   <td>本发明涉及一种基于激光雷达与扇状空间分割的路沿检测系统及其方法。方法包括：1.激光雷达扫描车辆周围环境,获取反射点云信息并转换到本地构建的三维坐标系中；2.对点云数据进行预处理,把每一帧点云中的地面数据分离并提取出来；3.根据激光雷达与点云的数据特征,将坐标系中的空间分割为扇状的结构体,根据地面信息和扇状结构体,识别道路延伸方向；4.使用并行的路沿检索算法,提取点云中的路沿候选点；5.对路沿候选点进行聚类,根据扇状空间特征,排除干扰点集合；6.对最后确定的路沿点做B样条曲线拟合,得到路沿检测结果。本发明适应性强,能适应各种形状的道路,能减少障碍物的影响,精度和还原度高,可靠性强,误差率低。</td>   <td>1.一种基于激光雷达与扇状空间分割的路沿检测系统,其特征在于,包括：点云采集模块,用于通过32/64线激光雷达扫描车辆周围环境,采集周围环境的点云数据并进行处理,将带有空间坐标、反射亮度和雷达环数的点云数据转换给本地坐标系中,将每一帧数据输出给地面分离模块；地面分离模块：用于从一帧点云数据中提取出当前点云集的道路路面,所述道路路面指的是点云空间中所有物体最贴近地面的点所组成的曲面,地面点云集合输出给路沿检测模块；扇状空间分割模块：用于根据激光雷达反射点云的特性,将三维坐标内的空间分成不同的扇状区域,扇状区域的特性由点云的数据特性所决定,能匹配点云的分布特征,根据地面检测结果,将扇状结构输出给道路延伸识别模块和路沿检测模块；道路延伸识别模块：用于接收地面点云集合和扇状空间结构,结合两者特征,检测出车辆的可行驶区域,以此判断自动驾驶场景中道路的延伸方向,并将结果输出给路沿检测模块；路沿检测模块：用于接收地面点云集合和扇状空间结构,根据道路的延伸方向将点云数据分类,通过方位角排序、点法线差聚类、点坐标数值滤波的方法,并行地从点云数据中提取各雷达扫描线检测得到的路沿,根据各扫描线检测得到的路沿特征点,进行基于欧几里得聚类方法的处理,得到多个路沿特征点集合,输出给路沿点筛选模块；路沿点筛选模块：用于接收聚类点集合,根据扇状空间结构,排除候选路沿特征点中的干扰点,得到可靠性高的路沿点结果,输出给路沿拟合模块；路沿拟合模块：用于接收最终的路沿点,根据路沿点连接关系,用基于B样条曲线拟合的算法求出相应的路沿,结合道路延伸方向,得到每一帧的路沿检测结果。</td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苏勤亮;                   李强       </td>   <td>中山大学</td>   <td>一种融合图像生成和多标签分类的异常图像检测方法</td>   <td>广东省</td>   <td>CN110766056A</td>   <td>2020-02-07</td>   <td>本发明提供一种融合图像生成和多标签分类的异常图像检测方法,该方法包括以下步骤：S1：图像重构特征提取以及降维处理；S2：利用S1得到的低维数据进行多分类概率计算,提取有效的类别概率特征；S3：将S1和S2得到的数据作为一分类器的输入得到数据异常的概率值。本方法极大地提升了高类别概率值在异常检测任务中的有效性,以及其在两类数据中的区分性,对于异常数据,可以更有效的获取特征,进而提升检测效果。</td>   <td>1.一种融合图像生成和多标签分类的异常图像检测方法,其特征在于,包括以下步骤：S1：图像重构特征提取以及降维处理；S2：利用S1得到的低维数据进行多分类概率计算,提取有效的类别概率特征；S3：将S1和S2得到的数据作为一分类器的输入得到数据异常的概率值。</td>   <td>G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         汤琪;              卢宇彤;              陈志广;                   肖侬       </td>   <td>中山大学</td>   <td>基于深度学习的时间序列相似度的计算方法、系统及介质</td>   <td>广东省</td>   <td>CN110766060A</td>   <td>2020-02-07</td>   <td>本发明公开了一种基于深度学习的时间序列相似度的计算方法、系统及介质,本发明基于深度学习的时间序列相似度的计算方法实施步骤包括：1)获取两个等长时间段的时间序列数据；2)将两个等长时间段的时间序列数据输入预先完成训练的基于深度学习的神经网络模型,得到两个等长时间段的时间序列数据之间的相似度。本发明综合了各种传统度量方法的优点,在时间序列相似度度量问题上比各个传统度量方法效果都好,可根据不同的需求以及不同的数据集,还可以去用同样的方法学习出适用于不同领域的数据相似度的度量方法,且针对不同问题不用再去考虑数据的内在特征而选择相似度计算方法。</td>   <td>1.一种基于深度学习的时间序列相似度的计算方法,其特征在于,实施步骤包括：1)获取两个等长时间段的时间序列数据；2)将两个等长时间段的时间序列数据输入预先完成训练的基于深度学习的神经网络模型,得到两个等长时间段的时间序列数据之间的相似度。</td>   <td>G06K9/62;G06N3/04;G06Q40/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>              周宇       </td>   <td>中山大学</td>   <td>断层几何结构获取方法、装置和计算机设备</td>   <td>广东省</td>   <td>CN110766794A</td>   <td>2020-02-07</td>   <td>本发明涉及一种断层几何结构获取方法、装置、计算机设备和计算机可读存储介质,该方法包括步骤：生成目标区域的数字高程模型；确定数字高程模型中的断层迹线,断层迹线用于表示数字高程模型中断层在地表的迹线；根据断层迹线,对目标区域的数字高程模型进行采样；生成基于采样的多个平面；获取多个平面的收敛平面；根据收敛平面确定目标区域的断层面结构。可以提高获取断层面结构的效率。</td>   <td>1.一种断层几何结构获取方法,其特征在于,包括步骤：生成目标区域的数字高程模型；确定所述数字高程模型中的断层迹线,所述断层迹线用于表示所述数字高程模型中断层在地表的迹线；根据所述断层迹线,对所述目标区域的数字高程模型进行采样；生成基于所述采样的多个平面；获取所述多个平面的收敛平面；根据所述收敛平面确定所述目标区域的断层面结构。</td>   <td>G06T17/05</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘坤华;              陈龙;              张亚琛;                   袁湛楠       </td>   <td>中山大学</td>   <td>一种基于GAN的三维地图修复方法</td>   <td>广东省</td>   <td>CN110766797A</td>   <td>2020-02-07</td>   <td>本发明涉及深度学习技术领域,更具体地,涉及一种基于GAN的三维地图修复方法。首先,对输入的原始待修复色彩图像和其对应的待修复伪色彩视差图进行缩放和归一化处理,使其大小为H*W；其次,把待修复色彩图像和待修复伪色彩视差图输入到生成模型；再次,生成模型完成对待修复色彩图像和待修复伪色彩视差图的修复,得到修复后的色彩图像和修复后的伪彩色视差图。本发明提供的基于GAN的三维地图修复方法中生成模型和判别模型均有卷积神经网络构成,卷积神经网络可以很好地提取图像特征,解决传统方法难处理无纹理或弱纹理的图像的问题。</td>   <td>1.一种基于GAN的三维地图修复方法,其特征在于,首先,对输入的原始待修复色彩图像和其对应的待修复伪色彩视差图进行缩放和归一化处理,使其大小为H*W；其次,把待修复色彩图像和待修复伪色彩视差图输入到生成模型；再次,生成模型完成对待修复色彩图像和待修复伪色彩视差图的修复,得到修复后的色彩图像和修复后的伪彩色视差图。</td>   <td>G06T17/05;G06T5/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苏卓;              梁曦文;                   周凡       </td>   <td>中山大学</td>   <td>一种基于深度语义分割的图像去雾方法</td>   <td>广东省</td>   <td>CN110766640A</td>   <td>2020-02-07</td>   <td>本发明公开了一种基于深度语义分割的图像去雾方法。本发明收集清晰无雾图和相应深度图的数据集,采用PSPNet网络模型构建语义分割模块,采用自动编码器构建去雾模块,并嵌入语义分割模块作为图像去雾模型；制定去雾模型训练策略进行模型训练；取有雾图及对应清晰图片所组成的测试集,对完整的去雾模型进行测试,将有雾图输入到语义分割模块中获得该有雾图的语义分割特征图,再将有雾图和对应语义分割特征图输入到去雾模块中,最后输出清晰无雾图。本发明方法能在较快时间内完成精度高的图像去雾任务,同时基于语义分割信息,能有效避免产生色差和光晕伪影现象,且在PSNR和SSIM指标上能比大部分现有图像去雾方法更好。</td>   <td>1.一种基于深度语义分割的图像去雾方法,其特征在于,所述方法包括：收集清晰无雾图和相应的深度图的数据集,室内图采用NYU-Depth v2数据集,室外图则采用RESIDE数据集；采用在ADE20K数据集下训练的PSPNet网络模型,并对该模型进行微调来构建语义分割模块；采用自动编码器来构建去雾模块,并嵌入语义分割模块作为完整的单张图像去雾模型；制定去雾模型训练策略,并进行模型训练,进一步修正网络参数得到最好的模型结果,即先设计损失函数,计算损失函数得到当前误差,通过反向传播修改网络参数；取有雾图及对应的清晰图片所组成的测试集,对完整的去雾模型进行测试,该最终模型接收一张输入的有雾图,经过语义分割模块后得到相应的语义分割标签图,将有雾图和得到的标签图连接起来并输入到去雾模块中得到对应的清晰结果图。</td>   <td>G06T5/00;G06T7/11</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;              李翔;              吴岸聪;                   赖剑煌       </td>   <td>中山大学</td>   <td>基于行人局部和整体融合匹配的无交叠视域摄像头之间的不完整行人匹配方法</td>   <td>广东省</td>   <td>CN105447465B</td>   <td>2020-02-07</td>   <td>本发明公开了一种基于行人局部和整体融合匹配的无交叠视域摄像头之间的不完整行人匹配方法,包括以下步骤：(1)建立行人局部匹配模型；(2)建立行人整体匹配模型；(3)融合局部模型和整体模型实现不完整行人的匹配。本发明在允许不对行人进行手动规整对齐的情况下,建立行人局部匹配模型和整体匹配模型,最后融合两种模型的匹配度信息,从而实现不完整行人匹配。本发明允许不对行人进行手动规整对齐,从而保持不完整行人的原始信息。同时,本发明对行人局部信息和全局信息进行融合匹配,实现优势互补,相比于直接将行人手动对齐到相同尺度建模的现有行人匹配方法,能够显著地提升不同摄像头之间不完整行人的匹配准确率。</td>   <td>1.一种基于行人局部和整体融合匹配的无交叠视域摄像头之间的不完整行人匹配方法,其特征在于,该方法包括下述步骤：(1)建立行人局部匹配模型；行人局部匹配模型是在允许无手动对齐的情况下,对不完整行人和完整行人在局部信息上的匹配关系进行建模得到的,根据行人局部匹配模型可以计算得到不完整行人和完整行人之间的局部匹配的相似度信息；建立行人局部匹配模型的具体方法为：对于摄像头cam<Sub>A</Sub>中检测到的不完整行人P<Sub>A</Sub>,其特征矩阵被表示为Y＝[y<Sub>1</Sub>,y<Sub>2</Sub>,…,y<Sub>n</Sub>],其中Y的每一列是P<Sub>A</Sub>的一个局部特征向量,n是P<Sub>A</Sub>局部特征向量的数量,对于摄像头cam<Sub>B</Sub>中检测到的C个完整行人<Image id="icf0001" he="87" wi="353" file="FDA0002164878080000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其特征矩阵被表示为<Image id="icf0002" he="85" wi="700" file="FDA0002164878080000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中D<Sub>c</Sub>的每一列是<Image id="icf0003" he="84" wi="74" file="FDA0002164878080000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>的一个局部特征向量,k<Sub>c</Sub>是<Image id="icf0004" he="86" wi="74" file="FDA0002164878080000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>局部特征向量的数量,首先组合C个完整行人的特征矩阵得到D＝[D<Sub>1</Sub>,D<Sub>2</Sub>,…,D<Sub>C</Sub>],再利用特征矩阵D为P<Sub>A</Sub>的n个局部特征向量y<Sub>i</Sub>,i∈[1,n]分别计算一个重构表达系数x<Sub>i</Sub>,使其满足y<Sub>i</Sub>＝Dx<Sub>i</Sub>,最后利用<Image id="icf0005" he="86" wi="73" file="FDA0002164878080000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>的特征矩阵D<Sub>c</Sub>对于P<Sub>A</Sub>的每个局部特征向量的重构误差之和来确定P<Sub>A</Sub>和<Image id="icf0006" he="85" wi="73" file="FDA0002164878080000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>之间的局部距离：          <Image id="icf0007" he="122" wi="700" file="FDA0002164878080000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,δ<Sub>c</Sub>用来从x<Sub>i</Sub>中选择与D<Sub>c</Sub>对应的重构系数；(2)建立行人整体匹配模型；(3)融合局部模型和整体模型实现不完整行人的匹配。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈彦呈;              陈湘萍;              周凡;              郑贵锋;                   林谋广       </td>   <td>中山大学</td>   <td>一种基于视觉特征的在线数学教育资源分类方法</td>   <td>广东省</td>   <td>CN110765872A</td>   <td>2020-02-07</td>   <td>本发明公开了一种基于视觉特征的在线数学教育资源分类方法。本发明根据数学教育资源数据,确定资源分类类别,并对数据进行解析,提取文本特征以及提取图形特征；将所述文本特征与图形特征进行向量化,并输入到随机森林模型中进行训练,得到训练好的模型后对未分类的数学教育资源进行分类。本发明利用机器学习的方法进行大量在线数学教育资源的分类,能有效地降低人力时间成本,同时能实时地对用户上传的资源进行分类。借助该方法,用户无需自己为上传的资源做分类工作,资源管理系统也无需聘请额外的人员做资源分类工作,同时,当训练的数据越多,利用机器学习进行分类的准确率也越高,且相比于手工分类而言花费的时间成本也越少。</td>   <td>1.一种基于视觉特征的在线数学教育资源分类方法,其特征在于,所述方法包括：根据数学教育资源数据,确定资源分类类别,并对数据进行解析,提取文本特征以及提取图形特征；将所述文本特征与图形特征进行向量化,并输入到随机森林模型中进行训练,得到训练好的模型后对未分类的数学教育资源进行分类。</td>   <td>G06K9/00;G06K9/62;G06N20/00;G06Q50/20;G06F40/289</td>  </tr>        <tr>   <td>中国专利</td>   <td>         耿立帅;                   朝红阳       </td>   <td>中山大学</td>   <td>基于深度摄像模组的物体三维重建方法</td>   <td>广东省</td>   <td>CN110751684A</td>   <td>2020-02-04</td>   <td>本发明涉及计算机视觉领域下的三维重建技术领域,涉及一种基于深度摄像模组的物体三维重建方法。本发明在体素哈希算法过程中采用了一种新的哈希方法：MD5,可以大幅提高数据插入、查找和索引的速度,减少碰撞；另外,本发明提出了一种新的内存分配方式,解决了现有方法中一次性分配固定大小内存的缺陷,从而能够在重建过程当中自动分配内存,实现动态扩展重构区域的目的；本发明采用了一种新的前后两帧对齐方式,计算前后两帧彩色图的orb特征点并挑选好的对应点对,利用对应点从深度图中得到前一帧的世界坐标以及后一帧的相机坐标,从而求解出后一帧的相机外参,实现前后两帧对齐,该方法能更加准确的找到对应点,减小三维重建中的漂移问题。</td>   <td>1.一种基于深度摄像模组的物体三维重建方法,采用基于体素哈希的算法实现三维重构；其特征在于,在体素哈希过程中采用一种新的哈希函数方法计算得到哈希值,新的哈希函数方法包括以下步骤：首先,首先经过如下公式将三维坐标转化为一维索引：          <Image id="icf0001" he="142" wi="700" file="FDA0002239410210000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,Δ为当前设备分辨率的大小,即一个体素的大小；然后,将计算好的一维索引数据通过MD5方法转化为哈希值。</td>   <td>G06T7/50;G06T17/20;G06T15/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         何金钰;                   朝红阳       </td>   <td>中山大学</td>   <td>一种半监督的三维点云手势关键点检测方法</td>   <td>广东省</td>   <td>CN110751097A</td>   <td>2020-02-04</td>   <td>本发明属于计算机视觉领域下的模式识别领域,更具体地,涉及一种半监督的三维点云手势关键点检测方法,采用这种方法可以利用未标注的数据获得准确的三维关键点信息；本发明提出了基于TOF模组生成三维点云进行手势关键点识别的方法,三维点云相对二维图像对于复杂场景和光线条件较差的环境,识别精度有较大的提升；本发明优化了点云数据的处理方式,先对手部点云进行平滑再采样,比采样后再平滑精度更高。</td>   <td>1.一种半监督的三维点云手势关键点检测方法,其特征在于,包括以下步骤：S1.构建RGB-D手势数据集：S11.由TOF模组拍摄手势训练集,分别获得2D图片和1:1对应的深度图；S12.设计二维图像的手部关键点检测网络进行训练；S13.基于以上模型在2D图片上进行手部关键点的识别；S14.将2D图像上的关键点对应到深度图,获得关键点的手势关键点深度坐标；S2.数据预处理：S21.将深度图的手部区域转为三维世界坐标；S22.通过重采样对手部三维点云进行平滑,通过对周围数据点进行高阶多项式插值来重建表面缺失的部分；S23.对平滑后的手部点云进行随机采样,最后获得1024个点；S24.根据当前点云求出点云的法线；S25.对点云进行归一化处理；S3.搭建点云检测网络,输入大小为Nx6的点云数据集,N为训练样本点云的大小,此处为1024,网络输出为21个关键点的三维坐标P；S4.手势关键点识别与分类；首先,基于距离阈值去除部分背景,然后,以三维点云输入网络,经过网络计算准确得到手部21个关键点的三维手势关键点坐标。</td>   <td>G06K9/00;G06K9/34;G06K9/62;G06T7/136</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              卢钦伟;                   谢晓华       </td>   <td>中山大学</td>   <td>一种基于光照、姿态生成对抗网络的人脸识别方法</td>   <td>广东省</td>   <td>CN110751098A</td>   <td>2020-02-04</td>   <td>本发明公开了一种基于光照、姿态生成对抗网络的人脸识别方法,包括步骤：(1)获取各种光照以及角度的人脸图像作为样本数据；(2)通过端到端训练生成对抗网络得到最优光照生成器；(3)通过端到端训练生成对抗网络得到最优姿态生成器；(4)设置目标光照以及姿态,进行人脸光照、姿态变换。本发明利用生成对抗网络方法及各种损失函数,可有效地将给定的人脸图像变换到指定光照条件以及指定姿态下,另外在姿态变换网络中引入了双路注意力机制,将人脸跟关键点信息分开处理,使得在进行姿态变换的同时,能够很好地保留原有图像中的光照信息。本发明具有参数量小、速度较快的特点。</td>   <td>1.一种基于光照、姿态生成对抗网络的人脸识别方法,其特征在于,包括步骤：(1)获取多种不同光照以及角度的人脸图像作为样本数据；(2)对人脸图像进行关键点检测；(3)生成对抗网络的光照对抗训练：从训练样本中随机选取一张图像作为输入人脸图像,得到图像I、身份标签I<Sub>id</Sub>、原始光照标签S<Sub>light</Sub>,再随机选择一个目标光照标签T<Sub>light</Sub>作为目标光照,将图像I跟T<Sub>light</Sub>输入到光照生成器G<Sub>light</Sub>得到图像I<Sub>t_fake</Sub>,接着将I<Sub>t_fake</Sub>跟原始光照标签S<Sub>light</Sub>输入到G<Sub>light</Sub>得到图像I<Sub>s_fake</Sub>,将原图像I和图像I<Sub>s_fake</Sub>的误差反馈给G<Sub>light</Sub>,这个误差称之为循环一致性损失；训练过程中对I、I<Sub>s_fake</Sub>、I<Sub>t_fake</Sub>进行真假判别、身份判别、光照判别；通过不断迭代上述过程,得到最优光照生成器G<Sub>light</Sub>；(4)生成对抗网络的姿态对抗训练：利用成对的、相同身份、光照条件下的训练数据进行训练,每一对中一张为正脸图像,一张为侧脸图像,分别表示为图像I<Sub>1</Sub>,包括图像I<Sub>1</Sub>中所有关键点坐标信息的heatmap图kp<Sub>1</Sub>,图像I<Sub>2</Sub>,包括图像I<Sub>2</Sub>中所有关键点坐标信息的heatmap图kp<Sub>2</Sub>,将I<Sub>1</Sub>跟kp<Sub>1</Sub>、kp<Sub>2</Sub>输入姿态生成器G<Sub>pose</Sub>,得到I’<Sub>1</Sub>,将I<Sub>2</Sub>跟kp<Sub>1</Sub>、kp<Sub>2</Sub>输入G<Sub>pose</Sub>得到I’<Sub>2</Sub>,然后将I<Sub>1</Sub>、I’<Sub>1</Sub>以及I<Sub>2</Sub>、I’<Sub>1</Sub>这两对分别进行身份保存损失函数计算,接着进行感知损失函计算,最后对I’<Sub>1</Sub>、I’<Sub>2</Sub>做一个全变分正则损失计算；姿态对抗训练中,身份真假判别器D<Sub>id</Sub>对[I<Sub>1</Sub>,I’<Sub>2</Sub>]、[I<Sub>1</Sub>,I<Sub>2</Sub>]、[I<Sub>2</Sub>,I’<Sub>1</Sub>]、[I<Sub>2</Sub>,I<Sub>1</Sub>]进行真假判别,姿态真假判别器D<Sub>pose</Sub>对[I<Sub>1</Sub>,kp<Sub>1</Sub>]、[I’<Sub>1</Sub>,kp<Sub>2</Sub>]、[I<Sub>2</Sub>,kp<Sub>2</Sub>]、[I’<Sub>2</Sub>,kp<Sub>1</Sub>]进行真假判别,以此来跟姿态生成网络对抗,不断迭代优化,最终得到最优姿态生成器G<Sub>pose</Sub>；(5)人脸光照、姿态变换：输入待变换原始人脸图像、目标光照标签,先将待变换原始人脸图像跟目标光照标签输入光照生成器,然后将光照生成器的输出再输入到姿态生成器中,得到最终目标图像。</td>   <td>G06K9/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谢晓华;              罗文杰;                   赖剑煌       </td>   <td>中山大学</td>   <td>一种自动求解数学题的方法和系统</td>   <td>广东省</td>   <td>CN110751137A</td>   <td>2020-02-04</td>   <td>本发明公开了一种自动求解数学题的方法和系统,方法包括：获取待求解的数学题信息,并转化成对应的图像矩阵信号；对图像矩阵信号进行数学题的定位、字符分割与识别,得到字符序列；对识别出的字符序列进行空间结构分析和语义分析,从而实现数学计算题自动求解。本发明识别字符过程中使用了定位、字符切割和字符细化方法,在字符识别上运用了字符识别的分类器。进行空间结构分析和语义分析时,生成一颗解析树,最后对解析树进行语法制导翻译从而实现计算题的自动求解。本发明避免了只能通过键盘输入数学计算题的弊端,只要定义好数学题的类型和符号,就可以自动求解该类型的题目,因此可处理更多类型的题目,有更好的实用性以及更高的准确性。</td>   <td>1.一种自动求解数学题的方法,其特征在于,包括步骤：(1)获取待求解的数学计算题信息,并转化成对应的图像矩阵信号；(2)对图像矩阵信号进行数学计算题的定位、字符分割与识别,得到字符序列；(3)结合字符序列的类别、大小、空间坐标信息,对数学计算题进行结构分析,方法是使用基于句法规则的方法来解析数学计算题,不断地合并字符序列,最终得到数学计算题的树形结构的表示即解析树；(4)根据符号的数学语义规则,定义解析树节点的属性以及计算方法；后序遍历解析树,自底向上地传递解析树节点的属性值,最后得到根节点的属性值；把根节点的属性值作为数学计算题的答案,完成数学计算题的自动求解。</td>   <td>G06K9/20;G06K9/34;G06K9/62;G06F17/10;G06F40/30;G06Q50/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈彦呈;              龙永浩;              陈湘萍;              李绿周;                   周凡       </td>   <td>中山大学</td>   <td>一种测试驱动的网页构件功能抽取方法</td>   <td>广东省</td>   <td>CN110750449A</td>   <td>2020-02-04</td>   <td>本发明公开了一种测试驱动的网页构件功能抽取方法。本发明根据网页JavaScript代码,生成抽象语法树,并依次对JavaScript代码进行层次划分,得到JavaScript代码的语句层次信息；根据用户操作序列和DOM状态,自动地生成网页构件外观测试用例；结合所述网页构件外观测试用例与构件原有的业务逻辑测试用例,生成构件所需功能的功能描述测试用例。基于所述的代码的语句层次信息,利用遗传算法分层地对网页构件进行代码抽取,根据测试的通过率和代码长度评估抽取结果的质量。本发明对于任意网页,用户可以自动化抽取所需功能的实现代码。通过利用测试用例,保证了该方法提取的功能代码运行正确且样式无误。同时还尽可能地精简了代码行数,从而提高网页构件加载速度。</td>   <td>1.一种测试驱动的网页构件功能抽取方法,其特征在于,所述方法包括：根据网页JavaScript代码,生成抽象语法树,并依次对JavaScript代码进行层次划分,得到JavaScript代码的语句层次信息；根据用户操作序列和DOM状态,自动地生成网页构件外观测试用例；结合所述网页构件外观测试用例与构件原有的业务逻辑测试用例,生成构件所需功能的功能描述测试用例。基于所述的代码的语句层次信息,利用遗传算法分层地对网页构件进行代码抽取,根据测试的通过率和代码长度评估抽取结果的质量。</td>   <td>G06F11/36;G06N3/12</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;                   张文晓       </td>   <td>中山大学</td>   <td>一种基于自适应双分支网络的低分辨率行人再识别方法</td>   <td>广东省</td>   <td>CN110738099A</td>   <td>2020-01-31</td>   <td>本发明公开了一种基于自适应双分支网络的低分辨率行人再识别方法,包括步骤：(1)获取低分辨率行人图片,输入到双分支网络,双分支网络包括超分辨率网络分支和生成对抗网络分支,低分辨率行人图片通过双分支网络后得到两张高分辨率图像；(2)将上述两个高分辨率图像进行融合,得到超分辨率图像；(3)将超分辨率图像输入到特征提取网络,输出行人特征向量；(4)将行人特征向量与目标数据集中行人图片的特征向量进行比对,根据相似度确定低分辨率行人图片中行人的身份。本发明结合自适应双分支网络和特征提取网络,进行端到端训练,具有超分辨率图片视觉效果好、行人再识别准确率高的特点,具有很强的应用价值。</td>   <td>1.一种基于自适应双分支网络的低分辨率行人再识别方法,其特征在于,包括步骤：(1)获取低分辨率行人图片,输入到双分支网络,所述双分支网络包括超分辨率网络分支和生成对抗网络分支,低分辨率行人图片通过超分辨率网络分支生成能反应行人整体形状的第一高分辨率图像,通过生成对抗网络分支生成细节清晰的第二高分辨率图像；(2)将上述第一高分辨率图像和第二高分辨率图像进行融合,得到一张超分辨率图像；(3)将超分辨率图像输入到特征提取网络,输出行人特征向量；(4)将行人特征向量与目标数据集中行人图片的特征向量进行比对,计算相似度,根据相似度确定低分辨率行人图片中行人的身份。</td>   <td>G06K9/00;G06K9/62;G06T3/40;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈刚;              孟海涛;                   黄凯       </td>   <td>中山大学</td>   <td>一种基于神经网络的双目立体视觉匹配方法及其运算框架</td>   <td>广东省</td>   <td>CN110738241A</td>   <td>2020-01-31</td>   <td>本发明涉及一种基于神经网络的双目立体视觉匹配方法及其运算框架,匹配方法包括如下步骤,构建神经网络运算框架、构建二值神经网络并进行训练；初始化神经网络运算框架；步骤三：将左图像和右图像输入二值神经网络进行图像特征提取,得到一串二值序列作为图像像素点的特征描述；使用二值神经网络代理卷积神经网络用于图像的特征提取,并设计专门针对二值神经网络的神经网络训练方式和运算的运算框架,使得双目立体视觉的匹配不仅有更高的精度,同时有更快的运算速度。</td>   <td>1.一种基于神经网络的双目立体视觉匹配方法,其特征在于,包括如下步骤：步骤一：构建神经网络运算框架、构建二值神经网络并进行训练；步骤二：将左图像和右图像输入二值神经网络进行图像特征提取,得到一串二值序列作为图像像素点的特征描述；步骤三：二值神经网络通过匹配算法对左图像和右图像进行匹配。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘宁;              周凡;              林谋广;                   雍琪       </td>   <td>中山大学</td>   <td>一种基于模糊数整体效用的审判风险分析方法</td>   <td>广东省</td>   <td>CN110738398A</td>   <td>2020-01-31</td>   <td>本发明公开了一种基于模糊数整体效用的审判风险分析方法。本发明首先对审判业务流程中可能出现的风险事件进行梳理,再对每一个风险事件的发生概率、影响程度、可修复性进行模糊数描述,最后通过计算整体效用得到精确的风险值,从而达到风险排序分级的目的。本发明为审判风险管理提供风险控制库,提高管理工作的效率；采用模糊数整体效用表示审判风险的风险值,得到更加精确的风险分级体系,为审判风险预警机制提供可靠保障。</td>   <td>1.一种基于模糊数整体效用的审判风险分析方法,其特征在于,所述方法包括：整理出审判工作中可能出现的风险事件,并从三个维度对风险事件进行评价得到对应的模糊数；聚合所述模糊数,通过模糊推理得到模糊风险值；对所述模糊风险值进行去模糊化得到精确风险值；根据所述精确风险值排序形成风险报告。</td>   <td>G06Q10/06;G06Q50/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘宁;              雍琪;              林格;                   陈小燕       </td>   <td>中山大学</td>   <td>一种基于模糊集理论的司法审判流程偏离预警方法</td>   <td>广东省</td>   <td>CN110738399A</td>   <td>2020-01-31</td>   <td>本发明公开了一种基于模糊集理论的司法审判流程偏离预警方法。本发明根据审判业务流程划分四大类审判风险,建立包含若干风险要素的风险层级结构；梳理案件审理流程,确定各个风险要素是否存在；聚合所有风险要素,计算不同类型风险的模糊风险值；每一个类型的模糊风险值分别与9个级别模糊数计算相似度；最高相似度所对应的级别为该类型风险的风险级别,如果级别高于中等,则认为当前流程出现偏离,审判系统将发起预警。本发明建立了完善的审判风险层级体系,为审判流程规范化监督工作提供保障；通过风险分析实现事前预警,扩展了审判全流程管理的方案；采用计算机技术对风险进行量化,减轻人力负担,提高审判管理工作效率。</td>   <td>1.一种基于模糊集理论的司法审判流程偏离预警方法,其特征在于,所述方法包括：根据审判业务流程划分四大类审判风险,包括质量风险、效率风险、舆情风险和廉政风险,建立包含若干风险要素的风险层级结构；梳理当前案件审理流程,确定各个风险要素是否存在,并用要素标识符表示；聚合所有风险要素,计算不同类型风险的模糊风险值；每一个类型的模糊风险值分别与9个级别模糊数计算相似度；最高相似度所对应的级别为该类型风险的风险级别,如果级别高于中等,则认为当前流程出现偏离,审判系统将发起预警,并将风险情况推送给审判管理部门进行处理。</td>   <td>G06Q10/06;G06Q50/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘刚;              陈志广;                   肖侬       </td>   <td>中山大学</td>   <td>基于生成对抗网络的复数域语音增强方法、系统及介质</td>   <td>广东省</td>   <td>CN110739002A</td>   <td>2020-01-31</td>   <td>本发明公开了一种基于生成对抗网络的复数域语音增强方法、系统及介质,本发明复数域语音增强方法的实施步骤包括：获取带噪声的语音；将语音采用傅里叶变换后再采用笛卡尔坐标表示得到带噪声的实数谱和虚数谱；将带噪声的实数谱和虚数谱输入预先完成训练的生成对抗网络的生成器,得到去除噪声后的纯净语音的实数谱和虚数谱；将纯净语音的实数谱和虚数谱基于逆傅里叶变换生成干净的语音。本发明能够从语音信号中更好地剔除噪声、生成干净的语音,有效解决相位难以预测的问题,能够有效提高增强后语音的听觉效果,可有效提高语音识别系统在噪声环境下的语音识别准确率。</td>   <td>1.一种基于生成对抗网络的复数域语音增强方法,其特征在于实施步骤包括：1)获取带噪声的语音；2)将语音采用傅里叶变换后再采用笛卡尔坐标表示得到带噪声的实数谱R和虚数谱I；3)将带噪声的实数谱R和虚数谱I输入预先完成训练的生成对抗网络的生成器,通过生成器的编码器Encoder将实数谱R和虚数谱I组成的输入IR编码为高语义特征Encoder<Sub>IR</Sub>；高语义特征Encoder<Sub>IR</Sub>经过生成器的自注意力机制层self-attention输出具有全局信息的特征S<Sub>IR</Sub>；通过生成器的解码器Decoder将特征S<Sub>IR</Sub>解码得到增强后纯净语音的实数谱和虚数谱IR′；4)将增强后纯净语音的实数谱和虚数谱IR′基于逆傅里叶变换生成干净的语音。</td>   <td>G10L21/0208;G10L25/30</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李明;                   郭利锋       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种纯光学指纹活体检测方法</td>   <td>广东省</td>   <td>CN105975936B</td>   <td>2020-01-31</td>   <td>本发明涉及一种光学指纹活体检测方法,包括以下步骤：数据采集：采集手指按压过程的一组连续指纹图片；数据筛选：对连续指纹图片的副本做一系列的前景分割步骤,找到前景区域最大的两幅连续指纹图片；活体静态质量检测：对筛选出来的两幅指纹图片中的一副进行脊线强度、方向相关性、局部能量、均值方差等特性的检测,确定质量是否符合标准；动态活体检测：对满足静态质量检测的两幅图片做差图,确定指纹的活体性。本发明通过设置数据采集、数据筛选、活体静态质量检测和动态活体检测来对指纹数据进行有效采集和分析处理,有效判别指纹的活体性,大大提高了纯光学指纹识别仪的识别精度,可有效筛选掉伪指纹。</td>   <td>1.一种纯光学指纹活体检测方法,其特征在于,包括以下步骤：数据采集：利用光学指纹识别仪采集手指按压过程的一组连续指纹图片；数据筛选：对连续指纹图片的副本进行前景分割,找到前景区域最大的连续指纹图片f1和f2；活体静态质量检测：对筛选出来的指纹图片f1和f2的脊线强度、方向相关性、局部能量、均值方差进行检测；动态活体检测：将通过活体静态质量检测的两幅指纹图片做差求出差图f,对差图f的强度特性、能量特性、方向特性等进行检测,最终确定指纹活体性,动态活体检测具体包括以下步骤：S1.将通过活体静态质量检测的两幅指纹图片相同的前景区分割出来,做差求出差图f；S2.对f求均值,值小于设定阈值判定为没发生偏移,反之为发生偏移；S3.对f的强度特性、能量特性、方向特性和直方图特性进行检测；S4.针对以上f的特性并结合有无发生偏移,确定指纹是否为活体性指纹。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         孙颖;              辛秦川;                   黄健锋       </td>   <td>中山大学</td>   <td>基于深度学习的单木级树种识别方法</td>   <td>广东省</td>   <td>CN110728197A</td>   <td>2020-01-24</td>   <td>本发明实施例公开了一种基于深度学习的单木级树种识别方法,所述方法包括：通过LiDAR点云数据获取冠层高度模型；通过局部最大值算法与冠层高度模型进行航空影像单木分割,并裁剪得到单株树木的块状影像；基于单株树木的块状影像利用深度卷积神经网络对单木树种进行识别。在本发明实施例中以LiDAR点云数据与高分辨率航空影像为基础,利用深度卷积神经网络图像分类技术进行单木尺度树种识别,实现了同时获取林区树木棵数及单株树木类型。</td>   <td>1.一种基于深度学习的单木级树种识别方法,其特征在于,所述方法包括：通过LiDAR点云数据获取冠层高度模型；通过局部最大值算法与冠层高度模型进行航空影像单木分割,并裁剪得到单株树木的块状影像；基于单株树木的块状影像利用深度卷积神经网络对单木树种进行识别。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨钦泰;              韩蓝青;              任勇;              吴庆武;              陈健宁;              邓慧仪;              孙悦奇;              袁联雄;              王玮豪;              郑瑞;              洪海裕;              孔维封;              黄雪琨;              袁田;              邱惠军;              李权;              黄桂芳;              叶俊杰;                   王伦基       </td>   <td>中山大学附属第三医院;清华珠三角研究院</td>   <td>基于数字病理玻片进行慢性鼻窦炎的分型方法及其系统</td>   <td>广东省</td>   <td>CN110728666A</td>   <td>2020-01-24</td>   <td>本发明公开了基于数字病理玻片进行慢性鼻窦炎的分型方法,包括下列步骤：图像采集,对慢性鼻窦炎鼻息肉玻片获取到数字化病理图像并进行勾画,生成大掩膜图像；图像预处理,得到小病理图和小掩膜图；建立训练集数据；建立深度学习量化预测模块,采用Inception V3模型及其在ImageNet数据集上进行训练得到模型参数,去掉此模型最后一层全连接层FC,并增加一层全连接层FC且其内只有一个神经元,不采用任何激活函数,设置损失函数采用均方误差MSE,设置学习率lr；整合玻片上所有小病理图片的嗜酸性粒细胞比例值,得到最终辅助诊断结果。本发明还公开了其系统。本发明通过学习训练快速得到病理图片上的嗜酸性粒细胞占比值,给出客观的、准确性高的辅助诊断结果。</td>   <td>1.一种基于数字病理玻片进行慢性鼻窦炎的分型方法,其特征在于包括下列步骤：图像采集：1)对慢性鼻窦炎鼻息肉玻片进行完整扫描获取到数字化病理图像；2)将所述数字化病理图像勾画出病变区域,从而得到勾画区域,并将所述勾画区域的位置生成xml格式的文件进行保存；3)生成一个与所述数字化病理图像的分辨率一致的大掩膜图像,并根据所述xml对其进行赋值,xml区域内为1,区域外为0,即勾画区域对应的像素值为1,其他区域对应的像素值为0；图像预处理：设置切图的图片分辨率,读取所述数字化病理图像和所述大掩膜图像,并按照所述设置的切图的图片分辨率分别对所述数字化病理图像和所述大掩膜图像进行切图,分别得到小病理图和小掩膜图,并且小病理图和小掩膜图的位置一一对应；计算每个小掩膜图的像素平均值P,并设定阈值G,并只保存小掩膜图的像素的平均值P≥G所对应的小病理图；若小掩膜图的像素的平均值P＜G所对应的小病理图则丢弃,其中P,G取值范围均是0到1；所述大掩膜图、小掩膜图均指mask图像,mask图像是将所述数字化病理图像中的病变区域的轮廓信息在坐标系中由x、y坐标值表示出来。设计训练数据集：每张小病理图片记做x,统计每张小病理图片的嗜酸性粒细胞个数及所有炎症细胞个数,嗜酸性粒细胞占炎症细胞的比例＝嗜酸性粒细胞个数/所有炎症细胞个数,统计的嗜酸性粒细胞数目N,非嗜酸性粒细胞数目M,每张图片对应的嗜酸性粒细胞比例S计算为：S＝N/(N+M),取值范围0％～100％,其中没有嗜酸性粒细胞为0％,全是嗜酸性粒细胞为100％,采用平均绝对误差MAE对前述的所有小病理图片及其对应的S,按照设定的比例分成训练集数据和测试集数据；建立深度学习量化预测模块：首先,采用深度学习keras框架下的Inception V3模型及其在ImageNet数据集上进行训练得到模型参数,该模型最后一层全连接层FC,其内有进行分类的神经元,并且采用softmax激活函数,去掉此模型所述的最后一层全连接层FC,并增加一层全连接层FC,该新增全连接层FC只有一个神经元,不采用任何激活函数,然后设置InceptionV3的模型的损失函数loss＝‘mse’,即采用均方误差MSE,设置学习率lr,用ImageNet数据集训练得到的开源参数对InceptionV3模型进行参数初始化,最后用所述训练集数据重新训练InceptionV3模型的参数,训练次数设定为n轮,每一轮均用所述测试集数据进行测试,将测试图片输入当前得到的模型进行预测得到嗜酸性粒细胞比例的预测值P1,并将所述预测值P1与测试数据的真实标签值即所述嗜酸性粒细胞比例S,计算平均绝对误差MAE,即每个图片的所述预测值P1与所述嗜酸性粒细胞比例S的绝对值的平均值；将n轮中MAE最小时对应的模型参数进行保存,从而得到嗜酸性粒细胞占比模型,所述n为自然数,n为1时,即只训练即得到嗜酸性粒细胞占比模型,n大于1时,按照前述训练次数的要求进行训练；整合玻片上所有小病理图片的嗜酸性粒细胞比例值,得到慢性鼻窦炎鼻息肉玻片的最终辅助诊断结果：设定所述慢性鼻窦炎鼻息肉玻片由N张小病理图片组成,N为自然数,分别由所述嗜酸性粒细胞占比模型训练得到每张小病理图片的嗜酸性粒细胞占比值Si,i为(1,N),则该玻片最终诊断结果为N个值的平均值,D＝∑Si/N)。</td>   <td>G06T7/00;G16H30/20;G06N3/04;G06N3/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郝春;                   阳庆玲       </td>   <td>中山大学</td>   <td>一种大数据人脸监控装置</td>   <td>广东省</td>   <td>CN209992981U</td>   <td>2020-01-24</td>   <td>本实用新型公开了人脸监控技术领域的一种大数据人脸监控装置,包括视频抓取装置和大数据库,视频抓取装置的信号输出端电性连接有无线通信传输模块,无线通信传输模块的信号输出端与大数据库的信号输入端连接,大数据库中设有人脸信息对比模块,人脸信息对比模块的信号输出端电性连接有对比信息验证模块,对比信息验证模块的信号输出端电性连接有报警模块；本实用新型通过建立包含商家数据库和公安系统数据库的大数据库对人脸识别进行对比监控,及时将出现在商家数据库中的公安系统标记人员信息通过无线通信传输模块发送到公安系统数据库中,构建基于大数据的人脸监控系统,构建更加智慧的系统,提供更具价值的服务。</td>   <td>1.一种大数据人脸监控装置,包括视频抓取装置(1)和大数据库(2),其特征在于：所述视频抓取装置(1)的信号输出端电性连接有无线通信传输模块(3),所述无线通信传输模块(3)的信号输出端与大数据库(2)的信号输入端连接,所述大数据库(2)中设有人脸信息对比模块(4),所述人脸信息对比模块(4)的信号输出端电性连接有对比信息验证模块(5),所述对比信息验证模块(5)的信号输出端电性连接有报警模块(6)。</td>   <td>G06K9/00;H04N7/18;G06F16/583</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              陈江滔;              陈荣军;              谢舜道;              朱雄泳;              曾衍瀚;                   路崇       </td>   <td>中山大学;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学花都产业科技研究院</td>   <td>一种可视化二维码的编码方法</td>   <td>广东省</td>   <td>CN107247984B</td>   <td>2020-01-24</td>   <td>本发明公开一种可视化二维码的编码方法,包括输入模块、图片预处理模块、图片通道提取模块、图片编码模块、图片通道合成模块和输出模块,提供基于信息隐藏的可视化二维码的编码方法,采用单位图像块的平均值作为运算目标,通过排序的方法确定编码的比特的方法。从美观性角度,此编码方法在保证解码鲁棒性的同时改善了图片编码的美观性,更符合人眼特性；从速度角度,此编码和解码方法节约了编码和解码的时间,更能适应现实环境的使用情况。</td>   <td>1.一种可视化二维码的编码方法,其特征在于,包括以下步骤：S1：输入模块：输入图像、输入信息和输入阈值δ；S2：图像预处理模块：根据比特流的长度重新调整图像的大小；S3：图像通道提取模块：在重新调整形成的图像上提取图像通道作为处理的对象；S4：图片编码模块：利用比特流对图像块编码；S5：图像通道合成模块：将图像通道进行合成,所有比特都编码后,得到一个编码后的图像<Image id="icf0001" he="71" wi="64" file="FDA0002223420290000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>与原来图像的CbCr颜色通道合成新的图像；S6：输出模块：在新的图像基础上加上可视化二维码的定位模块,形成可视化二维码；假设每个比特对应4×4图像块,每个编码单元均分为a、b、c和d四个区域,每个区域包含4个像素,分别统计每个模块亮度值的平均值M<Sub>a</Sub>、M<Sub>b</Sub>、M<Sub>c</Sub>和M<Sub>d</Sub>,通过这几个平均值值之间的排序关系来确定编码的是0或者1；把这四个值分为三个层次的比较：每个图像块统计M<Sub>a</Sub>、M<Sub>b</Sub>、M<Sub>c</Sub>和M<Sub>d</Sub>四个值,令<Image id="icf0002" he="84" wi="301" file="FDA0002223420290000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>首先对{M<Sub>a</Sub>,M<Sub>bc</Sub>,M<Sub>d</Sub>}进行排序,定义的编码规则为,当M<Sub>a</Sub>排在第二位,代表比特0；当M<Sub>a</Sub>排在其他位置,代表比特1；即编码的Bit为          <Image id="icf0003" he="139" wi="617" file="FDA0002223420290000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        为了保证图像的解码的鲁棒性,{M<Sub>a</Sub>,M<Sub>bc</Sub>,M<Sub>d</Sub>}三个值之间大小关系确定后,还需要让它们之间差值在一个定值之上,设置为δ,这个定值根据需要进行调整；依据编码的原则,对于输入的图像块和相应的比特,分为两大情况进行讨论,一种是排序正确的情况,即输入比特为’0’时,M<Sub>a</Sub>排在第二位或者输入比特为’1’时,M<Sub>a</Sub>排在第一位或者第三位；另外一种是排序错误的情况下,即输入比特为’0’时,M<Sub>a</Sub>排在第一位或者第三位,或者输入比特为’1’时,M<Sub>a</Sub>排在第二位。</td>   <td>G06K19/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘宁;                   高凯诗       </td>   <td>中山大学</td>   <td>一种垂直鱼眼镜头下行人属性分析方法及系统</td>   <td>广东省</td>   <td>CN110717544A</td>   <td>2020-01-21</td>   <td>本申请公开了一种垂直鱼眼镜头下行人属性分析方法及系统,将垂直鱼眼镜头获取的行人图片进行预处理,将预处理后的行人图片输入预设网络进行体态分类,获得每张行人图片的不同体态特征对应的概率；确定每张行人图片的体态分类损失和属性标签损失,对预设网络通过多次迭代的方式进行参数优化使得预设网络的损失进行收敛达到预设损失,通过优化后的所述预设网络确定行人图片的行人属性信息。通过垂直鱼眼镜头获取的行人图片对预设网络进行训练优化,使得训练后的网络的网络损失达到预设损失,利用了垂直鱼眼镜头下行人的特征对网络进行了改良,使得网络针对垂直鱼眼镜头下的行人有更好的属性分析结果,实现对垂直鱼眼镜头下的行人属性的分析。</td>   <td>1.一种垂直鱼眼镜头下行人属性分析方法,其特征在于,所述方法包括：将垂直鱼眼镜头获取的行人图片进行预处理,以使得每张行人图片都携带有行人属性标签；将预处理后的行人图片输入预设网络进行体态分类,获得每张行人图片的不同体态特征对应的概率；确定每张行人图片的体态分类损失和属性标签损失,所述体态分类损失和属性标签损失用于确定所述预设网络的损失；对所述预设网络通过多次迭代的方式进行参数优化使得所述预设网络的损失进行收敛达到预设损失,通过优化后的所述预设网络确定行人图片的行人属性信息。</td>   <td>G06K9/62;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨钦泰;              韩蓝青;              任勇;              吴庆武;              陈健宁;              邓慧仪;              孙悦奇;              袁联雄;              王玮豪;              郑瑞;              洪海裕;              孔维封;              黄雪琨;              袁田;              邱惠军;              李权;              黄桂芳;              叶俊杰;                   王伦基       </td>   <td>中山大学附属第三医院;清华珠三角研究院</td>   <td>病理图片的处理方法</td>   <td>广东省</td>   <td>CN110717908A</td>   <td>2020-01-21</td>   <td>本发明公开了病理图片的处理方法,包括图像采集：1)对病理玻片进行完整扫描获取到数字化病理图像；2)将数字化病理图像勾画出病变区域,并将勾画区域的位置生成xml格式的文件进行保存；3)生成一个与数字化病理图像的分辨率一致的大掩膜图像；图像处理：设置切图的图片分辨率,并对数字化病理图像和大掩膜图像切图,得到一一对应的小病理图和小掩膜图的位置；计算每个小掩膜图的像素平均值P,并设定阈值G,只保存P≥G所对应的小病理图；若P＜G,所对应的小病理图则丢弃,其中P,G取值范围均是0到1。本发明对数字化病理图像,能够基于深度学习技术使用GPU并行计算与统计,降低传统病理抽样统计的误差,提高病理诊断的效率与准确率。</td>   <td>1.一种病理图片的处理方法,其特征在于包括下列步骤：图像采集：1)对数字病理玻片进行完整扫描获取到数字化病理图像；2)将所述数字化病理图像勾画出病变区域,从而得到勾画区域,并将所述勾画区域的位置生成xml格式的文件进行保存；3)生成一个与所述数字化病理图像的分辨率一致的大掩膜图像,并根据所述xml对其进行赋值,xml区域内为1,区域外为0,即勾画区域对应的像素值为1,其他区域对应的像素值为0；图像处理：设置切图的图片分辨率,读取所述数字化病理图像和所述大掩膜图像,并按照所述设置的切图的图片分辨率分别对所述数字化病理图像和所述大掩膜图像进行切图,分别得到小病理图和小掩膜图,并且小病理图和小掩膜图的位置一一对应；计算每个小掩膜图的像素平均值P,并设定阈值G,并只保存小掩膜图的像素的平均值P≥G,所对应的小病理图；若小掩膜图的像素的平均值P＜G,所对应的小病理图则丢弃,其中P,G取值范围均是0到1；所述大掩膜图、小掩膜图就是指mask图像,所述mask图像是将所述数字化病理图像中的病变区域的轮廓信息在坐标系中由x、y坐标值表示出来。</td>   <td>G06T7/00;G06T7/11;G06T7/187</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              肖逢枝;              陈荣军;              谢舜道;              朱雄泳;                   曾衍瀚       </td>   <td>中山大学</td>   <td>一种二维条码初定位方法</td>   <td>广东省</td>   <td>CN110717500A</td>   <td>2020-01-21</td>   <td>本发明提供的一种二维条码初定位方法,包括：对原始二维码图像进行预处理并进行降采样处理；利用二值化算法对降采样处理后的图像进行处理,得到Min-Max二值化图片；根据二值化图片建立对应的积分图,通过边缘检测算子计算得到候选的矩形区域；计算窗口内边缘点的方向梯度直方图EOH,通过EOH特征判定,将符合特征判定的候选区域坐标映射到原始尺寸的灰度图上,输出二维条码的大致区域,完成初定位操作。本发明提供的一种二维条码初定位方法,实现快速自适应分离、提取出二维条码大致区域图像,有效减少了后续处理的计算量,加快整体识别效率,适用于移植到嵌入式实时系统,实现了对实际拍摄的光照不均、模糊、即便二维条码图像的初定位。</td>   <td>1.一种二维条码初定位方法,其特征在于：包括以下步骤：S1：采集摄像头拍摄的含有二维条码的原始图像并对原始图像进行预处理；S2：对预处理后的不同分辨率的图像进行降采样处理；S3：利用改进的Bernsen局部二值化算法对降采样处理后的图像进行处理,得到Min-Max二值化图片；S4：根据二值化图片建立对应的积分图,统计窗口内前景数,滤去分散的前景像素；S5：通过边缘检测算子计算得到候选的矩形区域,对候选的矩形区域进行边缘提取,计算窗口内边缘点的方向梯度直方图EOH,通过EOH特征判定,将符合特征判定的候选区域坐标映射到原始尺寸的灰度图上,输出二维条码的大致区域,完成初定位操作。</td>   <td>G06K9/46;G06K9/48</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谢晓华;              许伟鸿;                   赖剑煌       </td>   <td>中山大学</td>   <td>基于生成对抗网络的人脸光照迁移方法</td>   <td>广东省</td>   <td>CN110706152A</td>   <td>2020-01-17</td>   <td>本发明公开了一种基于生成对抗网络的人脸光照迁移方法,包括步骤：(1)获取正面人脸图像的样本数据；(2)从样本数据中选择两张图像分别作为目标人脸图像和参考光照图像,作为生成器的输入,生成器输出重光照图像,判别器将真实图像和重光照图像的误差反馈给生成器,分类器将目标人脸图像和重光照图像的身份信息的误差反馈给生成器。生成器、判别器和分类器进行反复对抗训练,得到最优人脸光照迁移模型；(3)人脸光照迁移。本发明采用生成对抗网络架构及损失函数,利用注意力机制使得模型能够有效地处理对局部光照细节。网络训练中不需要使用人脸的3D信息,也不需要对齐人脸,进行端到端训练,具有很好的重光照结果。</td>   <td>1.基于生成对抗网络的人脸光照迁移方法,其特征在于,(1)获取正面人脸图像的样本数据；(2)生成对抗网络的对抗训练：从样本数据中选择两张图像分别作为目标人脸图像和参考光照图像,将上述两张图像作为生成器的输入,生成器输出重光照图像；判别器将真实图像和当前生成的重光照图像的误差反馈给生成器,分类器将目标人脸图像和当前生成的重光照图像的身份信息的误差反馈给生成器；将当前生成的重光照图像作为新的当前目标人脸图像,将与原目标人脸具有相同光照效果的图像作为新的参考光照图像,重新输入到生成器中,生成器输出重构图像,将原目标人脸图像和重构图像的误差反馈给生成器,这个误差称之为循环一致性损失；生成器、判别器和分类器进行反复对抗训练,得到最优人脸光照迁移模型；(3)人脸光照迁移：利用训练好的最优人脸光照迁移模型进行人脸光照迁移,输入目标人脸图像和参考光照图像,即可输出光照迁移后带有参考光照的目标人脸图像。</td>   <td>G06T3/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         戈峰;              肖侬;              卢宇彤;              陈志广;                   邓楚富       </td>   <td>中山大学</td>   <td>基于FCN的腹部多器官核磁共振图像分割方法、系统及介质</td>   <td>广东省</td>   <td>CN110705555A</td>   <td>2020-01-17</td>   <td>本发明公开了一种基于FCN的腹部多器官核磁共振图像分割方法、系统及介质,本发明方法实施步骤包括获取输入图像并进行数据预处理、图像归一化操作,然后将其输入完成训练的高分辨率全卷积神经网络模型得到最终预测图,所述高分辨率全卷积神经网络模型被预先训练建立了归一化后的腹部多器官核磁共振图像及其对应的最终预测图的映射关系；将最终预测图使用激活函数激活得到预测得分图,且在每个像素位置取预测得分最高的类别作为该像素位置的预测标签类别,得到最终的分割预测图。本发明能够实现腹部多器官核磁共振图像的自动分割,例如按照无器官区、肝脏区、右肾区、左肾区和脾脏区五种不同区域类别对腹部多器官MR图像进行分割。</td>   <td>1.一种基于FCN的腹部多器官核磁共振图像分割方法,其特征在于实施步骤包括：1)获取输入的腹部多器官核磁共振图像并进行数据预处理、图像归一化操作；2)将归一化后的腹部多器官核磁共振图像输入完成训练的高分辨率全卷积神经网络模型得到最终预测图,所述高分辨率全卷积神经网络模型被预先训练建立了归一化后的腹部多器官核磁共振图像及其对应的最终预测图的映射关系；3)将最终预测图使用激活函数激活得到预测得分图,且在每个像素位置取预测得分最高的类别作为该像素位置的预测标签类别,得到最终的分割预测图。</td>   <td>G06K9/34;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              甄家杰;              刘凌波;                   李冠彬       </td>   <td>中山大学</td>   <td>一种基于深度学习的人群计数模型及其实现方法</td>   <td>广东省</td>   <td>CN110705344A</td>   <td>2020-01-17</td>   <td>本发明公开了一种基于深度学习的人群计数模型及其实现方法,所述方法包括：步骤S1,获取人群图像,对获取的人群图像进行预处理并利用标注信息产生对应的人群密度图；步骤S2,将输入的人群图像缩放成多个尺度版本,通过多个子网络提取各个尺度的特征,并利用特征增强模块增强各个尺度的特征；步骤S3,将多个子网络产生的特征结合,生成估计的人群密度图；步骤S4,利用估计的人群密度图与真实的人群密度图计算损失,更新模型参数；步骤S5,利用不同人群图像多次迭代式地进行步骤S1-S4的训练过程,直到符合停止的条件。</td>   <td>1.一种基于深度学习的人群计数模型,包括：预处理单元,用于获取人群图像,对获取的人群图像进行预处理后输出至特征提取单元,并利用标注信息产生对应的人群密度图；特征提取单元,用于将输入的人群图像缩放成多个尺度版本,通过多个子网络提取各个尺度的特征,并利用特征增强模块增强各个尺度的特征；估计人群密度图生成单元,用于将多个子网络产生的特征结合,生成估计的人群密度图；更新单元,用于根据所述估计人群密度图生成单元生成的估计的人群密度图与所述预处理单元生成的真实人群密度图计算损失,更新模型参数；迭代训练单元,用于多次迭代式地对不同人群图像进行所述预处理单元、特征提取单元、估计人群密度图生成单元以及更新单元的训练过程,直到满足设定的停止条件时停止训练。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         瞿毅力;              苏琬棋;              邓楚富;              王莹;              卢宇彤;              陈志广;                   肖侬       </td>   <td>中山大学</td>   <td>基于模块化GAN的多模态MRI与多模态CT的转换方法、系统及介质</td>   <td>广东省</td>   <td>CN110689561A</td>   <td>2020-01-14</td>   <td>本发明公开了一种基于模块化GAN的多模态MRI与多模态CT的转换方法、系统及介质,本发明转换方法包括根据所需执行的任务类型,选择GAN网络中训练好的模块来进行CT图-CT图模态转换、CT图-MRI图模态转换、MRI图-MRI图模态转换、MRI图-CT图模态转换、CT图-MRI病灶任务转换、MRI图-CT病灶任务。本发明考虑到MRI和CT内部子模态十分相似但MRI与CT两个模态又有巨大差异的情况,提出了一种采用模块化的条件GAN的转换方法,本发明可采用无监督学习方法,训练数据无需配准,在无需训练多个GAN的情况下能便利高校的实现单模态转换生成配准的多模态MRI和CT图。</td>   <td>1.一种基于模块化GAN的多模态MRI与多模态CT的转换方法,其特征在于实施步骤包括：1)判断需要执行的任务类型,若该任务为CT图-CT图模态转换则跳转执行步骤2),为CT图-MRI图模态转换则跳转执行步骤3),为MRI图-MRI图模态转换则跳转执行步骤4),为MRI图-CT图模态转换则跳转执行步骤5),为CT图-MRI病灶任务转换则跳转执行步骤6),为MRI图-CT病灶任务转换则跳转执行步骤7)；2)将完成训练后的GAN网络中的CT模态编码器与CT模态解码器组合可以得到一个CT内部多模态转换器,通过CT内部多模态转换器将输入的任意模态的CT图通转换生成目标模态的转换生成CT图；退出；3)将完成训练后的GAN网络中的CT模态编码器与MRI模态解码器组合可以得到一个CT-MRI多模态转换器,通过CT-MRI多模态转换器将输入的任意模态的CT图通转换生成目标模态的转换生成MRI图；退出；4)将完成训练后的GAN网络中的MRI模态编码器与MRI模态解码器组合可以得到一个MRI内部多模态转换器,通过MRI内部多模态转换器将输入的任意模态的MRI图通转换生成目标模态的转换生成MRI图；退出；5)将完成训练后的GAN网络中的MRI模态编码器与CT模态解码器组合可以得到一个MRI-CT多模态转换器,通过MRI-CT多模态转换器将输入的任意模态的MRI图通转换生成目标模态的转换生成CT图；退出；6)将完成训练后的GAN网络中的CT模态编码器与MRI病灶任务解码器组合即可得到一个MRI病灶任务处理器,通过MRI病灶任务处理器将输入的任意模态的CT图通转换生成MRI病灶任务；退出；7)将完成训练后的GAN网络中的MRI模态编码器与CT病灶任务解码器组合即可得到一个CT病灶任务处理器,通过CT病灶任务处理器将输入的任意模态的MRI图通转换生成CT病灶任务。</td>   <td>G06T7/30</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马艳阳;              单云霄;                   陈龙       </td>   <td>中山大学</td>   <td>一种三维空间中移动机器人定位系统及方法</td>   <td>广东省</td>   <td>CN110689572A</td>   <td>2020-01-14</td>   <td>本发明涉及一种三维空间中移动机器人定位系统及方法。其中,事件信息处理模块从事件相机读取事件流,并将事件流中的光照强度变化事件按照一定的时间窗组成事件图像帧；深度图像采集模块从深度相机中读取深度图像,并对其进行去噪等处理,从而生成深度图像帧；处理后的图像帧传输到信息融合模块,信息融合模块根据两个相机的内参以及它们之间的外参,计算出事件图像帧中每一个事件的深度信息,从而得到带深度的事件图像帧；VO模块利用带深度的事件图像帧实现三维空间中的定位。本发明利用事件相机的特性,可以适应黑暗环境、高动态范围环境等极端环境,并且该系统中传感器体积小、功耗低、易于安装等优点,适合搭载到移动机器人或自动驾驶系统上。</td>   <td>1.一种三维空间中移动机器人定位系统,其特征在于,包括：事件信息处理模块：按照滑动时间窗口的方法,累计一定时间间隔内每个像素位置发生事件的次数,形成初始事件图像帧；随后使用预先标定的事件相机内参对初始事件图像帧进行畸变矫正,矫正完成后使用高斯滤波对事件图像帧进行平滑处理,形成可用的事件图像帧,输出给信息融合模块；深度图像采集模块：用于实时从深度相机读取深度图,使用预先标定的深度相机内参进行畸变矫正,然后使用像素滤波的方法对深度图中的无效点进行深度恢复,处理完成后得到可用的深度图像帧,输出给信息融合模块；信息融合模块：用于从事件信息处理模块和深度图像处理模块获取可用的事件图像帧和深度图像帧,按照时间戳进行匹配,随后利用预先标定好的两个相机之间的外参,将深度图像帧中的像素投影到事件相机的像素坐标系,形成深度映射；根据生成的深度映射,事件图像帧中的事件结合深度信息生成带深度的事件图像帧,输出给VO模块；VO模块：用于在获取带深度的事件图像帧后,使用FAST特征点算法提取图像中的特征点,并利用LK光流法计算出上一个事件图像帧与当前事件图像帧中的特征点的对应关系；利用事件图像帧中的深度信息,计算出特征点在相机坐标系下的三维坐标,进而使用PnP算法求解出当前事件图像帧相对于上一个事件图像帧的相机相对位姿。</td>   <td>G06T7/73;G06T7/80;G06T7/55;G06T5/00;G01C21/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;              郭叙森;              许子潇;                   郭思璐       </td>   <td>中山大学</td>   <td>一种基于关键帧的三维物体检测与跟踪方法</td>   <td>广东省</td>   <td>CN110688905A</td>   <td>2020-01-14</td>   <td>本发明涉及一种基于关键帧的三维物体检测与跟踪方法,通过输入包含点云数据和图像数据的相邻两关键帧,首先使用特征提取网络对数据进行特征提取分别得到特征图,然后将特征图输入候选框提取网络得到两关键帧共享的候选框；之后通过共享候选框截取特征图相应特征进行特征融合,回归得到三维预测框；然后使用共享候选框截取特征图进行特征互相关得到相关特征,回归得到物体三维框在两关键帧的偏移量；通过插值算法得到所有帧的检测结果之后,对所有帧物体框进行关联,得到跟踪结果。本发明利用了流数据的冗余性,通过只对关键帧预测,大大减少了计算量,并且能够利用时序信息改善检测结果,提升了检测速度和更好的追踪目标。</td>   <td>1.一种基于关键帧的三维物体检测与跟踪方法,其特征在于,包括如下步骤：步骤一：输入前后两帧由点云数据和图像数据组成的关键帧数据,对数据进行预处理,其中的点云数据在俯视图方向上投影结构转化成BEV图；步骤二：将步骤一中的两关键帧数据进行特征提取,分别得到两关键帧的特征图,分别为点云特征图和图像特征图；步骤三：将步骤一中的两关键帧数据输入共享区域提取网络模块,生成能被两个关键帧共享的共享候选框集合；步骤四：步骤三中的共享候选框在所述特征图提取候选框特征,然后送入分类网络与框回归网络,得到物体的类别以及三维框位置；步骤五：步骤三中的共享候选框分别提取两关键帧的BEV图特征块,送入追踪模块提取对应候选框的相关特征,然后输入偏移回归网络得到两关键帧对应物体的三维框的偏移量；步骤六：根据物体的三维框和三维框对应的偏移量,运用插值法得到两个关键帧数据之间的其他帧数据的物体的三维框,从而得到所有帧中的物体的三维检测结果；步骤七：根据检测结果,对所有帧数据对应的物体相互关联,得到跟踪结果。</td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         邓楚富;              肖侬;              卢宇彤;              陈志广;              瞿毅力;              苏婉琪;                   王莹       </td>   <td>中山大学</td>   <td>基于条件生成对抗网络的多域图像转换方法、系统及介质</td>   <td>广东省</td>   <td>CN110675316A</td>   <td>2020-01-10</td>   <td>本发明涉及深度学习图像生成领域,具体涉及一种基于条件生成对抗网络的多域图像转换方法、系统及介质,本发明实施步骤包括输入待转换的x模态的原图x、y模态的原图y；采用预先训练好的条件提取器来生产x模态条件C<Sub>x</Sub>和y模态条件C<Sub>y</Sub>；将原图x、原图y、x模态条件C<Sub>x</Sub>、y模态输入预先训练好的条件生成对抗网络得到对应的图像转换结果。本发明利用特征提取器提取原图的特征,通过上采样及与零矩阵在通道上的拼接得到条件矩阵,在具有较高的独立性情况下,又保有每个模态输入自身的语义信息；本发明训练灵活,且对要转换的域的数量没有限制,所需参数少。</td>   <td>1.一种基于条件生成对抗网络的多域图像转换方法,其特征在于,实施步骤包括：1)输入待转换的x模态的原图x、y模态的原图y；2)针对原图x采用预先训练好的条件提取器来生产x模态条件C<Sub>x</Sub>,针对原图y采用预先训练好的条件提取器来生产y模态条件C<Sub>y</Sub>；3)将原图x、原图y、x模态条件C<Sub>x</Sub>、y模态条件C<Sub>y</Sub>输入预先训练好的条件生成对抗网络得到对应的图像转换结果。</td>   <td>G06T3/40;G06T7/33;G06T9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;              郭叙森;                   康德开       </td>   <td>中山大学</td>   <td>一种融合图像分割与分类的细胞图像语义分割方法</td>   <td>广东省</td>   <td>CN110675368A</td>   <td>2020-01-10</td>   <td>本发明涉及一种融合图像分割与分类的细胞图像语义分割方法,将细胞图像数据预处理后分别经过双线性细粒度分类神经网络和通过OSTU算法及填充算法进行处理,分别得到细胞分类模型和细胞分割图,将细胞分类模型对细胞分割图的前景连通区域进行预测,将预测结果赋给该连通区域从而得到逐区域的分类结果,结合分割得到的背景区域,最终得到细胞测试图像的语义分割结果。本发明融合了传统阈值方法以及深度学习方法实现对细胞图像的精确语义分割,与传统细胞图像分割方法相比,本发明还能够得到细胞的语义信息,并且是逐像素的语义类别,能够运用于细胞污染的鉴定以及隔离。</td>   <td>1.一种融合图像分割与分类的细胞图像语义分割方法,其特征在于,包括如下步骤：步骤一：构建细胞图像数据集,将细胞的相差显微镜数据按照细胞的类别分成七大类；步骤二：对图像数据进行预处理；步骤三：构建双线性细粒度分类神经网络,将步骤二中预处理后的图像输入双线性细粒度分类神经网络,双线性细粒度分类神经网络输出为图像中细胞的类别；步骤四：训练步骤三中的双线性细粒度分类神经网络使用梯度下降算法优化总体损失值,直到算法收敛且损失值不再下降后,保存网络参数,得到细胞分类模型；步骤五：将步骤二中预处理后的图像转化为细胞分割图；步骤六：使用步骤四中的细胞分类模型对步骤五中的细胞分割图每个前景连通区域进行采样预测,将预测结果赋给该连通区域从而得到逐区域的分类结果,结合分割得到的背景区域,最终得到细胞测试图像的语义分割结果。</td>   <td>G06T7/00;G06T7/11;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         钟俊勋;              成慧;                   何流       </td>   <td>中山大学</td>   <td>一种分布式完全覆盖的机器人编队覆盖方法</td>   <td>广东省</td>   <td>CN110675002A</td>   <td>2020-01-10</td>   <td>本发明涉及多移动机器人技术领域,更具体地,涉及一种分布式完全覆盖的机器人编队覆盖方法。本发明通过将问题转化为分布式约束优化问题,使得每个机器人在求解问题的过程能够将周围的机器人也引入,既在机器人间引入合作,使得最终得到的结果质量能显著优于现有的基于梯度的方法。并且在求解分布式约束优化问题时,本发明提出了使用梯度下降优化的连续最大和算法,此算法能够在保持非中心化和拥有多项式级的时间复杂度下,求出与传统的最大和算法相近的结果,从而能够成功地应用于解决对未知区域的完全覆盖问题,并且非中心化的性质使得算法的鲁棒性更高。</td>   <td>1.一种分布式完全覆盖的机器人编队覆盖方法,其特征在于,首先将如何最大化被释放机器人的总覆盖区域的问题转化为分布式约束优化问题,然后使用梯度下降优化的连续最大和算法进行求解,并通过在变量节点和效用函数节点间互相发送信息来实现优化；其中,转化后的分布式约束优化问题形式包括以下部分：1)实体<Image id="icf0001" he="65" wi="351" file="FDA0002228681410000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示各个机器人；2)变量节点<Image id="icf0002" he="64" wi="332" file="FDA0002228681410000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示各个机器人的位置状态；3)效用函数节点<Image id="icf0003" he="64" wi="347" file="FDA0002228681410000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示该机器人的分割后的覆盖区域的大小,由于二维空间的连续属性,使得效用函数节点U均为连续函数；效用函数节点会与能影响其取值的变量节点相连接；4)变量节点的取值范围<Image id="icf0004" he="72" wi="367" file="FDA0002228681410000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>5)优化问题的目标函数,即机器人群的总覆盖面积表示为：          <Image id="icf0005" he="151" wi="519" file="FDA0002228681410000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,向量x<Sub>j</Sub>表示所有与U<Sub>j</Sub>相连的变量节点集合。</td>   <td>G06Q10/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         梁湘国;              苏卓;              李波;              冷成财;                   罗笑南       </td>   <td>中山大学;南昌航空大学</td>   <td>一种基于卷积神经网络的灰阶图像着色方法及其装置</td>   <td>广东省</td>   <td>CN106855996B</td>   <td>2020-01-03</td>   <td>本发明实施例公开了一种基于卷积神经网络的灰阶图像着色方法及其装置,其中,该方法包括：收集图片集；选择向量化的卷积神经网络模型VCNN,并构造相应的网络结构；修改所述向量化的卷积神经网络模型VCNN；对所述图片集中的图片进行转换,获得相应YUV颜色空间的图片,从中随机抽取64x64的Y值块,同时抽取相应位置的U值块和V值块；训练网络,利用反向传播算法和随机梯度下降法更新网络参数；得到经过训练的网络后,利用网络进行着色,输入灰度块,输出相应的U值和V值,获得彩色图片。实施本发明实施例,解决了需要人为提供涂鸦或样例图片的缺点,实现全自动的图片着色,并解决了着色速度慢、着色效果不稳定的缺点,使得着色效果较自然而且稳定。</td>   <td>1.一种基于卷积神经网络的灰阶图像着色方法,其特征在于,所述方法包括：收集图片集；选择向量化的卷积神经网络模型VCNN,并构造相应的网络结构；修改所述向量化的卷积神经网络模型VCNN；对所述图片集中的图片进行转换,获得相应YUV颜色空间的图片,从中随机抽取64x64的Y值块,同时抽取相应位置的U值块和V值块,具体为：将下载的RGB颜色空间的图片转换为YUV颜色空间的图片；随机抽取YUV颜色空间的图片,再随机抽取图片中的64x64的区域,抽取出64x64的Y值作为训练网络的输入,以及相同位置的U值和V值,作为与训练网络的输出作对比的真实U,V值；训练网络,利用反向传播算法和随机梯度下降法更新网络参数；得到经过训练的网络后,利用网络进行着色,输入灰度块,输出相应的U值和V值,获得彩色图片。</td>   <td>G06T1/20;G06T1/60;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         印鉴;              林志平;                   刘威       </td>   <td>中山大学</td>   <td>一种基于用户评论的推荐算法</td>   <td>广东省</td>   <td>CN110648163A</td>   <td>2020-01-03</td>   <td>本发明提供一种基于用户评论的推荐算法,该方法基于深度学习的评论推荐系统,根据用户过去的浏览购买和评论记录和商品的评分记录及购买过该商品的顾客的评论,推测用户对该商品的喜好程度,最后推荐系统对评分进行排序,给用户推荐评分高的前N种商品。本发明利用自然语言处理技术对评论文本信息的进行向量化,并根据注意力机制算法计算每条评论的重要性和每条评论对潜在顾客的影响,采用卷积神经网络捕捉用户商品的交互特征矩阵,因此生成用户商品的交互向量,最后将生成交互向量输入到FM预测机预测评分,预测评分越高用户可能越喜欢该商品,最后根据评分进行排序给用户推荐评分前N个商品。</td>   <td>1.一种基于用户评论的推荐算法,其特征在于,包括以下步骤：S1：将用户ID,目标商品ID,用户评论集合,目标商品评论集合,用户购买过的商品ID集合,和购买过目标商品的用户ID集合编码为固定维度的向量；S2：根据注意力机制计算目标商品评论对用户的重要性,并结合用户购买历史记录与购买过目标商品的顾客的评论,用神经网络提取用户和目标商品的特征向量；S3：用S2步骤后生成的用户特征向量和商品特征向量通过向量的外积乘转化特征矩阵,用卷积神经网络CNN在特征矩阵张提取用户商品的交互特征,并编码为用户商品的交互向量；S4：将用户商品的交互向量输入到FM预测机,预测用户对目标商品的评分。并对其他商品进行上述操作,根据评分进行排序,给用户推荐评分高的前N个商品。</td>   <td>G06Q30/02;G06Q30/06;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              胡师艺;              成慧;                   王青       </td>   <td>中山大学</td>   <td>一种结合可见光和红外图像的多模态目标跟踪方法</td>   <td>广东省</td>   <td>CN106250878B</td>   <td>2019-12-31</td>   <td>本发明公开一种结合可见光和红外图像的多模态目标跟踪方法,步骤如下：分别获取可见光图像和红外图像；在任一模态下标定目标矩形框；两种模态下分别初始化目标模型；使用STRUCK算法分别跟踪T帧,在跟踪过程中判断是否要更新目标模型；跟踪T帧后,两种模态下分别往回跟踪T帧；分别计算两种模态下前向后向跟踪的误差；比较在这T帧两种模态下正向反向跟踪结果,选择可信度较高的模态作为这T帧的跟踪结果；这T帧下可信度比较小的模态位置更新为另一模态下的位置,并重新初始化目标模型；判断是否最后帧决定继续跟踪还是结束跟踪。本发明能在计算机上实现对视频的目标近乎实时跟踪,跟踪性能较普通方法也有很大的提高。</td>   <td>1.一种结合可见光和红外图像的多模态目标跟踪方法,其特征在于,步骤如下：分别获取可见光图像和红外图像；在任一模态下标定目标框；两种模态下分别初始化目标模型；使用STRUCK算法分别跟踪T帧,在跟踪过程中判断是否要更新目标模型；跟踪T帧后,两种模态下分别往回跟踪T帧；分别计算两种模态下前向后向跟踪的误差；比较在这T帧两种模态下正向反向跟踪结果,选择可信度较高的模态作为这T帧的跟踪结果；这T帧下可信度比较小的模态位置更新为另一模态下的位置,并重新初始化目标模型；判断是否最后帧决定继续跟踪还是结束跟踪；其具体过程如下：(1)读入图像：分别读入可见光和红外图像,分别将其转换成单通道图像后计算积分图；(2)在任一可见光或红外图像上手动框选目标框,框选后分别在可见光模态和红外模态这两种模态下通过粒子滤波采样正负样本,完成分类器的初始化；(3)读入下一帧的两种图像,在上一帧跟踪结果的基础上通过粒子滤波采样M个具有平移、尺度变化的样本,其中第一帧为步骤(2)中手动框选的位置；(4)分别采用在可见光模态和红外模态这两种模态的分类器在步骤(3)获得的样本中得到最好的样本作为该帧的跟踪结果；(5)判断分类器得到的最好的样本的分类得分是否大于预设的阈值,如果是,更新分类器,否则不更新；(6)判断该帧是否为预设T的倍数或者最后一帧,如果不是,则返回(3)；如果是,从该帧开始重新初始化两种模态的分类器,并从该帧开始,往回跟踪T帧,继续步骤(7)；(7)记这T帧的起始时间帧是τ<Sub>s</Sub>,终止时间是τ<Sub>e</Sub>,从τ<Sub>s</Sub>到τ<Sub>e</Sub>的正向跟踪结果的矩形框为：可见光模态下为<Image id="icf0001" he="100" wi="459" file="FDA0002085697550000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>在红外模态下为<Image id="icf0002" he="96" wi="457" file="FDA0002085697550000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>从τ<Sub>s</Sub>到τ<Sub>e</Sub>的跟踪结果的矩形框中心坐标为：可见光模态下<Image id="icf0003" he="96" wi="441" file="FDA0002085697550000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>红外模态下<Image id="icf0004" he="96" wi="442" file="FDA0002085697550000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>从τ<Sub>e</Sub>到τ<Sub>s</Sub>的反向跟踪结果的矩形框为<Image id="icf0005" he="74" wi="700" file="FDA0002085697550000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>中心坐标为<Image id="icf0006" he="79" wi="700" file="FDA0002085697550000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>(8)在这T帧下,计算可见光模态下跟踪结果正向、反向的平均重合率<Image id="icf0007" he="135" wi="577" file="FDA0002085697550000025.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中overlap函数是两个矩形交集面积和并集面积之比；计算红外模态下跟踪结果正向、反向的平均重合率<Image id="icf0008" he="135" wi="560" file="FDA0002085697550000026.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>(9)在这T帧下,计算可见光模态下跟踪结果中心的正向方向平均距离<Image id="icf0009" he="131" wi="444" file="FDA0002085697550000027.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>计算红外模态下的平均距离<Image id="icf0010" he="136" wi="453" file="FDA0002085697550000028.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>(10)在这T帧下,计算可见光模态下目标正向运动的总路程<Image id="icf0011" he="134" wi="418" file="FDA0002085697550000029.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>计算红外模态下目标正向运动的总路程<Image id="icf0012" he="132" wi="420" file="FDA00020856975500000210.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>(11)在两种模态下定义一个有效值：可见光模态下,有效值<Image id="icf0013" he="149" wi="603" file="FDA00020856975500000211.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>红外模态下的有效值为<Image id="icf0014" he="151" wi="596" file="FDA00020856975500000212.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>(12)如果<Image id="icf0015" he="74" wi="332" file="FDA00020856975500000213.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>那么可见光模态在这T帧的跟踪结果要好于红外模态,跟踪结果选择可见光模态的跟踪结果,红外模态在第e帧选择可见光的跟踪结果并重新初始化；否则选择红外模态的跟踪结果,可见光模态在第e帧的跟踪结果选择红外模态的并重新初始化；如果是最后一帧,则跟踪结束,否则返回步骤(3)；上述各式中：<Image id="icf0016" he="99" wi="89" file="FDA00020856975500000214.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示在模态m下从时间τ<Sub>s</Sub>到τ<Sub>s</Sub>的正向跟踪结果矩形框集合,元素<Image id="icf0017" he="101" wi="62" file="FDA00020856975500000215.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示在m模态下τ<Sub>i</Sub>时刻正向跟踪的矩形框；其中m＝1是可见光,m＝2是红外模态,i＝s,s+1,…,e；<Image id="icf0018" he="97" wi="86" file="FDA00020856975500000216.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示在模态m下从时间τ<Sub>s</Sub>到τ<Sub>s</Sub>的正向跟踪结果矩形中心坐标集合,元素<Image id="icf0019" he="100" wi="63" file="FDA0002085697550000031.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示在m模态下τ<Sub>i</Sub>时刻正向跟踪的矩形中心坐标,α是平衡参数,λ<Sub>1</Sub>和λ<Sub>2</Sub>是权重值；相应地,当标记的上方是←时表示的是反向跟踪。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              杨君;                   谢晓华       </td>   <td>中山大学</td>   <td>一种三维京剧脸谱自动化妆方法</td>   <td>广东省</td>   <td>CN106952221B</td>   <td>2019-12-31</td>   <td>本发明公开了一种三维京剧脸谱自动化妆方法,包括步骤：从输入源获取图像输入,进行人脸检测和人脸特征点定位并将特征点标注在图像上；对特征点进行人脸归一化处理；计算归一化后的人脸图像的深度信息,并根据深度信息重建三维点云；根据特征点信息对人脸图像进行三角分块操作；在准备好的京剧脸谱图片上,手工标注特征点,并对京剧脸谱进行相同的三角分块操作；将所述京剧脸谱上的每个三角块着色到人脸图像上；使用京剧脸谱将所述三维点云进行重新着色,并展现给用户。本发明自动计算出人脸特征点的坐标和人脸三维模型,运用计算机视觉和图像处理相关的技术,将事先手工标定好特征点的京剧脸谱描绘到目标人脸的三维模型上。</td>   <td>1.一种三维京剧脸谱自动化妆方法,其特征在于,包括步骤S1：从输入源获取图像输入,进行人脸检测和人脸特征点定位,如果输入图像中存在人脸,则将特征点标注在图像上；S2：对所述特征点进行人脸归一化处理；S3：计算归一化后的人脸图像的深度信息,并根据所述深度信息重建三维点云；步骤S2中所述归一化处理包括步骤：以两个眼球以及鼻尖的坐标为基准,进行仿射变换；步骤S3中计算归一化后的人脸图像的深度信息的方法包括步骤：归一化后的人脸图像与事先训练好的张量进行相乘,进行SVD分解操作,输出图像中每个像素点的深度信息；S4：根据所述特征点信息对人脸图像进行三角分块操作；S5：在准备好的京剧脸谱图片上,手工标注特征点,并对京剧脸谱进行和所述人脸图像相同的三角分块操作；S6：将所述京剧脸谱上的每个三角块进行仿射变化,着色到人脸图像上；步骤S6中人脸图像着色包括步骤：S61：根据脸谱中三角形顶点的坐标、以及真实人脸图像中特征点的位置计算出合适的仿射变换矩阵；S62：将仿射变换矩阵应用于脸谱图像,计算出人脸中每个点对应的RGB颜色值,RGB值为脸谱图像中的颜色值；S63：如果变换后的坐标点不在整点上,则采用双线性插值计算估计值；S7：根据人脸图像和三维点云的坐标对应关系,使用京剧脸谱将所述三维点云进行重新着色,并展现给用户；步骤S7中所述将所述三维点云进行重新着色包括步骤：S71：计算归一化变换所使用的仿射变换矩阵的逆变换矩阵；S72：将逆变换矩阵用于点云中每个点的坐标上,获取每个点对应人脸图像的坐标,从而得到对应的RGB颜色值；S73：如果变换后的坐标点不在整点上,则采用双线性插值计算估计值。</td>   <td>G06T3/00;G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         印鉴;              梁艺阐;                   周瑞莹       </td>   <td>中山大学</td>   <td>一种基于课程学习的深度学习机器阅读理解训练方法</td>   <td>广东省</td>   <td>CN110633730A</td>   <td>2019-12-31</td>   <td>本发明提供一种基于课程学习的深度学习机器阅读理解训练方法,本方法使用BERT预训练语言模型将(文章,问题,选项)三元组构建成一个序列,不需要单独对每一个元组进行操作。构成一个问题的四个选项序列输入到网络中,进行跟BERT一样的微调过程,经过全连接层和softmax分类层,选择最大概率选项作为预测答案,通过最大化正确答案的对数概率来反向更新模型的参数,使模型学习到文本信息。三阶段训练框架先在简单数据集微调,再在普通数据集微调,能由浅入深按顺序地学到文本知识,最后在困难数据集训练后的测试效果比融合学习(在简单和普通数据集混杂起来的集合上微调再在困难数据集上训练)的准确率要高出2.5％。</td>   <td>1.一种基于课程学习的深度学习机器阅读理解训练方法,其特征在于,包括以下步骤：S1：建立预训练语言模型在多选项阅读理解任务上的微调模型；S2：将建立好的微调模型依次在简单数据集和普通数据集中训练；S3：最后将微调好的模型用于困难数据集进行训练,然后进行测试。</td>   <td>G06K9/62;G06F16/33;G06F17/27;G06Q50/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              钟沈君;              谢舜道;                   陈荣军       </td>   <td>中山大学</td>   <td>一种双阶二维码的生成方法及其防复制验证方法</td>   <td>广东省</td>   <td>CN110633774A</td>   <td>2019-12-31</td>   <td>本发明提供的一种双阶二维码的生成方法,通过在普通二维码中嵌入S1纹理图案集,并同时嵌入S2纹理图案集,得到双阶二维码；本发明还提供一种双阶二维码的防复制验证方法,通过输入双阶二维码并进行纹理分类,得到图案匹配成纹理图案对；利用纹理图案对计算相关系数；将相关系数与验证阈值对比,完成双阶二维码的防复制验证。本发明提供的一种双阶二维码的生成方法及其防复制验证方法,充分利用了纹理二维码纹理图案对P&amp;S过程的敏感性,对普通二维码进行两个阶段的纹理图案嵌入而得到双阶二维码,通过相关系数的计算,直观准确地判断出所输入的二维码是真实或复制的,避免了印刷品在物理渠道被复制的可能性,且整个过程简单,容易进行推广。</td>   <td>1.一种双阶二维码的生成方法,其特征在于,包括以下步骤：S11：在普通二维码中嵌入S1纹理图案集,得到纹理二维码,具体为：使用S1纹理图案集,顺序替换普通矩阵式二维码数据区域中的所有暗模块；S12：在纹理二维码中嵌入S2纹理图案集,得到双阶二维码,具体为：使用S2纹理图案集替换纹理二维码数据区域中的部分暗模块。</td>   <td>G06K19/06;G06Q30/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨鑫;              齐新宇;              林承光;              黄晓延;                   夏云飞       </td>   <td>中山大学肿瘤防治中心(中山大学附属肿瘤医院、中山大学肿瘤研究所)</td>   <td>一种利用三维参数定量评价图像的方法</td>   <td>广东省</td>   <td>CN110634117A</td>   <td>2019-12-31</td>   <td>本发明公开了一种利用三维参数定量评价图像的方法,所述方法包括以下步骤：S1：扫描参数：利用测量模体对目标图像进行扫描,并获得若干个检测图像；S2：二维参数计算：分别计算所述若干个检测图像其对应的二维参数；S3：三维参数扩展计算：根据计算所得的各个图像的二维参数,得到各个图像的三维参数；结合各个所述三维参数评价目标图像。通过本技术方案,能够能更客观、更准确地评价图像质量。</td>   <td>1.一种利用三维参数定量评价图像的方法,其特征在于,所述方法包括以下步骤：S1：扫描参数：利用测量模体对目标图像进行扫描,并获得若干个检测图像；S2：二维参数计算：分别计算所述若干个检测图像其对应的二维参数；S3：三维参数扩展计算：根据计算所得的各个图像的二维参数,从所述检测图像中选取若干层相邻图像以及ROI,获得各个三维ROI体素平均值并进行计算,得到各个图像的三维参数；或根据计算所得的各个图像的二维参数,从所述检测图像中分别选取若干层相邻的图像,然后选取这些图像经过预处理后的图像并进行计算,得到各个图像的三维参数；结合各个所述三维参数评价目标图像。</td>   <td>G06T7/00;G06T7/13;G06T7/168</td>  </tr>        <tr>   <td>中国专利</td>   <td>         卢伟;                   曾令雯       </td>   <td>中山大学</td>   <td>一种半色调图像隐写分析方法</td>   <td>广东省</td>   <td>CN110619594A</td>   <td>2019-12-27</td>   <td>本发明提供的一种半色调图像隐写分析方法,包括：构造隐写图片,得到模拟的隐写图像数据集；确定模式块大小,统计模拟的隐写图像数据集中每张图片的模式块的直方图特征；根据模式块的直方图特征空间,对所有图像特征进行降维并构造变换矩阵；从数据集构造训练集和测试集,提取训练集、测试集图像模式块的直方图特征,利用变换矩阵计算训练集特征向量,输入SVM中进行分析模型的训练；将测试集特征向量输入到训练好的分析模型中,完成对图像隐写的分析。本发明提供的图像隐写分析方法,在选择大小合适的模式块的同时,采用PCA进行降维,对隐写的主要特征成分进行提取,降低特征维度,保持了更多的有效特征从而提高检测的准确率。</td>   <td>1.一种半色调图像隐写分析方法,其特征在于,包括以下步骤：S1：使用模拟嵌入的方法来构造M张隐写图片,得到一个模拟的隐写图像数据集,其中包含2M张图片；S2：确定模式块大小,统计模拟的隐写图像数据集中每张图片的模式块的直方图特征,得到模式块的直方图特征空间；S3：根据模式块的直方图特征空间,对所有图像特征进行降维并构造变换矩阵；S4：从数据集构造训练集,提取训练集图像模式块的直方图特征,利用变换矩阵计算训练集特征向量,输入SVM中进行分析模型的训练；S5：从数据集构造测试集,提取测试集图像模式块的直方图特征,利用变换矩阵计算测试集特征向量,将测试集特征向量输入到训练好的分析模型中,完成对图像隐写的分析。</td>   <td>G06T1/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         卢伟;                   黄韬       </td>   <td>中山大学</td>   <td>一种基于残差网络的超分辨图像检测方法</td>   <td>广东省</td>   <td>CN110619631A</td>   <td>2019-12-27</td>   <td>本发明提供的一种基于残差网络的超分辨图像检测方法,包括：选取图像,得到图像数据集；对图像数据集进行数据集增强并生成低分辨图像；对低分辨率图像进行处理,生成对应的超分辨图像集；构建残差网络模型,将超分辨图像集作为模型输入,对残差网络模型进行训练；将待检测的图像输入训练好的残差网络模型中,完成图像是否经过深度超分辨的检测。本发明提供的超分辨图像检测方法,通过构建残差网络模型对图像进行超分辨检测,直接对输入图像进行检测,无需保留中间结果,检测效率高；残差网络模型为全卷积结构,适应用任意尺寸的图像检测和各种不同的超分辨技术,普适性强；检测用时短,可以达到实施检测,大大提高检测效率。</td>   <td>1.一种基于残差网络的超分辨图像检测方法,其特征在于,包括以下步骤：S1：从公共数据集中选取图像,得到图像数据集；S2：对图像数据集进行数据集增强扩充,生成作为正样本的真实高分辨率图像；S3：将真实高分辨率图像利用双立方插值方法下采样生成低分辨图像；S4：利用经典的基于深度学习的超分辨方法对低分辨率图像进行处理,生成对应的超分辨图像集,作为负样本数据集；S5：构建残差网络模型,包括特征提取部分和特征分类检测部分,将负样本数据集作为模型输入,对残差网络模型进行训练；S6：将待检测的图像输入训练好的残差网络模型中,输出概率最大的分类对应的类别,即完成图像是否经过深度超分辨的检测。</td>   <td>G06T7/00;G06T3/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄炳升;              蔡洵;              曾英候;              陈亮亮;              冯仕庭;                   宋晨宇       </td>   <td>深圳大学;中山大学附属第一医院</td>   <td>基于深度学习的肝细胞癌磁共振图像分割系统和方法</td>   <td>广东省</td>   <td>CN110619635A</td>   <td>2019-12-27</td>   <td>本发明公开了基于深度学习的肝细胞癌磁共振图像分割系统和方法,该方法包括：获取肝细胞癌肿瘤患者的多序列磁共振成像图像；将获取的多序列磁共振成像图像输入至深度融合网络模型中,从而获得病灶分割结果图；所述深度融合网络模型包括深度卷积网络模块和多序列融合模块,所述深度卷积网络模块划分为多个序列通道,所述多序列融合模块用于融合所有序列通道处理多序列磁共振成像图像的处理结果。本发明通过深度融合网络模型来对多序列磁共振成像图像进行病灶分割,能够获得较好的分割效果,并且分割更加准确。本发明作为基于深度学习的肝细胞癌磁共振图像分割系统和方法可广泛应用于医学图像处理领域。</td>   <td>1.基于深度学习的肝细胞癌磁共振图像分割方法,其特征在于：包括以下步骤：获取肝细胞癌肿瘤患者的多序列磁共振成像图像；将获取的多序列磁共振成像图像输入至深度融合网络模型中,从而获得病灶分割结果图；所述深度融合网络模型包括深度卷积网络模块和多序列融合模块,所述深度卷积网络模块划分为多个序列通道,其中每个序列通道均用于处理所述多序列磁共振成像图像中的一个序列的磁共振成像图像,所述多序列融合模块用于融合所有序列通道处理多序列磁共振成像图像的处理结果。</td>   <td>G06T7/11;G06T7/30;G06K9/32;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         卢伟;                   马铭       </td>   <td>中山大学</td>   <td>基于边缘点频域空域特征结合图像模糊区域定位方法</td>   <td>广东省</td>   <td>CN110619647A</td>   <td>2019-12-27</td>   <td>本发明提供的基于边缘点频域空域特征结合图像模糊区域定位方法,对待测图像进行边缘点检测,得到不同尺度参数下滤波后的边缘点；在不同尺度的窗口下,对各个边缘点处的窗口图像做再模糊操作,提取原图窗口和再模糊图像窗口的DCT比值的频域特征；计算待测图像的共生矩阵,计算图像空域特征信息；将频域特征与图像空域特征信息进行加权融合并进行滤波操作,得到模糊响应图；用两个模糊响应阈值对模糊响应图进行处理,并将处理结果进行抠图计算,对得到的多尺度全像素点模糊相应图进行多尺度融合,输出融合后的模糊定位结果。本发明提供的模糊区域定位方法,实现了对数字图像中的模糊区域的精确定位,定位精度高。</td>   <td>1.基于边缘点频域空域特征结合图像模糊区域定位方法,其特征在于：包括以下步骤：S1：对待测图像进行边缘点检测,得到不同尺度参数下滤波后的边缘点；S2：在不同尺度的窗口下,对各个边缘点处的窗口图像做不同程度的再模糊操作,提取基于原图窗口和再模糊图像窗口的DCT比值的频域特征；S3：计算待测图像的共生矩阵,计算共生矩阵的能量、熵、对比度从而得到图像空域特征信息；S4：将频域特征与图像空域特征信息进行加权融合并进行滤波操作,得到模糊响应图；S5：用两个模糊响应阈值对模糊响应图进行处理,并将处理结果进行抠图计算,对得到的多尺度全像素点模糊相应图进行多尺度融合,输出融合后的模糊定位结果。</td>   <td>G06T7/12;G06T7/194;G06T7/73;G06T5/00;G06T5/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张晓春;              李熙莹;              陈振武;              邱铭凯;                   张枭勇       </td>   <td>深圳市城市交通规划设计研究中心有限公司;中山大学</td>   <td>一种车辆开关门动作识别方法、系统及存储介质</td>   <td>广东省</td>   <td>CN110619286A</td>   <td>2019-12-27</td>   <td>本发明公开了一种车辆开关门动作识别方法、系统及存储介质,方法包括：对每帧车辆图片进行车门状态特征提取与识别,得到每帧车辆图片的车门状态特征及车门状态识别结果；根据所述车门状态识别结果在确定识别到车门状态变化时,对连续帧的车门开关状态特征序列进行车门开关动作识别,连续帧包含车门状态发生变化的车辆图片帧。本发明将车辆开关门动作识别分解成车门状态识别以及车门开关动作识别两部分,在实际应用中可以对视频的每一帧图片先进行车门状态识别,并仅当确定车门状态变化时才进行连续帧的车门动作识别,不再需要在每次预测时都输入连续帧图片进行动作识别,从而更好地提高了动作识别的效率以及实用性,可广泛应用于图像处理领域。</td>   <td>1.一种车辆开关门动作识别方法,其特征在于：包括以下步骤：对每帧车辆图片进行车门状态特征提取与识别,得到每帧车辆图片的车门状态特征及车门状态识别结果；根据所述车门状态识别结果在确定识别到车门状态变化时,对连续帧的车门开关状态特征序列进行车门开关动作识别,所述连续帧包含车门状态发生变化的车辆图片帧。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         乔宇;              张秀兰;              宋迪屏;              李飞;              曲国祥;              刘亮希;                   熊健       </td>   <td>中国科学院深圳先进技术研究院;中山大学中山眼科中心</td>   <td>基于视野检查报告的数据处理方法、装置及设备</td>   <td>广东省</td>   <td>CN110619332A</td>   <td>2019-12-27</td>   <td>一种基于视野检查报告的数据处理方法包括：获取待处理的视野检查报告中的光敏感度数据图数据、模式偏差数值图数据和模式偏差概率图数据；将所述光敏感度数据图数据输入第一卷积神经网络模型得到第一训练分值,将模式偏差数值图数据输入第二卷积神经网络模型得到第二训练分值,将所述模式偏差概率图数据输入第三卷积神经网络模型得到第三训练分值；根据所述第一分值、第二分值、第三分值以及对应的权值计算得到分类结果。与通过图像对比的方式相比,识别精度更高,并且在模式偏差概率图和模式偏差数值图的基础上,引入保留有原始信息的光敏感度数据图,更加充分的利用视野检查报告信息,有利于提高所获取的分类结果的准确度。并且基于该分类结果,有利于提高最终报告的生成效率,并且结合其它检测信息,可以有助于医务人员快速的对病情进行评估。</td>   <td>1.一种基于视野检查报告的数据处理方法,其特征在于,所述基于视野检查报告的数据处理方法包括：获取待处理的视野检查报告中的光敏感度数据图数据、模式偏差数值图数据和模式偏差概率图数据；将所述光敏感度数据图数据输入第一卷积神经网络模型得到第一训练分值,将模式偏差数值图数据输入第二卷积神经网络模型得到第二训练分值,将所述模式偏差概率图数据输入第三卷积神经网络模型得到第三训练分值,其中第一卷积神经网络模型、第二卷积神经网络模型和第三卷积神经网络模型分别通过视野检查报告样本中的光敏感度数据图样本数据、模式偏差数值图样本数据和模式偏差概率图样本数据训练得到；根据所述第一分值、第二分值、第三分值以及对应的权值计算得到分类结果。</td>   <td>G06K9/34;G06K9/46;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              肖翔;              张伟;                   顾建权       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于两层时空邻域特征的动作识别方法</td>   <td>广东省</td>   <td>CN105956604B</td>   <td>2019-12-24</td>   <td>本发明公开一种基于两层时空邻域特征的动作识别方法,包括：对输入的视频,根据视频中连续帧的运动方向变化信息,提取运动变化模式特征作为视频的第一层原始特征。对第一层特征采用改进的词袋模型进行特征建模,得到第一层特征的向量表示。根据第一层中的每个局部兴趣点和最近邻若干兴趣点之间的时空关系,计算出第二层时空特征。对第二层特征采用改进的词袋模型进行特征建模,得到第二层特征的向量表示。将第一、二层的向量表示级联,形成视频最终的中层特征表达。采用支持向量机进行特征分类,得到识别准确率。本发明能有效地获取最近邻兴趣点的相对位置信息和类别信息,并结合了改进的词袋模型方法进行特征建模,显著提高了动作识别的准确率。</td>   <td>1.一种基于两层时空邻域特征的动作识别方法,其特征在于,包括以下步骤：(1)输入待识别的视频,根据视频中连续帧的运动方向变化信息,提取运动变化模式特征作为视频的第一层原始特征；(2)对第一层原始特征采用包含k-means++聚类方法的改进的词袋模型进行特征建模,得到第一层原始特征的向量表示；(3)根据第一层原始特征中的每个局部兴趣点和最近邻若干兴趣点之间的时空关系,计算出第二层时空特征；(4)对第二层时空特征同样采用改进的词袋模型进行特征建模,得到第二层时空特征的向量表示；(5)将第一层原始特征和第二层时空特征的向量表示级联起来,形成该视频最终的中层特征表达；(6)采用支持向量机(SVM)进行特征分类,最终输出动作视频的识别准确率；所述改进的词袋模型的具体实现包括数据聚类和计算统计频率直方图这两个步骤,其聚类是采用k-means++方法,k-means++方法的描述如下：(3-1)从输入的数据点集合中随机选择一个点作为第一个聚类中心；(3-2)对于数据集中的每一个点x,计算它与已选择的聚类中心中最近的聚类中心的距离D(x)；(3-3)选择一个新的数据点作为新的聚类中心,其选择的原则是：D(x)较大的点,被选取作为聚类中心的概率较大；(3-4)重复步骤(3-2)和(3-3)直到k个聚类中心被选出来；(3-5)利用这k个初始的聚类中心来运行标准的k-means算法。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王青;              周静文;              余伟江;              梁小丹;              林倞;                   肖侬       </td>   <td>中山大学</td>   <td>一种基于多域异质图引导的视觉问题常识推理模型及方法</td>   <td>广东省</td>   <td>CN110598573A</td>   <td>2019-12-20</td>   <td>本发明公开了一种基于多域异质图引导的视觉问题常识推理模型及方法,所述模型包括：预训练模型初始化单元,利用预训练模型对基础骨架网络和分类器参数进行初始化；视觉信息提取单元,用于利用检测器来提取输入图像的视觉信息的初始特征；上下文投票单元,用于提取全局视觉信息中未被标注的视觉隐含信息结合到视觉信息的局部特征之中；语言特征提取单元,用于利用自然语言预训练模型提取语言部分问题和答案的特征表示；多域特征推理融合单元,用于构造多域异质图,将视觉信息特征和语言信息特征利用多域异质图进行多域特征推理融合获得最终特征表示；分类单元,用于利用分类器对获得的特征处理后进行打分,选取得分高的选项为答案。</td>   <td>1.一种基于多域异质图引导的视觉问题常识推理模型,包括：预训练模型初始化单元,利用预训练模型对基础骨架网络和分类器参数进行初始化；视觉信息提取单元,用于利用检测器来提取输入图像的视觉信息的初始特征；上下文投票单元,用于提取全局视觉信息中未被标注的视觉隐含信息结合到视觉信息的局部特征之中；语言特征提取单元,用于利用自然语言预训练模型提取语言部分问题和答案的特征表示；多域特征推理融合单元,用于构造多域异质图,将视觉信息特征和语言信息特征利用多域异质图进行多域特征推理融合获得最终特征表示；分类单元,用于利用分类器对获得的特征处理后进行打分,选取得分高的选项为答案。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08;G06N5/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         何兆成;                   钟嘉明       </td>   <td>中山大学</td>   <td>一种基于蚁群算法的响应式公交服务规划方法</td>   <td>广东省</td>   <td>CN110598971A</td>   <td>2019-12-20</td>   <td>本发明涉及一种基于蚁群算法的响应式公交服务规划方法,通过以区域内出行者的出行时间、出行起终点等出行信息为基础,利用蚁群算法这一效果优、解释性强的启发式算法,规划包括发班时间、服务车型及途径站点在内的响应式公交服务方案,实现公交资源供给与公交出行需求的最优适配。本发明提出的方法具有数据驱动性,可规划得到稳定、优质的响应式公交服务方案；并且本发明提出具有良好的可解释性,便于使用者根据实际情况,理解、复现与改造方法,适用性广,可推广性强。</td>   <td>1.一种基于蚁群算法的响应式公交服务规划方法,其特征在于,包括以下步骤：步骤S1：获取用户公交出行的需求数据；步骤S2：基于需求数据构建响应式公交服务模型；步骤S3：基于蚁群算法对响应式公交服务模型进行优化,得到响应式公交的服务方案；步骤S4：根据响应式公交的服务方案求得响应式公交的服务方案的成本；步骤S5：判断响应式公交的服务方案的成本是否收敛,若是,以该方案作为最优响应式公交服务方案进行输出,若否,重新调整蚁群算法,返回步骤S3。</td>   <td>G06Q10/06;G06Q10/04;G06Q50/30;G06N3/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         卢伟;                   吕启越       </td>   <td>中山大学</td>   <td>一种图像区域复制粘贴篡改检测方法</td>   <td>广东省</td>   <td>CN110599478A</td>   <td>2019-12-20</td>   <td>本发明提供的图像区域复制粘贴篡改检测方法,包括：从待测图像提取特征点；根据特征点的局部图像块强度顺序计算其LIOP特征；利用Delaunay三角剖分Bowyer-Watson算法对特征点进行处理,计算每个三角形的LIOP描述子；进行三角形匹配并计算匹配后的三角形邻域；保留在三角形邻域内的特征点,形成特征点集合；生成特征点匹配对,并对特征点匹配对进行聚类,得到多个类别；计算每个类别的仿射矩阵；根据特征点匹配对和对应的仿射矩阵,计算对应区域变换前后的相关系数图,定位篡改区域。本发明提供的篡改检测方法,对旋转、缩放、JPEG压缩、添加噪声等具有更好的鲁棒性；速度快,实用性更强；便于更精确仿射变换矩阵,对匹配对的聚类操作能够应对多重复制的篡改操作。</td>   <td>1.一种图像区域复制粘贴篡改检测方法,其特征在于,包括以下步骤：S1：对待测图像进行预处理得到特征点,从而得到待提取特征的局部图像块；S2：将局部图像块根据像素强度进行子区域划分,计算每个像素点的局部强度顺序特征,得到对应特征点的LIOP特征；S3：利用Delaunay三角剖分Bowyer-Watson算法对特征点进行处理,生成Delaunay三角网,并分别计算每个三角形的三个顶点的LIOP描述子的平均值作为对应三角形的特征向量；S4：根据每个三角形的特征向量进行三角形匹配并计算匹配后的三角形邻域；S5：判断对应的特征点是否位于三角形邻域内；若是,执行步骤S6；否则,丢弃该特征点及对应的LIOP特征；S6：保留该特征点及对应的LIOP特征,形成特征点集合；S7：根据特征点集合进行特征点匹配,生成特征点匹配对,并对特征点匹配对进行聚类,得到多个类别；S8：计算每个类别的仿射矩阵；S9：根据特征点匹配对和对应的仿射矩阵,计算对应区域变换前后的相关系数图,定位篡改区域；S10：判断所有类是否计算完毕,若是,合并所有类的篡改区域作为检测结果；否则,执行步骤S9。</td>   <td>G06T7/00;G06T7/11;G06T5/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘文奇;              曾坤;              龚永义;                   罗笑南       </td>   <td>中山大学</td>   <td>一种基于卷积神经网络的图像头发识别方法及其装置</td>   <td>广东省</td>   <td>CN106611160B</td>   <td>2019-12-17</td>   <td>本发明实施例公开了一种基于卷积神经网络的图像头发识别方法及其装置,其中,该方法包括：收集图集；对图集中的每一张图片进行标记；对图集中的每一张图片进行预处理；检测图集中每一张图片的人像头部位置,获得训练图集对应的头部位置信息和测试图集对应的头部位置信息；基于全卷积网络构造头发全卷积网络；对头发全卷积网络进行训练；将输出的结果与头发区域遮蔽测试图集相比较,获得对头发全卷积网络的评估结果；将欲获取头发遮蔽图的图片输入头发全卷积网络,获得该图像头发区域的遮蔽图。可以解决现有技术中难以处理背景颜色与头发颜色相似的情况,以及无法在图像人脸偏转角度很大或人背面的情况下无法识别头发的问题。</td>   <td>1.一种基于卷积神经网络的图像头发识别方法,其特征在于,所述方法包括：收集图集,该图集包括训练图集和测试图集；对图集中的每一张图片进行标记,获得训练图集对应的头发区域遮蔽训练图集和测试图集对应的头发区域遮蔽测试图集；对图集中的每一张图片进行预处理,获得与训练图集对应的YCrCb训练图集和频率遮蔽训练图集、与测试图集对应的YCrCb测试图集和频率遮蔽测试图集；检测图集中每一张图片的人像头部位置,获得训练图集对应的头部位置信息和测试图集对应的头部位置信息；基于全卷积网络构造头发全卷积网络HFCN；将YCrCb训练图集、频率遮蔽训练图集及训练图集的头部位置信息进行编码,输入到头发全卷积网络,同时,将头发区域遮蔽训练图集输入头发全卷积网络,对头发全卷积网络进行训练,获得训练好的头发全卷积网络；将YCrCb测试图集、频率遮蔽测试图集及测试图集的头部位置信息进行编码,并输入到训练好的头发全卷积网络,将输出的结果与头发区域遮蔽测试图集相比较,获得对头发全卷积网络的评估结果；将欲获取头发遮蔽图的图片输入头发全卷积网络,获得该图像头发区域的遮蔽图。</td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              陈景文;              刘凌波;                   李冠彬       </td>   <td>中山大学</td>   <td>基于动态卷积网络的自然场景下人脸素描生成模型及方法</td>   <td>广东省</td>   <td>CN110580726A</td>   <td>2019-12-17</td>   <td>本发明公开了一种基于动态卷积网络的自然场景下人脸素描生成模型及方法,该方法包括：步骤S1,对所有卷积网络和全连接网络参数进行初始化；步骤S2,获取人脸图像,利用全卷积神经网络提取该人脸图像的层次化特征；步骤S3,将获得的特征利用转置卷积网络进行上采样,并利用可变形卷积网络挖掘人脸潜在区域和人脸变化形式信息；步骤S4,将特征分为多尺度区域,在各区域动态计算出自适应的滤波器权重,将滤波器权重与特征进行卷积计算得到新的特征,将多个尺度下所有区域特征组合生成高质量的人脸素描；步骤S5,根据生成的人脸素描与真实人脸素描的对比更新模型参数；步骤S6,多次迭代进行步骤S2-S5训练。</td>   <td>1.一种基于动态卷积网络的自然场景下人脸素描生成模型,包括：初始化单元,用于对模型的网络参数进行初始化；编码器单元,用于获取不经过预处理的自然场景下的人脸图像,利用全卷积神经网络提取该人脸图像的层次化特征；解码器单元,利用转置卷积网络逐渐将所述编码器单元生成的层次化特征上采样,并利用可变形卷积网络挖掘人脸潜在区域和人脸变化形式信息；人脸素描生成单元,用于将所述可变形卷积网络输出的特征分为多尺度区域,在各个区域动态计算出自适应的滤波器权重,并将滤波器权重与对应区域的特征进行卷积计算得到新的特征,将多个尺度下所有区域特征组合并生成高质量的人脸素描；更新单元,用于将所述人脸素描生成单元生成的人脸素描与真实人脸素描进行比较,通过优化目标函数的策略更新模型参数；迭代训练单元,用于多次迭代式地进行所述编码器单元、解码器单元、人脸素描生成单元以及更新单元的训练过程,直到训练过程收敛后或者达到最大迭代次数后得到最终模型。</td>   <td>G06T11/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王自鑫;              陈弟虎;              衣杨;                   黄侃       </td>   <td>中山大学</td>   <td>一种高层次综合工具中的指引文件自动生成方法及系统</td>   <td>广东省</td>   <td>CN106599370B</td>   <td>2019-12-13</td>   <td>本发明公开了一种高层次综合工具中的指引文件自动生成方法及系统,该方法包括有以下步骤：A、获取代码中的循环信息,若存在人工定义的循环展开因子,则直接执行步骤C,否则执行步骤B；B、根据循环信息对循环进行展开处理,通过设计空间探索计算得到循环展开因子；C、根据步骤A或步骤B中得到的循环展开因子生成高层次综合工具的指引文件。本发明利用循环展开因子自动生成高层次综合工具的循环指引文件,进而供高层次综合工具使用,无需人工编写循环的处理代码,大大减少设计人员的工作量和压力,显著提高了高层次综合工具生成硬件的效果。本发明作为一种高层次综合工具中的指引文件自动生成方法及系统可广泛应用于计算机与电路设计领域。</td>   <td>1.一种高层次综合工具中的指引文件自动生成方法,其特征在于：包括有以下步骤：A、获取代码中的循环信息,若存在人工定义的循环展开因子,则直接执行步骤C,否则执行步骤B；B、根据循环信息对循环进行展开处理,通过设计空间探索计算得到循环展开因子；C、根据步骤A或步骤B中得到的循环展开因子生成高层次综合工具的指引文件；所述步骤B具体包括以下子步骤：B1、根据循环信息建立循环依赖关系图；B2、计算最大迭代并行距离；B3、计算内存总线最大访问量；B4、通过设计空间探索计算得到循环展开因子；步骤B2中的最大迭代并行距离为指令间没有数据依赖的距离；步骤B3中的内存总线最大访问量根据最大迭代并行距离与访存指令能占满内存总线数的最大值计算得到；所述设计空间探索采用二分搜索的方式进行,所述设计空间探索的范围是从0到步骤A中得到的循环迭代距离,所述设计空间探索的约束条件为步骤B3中求得的资源消耗量。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘坤华;              陈龙;              袁湛楠;                   张彧       </td>   <td>中山大学</td>   <td>一种基于有向图的云制造服务优选数学模型建立方法</td>   <td>广东省</td>   <td>CN110569584A</td>   <td>2019-12-13</td>   <td>本发明提供了一种基于有向图的云制造服务优选数学模型建立方法,本发明以前k个最小和最大指标的服务组合方案为优选目标,对云制造服务组合方案分别建立目标函数和数学模型,并以结点具有资源代价的有向图进行表示,并在此基础上将结点具有资源代价的有向图转变为只有路径具有资源代价结点不具有资源代价的标准有向图,从而将服务组合优选问题转变为标准有向图的最短路径求解问题,方便服务组合方案的优选求解,提高求解效率,有效提高云制造企业生产效率,降低生产成本。</td>   <td>1.一种基于有向图的云制造服务优选数学模型建立方法,其特征在于,包括以下步骤：S1：分别建立设计服务组合方案、生产服务组合方案、产品服务组合方案和产品组合方案的前k个最小和最大指标服务组合方案数学模型；S2：根据前k个最小和最大指标服务组合方案数学模型将设计服务、生产服务、产品服务以及产品的服务组合过程通过结点具有资源代价的有向图表示；S3：将结点具有资源代价的有向图转变为只有路径具有资源代价结点不具有资源代价的标准有向图,根据标准有向图更新前k个最小和最大指标服务组合方案数学模型。</td>   <td>G06F17/50;G06F17/15;G06Q10/06;G06Q10/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘星成;                   钟思强       </td>   <td>中山大学</td>   <td>一种对对象基于区间数的基本概率分配生成方法</td>   <td>广东省</td>   <td>CN110569902A</td>   <td>2019-12-13</td>   <td>本发明涉及一种对对象基于区间数的基本概率分配生成方法,其包括：获取已知分类情况的训练样本形成训练集,每类训练样本包括至少一个属性特征；对所有属性特征进行归一化处理；提取各类训练样本中各个属性特征的最大值和最小值构建各个属性特征的区间数模型；将待测对象属性特征归一化处理；计算待测对象各个属性特征和区间数模型中各个区间数的相似度；根据训练集中不同属性特征数据的不确定程度确定每个属性特征权重；利用计算的相似度和属性特征权重确定待测对象的BPA；利用组合规则将待测对象的BPA进行融合得到待测对象最终的BPA。本发明克服现有的区间数相似度计算公式的缺陷,同时考虑到特征数据的不同权重,构造出更加合理的BPA生成方法。</td>   <td>1.一种对对象进行基于区间数的基本概率分配生成方法,其特征在于,所述方法包括步骤：S1、获取已知分类情况的原始训练样本形成训练集,每一类训练样本包括至少一个属性特征；S2、对所有训练样本的属性特征进行归一化处理；S3、提取各类训练样本中各个属性特征的最大值和最小值构建各个属性特征的区间数模型；S4、将待测对象的各个属性特征进行归一化处理；S5、计算待测对象归一化后的各个属性特征和区间数模型中各个区间数的相似度；S6、根据训练集中不同属性特征数据的不确定程度确定每个属性特征权重；S7、利用所计算的相似度和属性特征权重确定待测对象的基本概率分配BPA；S8、利用组合规则将待测对象的基本概率分配BPA进行融合得到待测对象最终的基本概率分配BPA。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              钟沈君;              谢舜道;                   陈荣军       </td>   <td>中山大学</td>   <td>一种基于增强相关系数的双阶二维码防伪认证方法</td>   <td>广东省</td>   <td>CN110570210A</td>   <td>2019-12-13</td>   <td>本发明提供的一种基于增强相关系数的双阶二维码防伪认证方法,包括以下步骤：选取双阶二维码扫描版本进行输入；对输入的双阶二维码进行纹理分类；根据分类结果,将纹理相同的图案匹配成纹理图案对；利用纹理图案对计算增强相关系数；将增强相关系数与认证阈值对比,完成双阶二维码的防伪认证。本发明提供的双阶二维码防伪认证方法,对输入的二阶二维码中的纹理图案进行分类并形成两种不同的纹理图案对,再根据增强相关系数计算双阶二维码的增强相关系数的均值,最后由均值判断双阶二维码的真伪,该方法对比传统的相关系数具有更大的防伪认证差值,认证鲁棒性强,同时增强相关系数的提高有效地增强了真伪分类的可靠性。</td>   <td>1.一种基于增强相关系数的双阶二维码防伪认证方法,其特征在于：包括以下步骤：S101：选取双阶二维码扫描版本进行输入；S102：对输入的双阶二维码进行纹理分类；S103：根据分类结果,将纹理相同的图案匹配成纹理图案对；S104：利用纹理图案对计算增强相关系数；S105：将增强相关系数与认证阈值对比,完成双阶二维码的防伪认证。</td>   <td>G06Q30/00;G06K7/14;G06T7/41</td>  </tr>        <tr>   <td>中国专利</td>   <td>         卢伟;                   吕文静       </td>   <td>中山大学</td>   <td>一种无参考对比度失真图像质量评价方法</td>   <td>广东省</td>   <td>CN110570420A</td>   <td>2019-12-13</td>   <td>本发明提供的一种无参考对比度失真图像质量评价方法,包括：对对比度失真图像提取多种颜色空间内的颜色矩和信息熵特征,构建描述图像失真的特征集；根据图像失真的特征集与先验分数结合构建训练集,构建图像质量评价的预测模型；提取待评价图像的对比度失真特征集,利用图像质量评价预测模型进行计算,预测待评价图像的图像质量。本发明提供的一种无参考对比度失真图像质量评价方法,既融合多颜色空间又使用颜色矩和信息熵特征联合,很好的保证检测的准确性和有效性,填补了无参考对比度失真图像质量评价领域的空缺。</td>   <td>1.一种无参考对比度失真图像质量评价方法,其特征在于,包括以下步骤：S1：对对比度失真图像提取多种颜色空间内的颜色矩和信息熵特征,构建描述图像失真的特征集；S2：根据图像失真的特征集与先验分数结合构建训练集,构建图像质量评价的预测模型；S3：提取待评价图像的对比度失真特征集,利用图像质量评价预测模型进行计算,预测待评价图像的图像质量。</td>   <td>G06T7/00;G06T7/90;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;              郭叙森;              古剑锋;              郭思璐;              杨铖章;                   许子潇       </td>   <td>中山大学</td>   <td>一种基于流数据的三维物体检测与跟踪方法</td>   <td>广东省</td>   <td>CN110570457A</td>   <td>2019-12-13</td>   <td>本发明涉及三维目标检测和追踪领域,更具体地,涉及一种基于流数据的三维物体检测与跟踪方法,通过输入包括点云数据和图像数据的前后两个关键帧,对数据进行特征提取得到特征图,并将两帧特征图做相关操作得到相关特征图。之后,将特征图输入候选框提取网络获得候选框；通过候选框在特征图和相关特征图获取特征块并输入回归网络,分别得到检测物体的三维框和三维框偏移量。通过插值法求出除关键帧之间的其他帧画面,并对所有帧中的目标进行关联,得到跟踪结果。本发明只需对关键帧进行检测,加快流数据检测的速度,满足自动驾驶环境对实时性的要求同时具有更好的稳定性；同时融合了点云信息和图像信息,优劣互补,提高检测物体的准确性。</td>   <td>1.一种基于流数据的三维物体检测与跟踪方法,其特征在于,包括如下步骤：步骤一：输入前后两帧由点云数据和图像数据组成的关键帧数据,对数据进行预处理,其中的点云数据在俯视图方向上投影结构转化成BEV图；步骤二：将步骤一中的两关键帧数据进行特征提取,得到特征图,并将提取的特征图输入区域特征提取模块,分别得到两个关键帧的候选框集合；步骤三：所述候选框在特征图中截取特征块和调整尺寸,然后输入分类网络与框回归网络,得到物体的类别和三维框位置；步骤四：对步骤二中提取的两个关键帧的数据做相关操作得到相关特征图,所述候选框在相关特征图中截取特征块和调整尺寸,然后输入回归网络得到两个关键帧对应物体的三维框的偏移量；步骤五：根据物体的三维框和三维框对应的偏移量,运用插值法得到两个关键帧数据之间的其他帧数据的物体的三维框,从而得到所有帧中的物体的三维检测结果；步骤六：根据检测结果,对所有帧数据对应的物体相互关联,得到跟踪结果。</td>   <td>G06T7/246;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         齐志新;              陈逸敏;                   张慧       </td>   <td>中山大学</td>   <td>基于极化雷达遥感影像的洪水淹没范围自动提取方法</td>   <td>广东省</td>   <td>CN110570462A</td>   <td>2019-12-13</td>   <td>本发明公开了基于极化雷达遥感影像的洪水淹没范围自动提取方法,包括以下步骤,S1：利用多尺度分割技术同时对灾前和灾后的极化雷达遥感影像进行分割,提取图像对象；S2：分别提取灾前和灾后极化雷达遥感影像中图像对象的像素平均值、几何和纹理属性；S3：使用变化矢量分析技术从灾前和灾后的极化雷达遥感影像中提取基于图像对象的特征向量,计算图像对象的变化矢量强度,确定变化地块；S4：基于步骤S2中提取的多种属性集合,利用基于决策树分类规则的层次非监督分类算法自动确定灾前和灾后的土地覆盖类别；S5：基于灾前和灾后土地覆盖分类结果以及变化矢量分析结果,通过分类后比较法确定变化地块的土地覆盖变化类别,得到洪水淹没区域。</td>   <td>1.基于极化雷达遥感影像的洪水淹没范围自动提取方法,其特征在于,包括以下步骤：步骤S1：利用多尺度分割技术同时对灾前和灾后的极化雷达遥感影像进行分割,提取图像对象；步骤S2：分别提取灾前和灾后极化雷达遥感影像中图像对象的像素平均值、几何和纹理属性；步骤S3：使用变化矢量分析技术从灾前和灾后的极化雷达遥感影像中提取基于图像对象的特征向量,计算图像对象的变化矢量强度,确定变化地块；步骤S4：基于步骤S2中提取的多种属性集合,利用基于决策树分类规则的层次非监督分类算法自动确定灾前和灾后的土地覆盖类别；步骤S5：基于灾前和灾后土地覆盖分类结果以及变化矢量分析结果,通过分类后比较法确定变化地块的土地覆盖变化类别,得到洪水淹没区域。</td>   <td>G06T7/246;G06T7/215;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              张俊轩;              王腾;              杨梁;                   王伟轩       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>基于知识迁移的多模态循环神经网络的图像文本描述方法</td>   <td>广东省</td>   <td>CN106650756B</td>   <td>2019-12-10</td>   <td>本发明提供一种基于知识迁移的多模态循环神经网络的图像文本描述方法,该方法通过多模态单元中的知识转移模型,很好地利用了现成图像分类器对大多数对象的识别能力以及现成语料库中的语法结构和语义关联性,能更准确地描述出图像中的目标对象以及使生成的句子描述语法结构更丰富,语义贴切,可读性更强。</td>   <td>1.一种基于知识迁移的多模态循环神经网络的图像文本描述方法,其特征在于,包括以下步骤：S1：在服务器中训练图像语义分类器；S2：在服务器中训练语言模型；S3：在服务器中预训练文本描述生成模型并生成描述句子；所述步骤S1的具体过程如下：S11：采集多种图像数据集：下载现成的数据集,包括ImageNet和MSCOCO,由于MSCOCO是一种图像与文本描述成对匹配的数据集,取其图像部分；S12：使用卷积神经网络,对采集的数据集中的每一张图片提取相应的图像特征f<Sub>I</Sub>；S13：制作一个标签集,选取1000个最常见的单词即覆盖了90％图像与文本描述成对匹配的训练集中使用到的单词,以及加上ImageNet图像分类中没有出现在成对匹配训练集中的对象的词,将两者组成需要用到的标签词库；S14：利用上个步骤制作好的标签词库,对每张图片采用多示例学习的方法为其添加上多个视觉概念标签：多示例学习中将各种多示例的集合定义为“包”,正包指的是包中至少有一个正示例,否则定义为负包,在这里把每张图片定义成一个包；对于MSCOCO数据集中每一张图片,根据其数据集中五个参考文本描述去给每个图像设定相对应的标签,如果一个图像中对应的五个参考文本描述中的任意一个提到了一个标签,则认为对应的图片是一个正包,否则认为该图片是负包；对于ImageNet数据集中的每一张图片,以其本来的标签作为单独的标签。</td>   <td>G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘洁;              施楚民;              冯俊杰;              黄璐;              陈炼翰;              李凯欣;                   余思远       </td>   <td>中山大学</td>   <td>一种环芯光纤优化设计的方法</td>   <td>广东省</td>   <td>CN110543746A</td>   <td>2019-12-06</td>   <td>本发涉及一种环芯光纤优化设计的方法,所述方法通过BP神经网络建立光纤设计参数与模式间耦合积分系数的非线性关系,同时在符合拉制工艺的结构限制条件下,利用遗传算法快速找到模间耦合系数最小的最优结构,从而实现低损耗、低模式组间耦合的环芯光纤的快速设计。所述方法创新性利用了环芯光纤的耦合积分系数作为优化目标进行神经网络的训练,打破了现有多模光纤模间实现低耦合理论上主要依靠增加模间有效折射率差的常规。同时将神经网络的输出值作为遗传算法的适应度函数值,相对于传统的电磁场仿真计算得到适应度函数值,具有更为快速的环芯光纤设计优化流程。</td>   <td>1.一种环芯光纤优化设计的方法,其特征在于,所述方法包括以下步骤：S1：根据光纤设计通用模型构建神经网络的训练样本,并对训练样本进行预处理；S2：根据S1预处理后的训练样本,构建并训练BP神经网络模型；S3：利用遗传算法寻找限定条件下的最优值：以各输入设计参量为个体产生种群,基于训练完成的BP神经网络,构建种群适应度函数,在限定条件下,寻找适应度值最高的个体来寻找各设计参量的最优值；S4、二次验证结果：根据遗传算法的优化结果获取若干组效果最优的参数组合,通过常规电磁场计算方法对参数进行二次验证,判断优化结果是否正确,若正确,则将结果返回给设计者,若不正确,则返回S2。</td>   <td>G06F17/50;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         瞿毅力;              苏琬棋;              邓楚富;              王莹;              卢宇彤;                   陈志广       </td>   <td>中山大学</td>   <td>基于条件生成对抗网络的多模态MRI转换方法、系统及介质</td>   <td>广东省</td>   <td>CN110544239A</td>   <td>2019-12-06</td>   <td>本发明公开了一种基于条件生成对抗网络的多模态MRI转换方法、系统及介质,本发明方法包括输入原始MRI图像,将原始MRI图像输入条件生成对抗网络的编码器得到语义特征图,并通过条件生成对抗网络的鉴别器识别原始MRI图像的模态类别；针对每一种原始MRI图像的模态类别以外的其他模态：生成该模态的条件向量,将语义特征图与该模态的条件向量连接,将连接后的结果输入条件生成对抗网络的解码器得到该模态的MRI转换图,从而最终得到所有其他模态的MRI转换图。本发明是无监督的,无需配准的多模态影像即可训练,同时能保证转换生成的多模态MRI是配准的,最后还能保证转换生成的MRI完好的保留了关键的病灶信息,还可以进一步根据需要加以进行检验。</td>   <td>1.一种基于条件生成对抗网络的多模态MRI转换方法,其特征在于实施步骤包括：1)输入原始MRI图像,将原始MRI图像输入条件生成对抗网络的编码器得到语义特征图,并通过条件生成对抗网络的鉴别器识别原始MRI图像的模态类别；2)针对每一种原始MRI图像的模态类别以外的其他模态：生成该模态的条件向量,将语义特征图与该模态的条件向量连接,将连接后的结果输入条件生成对抗网络的解码器得到该模态的MRI转换图,从而最终得到所有其他模态的MRI转换图。</td>   <td>G06T7/00;G06T7/33</td>  </tr>        <tr>   <td>中国专利</td>   <td>         瞿毅力;              王莹;              苏琬棋;              邓楚富;              卢宇彤;                   陈志广       </td>   <td>中山大学</td>   <td>生成配准的带病灶分割标签的多模态MRI的方法、系统及介质</td>   <td>广东省</td>   <td>CN110544275A</td>   <td>2019-12-06</td>   <td>本发明公开了一种生成配准的带病灶分割标签的多模态MRI的方法、系统及介质,本发明的方法包括获取正态分布的随机矩阵并输入生成对抗网络中训练好的结构特征解码器解码生成结构特征图；将结构特征图与随机选取的病灶分割标签图通过随机输入融合得到融合结果、并输入生成对抗网络中训练好的随机编码器得到编码；将编码输入生成对抗网络中训练好的各个模态的解码器,分别生成配准后的多模态MRI。本发明将生成对抗网络中的生成器模块化为编码器和解码器,通过多组编码器、解码器和鉴别器的组合训练,可接收一个符合设计规范的随机输入进而生成一组有病灶分割标签的配准的多模态MRI图像,可广泛应用于医学影像领域。</td>   <td>1.一种生成配准的带病灶分割标签的多模态MRI的方法,其特征在于实施步骤包括：1)从正态分布N(0,1<Sup>2</Sup>)获取随机矩阵Code<Sub>F,RM</Sub>；2)将随机矩阵Code<Sub>F,RM</Sub>输入生成对抗网络中训练好的结构特征解码器Decoder<Sub>F</Sub>解码生成结构特征图F<Sub>RM</Sub>；3)将结构特征图F<Sub>RM</Sub>与随机选取的病灶分割标签图L通过随机输入融合得到融合结果；4)将融合结果输入生成对抗网络中训练好的随机编码器Encoder<Sub>RM</Sub>得到编码Code<Sub>RM</Sub>；5)将编码Code<Sub>RM</Sub>输入生成对抗网络中训练好的各个模态i的i模态解码器Decoder<Sub>i</Sub>,分别生成配准后的i模态MRI i<Sub>g</Sub>。</td>   <td>G06T7/38;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         衣杨;              李芳;              吴斯扬;              涂冠平;              陈新耿;                   赵勇宪       </td>   <td>中山大学</td>   <td>基于改进粒子群优化的总拖期运输计划调度方法</td>   <td>广东省</td>   <td>CN106228265B</td>   <td>2019-12-03</td>   <td>一种基于改进粒子群优化的总拖期运输计划调度算法,步骤1,根据输入变量构建总拖期值数学模型,定义待加工工作数n和机器数m,以及每一待加工工作加工所需时间p<Sub>j</Sub>和交货期限d<Sub>j</Sub>,步骤2,根据总拖期值数学模型解构造图；步骤3,以构造图为依据,根据状态转移规则和信息素更新规则完成搜索,获得一使总拖期T最小的解。本发明在蚁群算法的基础上,针对并行多机总拖期运输计划调度问题(P//T)的特性,提出了基于启发式规则的改进蚁群算法(Heuristic Ant Colony Optimization,hACO),并对算法进行了性能优化的研究,以提高事件最优解的确定精度,以及保证对内部数据的处理筛选效率。</td>   <td>1.一种基于改进粒子群优化的总拖期运输计划调度方法,其特征在于：步骤1,根据输入变量构建总拖期值数学模型,定义待加工工作数n和机器数m,以及每一待加工工作加工所需时间p<Sub>j</Sub>和交货期限d<Sub>j,</Sub>；步骤2,根据总拖期值数学模型解构造图；步骤3,以构造图为依据,根据状态转移规则和信息素更新规则完成搜索,获得一使总拖期T最小的解；步骤2中,在t＝0时刻,按照机器的序号以递增方式优先选择工作加工；当出现两台机器同时完成工作加工,则依然按照机器序号的递增顺序优先选择工作加工；；步骤3中,状态转移规则为          <Image id="icf0001" he="150" wi="700" file="FDA0002227047330000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,q<Sub>0</Sub>(0≤q<Sub>0</Sub>≤1)是一个调节参数,q是随机产生的介于[0,1]之间的均匀分布随机数,η为启发式信息,τ为信息素,当q≤q<Sub>0</Sub>时,选择当前最优节点,当q&gt;q<Sub>0</Sub>选择随机方式探索；启发式信息确定方式为：spt<Sub>j</Sub>＝p<Sub>j</Sub>,将待加工工作按照短的加工时间非递减顺序排列,则η<Sub>ij</Sub>＝1/spt<Sub>j</Sub>；或启发式信息确定方式为,edd<Sub>j</Sub>＝d<Sub>j</Sub>,将待加工工作按照最早交货期限非递减顺序排列,则η<Sub>ij</Sub>＝1/edd<Sub>j</Sub>；或启发式信息确定方式为,mdd<Sub>j</Sub>＝max{C+p<Sub>j</Sub>,d<Sub>j</Sub>},C表示为目前已加工的工作加工总时间,将工作以非递减顺序排列工作,则η<Sub>ij</Sub>＝1/mdd<Sub>j</Sub>；spt为最短加工时间；edd为最早交货期限；mdd为修改的交货期限；α、β为决定信息素和启发式信息对选择概率的比重。</td>   <td>G06Q10/04;G06N3/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘凯;              刘洋;              柳林;                   李想       </td>   <td>中山大学</td>   <td>基于Gram-Schmidt的无人机影像与多光谱影像融合方法</td>   <td>广东省</td>   <td>CN106384332B</td>   <td>2019-12-03</td>   <td>本发明公开一种基于Gram-Schmidt变换的无人机影像与多光谱影像融合方法。其首先经过影像预处理获得具有相同像元尺寸的多光谱低空间分辨率遥感影像与三波段高空间分辨率无人机可见光影像这两套独立的多波段影像；之后对遥感影像进行多元线性回归、重构和Gram-Schmidt变换获得遥感影像GS成分,同时对无人机影像进行相同的重构和Gram-Schmidt变换得到无人机GS成分；之后对无人机GS成分进行梯度滤波得到纹理信息,并以一定权重叠加至遥感影像第1-4GS成分上；对增强结果进行Gram-Schmidt逆变换并去除冗余信息,即得到最终融合影像。本方法扩展了传统融合方法的单波段全色数据与多光谱影像融合的局限性,增加了融合数据的多样性,并实现了一种兼顾光谱保持性和信息质量的融合方法。</td>   <td>1.一种基于Gram-Schmidt变换的无人机影像与多光谱影像融合方法,其特征在于,包括：S1、影像预处理,包括影像配准、重采样至相同像元尺寸、相同空间范围裁剪,得到低空间分辨率多光谱遥感影像和相同范围的高空间分辨率三波段无人机光学影像；S2、对低空间分辨率多光谱遥感影像进行多元线性回归,获得模拟无人机影像；具体步骤包括：S21、在研究区范围内随机采样,样本数目为研究区范围内像元数目的0.8％-2％,样本内容为采样点处遥感影像N个波段的数据值和无人机影像RGB波段的数据值；S22、基于多元线性回归,分别得到相同范围的高空间分辨率三波段无人机光学影像RGB三个波段样本各自以遥感影像N波段样本为变量的回归系数；S23、将得到的回归系数应用于遥感影像N个波段分别得到模拟的无人机RGB波段；S3、对低空间分辨率多光谱遥感影像进行重构,并进行Gram-Schmidt变换得到遥感影像GS成分；具体步骤包括：S31、根据多光谱影像RGB波段范围内像元的均值与标准差对模拟无人机影像进行强度匹配,具体公式如下：          <Image id="icf0001" he="131" wi="589" file="FDA0002088898630000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中：<Image id="icf0002" he="73" wi="63" file="FDA0002088898630000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0003" he="71" wi="52" file="FDA0002088898630000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>分别为增强后的第i个波段和原始模拟无人机影像的第i个波段,μ<Sub>Mi</Sub>和<Image id="icf0004" he="69" wi="68" file="FDA0002088898630000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>分别为遥感影像和模拟无人机影像对应波段均值,σ<Sub>Mi</Sub>和<Image id="icf0005" he="67" wi="64" file="FDA0002088898630000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>分别为遥感影像和模拟无人机影像对应波段标准差,i＝R,G或B；S32、对强度匹配后的模拟无人机影像求均值波段,即在同一像元位置求取RGB波段像元的均值；S33、按模拟无人机影像均值波段,强度匹配后模拟无人机影像和原始遥感影像的顺序重构获得N+4波段的变换基础影像；S34、对变换基础影像进行Gram-Schmidt变换,得到N+4个遥感影像GS成分；S4、对相同范围的高空间分辨率三波段无人机影像进行重构,并进行Gram-Schmidt变换得到无人机影像GS成分；具体步骤包括：S41、根据多光谱影像RGB波段范围内像元的均值与标准差对无人机影像进行强度匹配,具体公式如下：          <Image id="icf0006" he="127" wi="587" file="FDA0002088898630000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中：χ<Sub>ei</Sub>和χ<Sub>i</Sub>分别为增强后的第i个波段和原始无人机影像的第i个波段,μ<Sub>Mi</Sub>和μ<Sub>Ui</Sub>分别为遥感影像和无人机影像对应波段均值,σ<Sub>Mi</Sub>和σ<Sub>Ui</Sub>分别为遥感影像和无人机影像对应波段标准差,i＝R,G或B；S42、对强度匹配后的无人机影像求均值波段；S43、按无人机影像均值波段,强度匹配后无人机影像的顺序重构获得4波段的变换基础影像；S44、对变换基础影像进行Gram-Schmidt变换,得到4个无人机影像GS成分；S5、对无人机影像GS成分进行梯度滤波得到纹理信息,并将其以权重w叠加至遥感影像的第1至4个GS成分上得到增强结果；S6、对增强结果进行Gram-Schmidt逆变换并去掉前4个冗余信息波段,即得到融合结果。</td>   <td>G06T3/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              林梓健;                   马东宏       </td>   <td>中山大学</td>   <td>一种单摄像头的行人跟踪方法</td>   <td>广东省</td>   <td>CN106682573B</td>   <td>2019-12-03</td>   <td>本发明公开一种单摄像头的行人跟踪方法。本发明采用分层次跟踪的策略,先把检测目标连接成稳定可靠的小段运动轨迹,然后通过匹配小段运动轨迹并填充小段运动轨迹间的空白段,形成最终的跟踪轨迹。该策略有利于解决跟踪过程中由于行人被短时间遮挡而引起的轨迹中断问题。在小段运动轨迹的构造中,本发明提出了基于级联思想的两步匹配方法,先采用高效的直方图匹配方法处理匹配难度小的情况,再进一步采用鲁棒的稀疏表示分类器解决目标表观相似引起的混淆问题。在小段运动轨迹之间的匹配中,本发明提出了基于稀疏表示的轨迹距离度量方法,对由环境变化和目标形变引起的目标表观变化情况有较好的鲁棒性。另外,本发明还提出计算量小的基于层次聚类的轨迹匹配方法,提高全局多轨迹匹配的效率。</td>   <td>1.一种单摄像头的行人跟踪方法,其特征在于：包括以下步骤：S1.对视频图像进行检测,将检测出来的行人称为观察目标；S2.设在当前帧t检测到有κ个观察目标,提取这κ个观察目标的特征；S3.设在当前帧t,同时存在若干条在当前帧t之前的运动轨迹,T＝{d<Sup>s</Sup>,d<Sup>s+1</Sup>,...,d<Sup>e</Sup>}表示一条运动轨迹,其中s、e分别为轨迹T的起始时刻和终止时刻；对于运动轨迹<Image id="icf0001" he="74" wi="468" file="FDA0002173843930000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>通过位置约束条件选出在当前帧t可能关联的观察目标,然后计算运动轨迹T<Sub>i</Sub>与可能关联的观察目标的相似度,将与运动轨迹T<Sub>i</Sub>相似度最高的观察目标作为该运动轨迹的最佳观察目标；S4.经过步骤S3的匹配后,每个观察目标遇到以下三种情况之一：A.观察目标不是任何运动轨迹的的最佳观察目标,因而未能成功配对；B.观察目标仅是一条运动轨迹的最佳观察目标,则将该观察目标与该运动轨迹配对；C.观察目标是多条运动轨迹的最佳观察目标,则利用稀疏表示分类器对观察目标进行分类,利用分类器确定该观察目标所属的运动轨迹,将观察目标与该运动轨迹配对；S5.执行步骤S4后,对运动轨迹进行更新：D.对于未成功配对到运动轨迹的观察目标,新建立一条以该观察目标为起点的运动轨迹；E.对于成功配对到运动轨迹的观察目标,将该观察目标连接到运动轨迹的末端；F.对于未成功配对到观察目标的运动轨迹,则利用该运动轨迹在当前帧t之前m帧的位置线性预测出其在当前帧t的位置,并将其称之为预测目标,若预测目标和运动轨迹的相似度大于阈值,则将预测目标连接到运动轨迹的末端；S6.重复执行步骤S3-S5,直到遍历完图像序列,低层次数据关联过程结束；S7.通过时空约束条件选出有可能关联的运动轨迹对(T<Sub>a</Sub>,T<Sub>b</Sub>)；S8.采集足够数量的行人图像；若图像中行人腿部是呈靠拢状态的,则标记为正样本,否则标记为负样本；对图像中行人腿部计算梯度方向直方图,并且以此作为特征,离线训练一个能够区分图像中行人腿部是否呈靠拢状态的支持向量机；S9.使用支持向量机判别T<Sub>a</Sub>、T<Sub>b</Sub>的每张图像中行人腿部是否呈靠拢状态,并选出判别结果为正的图像作为T<Sub>a</Sub>、T<Sub>b</Sub>的代表性图像子集(A<Sub>a</Sub>,A<Sub>b</Sub>)；S10.对运动轨迹对(T<Sub>a</Sub>,T<Sub>b</Sub>)的代表性图像子集(A<Sub>a</Sub>,A<Sub>b</Sub>)进行图像特征提取,然后基于提取的特征使用双向重构的策略来计算运动轨迹对(T<Sub>a</Sub>,T<Sub>b</Sub>)的距离,若运动轨迹对(T<Sub>a</Sub>,T<Sub>b</Sub>)的距离大于指定阈值,则舍弃该运动轨迹对；S11.基于层次聚类的轨迹匹配：将运动轨迹视为点,将S10得到的运动轨迹对视为边,边的权值取运动轨迹对的距离,由此构成一个无向图G＝(V,E),其中V＝{T<Sub>1</Sub>,T<Sub>2</Sub>,...,T<Sub>n</Sub>},E＝{(T<Sub>a</Sub>,T<Sub>b</Sub>,D(T<Sub>a</Sub>,T<Sub>b</Sub>))|D(T<Sub>a</Sub>,T<Sub>b</Sub>)＜θ<Sub>D</Sub>且1≤a,b≤n,n为运动轨迹数；聚类开始时,设类别数为n,每个类只包含一个点；每个点具有一个时间跨度Z<Sub>i</Sub>＝{s<Sub>i</Sub>,s<Sub>i</Sub>+1,...,e<Sub>i</Sub>}；每个类的时间跨度为该类所有点的时间跨度的并集；然后进行如下聚类：G.从边集中选出权值最小的边l；H.若边l的两端点所属类的时间跨度没有交集,则把这两个类聚合成一个类；I.从边集中删除边l,若边集为空集,聚类结束,否则返回G；J.聚类结束后,每一类表示一个行人目标；S12.轨迹修复：对于S11得到的每一个类,把所有观察目标的垂直坐标x、水平坐标y、检测窗口高度h分开处理,使用样条线分别拟合x-t曲线、y-t曲线、h-t曲线,以填补缺失的轨迹点并对轨迹进行平滑。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              岑杰鹏;                   王敬       </td>   <td>中山大学</td>   <td>一种基于动态时间规整与多核学习的动作识别方法</td>   <td>广东省</td>   <td>CN106845386B</td>   <td>2019-12-03</td>   <td>本发明针对视频的人体动作识别问题,提供了一种基于动态时间规整与多核学习的动作识别方法,该方法能充分利用动作序列的全局时间结构信息和局部特征的频率分布信息,主要的改进点在于：1)基于动态时间规整方法创建了动作平均模板,这一模板包含了BoW模型表示中忽略掉的动作序列的时间信息；2)通过增广特征多核学习的方法对动作平均模板表示和BoW表示进行结合,并通过引入学习权重调整两者的贡献度；通过以上两点改进,提高动作识别的准确率。</td>   <td>1.一种基于动态时间规整与多核学习的动作识别方法,其特征在于：包括以下步骤：一、建立BoW表示S11.记动作类别总数为C,令第j类动作的训练动作样本集为<Image id="icf0001" he="84" wi="687" file="FDA0002173039520000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中T<Sub>i</Sub><Sup>j</Sup>表示第j类动作的第i个训练动作样本,i＝1,2,..,N<Sub>j</Sub>,N<Sub>j</Sub>表示第j类动作的训练动作样本数；定义包含C类训练动作样本的集合<Image id="icf0002" he="118" wi="507" file="FDA0002173039520000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中<Image id="icf0003" he="147" wi="224" file="FDA0002173039520000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为训练动作样本总数；S12.对训练动作样本T<Sub>i</Sub>的每帧图像提取底层描述子,基于提取的底层描述子建立起训练动作样本T<Sub>i</Sub>的自相似矩阵SSM,然后基于自相似矩阵对每帧图像进行Z个不同时间尺度的SSM描述子提取；训练动作样本T<Sub>i</Sub>各帧图像提取的SSM描述子按照各帧顺序形成描述子序列<Image id="icf0004" he="84" wi="418" file="FDA0002173039520000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中Q<Sub>i</Sub>表示训练动作样本T<Sub>i</Sub>的帧数目,<Image id="icf0005" he="72" wi="59" file="FDA0002173039520000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示第k帧的Z个SSM描述子；S13.对各个训练动作样本进行步骤S12的操作；S14.从所有训练动作样本的Z个时间尺度下的描述子中随机选取e个SSM描述子,然后利用k-means算法将其聚类成p个簇,p&lt;&lt;e,得到包含有p个词汇的码本；S15.计算训练动作样本T<Sub>i</Sub>中各个SSM描述子与码本各个词汇之间的距离,然后将训练动作样本T<Sub>i</Sub>中的各个SSM描述子分别与距离最接近的词汇关联起来,即利用码本对SSM描述子进行量化,码本各个词汇关联的SSM描述子的数量形成一个直方图表示,即为训练动作样本T<Sub>i</Sub>的BoW表示；S16.对各个训练动作样本进行步骤S15的操作获取各个训练动作样本的BoW表示；二、建立动作平均模板表示S21.初始化j的值为1；S22.为第j类动作构建一个初始的空的平均模板<Image id="icf0006" he="71" wi="87" file="FDA0002173039520000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>初始化i的值为1；S23.若i＝1,令<Image id="icf0007" he="70" wi="188" file="FDA0002173039520000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中<Image id="icf0008" he="70" wi="57" file="FDA0002173039520000018.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为训练动作样本T<Sub>i</Sub><Sup>j</Sup>的SSM描述子序列,跳到步骤S26；否则,利用动态时间规整方法计算平均模板<Image id="icf0009" he="71" wi="78" file="FDA0002173039520000019.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>与描述子序列<Image id="icf0010" he="71" wi="61" file="FDA00021730395200000110.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>的累加距离：          <Image id="icf0011" he="73" wi="700" file="FDA0002173039520000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中c<Sub>k</Sub>＝(i<Sub>k</Sub>,j<Sub>k</Sub>)表示第k对帧,表示平均模板<Image id="icf0012" he="72" wi="81" file="FDA0002173039520000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>中的第i<Sub>k</Sub>帧与描述子序列<Image id="icf0013" he="73" wi="64" file="FDA0002173039520000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>中的第j<Sub>k</Sub>帧对齐,d(c<Sub>k</Sub>)表示第k对帧的SSM描述子的欧式距离,ω(c<Sub>k</Sub>)表示加权系数且ω(c<Sub>k</Sub>)＝i<Sub>k</Sub>-i<Sub>k-1</Sub>+j<Sub>k</Sub>-j<Sub>k-1</Sub>；S24.基于公式(1),由最后一对对齐帧回溯至最早一对对齐帧,获得最优路径p＝{c′<Sub>l</Sub>},其中c′<Sub>l</Sub>＝(i′<Sub>l</Sub>,j′<Sub>l</Sub>),表示平均模板<Image id="icf0014" he="73" wi="83" file="FDA0002173039520000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>中的第i′<Sub>l</Sub>帧与描述子序列<Image id="icf0015" he="71" wi="63" file="FDA0002173039520000025.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>中的第j′<Sub>l</Sub>帧对齐,对应的描述子映射集为<Image id="icf0016" he="86" wi="431" file="FDA0002173039520000026.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>S25.利用平均模板<Image id="icf0017" he="72" wi="109" file="FDA0002173039520000027.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>描述子序列<Image id="icf0018" he="72" wi="57" file="FDA0002173039520000028.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>计算新的平均模板<Image id="icf0019" he="71" wi="89" file="FDA0002173039520000029.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>          <Image id="icf0020" he="62" wi="700" file="FDA00021730395200000210.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,<Image id="icf0021" he="71" wi="119" file="FDA00021730395200000211.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示新的平均模板的第l帧的描述子,<Image id="icf0022" he="78" wi="142" file="FDA00021730395200000212.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示平均模板<Image id="icf0023" he="72" wi="78" file="FDA00021730395200000213.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>第i′<Sub>l</Sub>帧的描述子,<Image id="icf0024" he="78" wi="136" file="FDA00021730395200000214.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示描述子序列<Image id="icf0025" he="72" wi="58" file="FDA00021730395200000215.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>第j′<Sub>l</Sub>帧的描述子,L表示最优路径上对齐帧的数目,β＝1/i；S26.令i＝i+1然后执行步骤S23～S25,直至i＞N<Sub>j</Sub>,得到第j类动作的最终的平均模板R<Sub>j</Sub>；S27.令j＝j+1然后执行步骤S22～S26,直至j＞C；S28.通过步骤S21～S27的计算,获得C个平均模板组成的平均模板集合R＝{R<Sub>1</Sub>,R<Sub>2</Sub>...,R<Sub>C</Sub>},其中R<Sub>j</Sub>表示第j类动作的最终的平均模板；S29.对平均模板和训练动作样本进行量化：S291.从所有训练动作样本的描述子中随机选取e′个SSM描述子,然后利用k-means算法将其聚类成p′个簇,p′＜＜e′,得到包含有p′个词汇的码本；S292.分别计算训练动作样本T<Sub>i</Sub>的描述子序列中每帧的SSM描述子与步骤S291中获得的码本的各个词汇之间的距离,将每帧的SSM描述子分别与距离最接近的词汇关联起来,得到训练动作样本T<Sub>i</Sub>量化的描述子序列；S293.对各个训练动作样本进行步骤S292的操作；通过步骤S292中同样的方式对各个平均模板进行量化,可得到各个平均模板量化的描述子序列；S210.对训练动作样本T<Sub>i</Sub>的量化描述子序列利用动态时间规整方法计算其与各个平均模板的量化描述子序列的平均距离,训练动作样本T<Sub>i</Sub>的量化描述子序列到各个平均模板的量化描述子序列的平均距离构成一个C维向量,该C维向量为训练动作样本T<Sub>i</Sub>的平均模板表示；对动作样本集合T中各训练动作样本进行同样操作获取各训练动作样本的平均模板表示；S211.为Z个不同时间尺度分别建立平均模板表示,具体地,针对每一个时间尺度,在步骤S21～S210中利用该时间尺度的描述子进行该时间尺度下的动作平均模板的构建、码本的构建以及平均模板表示的构建；将某个训练动作样本在Z个时间尺度下分别获得的平均模板表示拼接成一个向量,作为该训练动作样本最终的平均模板表示；三、结合BoW表示和平均模板表示的动作表示S31.利用增广特征多核学习(AFMKL)结合BoW表示和平均模板表示,增广特征多核学习的决策函数如下：          <Image id="icf0026" he="83" wi="700" file="FDA0002173039520000031.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中x表示BoW表示,x′表示平均模板表示,ω和β表示学习权重,<Image id="icf0027" he="51" wi="46" file="FDA0002173039520000037.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示对BoW表示的非线性映射函数,φ表示对平均模板表示的非线性映射函数,b为偏置项,d<Sub>1</Sub>和d<Sub>2</Sub>为对BoW表示、平均模板表示进行加权的系数；S32.通过最小化结构风险函数,建立以下的最优化问题：          <Image id="icf0028" he="100" wi="616" file="FDA0002173039520000032.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0029" he="134" wi="280" file="FDA0002173039520000033.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        d<Sub>m</Sub>≥0,m＝1,2.其中<Image id="icf0030" he="102" wi="700" file="FDA0002173039520000034.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>          <Image id="icf0031" he="68" wi="700" file="FDA0002173039520000035.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        s.t.表示服从后面的约束,d＝[d<Sub>1</Sub>,d<Sub>2</Sub>]<Sup>T</Sup>表示加权系数向量,<Image id="icf0032" he="120" wi="259" file="FDA0002173039520000036.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示二次正则化项,x<Sub>i</Sub>表示第i个训练动作样本的BoW表示,x′<Sub>i</Sub>表示第i个训练动作样本的平均模板表示,y<Sub>i</Sub>∈{+1,-1}表示第i个训练动作样本的正负标签,ξ＝(ξ<Sub>1</Sub>,ξ<Sub>2</Sub>,...,ξ<Sub>N</Sub>)<Sup>T</Sup>表示松弛变量向量,ξ<Sub>i</Sub>表示第i个训练动作样本的松弛变量,λ表示惩罚参数,N为训练动作样本的数目；S33.为式(3)中每个不等式约束引入拉格朗日乘子α<Sub>i</Sub>,并记α＝(α<Sub>1</Sub>,α<Sub>2</Sub>,...,α<Sub>N</Sub>)<Sup>T</Sup>为对偶变量,将式(3)中的优化问题转换为其对偶形式：          <Image id="icf0033" he="84" wi="700" file="FDA0002173039520000041.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0034" he="133" wi="292" file="FDA0002173039520000042.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        0≤α<Sub>i</Sub>≤λ,i＝1,...,N.其中,α<Sub>i</Sub>和α<Sub>j</Sub>分别表示对第i个训练动作样本、第j个训练动作样本构成的不等式约束所引入的拉格朗日乘子；<Image id="icf0035" he="57" wi="700" file="FDA0002173039520000043.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为核函数；S34.对公式(2)在训练动作样本集上进行优化求解：S341.固定加权系数向量d,(4)中的对偶问题转换成关于对偶变量α的优化问题,此时利用标准的SVM的求解方法对对偶变量α进行求解；S342.固定对偶变量α,利用梯度下降的方法对加权系数向量d进行求解；S343.迭代地进行S341和S342,直至式(2)收敛或达到最大迭代数；S35.利用步骤S34确定加权系数向量d和对偶变量α后,得到最终的决策函数：          <Image id="icf0036" he="91" wi="700" file="FDA0002173039520000044.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        四、对测试动作样本进行动作识别S41.利用第一部分的内容求取测试动作样本的BoW表示；S42.利用第二部分的内容求取测试动作样本的平均模板表示；S43.将测试动作样本的BoW表示、平均模板表示输入至最终的决策函数中,决策函数输出分类结果。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              林大钧;              许丹丹;              林梓健;                   柯博       </td>   <td>中山大学</td>   <td>一种结合时空拓扑估计的跨摄像机目标匹配与跟踪方法</td>   <td>广东省</td>   <td>CN106846378B</td>   <td>2019-12-03</td>   <td>本发明提供的方法主要有两个发明点,一是研究基于特征匹配的目标匹配跟踪算法,采用多种具有互补性质的表观特征建立匹配模型,并将多种特征的匹配结果进行决策级的融合；二是提出一种无监督的拓扑估计算法,使系统能够在匹配与跟踪的过程中自动建立监控网络的时空拓扑关系,同时利用时空拓扑约束极大地提高了匹配与跟踪的准确度。本发明对跨摄像机目标匹配中由于遮挡、环境、光照等变化带来的干扰具有较强的鲁棒性,有利于实现多摄像机视频监控系统对目标的鲁棒协同跟踪。</td>   <td>1.一种结合时空拓扑估计的跨摄像机目标匹配与跟踪方法,其特征在于：包括以下步骤：S1.将两两摄像机之间的转移概率初始化为<Image id="icf0001" he="124" wi="93" file="FDA0002107492270000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>M为监控网络中摄像机的数量；时间窗口设置为TW＝τ,τ为预设值,两两摄像机间的转移计数器初始化为0；S2.设当前摄像机为C<Sub>i</Sub>,摄像机C<Sub>i</Sub>内的行人目标表示为O<Sub>i,a</Sub>；在设定的时间窗口内搜索其他摄像机中出现的行人目标；S3.设在摄像机C<Sub>j</Sub>中搜索得到的行人目标为O<Sub>j,b</Sub>,i≠j,1≤j≤M,计算行人目标O<Sub>i,a</Sub>、O<Sub>j,b</Sub>之间的匹配概率：          <Image id="icf0002" he="121" wi="700" file="FDA0002107492270000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0003" he="77" wi="79" file="FDA0002107492270000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为表示O<Sub>i,a</Sub>、O<Sub>j,b</Sub>之间转移关系的变量,当O<Sub>i,a</Sub>、O<Sub>j,b</Sub>之间存在转移关系时,<Image id="icf0004" he="77" wi="79" file="FDA0002107492270000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>取1,否则取0；<Image id="icf0005" he="89" wi="386" file="FDA0002107492270000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示目标O<Sub>i,a</Sub>、O<Sub>j,b</Sub>存在转移关系的后验概率,对应了目标O<Sub>i,a</Sub>、O<Sub>j,b</Sub>的匹配概率；<Image id="icf0006" he="96" wi="389" file="FDA0002107492270000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示在转移关系<Image id="icf0007" he="83" wi="80" file="FDA0002107492270000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>的条件下目标O<Sub>i,a</Sub>、O<Sub>j,b</Sub>的表观特征分布似然函数,定义为与目标表观相似度函数L(O<Sub>i,a,</Sub>O<Sub>j,b</Sub>)成正比,<Image id="icf0008" he="88" wi="192" file="FDA0002107492270000018.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示两个目标存在转移关系的先验概率,通过计算摄像机C<Sub>i</Sub>到摄像机C<Sub>j</Sub>的目标转移概率得到；P<Sub>i,j</Sub>(O<Sub>i,a</Sub>,O<Sub>j,b</Sub>)表示目标O<Sub>i,a</Sub>,O<Sub>j,b</Sub>的联合概率分布,是后验概率的归一化因子；S4.对监控网络中除摄像机C<Sub>i</Sub>外的所有摄像机执行步骤S2、S3,然后对得到的各个摄像机的行人目标与摄像机C<Sub>i</Sub>的行人目标的匹配概率进行排序,将匹配概率最高的前m个行人目标作为候选匹配目标,对应的摄像机作为候选匹配摄像机,m的取值为1或2：          <Image id="icf0009" he="162" wi="608" file="FDA0002107492270000019.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,s<Sub>1</Sub>为最高的匹配概率,s<Sub>2</Sub>为次高的匹配概率,s<Sub>τ</Sub>为设定的阈值；S5.计算摄像机之间的转移次数w<Sub>p</Sub>：          <Image id="icf0010" he="183" wi="694" file="FDA0002107492270000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        s<Sub>p</Sub>表示最高的匹配概率或次高的匹配概率,1≤p≤m；当m＝1时,将摄像机C<Sub>i</Sub>到摄像机C<Sub>g</Sub>之间的转移计数N<Sub>ig</Sub>增加w<Sub>1</Sub>；当m＝2时,将摄像机C<Sub>i</Sub>到摄像机C<Sub>g</Sub>之间的转移计数N<Sub>ig</Sub>增加w<Sub>1</Sub>,然后将摄像机C<Sub>i</Sub>到摄像机C<Sub>k</Sub>之间的转移计数N<Sub>ik</Sub>增加w<Sub>2</Sub>；其中摄像机C<Sub>g</Sub>、摄像机C<Sub>k</Sub>分别为与C<Sub>i</Sub>匹配概率最高、次高的摄像机；S6.将各个摄像机作为当前摄像机然后执行步骤S2～S5；S7.计算摄像机C<Sub>i</Sub>到摄像机C<Sub>j</Sub>之间的转移概率：          <Image id="icf0011" he="162" wi="518" file="FDA0002107492270000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>N<Sub>ij</Sub>表示根据步骤S2～S6计算得到的摄像机C<Sub>i</Sub>到摄像机C<Sub>j</Sub>之间的转移次数；S8.根据步骤S7计算两两摄像机之间的转移概率；S9.记录摄像机C<Sub>i</Sub>到摄像机C<Sub>j</Sub>所有匹配的行人目标之间的时间间隔,构成时间序列T<Sub>ij</Sub>,然后利用自适应的Parzen窗算法估计出摄像机C<Sub>i</Sub>与摄像机C<Sub>j</Sub>之间的转移时间分布的概率密度曲线,取与曲线峰值对应的时间差值作为时间窗口大小T(C<Sub>j</Sub>|C<Sub>i</Sub>)的估计值；S10.根据步骤S9估算出两两摄像机之间的时间窗口大小；S11.更新摄像机C<Sub>i</Sub>、摄像机C<Sub>j</Sub>之间的转移概率：P<Sub>ij</Sub>(k)＝(1-α)P<Sub>ij</Sub>(k-1)+αP(C<Sub>j</Sub>|C<Sub>i</Sub>)P<Sub>ij</Sub>(k)表示第k次迭代得到的摄像机C<Sub>i</Sub>到摄像机C<Sub>j</Sub>的转移概率,α是更新因子,0≤α≤1,P<Sub>ij</Sub>(k-1)表示第k-1次迭代得到的摄像机C<Sub>i</Sub>到摄像机C<Sub>j</Sub>的转移概率,当k＝1时,<Image id="icf0012" he="123" wi="324" file="FDA0002107492270000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>S12.根据步骤S11对两两摄像机之间的转移概率进行更新；S13.更新摄像机C<Sub>i</Sub>与摄像机C<Sub>j</Sub>之间的时间窗口：T<Sub>ij</Sub>(k)＝(1-η)T<Sub>ij</Sub>(k-1)+ηT(C<Sub>j</Sub>|C<Sub>i</Sub>)其中T<Sub>ij</Sub>(k)表示第k次迭代得到的摄像机C<Sub>i</Sub>与摄像机C<Sub>j</Sub>之间的时间窗口,T<Sub>ij</Sub>(k-1)表示第k-1次迭代得到的摄像机C<Sub>i</Sub>与摄像机C<Sub>j</Sub>之间的时间窗口,当k＝1时,T<Sub>ij</Sub>(k-1)＝τ,η表示更新因子；S14.判断是否达到了设定的迭代次数,若是则输出转移概率以及时间窗口估计值,完成拓扑结构估计,结束迭代,否则令k＝k+1然后执行步骤S6～S14。</td>   <td>G06T7/292;G06T7/246;G06T7/277;H04N7/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李中华;                   何冠男       </td>   <td>中山大学</td>   <td>一种RFID阅读器防碰撞协议的性能评估方法</td>   <td>广东省</td>   <td>CN110532823A</td>   <td>2019-12-03</td>   <td>本发明涉及RFID射频识别领域,更具体地,涉及一种RFID阅读器防碰撞协议的性能评估方法。首先,RFID阅读器在服务器的控制下,发送指定信号探测与周围阅读器的碰撞情况。然后,服务器根据所有阅读器的碰撞信息进行图论建模,计算最大独立集。与此同时,所有RFID阅读器安装同一个RFID阅读器防碰撞协议,同时开始执行并记录状态信息。最后,服务器将所有阅读器的状态信息和最大独立集进行比对,计算状态信息和最大独立集的接近程度。根据接近程度,选择出性能最优的RFID阅读器防碰撞协议。本发明从简易部署,快速评估的角度出发,根据图论原理和RFID系统特点,能够针对众多的RFID阅读器防碰撞协议进行性能评估,并能选择出性能最优的协议。</td>   <td>1.一种RFID阅读器防碰撞协议的性能评估方法,其特征在于,包括以下阶段：RFID阅读器碰撞信息收集阶段、RFID服务器图论建模阶段、RFID阅读器防碰撞协议测试阶段以及比对与选择阶段；RFID阅读器碰撞信息收集阶段：RFID阅读器在RFID服务器的统一协调下,每个RFID阅读器根据读取RFID标签功率向周围发送碰撞侦测信号,接收到碰撞侦测信号的RFID阅读器根据碰撞侦测信号的信号强度判断与发送碰撞侦测信号的RFID阅读器同时读取标签时是否会发生碰撞,并将碰撞信息发送到RFID服务器；RFID服务器图论建模阶段：RFID服务器汇总所有RFID阅读器的碰撞信息,根据图论知识进行建模,并计算得到当前碰撞信息下的最大独立集；RFID阅读器防碰撞协议测试阶段：每个RFID阅读器安装相同的防碰撞协议,并同时开始运行,每个RFID阅读器根据RFID阅读器防碰撞协议进行状态转换,状态包括读取标签成功状态、静默状态以及读取标签失败状态,并记录相关状态信息,当测试时间到后,每个RFID阅读器将状态信息上传至RFID服务器,同时每个RFID阅读器安装新的防碰撞协议,重新开始测试；比对与选择阶段：RFID服务器汇总所有RFID阅读器的状态信息,并与最大独立集进行比对,计算每个协议接近最大独立集的程度,根据接近程度的大小选择出性能最优的RFID阅读器防碰撞协议。</td>   <td>G06K7/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;              吴箫;                   印鉴       </td>   <td>中山大学</td>   <td>一种基于轻量化卷积神经网络的驾驶员行为识别方法</td>   <td>广东省</td>   <td>CN110532878A</td>   <td>2019-12-03</td>   <td>本发明为基于轻量化卷积神经网络的驾驶员行为识别方法,获取驾驶行为公开数据集,得到不同驾驶行为分类的图片；对图片预处理,随机打乱数据集并划分训练集和测试集；对训练集进行数据增强；设计轻量化卷积神经网络,将训练集输入数据输入网络进行特征提取；用分类器对提取到的特征向量进行各个驾驶员行为类别的概率预测,根据训练集类别标签对预测的概率计算损失函数,通过反向传播指导网络下一步训练方向；训练完成后保存训练好的驾驶员行为分类模型并保存。本发明通过采用卷积模块和增强通道间的信息流通设计出轻量化卷积神经网络,训练出体积小、运算简单且准确率高的驾驶员行为分类模型,适用于车载移动端进行驾驶行为的识别分类。</td>   <td>1.一种基于轻量化卷积神经网络的驾驶员行为识别方法,其特征在于,包括以下步骤：S1、获取公开的驾驶行为数据集,得到对应不同驾驶行为分类下的一系列图片；S2、对图片进行预处理,再随机打乱数据集,对打乱后的数据集进行划分,得到训练集和测试集；S3、对训练集进行数据增强处理,以增加训练样本的多样性；S4、设计轻量化卷积神经网络,将训练集输入数据输入轻量化卷积神经网络进行特征提取；S5、用分类器对提取到的特征向量进行各个驾驶员行为类别的概率预测,根据训练集类别标签对预测的概率计算损失函数,并通过反向传播指导轻量化卷积神经网络的下一步训练方向；S6、训练完成后保存训练好的驾驶员行为分类模型。</td>   <td>G06K9/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘星成;                   赵莹莹       </td>   <td>中山大学</td>   <td>一种基于不同类别的联合距离均值的分类方法</td>   <td>广东省</td>   <td>CN110533104A</td>   <td>2019-12-03</td>   <td>本发明涉及分类算法领域,更具体地,涉及一种基于不同类别的联合距离均值的分类方法。其包括步骤：S1、将训练样本按照类别分成不同的子集；S2、计算测试样本到每个子集中的训练样本的距离；S3、将距离按照大小顺序排列,从每一类中取出距离最小的k个训练样本；S4、记录每一类中所取出的k个训练样本到测试样本的距离,并求平均值,根据平均值得到判别距离；S5、将测试样本归为判别距离最小的那一类；S6、若测试样本分类完毕,则结束分类步骤,否则返回S2,继续执行分类步骤,直至测试样本分类完毕。本分类方法保留了K最近邻分类方法简单易行的特点,且对比现有K最近邻分类方法取得更好的分类效果。</td>   <td>1.一种基于不同类别的联合距离均值的分类方法,其特征在于,所述方法包括步骤：步骤1、将训练样本按照类别分成不同的子集；步骤2、计算测试样本到每一个子集中的训练样本的距离；步骤3、将距离按照大小/小大顺序排列,从每一类中取出后/前k个训练样本；步骤4、记录每一类中所取出的k个训练样本到测试样本的距离,并求平均距离值,根据平均距离值得到判别距离；步骤5、将测试样本归为判别距离最小的那一类；步骤6、若测试样本分类完毕,则结束分类步骤,否则返回步骤2,继续执行分类步骤,直至测试样本分类完毕。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张贺晔;              李吉平;                   吴万庆       </td>   <td>中山大学</td>   <td>内窥镜图像的成像方法、装置、设备及介质</td>   <td>广东省</td>   <td>CN110533612A</td>   <td>2019-12-03</td>   <td>本申请提供了一种内窥镜图像的成像方法、装置、设备及介质,所述方法涉及对内窥镜图像中R,G,以及B通道分量的通道图像进行调整,包括：获取内窥镜图像数据,并将所述内窥镜图像数据进行RGB通道分解,并生成通道图像,所述通道图像包括R通道图像,G通道图像,以及B通道图像；将所述通道图像分别输入至预设的调整函数模型；获取所述调整函数模型输出的通道增强图像,所述通道增强图像包括R通道增强图像,G通道增强图像,以及B通道增强图像；依据所述R通道增强图像,所述G通道增强图像,以及所述B通道增强图像生成内窥镜图像。该方法通过对图像的R、G、B通道分量独立进行调整,使得组织背景和血管的色调产生明显的对比效果。</td>   <td>1.一种内窥镜图像的成像方法,所述方法涉及对内窥镜图像中R,G,以及B通道分量的通道图像进行调整,其特征在于,包括：获取内窥镜图像数据,并将所述内窥镜图像数据进行RGB通道分解,并生成通道图像,所述通道图像包括R通道图像,G通道图像,以及B通道图像；将所述通道图像分别输入至预设的调整函数模型；获取所述调整函数模型输出的通道增强图像,所述通道增强图像包括R通道增强图像,G通道增强图像,以及B通道增强图像；依据所述R通道增强图像,所述G通道增强图像,以及所述B通道增强图像生成内窥镜图像。</td>   <td>G06T5/00;G16H30/40;A61B1/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张贺晔;              刘子龙;              李吉平;                   吴万庆       </td>   <td>中山大学</td>   <td>超声弹性图像的成像方法、装置、设备及介质</td>   <td>广东省</td>   <td>CN110517249A</td>   <td>2019-11-29</td>   <td>本申请提供了一种超声弹性图像的成像方法、装置、设备及介质,包括：利用人工神经网络的自学习能力,建立超声弹性图像中的射频数据与病变特征之间的对应关系；获取患者的当前超声弹性图像的当前射频数据；通过所述对应关系,确定与所述当前射频数据对应的当前病变特征；具体地,确定与所述射频数据对应的当前病变特征,包括：将所述对应关系中与所述当前射频数据相同的射频数据所对应的病变特征,确定为所述当前病变特征。通过本方法可以直接通过射频信号重建超声弹性成像；并且仅以计算模拟为训练数据就可以从真实的射频数据中推断出真实的弹性成像分布；本方法的神经网络框架可以直接从超声射频数据产生位移场和应变场。</td>   <td>1.一种超声弹性图像的成像方法,其特征在于,包括：利用人工神经网络的自学习能力,建立超声弹性图像中的射频数据与病变特征之间的对应关系；获取患者的当前超声弹性图像的当前射频数据；通过所述对应关系,确定与所述当前射频数据对应的当前病变特征；具体地,确定与所述射频数据对应的当前病变特征,包括：将所述对应关系中与所述当前射频数据相同的射频数据所对应的病变特征,确定为所述当前病变特征。</td>   <td>G06T7/00;G16H50/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         单云霄;              张彧;                   陈龙       </td>   <td>中山大学</td>   <td>一种基于激光雷达和PTZ摄像机的目标追踪方法</td>   <td>广东省</td>   <td>CN110517284A</td>   <td>2019-11-29</td>   <td>本发明涉及一种基于激光雷达和PTZ摄像机的目标追踪方法。包括人工标定PTZ摄像机在多个静态姿态下与激光雷达的不同转移矩阵,基于这些转移矩阵推导PTZ摄像机动态情况下姿态与转移矩阵的映射关系；在追踪中利用IMU和PTZ摄像机姿态查询获取PTZ摄像机的实时姿态,基于前述映射关系实时确定PTZ摄像机和激光雷达的转移矩阵；根据实时确定的转移矩阵将点云转换成稀疏深度图,与摄像机图像进行特征融合后使用KCF算法进行目标追踪；基于目标追踪的模块的反馈,实时调整PTZ摄像机的姿态,保持追踪目标始终在画面中。本方法采用PTZ摄像机进行目标追踪,克服了智能机器人所搭载传统摄像机观测方向与机器人运动方向高度耦合的缺陷,提升了目标追踪算法的鲁棒性和准确性。</td>   <td>1.一种基于激光雷达和PTZ摄像机的目标追踪方法,其特征在于,包括以下步骤：S1.采集PTZ摄像机在多个静态姿态下的图像与激光雷达对应的点云；S2.利用上述图像和点云,在PTZ摄像机每个静态姿态下进行联合标定确定雷达坐标系与图像坐标系的关系；S3.基于上述转移矩阵推导PTZ摄像机动态情况下姿态与转移矩阵的映射关系；S4.在追踪中对IMU和PTZ摄像机姿态查询结果进行融合,获取PTZ摄像机的实时姿态,并基于上述映射关系实时确定PTZ摄像机和激光雷达的转移矩阵；S5.根据实时确定的转移矩阵将点云转换成稀疏深度图,与摄像机图像进行特征融合后使用KCF算法进行目标追踪；S6.根据上述目标追踪模块的反馈结果,实时调整PTZ摄像机的姿态,保持追踪目标在画面中心。</td>   <td>G06T7/20;G06K9/62;G01S17/66;G01S7/497</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林俊浩;              单云霄;                   陈龙       </td>   <td>中山大学</td>   <td>一种基于卷积神经网络的道路裂缝自动检测方法</td>   <td>广东省</td>   <td>CN110503637A</td>   <td>2019-11-26</td>   <td>本发明涉及图像识别与深度学习技术领域,更具体地,涉及一种基于卷积神经网络的道路裂缝自动检测方法。本发明基于轻量级的卷积神经网络,仅需对数据进行手动标注,根据检测衡量标准精确的和召回率来筛选并保持网络权重,在进行检测时,对图片进行(平方)压缩后,再切分成27*27大小进行检测,根据初步检测的结果对相应的裂缝区域进行多次旋转变换后再检测,更新检测结果,得到每一帧中裂缝的位置标出并响铃提示。本发明的模型是轻量级的,具有很高的召回率和精确度,可用于实时的裂缝检测,对于检测到的裂缝我们会记录其位置信息并标出,同时可以响铃警示工作人员,比以上提出的发明更具有实用性。</td>   <td>1.一种基于卷积神经网络的道路裂缝自动检测方法,其特征在于,包括以下步骤：S1.采集含有裂缝的图片数据,使用图像处理软件对图片进行像素级别的标注；S2.从原始图片中提取出w*h大小的小图片,利用统计的方法,根据小图片中裂缝像素的总数来划分正负样本并根据一定比例划分为训练集和测试集,对训练集的正样本进行旋转,翻转操作,增加训练集数据的多样性；S3.采用迁移学习的思想,边训练边测试,取在测试集中准确率和召回率综合最好的网络权重进行保存,得到一个二分类器；S4.使用摄像头获取路面的图像,将输入的每一帧图片进行4x4像素邻域内的双立方插值,将像素改变为W*H；S5.将图片等分为w*h大小的检测单元并记录其位置信息,利用步骤S3训练出来的二分类器对每一个检测单元分类,输出该单元是否为裂缝单元；S6.对检测为裂缝的区域分别进行多个角度的旋转,得到多个新的样本,再次进行检测,若有两个以上被分类为裂缝,则确信其为裂缝,否则更新其为非裂缝；S7.根据步骤S6优化后的裂缝结果,标出裂缝单元在原图片的位置,如有裂缝单元,则给出提示。</td>   <td>G06T7/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         徐金龙;              陈小柏;                   虞志益       </td>   <td>中山大学</td>   <td>一种基于深度学习技术的视觉辅助系统</td>   <td>广东省</td>   <td>CN110490087A</td>   <td>2019-11-22</td>   <td>本发明涉及一种基于深度学习技术的视觉辅助系统,包括数据采集单元、数据预处理单元、环境感知单元和用户接口单元；其中数据采集单元用于感知周围环境的信息,并将感知到的数据传输至数据预处理单元；数据预处理单元对接收到的数据进行预处理,然后将预处理好的数据传输至环境感知单元；环境感知单元接收数据后进行基于深度学习技术的障碍物检测,得到周围环境中障碍物的类别、位置及距离,然后将检测到的周围环境的障碍物信息处理成文本信息,并将文本信息送入用户接口单元；用户接口单元将文本信息转化成语音,供用户获知周围的环境的障碍物情况。</td>   <td>1.一种基于深度学习技术的视觉辅助系统,其特征在于：包括数据采集单元、数据预处理单元、环境感知单元和用户接口单元；其中数据采集单元用于感知周围环境的信息,并将感知到的数据传输至数据预处理单元；数据预处理单元对接收到的数据进行预处理,然后将预处理好的数据传输至环境感知单元；环境感知单元接收数据后进行基于深度学习技术的障碍物检测,得到周围环境中障碍物的类别、位置及距离,然后将检测到的周围环境的障碍物信息处理成文本信息,并将文本信息送入用户接口单元；用户接口单元将文本信息转化成语音,供用户获知周围的环境的障碍物情况。</td>   <td>G06K9/00;G06N3/04;G10L13/02;G01S15/02</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林凯荣;              兰甜;              梁汝豪;                   卢鹏宇       </td>   <td>中山大学</td>   <td>一种基于CPP的水文模型参数动态率定方法</td>   <td>广东省</td>   <td>CN110490228A</td>   <td>2019-11-22</td>   <td>本发明涉及一种基于CPP的水文模型参数动态率定方法。包括：S1.将率定期在年尺度上划分为多个子期,并计算所有子期的气象聚类指标和下垫面聚类指标；基于聚类指标与流量之间的非线性关系来筛选出候选聚类指数；S2.采用PCA算法消除聚类指标之间的多重共线性；S3.对所有年份的每个子期的聚类指标值进行平均；并根据气象指标和下垫面指标依次进行两次聚类操作,最终将年内水文过程划分为四个子期进行率定；S4.采用改进的并行率定方案在每个子期对TOPMODEL模型参数进行独立地优选,并组合以生成连续的流量序列值；S5.利用多指标综合评价体系评估水文模型在不同流量条件下的模拟性能。本发明有效提高了水文模型在变化环境下的预测能力。</td>   <td>1.一种基于CPP的水文模型参数动态率定方法,其特征在于,包括以下步骤：S1.将率定期在年尺度上划分为多个子期,并计算所有子期的气象聚类指标和下垫面聚类指标；基于聚类指标与流量之间的非线性关系来筛选出候选聚类指数；S2.采用PCA算法消除聚类指标之间的多重共线性；S3.对所有年份的每个子期的聚类指标值进行平均；并根据气象指标和下垫面指标依次进行两次聚类操作,最终将年内水文过程划分为四个子期进行率定,即干旱期,雨期I,雨期II和雨期III；S4.采用改进的并行率定方案在每个子期对TOPMODEL模型参数进行独立地优选,并组合以生成连续的流量序列值；S5.利用多指标综合评价体系评估水文模型在不同流量条件下的模拟性能。</td>   <td>G06K9/62;G06Q10/04;G06Q10/06;G06Q50/26;G01W1/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         蔡庆玲;              孙玮;              梁伟霞;              何鸿奇;                   林进可       </td>   <td>中山大学</td>   <td>一种基于USFaster R-CNN的甲状腺超声图像结节自动定位识别方法</td>   <td>广东省</td>   <td>CN110490892A</td>   <td>2019-11-22</td>   <td>本发明公开了一种基于USFaster R-CNN的甲状腺超声图像结节自动定位识别方法,属于人工智能和深度学习领域。该方法包括甲状腺超声图像的预处理,深度神经网络模型的搭建,网络模型训练和优化,其中深度神经网络模型包括底层卷积特征提取网络,候选框生成网络,特征图池化层、分类与候选框回归网络。利用深度学习方法,实现甲状腺超声图像特征自动提取,候选框自动生成,筛选,位置修正。实现甲状腺结节的自动定位识别功能。本发明可以有效辅助医生进行甲状腺超声图像诊断,提高诊断的客观性和准确率,有效降低医生的工作量以及小目标结节的漏检率。</td>   <td>1.一种基于USFaster R-CNN的甲状腺超声图像结节自动定位识别方法,其特征在于,包括以下步骤：(1)首先对甲状腺超声图像进行预处理,截取出待处理区域,去除超声图像中与甲状腺图像无关信息,用图像标注工具labelImg,根据诊断医生的标记以及诊断报告框出超声图像中的甲状腺结节,并进行结节种类的标注,制作成xml文件作为训练集的ground true标签,然后去除图像中的诊断医生的人工标注,以免影响对网络的训练效果,最后将超声图像以及对应的xml文件分成训练集,验证集和测试集；(2)用大型数据集ImageNet数据集预先训练已经搭建好的USFaster R-CNN深度神经网络,得到训练好的网络参数,然后在用甲状腺超声图像训练集训练网络时将预先训练好的网络参数迁移到USFaster R-CNN网络中,这样使得网络收敛得更快并且具有更强的泛化能力；(3)当初步训练好网络模型后使用验证集对训练的超参数进行微调和优化,使模型达到最好的识别和定位效果；(4)将待识别与定位的甲状腺超声图像,即测试集,读取到USFaster R-CNN网络,实现甲状腺结节的自动定位与识别。</td>   <td>G06T7/136</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘广;              吕中荣;              汪利;                   刘济科       </td>   <td>中山大学</td>   <td>基于ERSA算法的结构参数识别方法</td>   <td>广东省</td>   <td>CN110472315A</td>   <td>2019-11-19</td>   <td>本发明主要涉及增强的响应灵敏度(ERSA)算法在结构参数识别这一领域的工程应用,具体是一种基于ERSA算法的结构参数识别方法,主要步骤如下：(一)通过对系统参数进行灵敏度分析,得到系统关于未知参数的灵敏度方程。(二)利用测量的响应数据和基于当前参数计算得到的响应数据构建最小二乘目标函数。(三)采用ERSA算法迭代优化这一目标函数,直到满足循环结束条件为止。(四)最后得到的最优解即为参数识别结果。该方法相较于传统的群智能方法而言,无需随机搜索,利用少量迭代步数即可得到精准的识别结果。</td>   <td>1.一种基于ERSA算法的结构参数识别方法,其特征在于,包括以下步骤：1)将系统的结构进行建模,未知参数包含在系统的动力学微分方程中；2)对于结构参数进行灵敏度分析,并基于当前给定参数计算出响应,对比测量得到的响应构建目标函数；3)利用ERSA算法不断优化目标函数,直到满足设定的终止条件,得到最优解即为参数识别结果。</td>   <td>G06F17/50;G06F17/11;G06F17/13;G06F17/15;G06F17/16</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;                   洪赛丁       </td>   <td>中山大学</td>   <td>基于小波变换和迁移学习的视网膜OCT图像分类方法</td>   <td>广东省</td>   <td>CN110472530A</td>   <td>2019-11-19</td>   <td>本发明属于计算机视觉、医学图像处理技术领域,为基于小波变换和迁移学习的视网膜OCT图像分类方法,包括步骤：对视网膜OCT图像进行小波变换,得到低频细节分量、水平细节分量、垂直细节分量、对角线细节分量四条子带；对四条子带的图像,基于迁移学习进行子带特征的提取；将所提取的四个子带图像的特征进行级联和特征融合,然后将级联和特征融合后的特征作为训练集输入到随机森林中,进行训练、分类和预测结果。本发明充分利用了视网膜OCT图像原本的信息,在一定程度上能减少训练参数、加速运行,提高最终分类预测的准确率。</td>   <td>1.基于小波变换和迁移学习的视网膜OCT图像分类方法,其特征在于,包括步骤：S1、对视网膜OCT图像进行小波变换,得到低频细节分量、水平细节分量、垂直细节分量、对角线细节分量四条子带；S2、对四条子带的图像,基于迁移学习进行子带特征的提取；S3、将所提取的四个子带图像的特征进行级联和特征融合,然后将级联和特征融合后的特征作为训练集输入到随机森林中,进行训练、分类和预测结果。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         孔雀屏;              陈湘萍;              黄袁;              刘聪;                   周凡       </td>   <td>中山大学</td>   <td>一种高效阅读智能合约辅助方法</td>   <td>广东省</td>   <td>CN110473092A</td>   <td>2019-11-19</td>   <td>本发明公开了一种高效阅读智能合约辅助方法,用户先输入所有已读合约,提取其标识符和Token序列；之后用户输入将读合约,提取其标识符和Token序列,与数据库中所有智能合约的标识符、Token序列比较,计算出将读合约与所有已读合约的综合相似度,输出综合相似度前十的合约,并分别标记输出合约与将读合约不同的部分,辅助用户高效细读大量的智能合约。通过该方法,可快速找到与已看合约不同的部分,从而节省阅读时间,无论是编程人员还是非编程人员,都能提高细读大量智能合约的效率。另外,编程人员也可借助该工具,快速学习编写智能合约。同类型的功能在实现上会有些许的差异,利用该方法可快速获取差异,从而设计出更安全,功能更全面的智能合约。</td>   <td>1.一种高效阅读智能合约辅助方法,其特征在于,所述方法包括：用户先输入所有已读合约,预处理已读合约后,分别提取已读合约的标识符和Token序列,并把它们存储在数据库中；用户输入将读合约,预处理将读合约后,提取将读合约的标识符和Token序列,分别把将读合约的标识符、Token序列与数据库中所有智能合约的标识符、Token序列比较,计算出将读合约与所有已读合约的语义相似度和语法相似度；把语义相似度和语法相似度线性组合成综合相似度,输出综合相似度前十的合约,并分别标记输出合约与将读合约不同的部分,辅助用户高效细读大量的智能合约。</td>   <td>G06Q40/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         梁小丹;              王绍菊;              林冰倩;                   林倞       </td>   <td>中山大学</td>   <td>一种可自动定制的医学病灶检测架构及方法</td>   <td>广东省</td>   <td>CN110473195A</td>   <td>2019-11-19</td>   <td>本发明公开了一种可自动定制的医学病灶检测架构及方法,该检测架构包括：候选特征提取模块,对医学图像进行特征提取；病灶检测网络头部自动定制模块,定义搜索空间将候选区域间的感知关系合并在一起,并利用可微NAS算法得到最佳病灶检测网络头部；病灶检测网络头部最优模块,利用一卷积层,并经过一个标准细胞和两个收缩细胞得到新的候选特征,通过两连接层对候选特征进行二元分类和预测框回归,将二元分类中候选特征分类的权重M作为高层次语义信息输出至知识迁移模块；知识迁移模块,结合语义关系并在不同的区域内传递相关的上下文信息,得到增强的候选特征,并将增强的候选特征和原候选特征合并,最后通过全连接层进行多元分类和回归。</td>   <td>1.一种可自动定制的医学病灶检测架构,包括：候选特征提取模块,用于对输入的医学图像进行特征提取,提取出图像的候选特征；病灶检测网络头部自动定制模块,用于根据医学图像特性、病灶特征和目标检测的相关知识,定义新的搜索空间,所述搜索空间包括大量具有灵活感受野、跳层连接等子网络架构的先进操作,并增加一个非局部操作,将候选区域间的感知关系合并在一起,根据候选特征并利用可微NAS算法在设计的搜索空间中搜索合适的操作和连接方式使其组成一个适合医学图像的最佳病灶检测网络头部；病灶检测网络头部最优模块,为所述病灶检测网络头部自动定制模块定制的最佳病灶检测网络头部,将所述候选特征提取模块输出的候选特征,首先经过一个卷积核为3×3的卷积层,然后经过一个标准细胞和两个收缩细胞得到新的候选特征,并通过两个连接层对新的候选特征进行二元分类和预测框回归,将二元分类中候选特征分类的权重M作为高层次语义信息输出至知识迁移模块；知识迁移模块,在最佳病灶检测网络头部学习到的区域关系图的基础上,结合语义关系,并在不同的区域内传递相关的上下文信息,以得到一个增强的候选特征,并将增强后的候选特征和原来的候选特征合并在一起去共享多种病灶类型的相关信息,最后通过全连接层进行多元分类和回归。</td>   <td>G06T7/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         傅恺延;                   钟任新       </td>   <td>中山大学;公安部交通管理科学研究所</td>   <td>基于线性策略的交叉熵算法的交通仿真模型参数标定方法</td>   <td>广东省</td>   <td>CN105930565B</td>   <td>2019-11-19</td>   <td>本发明公开一种基于线性策略的交叉熵算法的交通仿真模型参数标定方法,具体的步骤如下：以跟驰模型为例,选取两车之间的车头间距以及后车速度作为评价指标,确定目标函数的形式,从而确定参数标定的目标；采集所需要的交通数据；根据所确定的跟驰模型以及交通数据,确定待标定的参数及其对应的有效取值范围；利用交叉熵算法对上述跟驰模型进行参数标定计算；计算过程中采用线性策略确定样本数量：设下一迭代的样本的生成数量与当前迭代的样本的方差存在一个负的线性关系,即当前样本方差越大,下一次迭代时样本的生成数量越大,方差越小,样本生成数量越小；直到样本的方差小于交叉熵算法预设的阀值,最后输出样本的均值作为标定的最优值。</td>   <td>1.一种基于线性策略的交叉熵算法的交通仿真模型参数标定方法,其特征在于采用线性关系生成样本量,具体的步骤如下：a、选择跟驰模型作为交通仿真模型,选取两车之间的车头间距以及后车的速度作为评价指标,确定目标函数的形式,从而确定参数标定的目标；步骤a中选取两车间的车头间距以及后车的速度作为评价指标,确定目标函数的形式,具体为：在跟驰模型中以两车间的车头间距以及后车的速度作为评价指标,则目标函数中,计算的就是模型仿真的两车间的车头间距以及后车的速度与观测得到的数据之间的差异,具体形式为：          <Image id="icf0001" he="89" wi="700" file="FDA0002097187790000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,S(x)表示目标函数的值,x表示跟驰模型的参数集；T表示总的仿真时长,t表示某个仿真时刻；<Image id="icf0002" he="77" wi="145" file="FDA0002097187790000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示在t时刻跟驰模型计算得出的车头间距,<Image id="icf0003" he="79" wi="84" file="FDA0002097187790000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示在t时刻观测得到的车头间距；<Image id="icf0004" he="76" wi="148" file="FDA0002097187790000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0005" he="78" wi="82" file="FDA0002097187790000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>分别表示在t时刻模型计算得出的速度值和观测得到的速度值；λ为权重值,λ越高,表示在目标函数中车头间距的比重越高,反之,速度值的比重越高；b、采集所需要的交通数据,并确定跟驰模型的形式；根据所确定的跟驰模型以及实地测量数据,确定待标定的参数及其对应的有效取值范围；c、利用交叉熵算法对上述跟驰模型进行参数标定计算,并采用线性策略确定样本数量；所述步骤c利用交叉熵算法对上述跟驰模型进行参数标定计算,并采用线性策略确定样本数量,具体实现过程为：c1、设下一次迭代的随机样本的数量与当前迭代的样本的方差存在简单的负相关关系,即当前样本方差越大,下一次迭代生成的样本数量越大,当前样本方差越小,下一次迭代生成的样本数量越小；c2、初始化样本的概率密度函数,此处的样本指代跟驰模型的待标定参数；c3、迭代开始,首先按照线性关系确定初次迭代要生成的样本的数量；c4、假设样本的分布为独立正态分布,按照该分布生成一定数量的随机样本；c5、将每个样本分别输入跟驰模型,模拟驾驶员的驾驶情况,并收集每一时刻后车驾驶员的驾驶速度和绝对位置；通过对比仿真得出的和观测得到的后车的驾驶速度和绝对位置,计算目标函数式,得到评价每一个样本的目标函数值；c6、将得到的目标函数值从小到大依次排列,把产生最小目标函数值的前5％的样本标记为精英样本；即精英样本就是在每次迭代中,能够产生最小目标函数值的参数集；c7、判断当前的精英样本是否满足设定的迭代停止条件；当不满足停止条件时,将根据精英样本的信息更新正态分布的均值和方差,并且返回步骤c3,继续进行迭代；直到精英样本聚拢于最优样本附近,退出循环,记录最优样本,并且结束交叉熵算法。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李惠川;                   刘树郁       </td>   <td>中山大学</td>   <td>基于条件生成对抗网络的图像风格迁移方法</td>   <td>广东省</td>   <td>CN110458216A</td>   <td>2019-11-15</td>   <td>本发明涉及一种基于条件生成对抗网络的图像风格迁移方法,包括以下步骤：S1.构造条件生成对抗网络的图像生成器G和判别器D；S2.收集不同风格的图像并对其进行预处理和数据增强,制定训练策略,对图像生成器G和判别器D进行训练；S3.利用训练好的图像生成器G和判别器D进行图像风格的迁移。</td>   <td>1.一种基于条件生成对抗网络的图像风格迁移方法,其特征在于：包括以下步骤：S1.构造条件生成对抗网络的图像生成器G和判别器D；S2.收集不同风格的图像并对其进行预处理和数据增强,制定训练策略,对图像生成器G和判别器D进行训练；S3.利用训练好的图像生成器G和判别器D进行图像风格的迁移。</td>   <td>G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林浩添;              吕健;              张凯;              曾思明;              徐帆;                   陈琦       </td>   <td>中山大学中山眼科中心;广西壮族自治区人民医院</td>   <td>一种眼部图片和属性信息的分析方法和系统</td>   <td>广东省</td>   <td>CN110458806A</td>   <td>2019-11-15</td>   <td>本发明涉及一种眼部图片和属性信息的分析方法和系统,获取用户患有角膜炎的眼部图片和所述用户的属性信息；根据所述眼部图片的炎症浸润和/或溃疡形状,判别所述眼部图片的分类信息；根据所述属性信息和所述分类信息,获取所述用户所患有的角膜炎属于不同类别的概率。本发明无需专业的眼科医生,只需上传眼部图片和属性信息,即可以获知用户所患角膜炎类型的概率,为用户或者医生作出诊疗决策提供可靠的参考数据,并且可以推广到基层医院,普及更多的基层人群,弥补现行医疗资源分布不均的缺陷,为专业的眼科医生减轻负担。</td>   <td>1.一种眼部图片和属性信息的分析方法,其特征在于,包括以下步骤：获取用户患有角膜炎的眼部图片和所述用户的属性信息；根据所述眼部图片的炎症浸润和/或溃疡形状,判别所述眼部图片的分类信息；根据所述属性信息和所述分类信息,获取所述用户所患有的角膜炎属于不同类别的概率。</td>   <td>G06T7/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>              郑昱       </td>   <td>中山大学</td>   <td>一种基于社交媒体的社会群体认知指数构建方法</td>   <td>广东省</td>   <td>CN110442865A</td>   <td>2019-11-12</td>   <td>本发明属于社交媒体语义分析技术领域,具体涉及一种基于社交媒体的社会群体认知指数构建方法；本发明提出了基于社交媒体的社会群体认知指数的构建方法,通过对于非社会群体认知特征主题与社会群体认知特征的差异分析,在不同地域和不同时间内,分别获取基于社交媒体的社会群体认知表达性指数和基于社交媒体的社会群体认知能动性指数,得到可以进行不同时间、地域比较的社会群体认知指数；消除了个体主观因素对数据收集的影响,有助于更为精确地进行社会群体认知状况评估。本发明在多层级水平上构建社会认知指数,有助于根据不同地域特征进行当地居民的社会群体认知数据纵向采集,实现区域社会群体认知的多维度比较评估。</td>   <td>1.一种基于社交媒体的社会群体认知指数构建方法,其特征在于：包括以下步骤：(1)目标群体选取：根据9个社会阶层分类确定11个目标群体特征词,包括领导、经理、老板、专家、教授、医生、农民、服务员、工人、白领、公务员,获取社交媒体账号所发布的推文中包含目标群体特征词的推文；(2)数据预处理：对社交媒体账号所发布的推文做分词处理,去除无用符号,得到账号特征词；删除无文字内容推文；采用词袋模型来表征账号,词袋模型是基于自然语言处理和信息检索下被简化的一种表达模型,此模型无需考虑文法以及词的顺序；(3)特征词选取：根据社会认知基本维度中文形容词词库的两个基本维度：能动性维度和表达性维度的特征词进行选取；按照社会认知基本维度中文形容词词库,其中表达性维度的特征词包括友好、善良、可靠、热情、和蔼、真诚,能动性维度的特征词包括有能力、自信、上进、高效、聪明、努力；(4)特征词扩展：寻找社会认知两个基本维度特征词的近义词,对已有特征词进行扩展；利用哈工大信息检索研究室同义词词林扩展版HIT IR-Lab Tongyici Cilin(extended)进行特征词的扩展,HIT IR-Lab Tongyici Cilin(extended)按照树状的层次结构把同义词分成了大、中、小三类；(5)数据库建立：选取目标时间区间的社交媒体推文构建数据库,包括社会群体认知两个基本维度所涉及的特征词,建立目标数据库,命名SC数据库(social cognition)；随机抽样相等数量的非社会群体认知基本维度的特征词其中,表达主题包括事件和经历,生活方式,运动、社区参与和实践活动,建立控制变量数据库,命名CV数据库(control variable)；(6)社会群体认知模型构建：采用机器学习技术中的K近邻算法分类识别两种基本维度的特征词,基于SC和CV数据库作为二分类别框架建构模型,采用多层级线性回归模型,建立SC数据库模型；基于CV数据库的社会群体认知模型构建：采用K近邻算法分类识别两种基本维度的特征词,基于SC和CV数据库作为二分类别框架建构模型,采用多层级线性回归模型,建立CV数据库模型；(7)社会群体认知指数构建：基于社会群体认知模型,确定社交媒体账号使用者社会群体认知表达与非社会群体认知表达在其推文中的频次；构建社会群体认知指数(SocialGroup Cognition Index,SGCI),社会群体认知指数的计算是基于社交媒体账号社会群体认知与非社会群体认知之间的差异,包括社会群体认知表达性指数(SGCI_communion)和社会群体认知能动性指数(SGCI_agency)。</td>   <td>G06F17/27;G06F16/21;G06F16/28;G06K9/62;G06Q50/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         许深颐;              王若梅;              周凡;                   林格       </td>   <td>中山大学</td>   <td>基于传感器的分离式部署的人体行为识别健康管理系统</td>   <td>广东省</td>   <td>CN110443145A</td>   <td>2019-11-12</td>   <td>本发明公开了基于传感器的分离式部署的人体行为识别健康管理系统。本发明系统包括客户端的用户交互模块、数据采集模块,服务端的模型识别模块、数据分析模块、建议模块。客户端部署在智能手机或者智能手环等用户个人终端上,用于与用户交互及采集用户的行为数据。服务端部署在远程的主机或服务器上,用于识别人体的行为数据以及进行相应的数据分析,从而提供建议。客户端与服务端通过网络进行通信。本发明提供了便于部署的分离式的人体行为识别系统,提供了更加全面的识别行为的种类以及更加完善有效的服务提醒,同时也进一步提高了识别的准确度以及速度,便于针对用户提供更加深度个性化的服务。</td>   <td>1.基于传感器的分离式部署的人体行为识别健康管理系统,其特征在于,所述系统包括：由客户端与服务端两部分构成,客户端包括用户交互模块、数据采集模块,部署在智能手机或者智能手环等用户个人终端上,用于与用户交互和采集用户的行为数据。服务端包括模型识别模块、数据分析模块、建议模块,部署在远程的主机或服务器上,用于识别人体的行为数据以及进行相应的数据分析,从而提供建议。客户端与服务端通过网络进行通信。</td>   <td>G06K9/00;G06K9/62;G16H20/30;G16H50/30</td>  </tr>        <tr>   <td>中国专利</td>   <td>         邱斌;              苏卓;                   周凡       </td>   <td>中山大学</td>   <td>一种基于深度学习的图像去雾方法</td>   <td>广东省</td>   <td>CN110443759A</td>   <td>2019-11-12</td>   <td>本发明公开了一种基于深度学习的图像去雾方法。本发明通过收集自然图片作为无雾图,然后在图片的不同区域随机生成不同取值的透射率和大气光,并通过大气光散射模型合成人工有雾图作为训练集；构建用于预测雾浓度图的卷积神经网络；之后利用训练集训练所述卷积神经网络；最后输入待处理有雾图,利用所述卷积神经网络计算得到雾浓度图,用有雾图减去雾浓度图即得到最终的去雾图。本发明是一种自适应的去雾方法,产生的去雾结果比较自然,且鲁棒性强,适用范围广,可同时应用于室内和自然场景的去雾中；本方法是一种全自动、端到端的去雾方法,不需要后处理步骤。</td>   <td>1.一种基于深度学习的图像去雾方法,其特征在于,所述方法包括：收集自然图片作为无雾图,然后在图片的不同区域随机生成不同取值的透射率和大气光,并通过大气光散射模型合成人工有雾图作为训练集；构建用于预测雾浓度图的卷积神经网络；利用训练集训练所述用于预测雾浓度图的卷积神经网络；输入待处理有雾图,利用所述用于预测雾浓度图的卷积神经网络计算得到雾浓度图,用有雾图减去雾浓度图即得到最终的去雾图。</td>   <td>G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;                   刘稳       </td>   <td>中山大学</td>   <td>结合生成对抗网络的半监督视网膜OCT图像层分割方法</td>   <td>广东省</td>   <td>CN110443815A</td>   <td>2019-11-12</td>   <td>本发明属于计算机视觉、图像处理技术,为结合生成对抗网络的半监督视网膜OCT图像层分割方法,包括步骤：准备视网膜OCT图像数据,将部分病人的标注图片和全部病人的未标注图片作为训练集,其余病人的标注图片作为测试集；构建生成对抗网络,生成对抗网络包括分割网络和鉴别器网络,分割网络的输出端与鉴别器网络的输入端连接；设计生成对抗网络的损失函数；设置评估指标；利用所设计的损失函数,引入所准备的训练集,对生成对抗网络进行训练。本发明同时利用标注数据和未标注数据对生成对抗网络进行训练,增强了网络的鲁棒性,提高了语义分割的准确率。</td>   <td>1.结合生成对抗网络的半监督视网膜OCT图像层分割方法,其特征在于,包括以下步骤：S1、准备视网膜OCT图像数据,将部分病人的标注图片和全部病人的未标注图片作为训练集,其余病人的标注图片作为测试集；S2、构建生成对抗网络,生成对抗网络包括分割网络和鉴别器网络,分割网络的输出端与鉴别器网络的输入端连接；S3、设计生成对抗网络的损失函数；S4、设置评估指标；S5、利用所设计的损失函数,引入步骤S1所准备的训练集,对生成对抗网络进行训练。</td>   <td>G06T7/11;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         曾坤;              卢建业;                   周凡       </td>   <td>中山大学</td>   <td>一种用于立体图像拼接的关联图构造方法</td>   <td>广东省</td>   <td>CN110443838A</td>   <td>2019-11-12</td>   <td>本发明公开了一种用于立体图像拼接的关联图构造方法。本发明根据立体图像左、右视图和视差图构成的立体信息,将立体图像视图间的2D匹配点对升维成3D匹配点对,采用基于3D匹配点对的概率模型判断立体图像间的匹配关系,从而构建多幅立体图像匹配关系的关联图,用以引导立体图像进行自动化拼接。本发明基于传统的用于平面图像拼接的关联图构造方法进行改进,开拓性地从3D视角检测立体图像间的匹配关系,给出了一种简洁有效的关联图构造方法,为多副立体图像的拼接提供可靠的匹配关系关联图,避免需要用户指定立体图像的拼接顺序,为自动化立体图像拼接提供必要前提。</td>   <td>1.一种用于立体图像拼接的关联图构造方法,其特征在于,所述方法包括：步骤一,输入一组立体图像的左视图、右视图和视差图,并给出立体相机内参；步骤二,为每一幅立体图像分配一个索引号；步骤三,以索引号为节点,构建任意两个节点均相连的关联图；步骤四,对关联图中每一条边两端节点所对应的立体图像,提取左、右视图中的SIFT特征点,获得立体图像间的2D匹配点对；步骤五,基于视差图和相机内参,将步骤四中获得的2D匹配点对升维成3D匹配点对；步骤六,选择关联图中的一条边,基于3D匹配点对,采用概率模型检测该边两端节点所对应的立体图像是否具有匹配关系,若无匹配关系,则从关联图中删除该条边；步骤七,重复执行步骤六,直至关联图中的每一条边均被检测；步骤八,检测关联图中是否存在孤立点,若存在,则删除孤立点；步骤九,输出最终关联图。</td>   <td>G06T7/33;G06T3/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王国利;              尚保林;              许沥文;              黄开德;                   郭雪梅       </td>   <td>中山大学</td>   <td>一种基于探测信号的3D成像与人体识别方法</td>   <td>广东省</td>   <td>CN106919931B</td>   <td>2019-11-08</td>   <td>本发明公开了一种基于探测信号的3D成像与人体识别方法,是基于无线探测网络中采集到的探测信号强度值进行3D成像与人体识别,首先通过记录信号接收器检测到的信号强度值,将数据上传到上位机进行反投影,然后进行规范化标准操作,将得到随时间变化的剖面图合并起来输出3D成像图,最后将成像图与数据库存储的图像进行比照,得到识别结果。总之,相比现有技术,本发明方法简单、新颖,能很好地得到3D成像结果并有效识别目标。</td>   <td>1.一种基于探测信号的3D成像与人体识别方法,其特征在于：首先,通过记录信号接收器检测到的信号强度值,将数据上传到上位机进行3D成像,并将成像图与数据库存储的图像进行比照,最后输出成像以及识别结果；其具体包括以下步骤：1)在兴趣区域部署信号发生器与信号接收器,每一个信号接收器均能接收到至少一个信号发生器产生的信号,采集并存储每个信号发生器标识ID和对应的信号强度值读数；其中,在兴趣区域部署信号发生器与信号接收器需要满足预定条件：在兴趣区域范围内,存在信号发生器生成的信号穿过被成像目标；2)将数据传送到上位机进行预处理,包括清洗、过滤；3)兴趣区域里没有兴趣目标,即在空场景离线条件下,采集并保存传感器的数据,作为区域信号强度的基准值y<Sub>0</Sub>；4)兴趣区域存在兴趣目标,即在线条件下,采集并保存t时刻信号接收器数据,作为区域信号强度的实时测量值<Image id="icf0001" he="93" wi="122" file="FDA0002136348680000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>即在线条件下的信号强度观测值；5)向量化处理信号传输网络中的信号强度值,得到相应的观测值,然后采用信号强度在空场景离线条件以及存在目标的在线条件下的变差值作为观测值y；具体地,          <Image id="icf0002" he="83" wi="700" file="FDA0002136348680000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,<Image id="icf0003" he="75" wi="93" file="FDA0002136348680000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为在线条件下的信号强度观测值,y<Sub>0</Sub>为基准值,维度均为R<Sup>N×1</Sup>,N为门竖直平面上的从信号发生器到信号接收器形成的信号链路总数；6)将兴趣区域分为满足预定条件大小的像素块,根据信号发生器与信号接收器的个数,计算出穿过第x<Sub>i</Sub>像素块的直射信号个数,记为cnt(x<Sub>i</Sub>),x<Sub>i</Sub>∈x,i∈[1,2,3,4...],x为所有像素块构成的衰减成像图；其中,所述预定条件大小为使获得成像效果最好的像素块大小；7)将当前t时刻检测到的信号强度值<Image id="icf0004" he="73" wi="94" file="FDA0002136348680000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>与基准数据进行比较,得到信号强度变化量y；8)利用信号的传播特性得出的椭圆模型设计测量矩阵,构建出成像的数学模型y＝φx+n,得到在t时刻第x<Sub>i</Sub>像素块的阴影衰减值；其中,y为步骤7)变差法得到的当前时刻信号强度的变化量,φ为根据椭圆模型设计的测量矩阵,n为测量噪声；由椭圆模型设计得到的测量矩阵,其具体地表示及含义为：<Image id="icf0005" he="95" wi="408" file="FDA0002136348680000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>φ的每个列向量表示特定某像素对所有链路的权重因子,T为对向量求转置,N为门禁竖直平面上的从信号发生器到信号接收器形成的信号链路总数；9)将得到的阴影衰减值进行规范化标准操作,所述规范化标准操作是指为减小因步骤6)中每个像素块的cnt(x<Sub>i</Sub>)不一致给衰减值带来的影响,设定第一阈值,进行均值处理,其中具体阴影衰落密度估计为：          <Image id="icf0006" he="173" wi="700" file="FDA0002136348680000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,p′(x<Sub>i</Sub>,t)为计算得到的阴影衰减值,α为设定的第一阈值；10)设定第二阈值去除伪影,以提高成像质量,其具体表达式为：          <Image id="icf0007" he="149" wi="700" file="FDA0002136348680000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,β为设定的第二阈值,阴影衰落密度不大于β时,认为该处为伪影,将其密度置0,从而提高成像质量；11)利用不同t时刻的数据得到随时间变化的剖面图,将所有剖面图合并起来就能够获得运动目标的3D影像,即为：          <Image id="icf0008" he="64" wi="700" file="FDA0002136348680000031.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,<Image id="icf0009" he="73" wi="597" file="FDA0002136348680000032.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>分别为在时刻t<Sub>1</Sub>,t<Sub>2</Sub>,…t<Sub>M</Sub>时的阴影衰落密度；12)建立3D影像数据库,将当前3D影像与数据库中的影像比对以识别目标身份,计算模块将3D影像数据传送到控制模块,并由控制模块决定是否触发警报模块以及确定警报级别；其中,所述计算模块完成信号存储、计算、成像任务；所述控制模块完成配置、判断、定时任务；所述警报模块完成提醒、警告、紧急、报警任务；所述影像比对方式为提取当前3D影像的形状特征与运动边缘直方图特征,形成该运动3D影像的最终表达方式,并与数据库作比较,最高相似度低于阈值,则认为能触发警报模块。</td>   <td>G06K9/00;G06T15/00;G06K9/32</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李茁;                   马宇       </td>   <td>中山大学</td>   <td>压水堆组件形状因子参数化方法</td>   <td>广东省</td>   <td>CN110427681A</td>   <td>2019-11-08</td>   <td>本发明是压水堆组件形状因子参数化方法。本发明组件形状因子参数化方法以本征正交分解为基础,在于将组件形状因子分解为具有空间特征的本征正交基及随组件状态参数变化的本征正交系数两部分,将本征正交系数作为一种组件均匀化少群常数进行少群常数参数化,以快速获取准确的组件形状因子为目的。本发明将组件形状因子的空间特性存储于本征正交基中,可以提高组件形状因子参数化的计算精度；将本征正交系数作为一种组件均匀化少群常数进行少群常数参数化,可以明显减少组件形状因子参数化的计算量提高计算速度；本征正交系数随组件状态参数的变化规律更单调,更易于进行参数化,进而可以提高组件形状因子参数化的计算精度。</td>   <td>1.一种压水堆组件形状因子参数化方法,其特征是：包括如下步骤：步骤一：确定压水堆组件工况点数,视压水堆组件形状因子为二维矩阵；步骤二：对多个二维矩阵进行本征正交分解,得到本征正交基,以及本征正交系数；步骤三：取前M个本征正交基以及对应的本征正交系数,将本征正交基与本征正交系数相乘,得到乘积；步骤四：比较本征正交基与本征正交系数的乘积与压水堆组件形状因子,计算压水堆组件形状因子误差；步骤五：判断组件形状因子误差是否满足要求,重复步骤三至步骤五,直至搜索到满足要求的本征正交基的最小数量,并存储最小数量的一组本征正交基；步骤六：对M个本征正交系数进行常规截面参数化,得到M个本征正交系数截面参数化结果,存储本征正交系数参数化的结果；步骤七：根据存储的本征正交系数参数化结果,对本征正交系数进行回代,回代后的本征正交系数与存储的本征正交基相乘,得到压水堆组件形状因子。</td>   <td>G06F17/50;G06F17/16;G21C13/02</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄以华;                   张睿       </td>   <td>中山大学</td>   <td>一种基于二次迁移学习的虹膜身份验证方法</td>   <td>广东省</td>   <td>CN110427804A</td>   <td>2019-11-08</td>   <td>本发明提出一种基于二次迁移学习的虹膜身份验证方法,包括以下步骤：采集眼球虹膜图片,进行预处理后划分为训练集和测试集；将训练集图片输入已进行预训练的深度卷积神经网络中进行迁移学习的分类训练；对训练集图片进行三元组的构造,去除深度卷积神经网络中的全连接层,再输入三元组对深度卷积神经网络进行二次迁移学习训练；将测试集图片输入深度卷积神经网络,输出特征向量后与其对应的身份信息进行验证,若验证成功,深度卷积神经网络完成训练；否则,调整结构参数后对深度卷积神经网络重新进行二次迁移学习训练；将待验证的图片输入完成训练的深度卷积神经网络中,将输出特征向量与存储有人员身份的数据库进行距离匹配,输出身份验证结果。</td>   <td>1.一种基于二次迁移学习的虹膜身份验证方法,其特征在于,包括以下步骤：S1：采集眼球虹膜图片,对所采集的图片进行预处理后,划分为训练集和测试集；S2：将所述训练集图片输入已进行预训练的深度卷积神经网络中进行迁移学习的分类训练,得到完成迁移学习训练的深度卷积神经网络；S3：对所述训练集图片进行三元组的构造,并将S2步骤中完成迁移学习分类训练的深度卷积神经网络中的全连接层去除,再输入训练集图片的三元组对深度卷积神经网络进行二次迁移学习训练；S4：将所述测试集图片输入所述深度卷积神经网络,输出相应的特征向量后与测试集中对应的身份信息进行验证,若验证结果为成功,深度卷积神经网络完成训练；若验证结果为失败,则对深度卷积神经网络的结构参数进行调整后,跳转执行S3步骤；S5：将待身份验证的眼球虹膜图片输入所述完成训练的深度卷积神经网络中,输出得到相应的输出特征向量,然后将输出特征向量与存储有人员身份的数据库进行距离匹配,输出身份验证结果。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖正首;                   黄林冲       </td>   <td>中山大学</td>   <td>一种颗粒材料的颗粒状态分析方法、装置及设备</td>   <td>广东省</td>   <td>CN110414116A</td>   <td>2019-11-05</td>   <td>一种颗粒材料的颗粒状态分析方法包括：获取待分析的颗粒材料中的颗粒的几何形态,通过傅里叶级数表征所述颗粒材料的数学描述,所述数学描述包括颗粒形状描述符和颗粒位置描述符；根据所述颗粒材料的数学描述,判断颗粒之间是否接触；如果颗粒之间有接触,则根据所述颗粒材料的数学描述,计算颗粒之间的接触特征；根据所述颗粒之间的接触特征,计算颗粒之间的接触力,根据所计算的接触力更新颗粒的运动状态,得到颗粒材料的宏观力学响应。通过该数学描述方式,能够适用于任意形状颗粒,而且颗粒形状描述符保持常量,独立于颗粒的位置描述符,在颗粒发生移动或旋转时,不需要更新颗粒的形状描述符,运算量较低,计算效率较高。</td>   <td>1.一种颗粒材料的颗粒状态分析方法,其特征在于,所述颗粒材料的颗粒状态分析方法包括：获取待分析的颗粒材料中的颗粒的几何形态,通过傅里叶级数表征所述颗粒材料的数学描述,所述数学描述包括颗粒形状描述符和颗粒位置描述符；根据所述颗粒材料的数学描述,判断颗粒之间是否接触；如果颗粒之间有接触,则根据所述颗粒材料的数学描述,计算颗粒之间的接触特征；根据所述颗粒之间的接触特征,计算颗粒之间的接触力,根据所计算的接触力更新颗粒的运动状态；根据所获取的颗粒当前的状态以及颗粒在下一时刻的状态,通过时间步的迭代,获取颗粒在一时间段的力学响应与状态演化。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郭素素;              谢晓华;                   龚文勇       </td>   <td>中山大学</td>   <td>一种猴脸多属性联合识别方法</td>   <td>广东省</td>   <td>CN110414298A</td>   <td>2019-11-05</td>   <td>本发明公开一种猴脸多属性联合识别方法,包括：构建猴脸样本数据库：采集猴子视频和图片资料进行处理,基于Matlab生成猴脸样本数据库,且所有样本都具有猴属性类别的人工标记；生成人脸属性识别预训练模型：根据恒等映射的扰动的学习建立人脸属性识别预训练模型的ResNet50；训练猴脸多属性联合识别模型：基于MXNet平台,在人脸属性识别预训练模型的基础上,再利用猴脸样本数据库进行调优训练,获得猴脸多属性联合识别模型。本发明可以根据年龄和性别属性的共同特征实现同时识别两个属性的目标。</td>   <td>1.一种猴脸多属性联合识别方法,其特征在于,包括如下步骤：S10 构建猴脸样本数据库：采集猴子视频和图片资料进行处理,基于Matlab生成猴脸样本数据库,且所有样本都具有猴属性类别的人工标记；S20 生成人脸属性识别预训练模型：根据恒等映射的扰动的学习建立人脸属性识别预训练模型ResNet50；S30 训练猴脸多属性联合识别模型：基于MXNet平台,在人脸属性识别预训练模型的基础上,再利用猴脸样本数据库进行调优训练,获得猴脸多属性联合识别模型。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张嫚宁;              谢晓华;                   龚文勇       </td>   <td>中山大学</td>   <td>一种基于计算机视觉的猴脸亲缘关系分析方法</td>   <td>广东省</td>   <td>CN110414299A</td>   <td>2019-11-05</td>   <td>本发明公开一种基于计算机视觉的猴脸亲缘关系分析方法,包括基于手工标记的猴脸图像,运用Faster R-CNN模型训练猴脸检测器；基于猴脸检测器获取猴脸坐标并保存猴脸图像,结合已有的猴子身份,种群信息,建立一个猴脸图像数据库；设计猴脸亲缘关系验证算法,通过对深度卷积神经网络的训练对猴脸分析目标特征进行猴脸亲缘关系分析。本发明方法应用卷积神经网络挖掘利用更多猴脸信息来达到更准确的亲缘关系分析,对测试样本和训练样本之间的相关性无要求,当猴脸姿势、尺度发生较大变化时,仍然能准确判断。</td>   <td>1.一种基于计算机视觉的猴脸亲缘关系分析方法,其特征在于,包括如下步骤：S10基于手工标记的猴脸图像,运用Faster R-CNN模型训练猴脸检测器；S20基于猴脸检测器获取猴脸坐标并保存猴脸图像,结合已有的猴子身份,种群信息,建立一个猴脸图像数据库；S30设计猴脸亲缘关系验证算法,通过对深度卷积神经网络的训练对猴脸分析目标特征进行猴脸亲缘关系分析。</td>   <td>G06K9/00;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈汉嵘;              谢晓华;                   韦宝典       </td>   <td>中山大学</td>   <td>一种基于双摄头的列车车厢人群密度估计方法</td>   <td>广东省</td>   <td>CN110414301A</td>   <td>2019-11-05</td>   <td>本发明公开一种基于双摄头的列车车厢人群密度估计方法,包括：提出多视角人群密度估计网络,该网络由两部分组成,一部分是参数共享的卷积神经网络,另一部分是全连接层,该网络能区分当前列车车厢的人群密度等级。模型训练阶段,使用具有5类密度等级的样本进行迭代优化；模型应用阶段,依照地铁实际运行情况有规律抽样估计。本发明基于深度学习方法估计人群密度,采用卷积神经网络自动学习特征来取代以往手工设计的特征,以提高人群密度估计的准确率和鲁棒性。</td>   <td>1.一种基于双摄头的列车车厢人群密度估计方法,其特征在于,包括如下步骤：S10准备训练样本：建立包含4个参数共享的卷积层和5个全连接层的神经网络,输入同一车厢内相同时刻的两个不同视角的视频帧,训练具有密度等级的标签的样本,其中卷积层用于提取视频的特征向量,全连接层用于将卷积层所提取出的特征向量按密度等级进行分类；S20神经网络训练：数次迭代优化训练神经网络；S30应用阶段：截取双摄头拍摄的当前列车车厢的视频帧分别输入至优化后的神经网络,得到当前列车车厢的图像分类结果。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张贺晔;              李吉平;                   吴万庆       </td>   <td>中山大学</td>   <td>胶囊内窥镜图像的分类方法、装置、设备及介质</td>   <td>广东省</td>   <td>CN110414607A</td>   <td>2019-11-05</td>   <td>本申请提供了一种胶囊内窥镜图像的分类方法,所述方法涉及对胶囊内窥镜图像进行病变图像或非病变图像的区分,包括：利用人工神经网络的自学习能力,建立胶囊内窥镜图像中的图像特征与病变图像之间的对应关系；获取患者的当前胶囊内窥镜图像的当前图像特征；通过所述对应关系,确定与所述当前图像特征对应的当前病变图像；具体地,确定与所述图像特征对应的当前病变图像,包括：将所述对应关系中与所述当前图像特征相同的图像特征所对应的病变图像,确定为所述当前病变图像。泛化能力强,对胶囊内窥镜图像分类效果好,同时分类时间相比现有的方法更短,当有需要分类其他状况时人工神经网络可以完成多分类情况。</td>   <td>1.一种胶囊内窥镜图像的分类方法,所述方法涉及对胶囊内窥镜图像进行病变图像或非病变图像的区分,其特征在于,包括：利用人工神经网络的自学习能力,建立胶囊内窥镜图像中的图像特征与病变图像之间的对应关系；获取患者的当前胶囊内窥镜图像的当前图像特征；通过所述对应关系,确定与所述当前图像特征对应的当前病变图像；具体地,确定与所述图像特征对应的当前病变图像,包括：将所述对应关系中与所述当前图像特征相同的图像特征所对应的病变图像,确定为所述当前病变图像。</td>   <td>G06K9/62;G06K9/46;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张贺晔;              马成龙;              吴万庆;                   陈镇阳       </td>   <td>中山大学</td>   <td>心脏临床指标的检测方法、装置、设备及介质</td>   <td>广东省</td>   <td>CN110400298A</td>   <td>2019-11-01</td>   <td>本申请提供了一种心脏临床指标的检测方法、装置、设备及介质,包括：利用人工神经网络的自学习能力,建立心脏MR图像与指定参数之间的对应关系；其中,指定参数包括心脏CT图像,针对心脏MR图像的心脏临床指标,以及针对心脏CT图像的心脏临床指标；获取患者的当前心脏MR图像；通过对应关系,确定与当前心脏MR图像对应的当前指定参数；具体地,确定与当前心脏MR图像对应的当前指定参数,包括：将对应关系中与当前心脏MR图像相同的心脏MR图像所对应的指定参数,确定为当前指定参数。可用于MR与CT的不同成像模态下多类型心脏临床指标的评估。挖掘和表征多类型心脏临床指标之间的复杂关系、获取任务相关性并且实现其在不同成像模态的转移。</td>   <td>1.一种心脏临床指标的检测方法,其特征在于,包括：利用人工神经网络的自学习能力,建立心脏MR图像与指定参数之间的对应关系；其中,所述指定参数包括心脏CT图像,针对心脏MR图像的心脏临床指标,以及针对心脏CT图像的心脏临床指标；获取患者的当前心脏MR图像；通过所述对应关系,确定与所述当前心脏MR图像对应的当前指定参数；具体地,确定与所述当前心脏MR图像对应的当前指定参数,包括：将所述对应关系中与所述当前心脏MR图像相同的心脏MR图像所对应的指定参数,确定为所述当前指定参数。</td>   <td>G06T7/00;G16H50/30;G16H50/70</td>  </tr>        <tr>   <td>中国专利</td>   <td>         何明光;                   李治玺       </td>   <td>中山大学中山眼科中心</td>   <td>一种基于眼部特征的脑卒中人工智能筛查方法</td>   <td>广东省</td>   <td>CN110400301A</td>   <td>2019-11-01</td>   <td>本发明涉及了一种人工智能技术领域,尤其是涉及了一种基于眼部特征的脑卒中人工智能筛查方法,其具体步骤是：(1)标注及预处理已收集的眼底彩照；(2)构建及优化深度卷积神经网络模型；(3)获取待识别患者眼底图像,利用深度卷积神经人工智能模型对眼底特征进行识别,获得有无脑卒中的眼底图像分类结果；该方法能高效、便捷、无创地进行脑卒中高危人群筛查。</td>   <td>1.一种基于眼部特征的脑卒中人工智能筛查方法,其特征在于：(1)对数据库中,已收集并标注是否患有脑卒中的眼底彩照进行预处理,得到模型的训练集；(2)构建模块一：深度卷积神经网络模型,使用预处理后的训练集对深度卷积神经网络进行训练；(3)构建模块二：结合患者血糖、血脂等多模态数据,通过决策模块整合对训练模型进行进一步优化；(4)使用待测眼底图像,将待测眼底图像输入到训练后的深度卷积神经网络中,进而对有无脑卒中进行判别。</td>   <td>G06T7/00;G16H30/20;G16H50/20;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李明;                   赵志洁       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>构音障碍检测方法和系统</td>   <td>广东省</td>   <td>CN105719662B</td>   <td>2019-10-25</td>   <td>本发明涉及一种构音障碍检测方法和系统。上述构音障碍检测方法,包括如下步骤：读取电磁发音仪器产生的数据,获取音频数据及其同步的运动坐标数据；根据所述音频数据从所述运动轨迹信息上提取各个字词读音对应的子运动轨迹信息；将所述子运动轨迹信息与参考语音库中各个字词读音对应的参考运动轨迹信息进行特征运算,获取相似概率值；其中所述参考语音库为包括所述各字词正常发音的语音数据库；根据相似概率值对所述用户进行构音障碍检测。本发明提供的构音障碍检测方法和系统,可以利用数据中的各字词读音以及相应的子运动轨迹信息,使检测结果的准确性得到提高。</td>   <td>1.一种构音障碍检测方法,其特征在于,包括如下步骤：读取电磁发音仪器产生的语音数据,根据所述语音数据获取音频数据及其对应的运动轨迹信息；其中,所述电磁发音仪器的传感器安装在用户的发音位置,所述语音数据为用户根据设定字词进行发音时,电磁发音仪器在用户发音感应位置获取的数据；所述设定字词是指参考语音库所包括正常发音所对应的一个或者多个字词；根据所述音频数据从所述运动轨迹信息上提取各个字词读音对应的子运动轨迹信息；将所述子运动轨迹信息与所述参考语音库中各个字词读音对应的参考运动轨迹信息进行特征运算,获取相似概率值；其中所述参考语音库为包括所述各个字词正常发音的语音数据库；根据相似概率值对所述用户进行构音障碍检测。</td>   <td>G10L25/51;G10L25/66</td>  </tr>        <tr>   <td>中国专利</td>   <td>         衣杨;              程阳;              许楚萍;              兰天翔;                   王昭       </td>   <td>中山大学</td>   <td>基于显著轨迹和时空演化信息的视频人体行为识别方法</td>   <td>广东省</td>   <td>CN106529477B</td>   <td>2019-10-25</td>   <td>本发明提供一种基于显著轨迹和时空演化信息的视频人体行为识别方法,该方法充分利用视频中的光流信息,在改进密集轨迹的基础上,通过定义轨迹的静态显著性和动态显著性,并以线性融合方式,计算得到轨迹的组合显著性,从而有效移除背景运动轨迹,提取前景运动轨迹；针对传统基于底层视觉特征表示方法忽略了行为视频中丰富的中高层语义信息问题,提出中层视觉特征表示即轨迹束,从中提取人体行为时空演化信息作为视频特征表示,有效去除背景轨迹,提取前景运动轨迹并显著提高算法的识别效果。</td>   <td>1.一种基于显著轨迹和时空演化信息的视频人体行为识别方法,其特征在于,包括以下步骤：S1：对每一帧视频图像进行人体检测并构建多尺度时空金字塔,再对视频帧中的时空兴趣点密集采样,并判断时空兴趣点所在后续帧的位置,将其加入到轨迹序列中,采用中心点-外围显著性方法计算视频帧的静态显著性和动态显著性,并通过线性融合方式得到视频帧的组合显著性；S2：将轨迹显著性定义为轨迹每点在组合显著性图像中显著性的均值,计算显著性阈值,当轨迹显著性小于该阈值时,则认为是背景轨迹而予以删除,从而有效提取前景运动轨迹；S3：采用最大似然估计算法建立高斯混合模型,对于每一视频帧序列,利用提取到的前景运动轨迹,生成中层视觉特征表示即轨迹束；S4：根据视频中所有的特征表示轨迹束,利用大数据线性分类模型,求解分类超平面,提取其中的运动时空演化信息作为视频特征表示；所述步骤S1的具体过程如下：S11：对给定视频帧,计算梯度矩阵,初始化密集采样时空兴趣点作为轨迹起始点；S12：将第j帧上每个特征点p<Sub>j</Sub>＝(x<Sub>j</Sub>,y<Sub>j</Sub>),通过中值滤波后的密集光流场f＝(u<Sub>t</Sub>,v<Sub>t</Sub>)跟踪至第j+1帧,公式计算如下：          <Image id="icf0001" he="81" wi="700" file="FDA0002138589250000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        S13：采用中心点-外围区域的灰度差异计算单帧的静态显著性C<Sub>A</Sub>,公式计算如下：C<Sub>A</Sub>(x<Sub>li</Sub>)＝|g(x<Sub>li</Sub>)-g(A(x<Sub>li</Sub>))|其中g为经过高斯滤波处理的灰度图像,g(A(x<Sub>li</Sub>))是点x<Sub>li</Sub>的外围区域的灰度平均值；记C<Sub>M</Sub>为单帧的运动显著性,计算中心点和外围区域光流直方图的卡方距离,得到单帧的运动显著性,公式计算如下：          <Image id="icf0002" he="98" wi="700" file="FDA0002138589250000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中h<Sub>j</Sub>(x<Sub>li</Sub>)为点x<Sub>li</Sub>处,HOF描述符第j个bin的值,而h<Sub>j</Sub>(A(x<Sub>li</Sub>))为外围区域的光流平均值；记C<Sub>C</Sub>为单帧的组合显著性,通过线性组合的方式来计算C<Sub>C</Sub>：          <Image id="icf0003" he="68" wi="700" file="FDA0002138589250000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image></td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘镇;              王向东;              明伟华;                   周翠英       </td>   <td>中山大学</td>   <td>一种地质剖面图同岩性地层间界线接触在线检测与消去方法</td>   <td>广东省</td>   <td>CN110378984A</td>   <td>2019-10-25</td>   <td>本发明涉及一种地质剖面图同岩性地层间界线接触在线检测与消去方法,属于地理信息技术与地质学的交叉领域。其特征是：根据已获取的二维同岩性数组、一维地层序列数组、二维层深度数组,建立了相应的二维标志位数组,该二维标志位数组可以用来确定相同岩性但是不同层号地层间界线是否接触,也即是该界线是否需要绘制,有效的解决了地层界线的接触检测,并采用单一地层线相邻钻孔间单独绘制的方法绘制界线,能更精确的控制所有地层界线的显隐。</td>   <td>1.一种地质剖面图同岩性地层间界线接触在线检测与消去方法,其特征在于包括以下步骤：(1)将需要绘制的地层层号按从上到下的绘制顺序排列在一维层序列数组A(1,a)中；(2)将岩性相同的层号存储在一维同岩性数组中,若有多个岩性都存在这种情况时,该数组扩展成为二维同岩性数组B(i,b)；(3)将每个钻孔的层深度信息(包括层顶深度和层底深度)统一于一个二维层深度数组D(n+1,h)中,其层序列和一维层序列数组保持一致,并多出一组；(4)创建一个比二维层深度数组小一列的,默认值为1的二维标志位数组E(n+1,h-1),该数组与层深度数组对应；(5)将二维同岩性数组中的每一个一维同岩性数组拿出对一维层序列数组进行查验,如果该一维层序列数组中存在两个或以上同岩性数组中的层号,则将该层号在一维层序列数组中的位置取出存放于一个一维同岩性位置数组C(1,c)中；(6)进行地层边界接触检测,用一维同岩性位置数组信息顺序提取同岩性层号的层深度信息,如果相邻两钻孔的该层层底深度与后续某一层层顶部深度相同,那么令与该层的层底和后续某一层层顶的相同位置的标志位数组为0,并令两层中间的标志位全部为0；(7)绘制界线,如果与两相邻钻孔的深度层对应的层标志位为1,那么该深度层界线需要绘制,如果与两相邻钻孔的深度层对应的层标志位为0,那么该界线不需要绘制。</td>   <td>G06T11/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         邹小兵;              潘悦然;                   蔡昆京       </td>   <td>昆山杜克大学;中山大学附属第三医院</td>   <td>基于微笑范式和音视频行为分析的孤独症早期筛查系统</td>   <td>江苏省</td>   <td>CN110363129A</td>   <td>2019-10-22</td>   <td>本发明公开了一种基于微笑范式和音视频行为分析的孤独症早期筛查系统,包括：数据采集模块,用于采集试验全程的音视频数据；预处理模块,用于同步对齐采集的音视频,分段落标记不同逗笑刺激的时间和类型；特征提取模块,对预处理获取数据,分别进行逐帧分析各项特征；训练分类模块,用于对分段落视频数据提取的特征训练分类器,及整个范式训练孤独症风险系数预测分类器模型；预测模块,对提取的特征通过使用分类器模型,分段落打分,并对整个范式打分。本发明使用与孤独症的早期筛查,使筛查试验更标准化、结构化,试验评估更准确更易读易解释。</td>   <td>1.一种基于微笑范式和音视频行为分析的孤独症早期筛查系统,其特征在于,包括：数据采集模块,用于采集逗笑试验过程中被试者、评估者和道具的多个RGB-D摄像头视角多声道的音视频多模态数据；预处理模块,用于同步采集到的多视角多声道的音视频数据,检测并标记不同逗笑刺激发出的时间以供后续分析；特征提取模块,用于对预处理获取的被试者、评估者全部角度的视频数据段落,分别进行逐帧分析处理,获取脸部、朝向、目光和手势的特征；训练分类模块,对分段落视频数据提取的特征进行训练,得到分段落范式打分的分类器模型及整个范式的孤独症预测分类器模型；预测模块,采用特征提取模块对整个范式视频进行分段落的脸部、朝向、目光和手势特征提取,并使用分类器模型对范式分段落进行打分,对测试者的孤独症风险进行评估预测。</td>   <td>G06K9/00;G16H50/20;G16H50/30;G10L15/26;G10L25/66</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张贺晔;              吴欣洋;              吴万庆;                   陈镇阳       </td>   <td>中山大学</td>   <td>免造影剂的心肌梗死面积的检测方法、装置、设备及介质</td>   <td>广东省</td>   <td>CN110363755A</td>   <td>2019-10-22</td>   <td>本申请提供了一种免造影剂的心肌梗死面积的检测方法、装置、设备及介质,应用于通过无造影剂获取的心脏磁共振图像序列的心肌梗死面积检测,包括：利用人工神经网络的自学习能力,建立心脏磁共振图像序列中的运动特征与心肌梗死面积之间的对应关系；获取患者的当前心脏磁共振图像序列的当前运动特征；通过对应关系,确定与当前运动特征对应的当前心肌梗死面积；具体地,确定与运动特征对应的当前心肌梗死面积,包括：将对应关系中与当前运动特征相同的运动特征所对应的心肌梗死面积,确定为当前心肌梗死面积。患者拍摄心脏磁共振图像时无需使用造影剂,避免了中毒风险；与其他无造影剂技术相比,本发明效率更高,结果更精准,成本更低。</td>   <td>1.一种免造影剂的心肌梗死面积的检测方法,应用于通过无造影剂获取的心脏磁共振图像序列的心肌梗死面积检测,其特征在于,包括：利用人工神经网络的自学习能力,建立心脏磁共振图像序列中的运动特征与心肌梗死面积之间的对应关系；获取患者的当前心脏磁共振图像序列的当前运动特征；通过所述对应关系,确定与所述当前运动特征对应的当前心肌梗死面积；具体地,确定与所述运动特征对应的当前心肌梗死面积,包括：将所述对应关系中与所述当前运动特征相同的运动特征所对应的心肌梗死面积,确定为所述当前心肌梗死面积。</td>   <td>G06T7/00;G06T7/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         罗洁;              林泽帆;                   林佳吟       </td>   <td>中山大学</td>   <td>一种基于CT图像的多触点电极自动分割方法</td>   <td>广东省</td>   <td>CN110363778A</td>   <td>2019-10-22</td>   <td>本发明涉及医学影像处理技术领域,一种基于CT图像的多触点电极的自动分割方法,包括以下步骤：S1.图像预处理；S2.利用电极灰度与人体组织不同的特点初步分割采用阈值分割进行；S3.利用形态学方法,将阈值分割后图像中属于同一电极的触点连接起来,并对识别出的电极进行编号；S4.利用统计学方法,将电极主轴提取出来并根据电极规格分割触点,得到触点分割结果。本方法是一种具有高鲁棒性、易实现的基于CT图像的多触点电极的自动分割方法。该方法可以在不同CT金属伪影条件下,更有效的去除各种干扰,稳健地对多触点脑外科植入电极进行自动分割。该方法考虑了CT图像中各类可引起干扰的情况,对于不同CT伪影强度有较好的适应性。</td>   <td>1.一种基于CT图像的多触点电极的自动分割方法,其特征在于,包括以下步骤：S1.图像预处理；S2.对预处理后的图像,利用电极灰度与人体组织不同的特点采用阈值分割方式进行初步分割；S3.利用形态学方法,将阈值分割后图像中属于同一电极的触点连接起来,并对识别出的电极进行编号；S4.利用统计学方法,将电极主轴提取出来并根据电极规格分割触点,得到触点分割结果。</td>   <td>G06T7/11;G06T7/136;G06T7/187</td>  </tr>        <tr>   <td>中国专利</td>   <td>         朱斌;              高冠明;                   陆永恩       </td>   <td>佛山市第一人民医院(中山大学附属佛山医院)</td>   <td>一种麻精类药品二维码防伪认证方法及装置</td>   <td>广东省</td>   <td>CN110348860A</td>   <td>2019-10-18</td>   <td>本发明公开了一种麻精类药品二维码防伪认证方法及装置,针对现有方法中存在的二维码标签的防伪认证认证方法的不足之处,认证方法的后端服务器的数据库存储二维码标签的相关信息；令二维码解码器具有一个伪随机数发生器PRNG,并能执行Hash计算,异或(XOR)逻辑操作。在流通过程中防止合法标签被重复使用,可以完全保证药品的合法性,加了强医疗机构麻醉药品、精神药品处方管理,保证了患者正常医疗需求,抵御麻醉药品、精神药品流入非法渠道,在物流、仓储、开药的过程中可以实现药品溯源,可以实现药品或处方单的防伪溯源。</td>   <td>1.一种麻精类药品二维码防伪认证方法,其特征在于,所述方法包括以下步骤：步骤1,将二维码标签在数据库中进行初始化；步骤2,二维码解码器生成随机数；步骤3,通过随机数读取二维码标签并向数据库发送查询消息认证请求；步骤4,认证请求通过后授权访问消息和二维码标签相关信息传输到二维码解码器；步骤5,二维码解码器根据授权访问消息和二维码标签相关信息对二维码标签进行验证。</td>   <td>G06Q30/00;G06K7/14;G06K19/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;              白善荣;                   夏俊       </td>   <td>中山大学</td>   <td>一种白内障手术显微镜下单目深度估计方法</td>   <td>广东省</td>   <td>CN110349197A</td>   <td>2019-10-18</td>   <td>本发明涉及一种白内障手术显微镜下单目深度估计方法,通过显微镜系统拍摄两张图像,通过对比计算获得第一张图像中准确的测试物点,再通过代价函数找到测试物点的匹配点,获得两个点之间的视差,从而计算出测试物点的深度。通过显微镜系统获得测试物点的平面坐标后,通过计算获得测试物点的深度,从而在手术的过程的位置转换为三维坐标,为自动手术提供位置的定位。</td>   <td>1.一种白内障手术显微镜下单目深度估计方法,其特征在于,包括以下步骤：步骤一：使用显微镜摄像系统以a毫米垂直位移拍摄两幅图像；步骤二：对每幅图像进行预处理,转换为具有灰度值的图像,并对第一幅图像进行角点检测,得到若干候选角点,并且得到所述角点的坐标；步骤三：若干所述角点以各自为中心,R<Sub>j</Sub>为半径,求出相对于角点的黑点和白点,并且黑点位于角点白色像素区域和白点位于角点的黑色区域,且黑点的像素灰度&lt;0.5,白点的像素灰度&gt;0.5,则该角点为测试物点P′<Sub>i</Sub>,黑点和白点位置的计算公式如下：          <Image id="icf0001" he="191" wi="700" file="FDA0002117644790000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0002" he="192" wi="688" file="FDA0002117644790000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0003" he="186" wi="700" file="FDA0002117644790000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0004" he="186" wi="690" file="FDA0002117644790000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0005" he="68" wi="700" file="FDA0002117644790000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,X<Sub>blacki</Sub>和Y<Sub>blacki</Sub>分别为黑点的坐标值,X<Sub>whitei</Sub>和Y<Sub>whitei</Sub>分别白点的坐标值,X<Sub>j</Sub>和Y<Sub>j</Sub>是以角点为中心,R<Sub>j</Sub>为半径的范围内所有像素点的各自坐标,I为该点的灰度值,range为选取的半径最大值；X<Sub>i</Sub>、Y<Sub>i</Sub>分别为测试物点P′的坐标值；步骤四：对第一幅图像P′<Sub>i</Sub>向第二幅图像进行匹配,得到匹配点P″<Sub>i</Sub>,P′<Sub>i</Sub>和P″<Sub>i</Sub>之间的的像素距离；步骤五：测试物点的深度计算公式如下：          <Image id="icf0006" he="113" wi="349" file="FDA0002117644790000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,r为测试点P′与显微镜放大中心之间的距离,Δr为P′<Sub>i</Sub>和P″<Sub>i</Sub>之间的的像素距离,称为视差。</td>   <td>G06T7/55</td>  </tr>        <tr>   <td>中国专利</td>   <td>         潘子潇;              谢晓华;                   尹冬生       </td>   <td>中山大学</td>   <td>一种跨摄像头的行人轨迹匹配方法</td>   <td>广东省</td>   <td>CN106887014B</td>   <td>2019-10-15</td>   <td>本发明涉及一种跨摄像头的行人轨迹匹配方法,包括以下步骤：S1.提取目标摄像头的一条行人轨迹作为目标轨迹,然后将其余摄像头在该时间段内出现的所有行人轨迹作为候选轨迹；S2.使用中国连锁餐厅过程训练分层狄利克雷过程,提取所有轨迹的全局运动模式特征,同时获得目标轨迹和各条候选轨迹在全局运动模式特征上的特征权重；S3.分别计算目标轨迹特征权重与各条候选轨迹特征权重之间的余弦距离作为相似性度量,然后将余弦距离最小的候选轨迹作为目标轨迹的匹配轨迹。</td>   <td>1.一种跨摄像头的行人轨迹匹配方法,其特征在于：包括以下步骤：S1.提取目标摄像头的一条行人轨迹作为目标轨迹,然后将其余摄像头在该时间段内出现的所有轨迹作为候选轨迹；S2.使用中国连锁餐厅过程训练分层狄利克雷过程,提取所有轨迹的全局运动模式特征,同时获得目标轨迹和各条候选轨迹在全局运动模式特征上的特征权重；S3.分别计算目标轨迹特征权重与各条候选轨迹特征权重之间的余弦距离作为相似性度量,然后将余弦距离最小的候选轨迹作为目标轨迹的匹配轨迹；所述中国连锁餐厅过程训练分层狄利克雷过程,提取所有轨迹的全局运动模式特征,同时获得目标轨迹和各条候选轨迹在全局运动模式特征上的特征权重的具体过程如下：S11.定义t<Sub>ji</Sub>为第j个轨迹中第i个观察值x<Sub>ji</Sub>所属的集合,t<Sub>ji</Sub>的取值有如下关系：          <Image id="icf0001" he="119" wi="700" file="FDA0002019074000000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        t、k、θ、x分别表示所有t<Sub>ji</Sub>、k<Sub>jt</Sub>、θ<Sub>k</Sub>、x<Sub>ji</Sub>组成的集合,α<Sub>0</Sub>表示狄利克雷过程G<Sub>j</Sub>的集中参数,t表示t<Sub>ji</Sub>的实际取值,f(·)表示多项式分布的概率密度函数,<Image id="icf0002" he="75" wi="77" file="FDA0002019074000000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示去除第i个观察值后,第j个轨迹中集合t的观察值数量,t<Sup>-ji</Sup>表示去除集合t<Sub>ji</Sub>后,第j个轨迹的其余观察值集合；S12.定义k<Sub>jt</Sub>为第j个轨迹中集合t的运动模式,定义S<Sub>t</Sub>为集合t上的观察值,k<Sub>jt</Sub>的取值有如下关系：          <Image id="icf0003" he="140" wi="700" file="FDA0002019074000000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        k<Sup>-jt</Sup>表示去除观察值集合t后,第j个轨迹的其余观察值集合所属的运动模式集合；γ为狄利克雷过程G<Sub>0</Sub>的集中参数；<Image id="icf0004" he="73" wi="94" file="FDA0002019074000000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示所有轨迹的观察值集合中去除集合t后,余下的集合中属于运动模式k的数量；S13.定义θ<Sub>k</Sub>为第k种运动模式,定义S<Sub>k</Sub>为所有轨迹中属于运动模式k的观察值,θ<Sub>k</Sub>的取值有如下关系：          <Image id="icf0005" he="126" wi="700" file="FDA0002019074000000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        h(θ<Sub>k</Sub>)表示基分布H的概率密度函数,θ<Sup>-k</Sup>表示去除运动模式k后,其余运动模式的取值的集合；S14.统计第j个轨迹中所有观察值集合的运动模式种类情况,即为特征权重{π}：          <Image id="icf0006" he="177" wi="345" file="FDA0002019074000000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        T表示第j个轨迹中集合t的元素个数,其中<Image id="icf0007" he="78" wi="125" file="FDA0002019074000000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是狄拉克δ函数,具有如下性质：          <Image id="icf0008" he="166" wi="475" file="FDA0002019074000000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image></td>   <td>G06T7/292;G06T7/246;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李熙莹;                   陈思嘉       </td>   <td>中山大学</td>   <td>一种基于彩色立体标定物的无人机标定方法及系统</td>   <td>广东省</td>   <td>CN106651961B</td>   <td>2019-10-11</td>   <td>本发明公开了一种基于彩色立体标定物的无人机标定方法及系统,方法包括：将彩色棋盘格立体标定物放置到待拍摄场景内；采用无人机至少从3个不同的方位拍摄彩色棋盘格立体标定物的图像；根据拍摄的彩色棋盘格立体标定物的图像采用灭点理论线性求解出无人机摄像机内参数；根据无人机摄像机内参数采用坐标投影变换方法确定无人机摄像机的空间位置和图像几何约束关系。本发明采用了彩色棋盘格立体标定物来进行摄像机标定,易于准确测量、检测精度高、便于安放和通用性强；只需至少从3个不同的方位拍摄彩色棋盘格立体标定物的图像并结合灭点理论得到无人机摄像机内参数来完成摄像机内参数的标定,使用起来更方便。本发明可广泛应用于计算机视觉领域。</td>   <td>1.一种基于彩色立体标定物的无人机标定方法,其特征在于：包括以下步骤：将彩色棋盘格立体标定物放置到待拍摄场景内,所述彩色棋盘格立体标定物为封闭式立体结构,所述彩色棋盘格立体标定物包含有至少一个顶面和一个侧面,所述彩色棋盘格立体标定物的每个表面采用彩色与白色相间、彩色与黑色相间、不同彩色相间或黑色与白色相间的棋盘格图案且任意相邻两个表面的棋盘格图案的颜色组合均不相同；采用无人机至少从3个不同的方位拍摄彩色棋盘格立体标定物的图像；根据拍摄的彩色棋盘格立体标定物的图像采用灭点理论线性求解出无人机摄像机内参数；根据无人机摄像机内参数采用坐标投影变换方法确定无人机摄像机的空间位置和图像几何约束关系；所述根据拍摄的彩色棋盘格立体标定物的图像采用灭点理论线性求解出无人机摄像机内参数这一步骤,其包括：确定无人机摄像机的模型以及空间点齐次坐标与图像点齐次坐标的映射关系：若无人机摄像机的模型为针孔相机模型,则任一空间点P的空间点齐次坐标M与图像点齐次坐标m的映射关系表达式为：λm＝K[RT]M,其中,λ为给定的比例因子,K为无人机摄像机的内参数矩阵,R为世界坐标系到摄像机坐标系的旋转矩阵,T为世界坐标系到摄像机坐标系的平移向量；根据拍摄的彩色棋盘格立体标定物的图像和灭点理论的约束方程,采用最小二乘法和张正友平面标定法线性求解出无人机摄像机的内参数矩阵。</td>   <td>G06T7/80;G06T7/90;G01C11/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张琳;                   刘恒       </td>   <td>中山大学</td>   <td>基于循环谱估计的深度学习智能调制识别方法</td>   <td>广东省</td>   <td>CN110321953A</td>   <td>2019-10-11</td>   <td>本发明提供的基于循环谱估计的深度学习智能调制识别方法,包括以下步骤：根据载波频率和码元速率生成调制信号；对调制信号进行循环谱估计,提取循环谱函数的截面图；将截面图作为特征,训练深度神经网络；利用深度神经网络对未知信号的调制方式进行识别。本发明提供的基于循环谱估计的深度学习智能调制识别方法,将循环谱估计与深度神经网络结合起来,利用神经网络的智能处理能力和循环谱较好的分类识别能力,提高了整个信号调制识别系统的性能；仅利用了循环谱函数的截面图,省去了提取循环谱特征的步骤,降低了算法时间复杂度。</td>   <td>1.基于循环谱估计的深度学习智能调制识别方法,其特征在于,包括以下步骤：S1：根据载波频率和码元速率生成调制信号；S2：对调制信号进行循环谱估计,提取循环谱函数的截面图；S3：将截面图作为特征,训练深度神经网络；S4：利用深度神经网络对未知信号的调制方式进行识别。</td>   <td>G06K9/62;G06N3/02</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈旭;              周知;                   武琼       </td>   <td>中山大学</td>   <td>基于因子图模型的移动用户位置预测方法</td>   <td>广东省</td>   <td>CN110322067A</td>   <td>2019-10-11</td>   <td>本发明公开了一种基于因子图模型的移动用户位置预测方法,步骤如下：对用户的移动轨迹数据进行聚类,并剔除一些少见的位置点,得到用户的候选位置集合；从用户的历史记录日志中提取用户的时间特征,位置特征,网络状态特征和社交特征；对用户不同类型特征之间的关联情况用因子进行定义和表示,并构建因子图模型；训练该模型,并对用户下一时刻的位置进行预测。该发明综合考虑用户相邻时刻位置之间的时空关联性,以及用户的网络状态、社交行为对位置的影响,并用不同类型的因子来刻画不同类型特征对位置的影响,利用因子图模型将不同类型特征之间的关联进行有效地融合,该方法能显著提高移动用户位置预测的准确率。</td>   <td>1.一种基于因子图模型的移动用户位置预测方法,其特征在于,所述的移动用户位置预测方法包括下列步骤：S1、提取用户候选位置集合,即对用户的移动轨迹数据进行聚类,剔除用户不常去的位置点,从而产生该用户的候选位置点,组成候选位置集合；S2、提取用户在不同时刻下的网络状态,不同社交行为类型的频率以及位置点,得到用户的时间特征、网络状态特征、社交特征以及位置特征；S3、建立移动用户行为序列,即将提取到的用户特征按照时间顺序转化为序列,序列中每个元组的格式为[时间特征,位置特征,网络状态特征,社交特征]；S4、对不同类型特征之间的关联情况进行定义,并利用因子图模型将所有特征有效地融合到一个统一的框架中；S5、对构建的因子图模型进行训练,并预测用户下一时刻的位置点。</td>   <td>G06Q10/04;G06K9/62;H04W4/02;H04W4/029</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;                   陈崇雨       </td>   <td>中山大学</td>   <td>一种基于稀疏正源分离模型的图像去模糊算法</td>   <td>广东省</td>   <td>CN110313016A</td>   <td>2019-10-08</td>   <td>一种基于稀疏正源分离模型的图像去模糊算法,用于对光学显微成像系统采集到的因衍射效应与光学偏差而产生的模糊图像进行处理,可以在单次感光成像并且不增加外部成像设备的情况下,将光学显微系统的空间分辨率提升到纳米量级。在所述方法中,显微成像的模糊过程被表示为成像系统点扩散函数的线性组合。将该过程嵌入正源分离的优化框架中,对其加入稀疏性约束并求解以去除模糊,从而实现高分辨显微成像。所述方法还包括对实际光学显微镜的预处理步骤,通过去除显微图像中的背景散射等干扰,使得输入的模糊图像更符合所提的成像模型。有关实验表明,将单个模糊显微图像作为输入,所述方法取得了比其他方法更好的细节解析性能。</td>   <td>PCT国内申请,权利要求书已公开。</td>   <td>G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              何彦东;              邓铭豪;              陈荣军;              谢舜道;              苏宏雄;              朱雄泳;                   曾衍瀚       </td>   <td>中山大学</td>   <td>基于改进Retinex与对数图像处理的低照度图像增强方法</td>   <td>广东省</td>   <td>CN110298796A</td>   <td>2019-10-01</td>   <td>本发明提供了一种基于改进Retinex与对数图像处理的低照度图像增强方法,包括：计算样本图像的亮通道值,将该值作为样本图像的光照分量；利用现有的对数图像处理模型下的背景强度对光照分量进行自适应局部调整；结合Sobel边缘检测方法,对局部调整后的光照分量进行滤波细化；根据细化后的光照分量,基于Retinex理论得到增强图像。本发明提供一种基于改进Retinex与对数图像处理的低照度图像增强方法,在现有的对数图像处理模型下结合了Retinex理论,对样本图像进行了加强,有效地解决了Retinex算法可能存在的光晕效应以及过增强的问题,该方法能适应更多样的光照环境。</td>   <td>1.基于改进Retinex与对数图像处理的低照度图像增强方法,其特征在于,包括以下步骤：S1：计算样本图像的亮通道值,将该值作为样本图像的光照分量；S2：利用现有的对数图像处理模型下的背景强度对光照分量进行自适应局部调整；S3：结合Sobel边缘检测方法,对局部调整后的光照分量进行滤波细化；S4：根据细化后的光照分量,基于Retinex理论得到增强图像。</td>   <td>G06T5/00;G06T7/13</td>  </tr>        <tr>   <td>中国专利</td>   <td>         江颖;                   刘婷       </td>   <td>中山大学</td>   <td>一种基于各向异性基函数建立SPECT重构的方法</td>   <td>广东省</td>   <td>CN110298900A</td>   <td>2019-10-01</td>   <td>本发明公开了一种基于各向异性基函数建立SPECT重构的方法,包括下述步骤：S1：基于各向异性基函数建立SPECT重构的离散模型；S2：采用最大似然估计准则模拟SPECT重构,得到离散模型参数的估计值,建立SPECT重构的优化模型；S3：在SPECT重构的优化模型中引入两类不同的正则项,分别是HDCR和HFNR,建立描述SPECT重构的最小泛函；S4：采用不动点算法建立迭代模型,求解SPECT重构的优化模型。本发明建立SPECT重构的方法能够在不损坏图像质量的前提下减少SPECT检查中使用的放射试剂的剂量,并且能够有效消除由全变差正则化所导致的阶梯效应,同时能保持图像细节以及真实边界。</td>   <td>1.一种基于各向异性基函数建立SPECT重构的方法,其特征在于,包括下述步骤：S1：基于各向异性基函数建立SPECT重构的离散模型：S11：SPECT重构的离散模型采用连续积分方程表示,所述连续积分方程根据示踪剂分布活性函数和积分核函数构建,所述积分核函数包括衰减校正,康普顿散射校正和系统空间分辨率校正的影响；S12：采用各向异性基函数逼近连续积分方程中的示踪剂分布活性函数,构建离散积分模型；S13：构造分片多项式子空间X的基底函数和SPECT系统矩阵；S14：将离散积分模型表示为线性方程；S2：采用最大似然估计准则模拟SPECT重构,得到离散积分模型参数的估计值,建立SPECT重构的优化模型；S3：在SPECT重构的优化模型中引入两类不同的正则项,分别是HDCR和HFNR,建立描述SPECT重构的最小泛函；S4：采用不动点算法建立迭代模型,求解SPECT重构的优化模型。</td>   <td>G06T11/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         关学新;              沈勇婷;              徐文兵;                   姚清河       </td>   <td>中山大学</td>   <td>基于燃料电池热电联产系统的简易试验模型</td>   <td>广东省</td>   <td>CN110298127A</td>   <td>2019-10-01</td>   <td>本发明提供一种基于燃料电池热电联产系统的简易试验模型,属于氢气制备装置的模拟仿真的技术领域。本发明的简易试验模型,包括了微控制器、热力仿真器和电力仿真器,微控制器内设置有燃料电池动态模型的模拟模块,热力仿真器和电力仿真器分别为模拟燃料电池发热特性和电输出特性的仿真电路,且均受到燃料电池动态模型的模拟模块的控制。本发明可以避免真实的燃料电池测试组件,便面高成本和高安全风险的问题,还优化了电路结构,能够同时对电学和热力学特性进行研究,使得通过不同的模拟模块,就可以实现对不同的燃料电池进行测试,进一步降低生产成本。</td>   <td>1.一种基于燃料电池热电联产系统的简易试验模型,包括数字控制器、电力仿真器；所述数字控制器内置有燃料电池动态模型的模拟模块；所述电力仿真器为模拟燃料电池电特性的电路；数字控制器与电力仿真器之间具有通信线路；其特征在于：还包括热力仿真器,所述热力仿真器为模拟燃料电池内部热力特性的电路。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         余顺争;                   罗经伦       </td>   <td>中山大学</td>   <td>一种云安全服务功能树网络入侵检测系统</td>   <td>广东省</td>   <td>CN110298381A</td>   <td>2019-10-01</td>   <td>本发明涉及网络监控领域,更具体的,涉及一种云安全服务功能树网络入侵检测系统,所述的系统包括服务树拓扑编排模块、服务树拓扑映射模块、流特征数据库模块、全局资源监控模块；本发明利用网络功能虚拟化技术提供云安全资源；根据云安全态势灵活定制安全防御策略,在靠近网络攻击来源的方向部署云安全服务功能树,对可疑网络流量进行逐步细分识别；依据安全防御策略,可在细分后的云安全服务功能树分支中,依据当前分支的网络流特性调度相应的云安全VNF进行更细粒度处理,大大提高了网络的安全性。</td>   <td>1.一种云安全服务功能树网络入侵检测系统,其特征在于,包括服务树拓扑编排模块(1)、服务树拓扑映射模块(2)、流特征数据库模块(3)、全局资源监控模块(4)；所述流特征数据库模块(3)用于存储网络攻击的网络流特征数据,结合云安全态势选取相应的网络攻击流特征数据集构建相应的训练集；所述的服务树拓扑编排模块(1)用于结合云安全态势构建决策树分类模型,使用流特征数据库模块(3)中构建好的训练集对决策树模型进行训练与剪枝,将训练好的决策树分类模型传递给服务树拓扑映射模块(2)；所述的服务树拓扑映射模块(2)用于接收服务树拓扑编排模块(1)所构建的决策树分类模型,将决策树分类模型的决策规则匹配节点映射到相应的服务功能树节点中,在服务功能树节点完成对网络流量的匹配与分类；所述的全局资源监控模块(4)用于对流特征数据库模块(3)、服务树拓扑编排模块(1)以及服务树拓扑映射模块(2)中全网范围内的资源进行监控与维护,在云安全服务功能树拓扑的映射以及虚拟逻辑网络的构建过程中,提供底层基础设施的实际承载能力信息,优化VNF资源的映射和部署。</td>   <td>G06K9/62;H04L29/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              连丽娜;                   董佳羽       </td>   <td>中山大学</td>   <td>一种基于稀疏编码的人脸识别方法</td>   <td>广东省</td>   <td>CN106845376B</td>   <td>2019-10-01</td>   <td>本发明涉及一种基于稀疏编码的人脸识别方法,传统的稀疏编码方法往往事先假定编码残余满足一定的概率分布形式,如拉普拉斯分布或高斯分布,在此基础上提出了<I>l</I><Sub><I>1</I></Sub>或<I>l</I><Sub><I>2</I></Sub>范式来求解稀疏编码系数。这样的处理手段在一些情况下会严重影响人脸识别的鲁棒特性,特别是当存在遮挡、噪声或其他形式的干扰时。本发明提供的方法针对传统稀疏编码方法的不足之处,引入迭代优化思想,其主要解决了以下两个问题：一是无需预先假定残差的分布形式,避免了不合理的残差分布函数对人脸识别鲁棒性带来的影响；二是有选择地保留了一部分有用的像素点进行识别,在大大减少了计算量的同时,更好地解决了遮挡和像素损坏等问题,获得了更鲁棒的识别性能。</td>   <td>1.一种基于稀疏编码的人脸识别方法,其特征在于：包括以下步骤：S1.设训练样本集中包括有k个已知对象,其中第i个对象所包含的n<Sub>i</Sub>个训练样本表示为矩阵<Image id="icf0001" he="83" wi="627" file="FDA0002107478500000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中i＝1,2,…,k,v<Sub>ij</Sub>∈R<Sup>m</Sup>,j＝1,2,...,n<Sub>i</Sub>,v<Sub>ij</Sub>表示第i个对象的第j个训练样本对应的列向量,m表示v<Sub>ij</Sub>的维度,则训练样本集A可表示为：          <Image id="icf0002" he="64" wi="700" file="FDA0002107478500000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中<Image id="icf0003" he="143" wi="188" file="FDA0002107478500000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示训练样本的总个数；S2.设测试样本表示为y,令重构样本y<Sub>rec</Sub>初始化为所有训练样本的均值；S3.计算y与y<Sub>rec</Sub>之间的残差e＝y-y<Sub>rec</Sub>；S4.定义对角矩阵P＝diag(p<Sub>1</Sub>,p<Sub>2</Sub>,…,p<Sub>m</Sub>)；其中<Image id="icf0004" he="153" wi="403" file="FDA0002107478500000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>e<Sub>d</Sub>表示残差e的第d个分量,d＝1,2,…,m,τ表示设定的判决阈值；S5.定义Ψ＝{d|p<Sub>d</Sub>＝1,d∈{1,2,…,m}}为重构误差小于判决阈值的像素点位置集合,设Ψ中有c个元素,即Ψ＝{ψ<Sub>1</Sub>,ψ<Sub>2</Sub>,…,ψ<Sub>c</Sub>}；S6.从对角矩阵P中选择第ψ<Sub>1</Sub>,ψ<Sub>2</Sub>,…,ψ<Sub>c</Sub>行的元素构造投影矩阵P<Sup>*</Sup>∈R<Sup>c×m</Sup>；S7.计算编码向量x：<Image id="icf0005" he="103" wi="686" file="FDA0002107478500000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中P<Sup>*</Sup>y-P<Sup>*</Sup>Ax用来衡量鲁棒编码后的重构人脸向量与输入人脸向量y之间的偏差,λ代表拉格朗日乘子,||·||<Sub>1</Sub>表示l<Sub>1</Sub>范式约束；S8.计算重构样本y<Sub>rec</Sub>：y<Sub>rec</Sub>＝Ax；S9.如已收敛或达到最大迭代次数,则输出最后一次迭代得到的编码向量x：<Image id="icf0006" he="86" wi="529" file="FDA0002107478500000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中x<Sub>i</Sub>表示对象i的系数,<Image id="icf0007" he="93" wi="562" file="FDA0002107478500000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>否则再次执行步骤S3～S8；S10.将编码向量x中除x<Sub>i</Sub>外的成员替换为0,得到向量δ<Sub>i</Sub>∈R<Sup>n</Sup>,i的初始值为1,然后对测试样本y进行重构：          <Image id="icf0008" he="79" wi="257" file="FDA0002107478500000018.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0009" he="72" wi="58" file="FDA0002107478500000019.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示对象i重构的测试样本；S11.令i＝i+1,然后重复执行步骤S10,直至i&gt;k；S12.求取<Image id="icf0010" he="64" wi="240" file="FDA0002107478500000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>与测试样本y的重构误差,测试样本y所属的对象为重构误差最小的<Image id="icf0011" he="64" wi="50" file="FDA0002107478500000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>对应的对象：          <Image id="icf0012" he="98" wi="700" file="FDA0002107478500000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image></td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘丙军;              杨子博;              邱江潮;              谭学志;                   彭为       </td>   <td>中山大学</td>   <td>一种基于河道测站控制分析的多级水位流量关系确定方法</td>   <td>广东省</td>   <td>CN110287579A</td>   <td>2019-09-27</td>   <td>本发明涉及一种基于河道测站控制分析的多级水位流量关系确定方法,包括：S1.基于水流运动规律,通过分析水文测站的水力学属性和几何属性,建立水位流量模型；S2.根据河道实际情况,对复式河道水位变动时的控制阶段进行识别和组合,将复式河道等效划分为多个标准几何图形；S3.基于复式河道划分出的多阶段控制,建立相应的控制矩阵并求解出联合方程。本发明确定的水位流量关系曲线通过对测站水力学属性和几何属性的分析,充分考虑了测站断面的水流运动规律,使水位流量关系拟合线型的选取更加客观,拟合参数具有明确的物理意义,且可根据测站特性将水位变幅分成若干段,体现出复式河道水位流量的非线性关系,从而确定不同段拟合参数的合理范围。</td>   <td>1.一种基于河道测站控制分析的多级水位流量关系确定方法,其特征在于,包括以下步骤：S1.基于水流运动规律,通过分析水文测站的水力学属性和几何属性,建立水位流量模型；首先,将断面平均流速及断面面积分别用水位表示,得到流量关于水位表达式的一般形式：Q＝a(H-b)<Sup>c</Sup>    (1.1)式中Q为流量,H为水位,a为系数,c为指数,b为零流水位；然后,根据测站控制类型与断面形状进一步推导具体的水位流量关系式；包括测站控制识别、测站水力学属性分析、测站集合属性分析,对基于河道测站控制分析的水位流量关系式进行确定；S2.根据河道实际情况,对复式河道水位变动时的控制阶段进行识别和组合,将复式河道等效划分为多个标准几何图形；S3.基于复式河道划分出的多阶段控制,建立相应的控制矩阵并求解出联合方程。</td>   <td>G06F17/50;G06F17/16;G06F17/12;G01F1/00;G01C13/00;E02B1/02;E02B1/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;                   钟圳伟       </td>   <td>中山大学</td>   <td>一种基于相对速度的社会力模型的人群仿真方法</td>   <td>广东省</td>   <td>CN110276123A</td>   <td>2019-09-24</td>   <td>本发明涉及计算机图形学、人群仿真技术,具体为基于相对速度的社会力模型的人群仿真方法,包括以下步骤：使用改进后的基于相对速度的社会力模型对仿真实验进行模拟计算；得出仿真模拟数据,包括行人的速度大小、速度方向和位置；根据仿真模拟数据进行仿真建模,显示仿真结果；所述改进后的基于相对速度的社会力模型,与现有基于相对速度的社会力模型公式相比,其计算公式添加了行人当前位置下的社会心理力的计算项。本发明通过对社会力模型结合相对速度进行改进,能有效减少行人震荡问题和避免行人重叠问题,使得社会力模型在人群疏散仿真的应用上更具有真实性。</td>   <td>1.一种基于相对速度的社会力模型的人群仿真方法,其特征在于,包括以下步骤：使用改进后的基于相对速度的社会力模型对仿真实验进行模拟计算；得出仿真模拟数据,包括行人的速度大小、速度方向和位置；根据仿真模拟数据进行仿真建模,显示仿真结果；所述改进后的基于相对速度的社会力模型,与现有基于相对速度的社会力模型公式相比,其计算公式添加了行人当前位置下的社会心理力的计算项。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马争鸣;              冯子健;                   张扬       </td>   <td>中山大学</td>   <td>一种基于准直测地线分解与低维嵌入的数据降维算法</td>   <td>广东省</td>   <td>CN110276381A</td>   <td>2019-09-24</td>   <td>本发明涉及机器学习领域中流形学习的数据降维问题,提出了一种基于准直测地线分解与低维嵌入的数据降维算法。首先提出准直测线的概念以及分解原则,对高维数据集进行准直测地线分解,之后根据每个样本点所在准直测地线的情况利用起点、方向与测地距离进行低维预测,位于多条准直测地线的样本点会有多个预测值,通过最小化预测误差建立方程,得到低维空间中起点矩阵与方向矩阵的优化解,最后根据求出的优化解对各样本点进行迭代得到高维数据集的低维表示。本算法保证了分解后准直测地线总数远小于样本点总数,且准直测地线网络具有稳定的结构,利用较小的计算量得到稳定、准确的结果。</td>   <td>1.一种基于准直测地线分解与低维嵌入的数据降维算法,其特征在于：A.提出准直测地线的概念以及分解原则：最优测地线任意处的角度要大于等于给定值；测地线长度要大于等于3；每条最优测地线都必须与至少一条其余的最优测地线相交；最优测地线上未被覆盖的样本点数要尽可能多；B.每次随机选取一点为起点,遍历该点出发的所有测地线,依据上述原则进行筛选,选出一条最优测地线；C.重复权利要求1中A与B选择最优测地线,直至所有的最优测地线覆盖整个样本集,对于特殊数据可进行重复覆盖得到更为稳定的准直测地线网络；D.每个样本点可以由经过其的准直测地线的起点、方向与测地距离得到,位于多条准直测地线上的样本点则有多个预测值；E.求出各点的预测误差,通过最小化该误差求出起点矩阵与方向矩阵的优化解；F.根据求出的起点、方向与测地距离得到高维数据集的低维表示。</td>   <td>G06K9/62;G06N20/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         詹杰民;              喜扬洋;              胡文清;              林凯;              罗莹莹;                   余炜光       </td>   <td>中山大学</td>   <td>一种上呼吸道凹凸榫卯装配的实验装置及该装置建模方法</td>   <td>广东省</td>   <td>CN110264560A</td>   <td>2019-09-20</td>   <td>本专利涉及临床医学、生物力学和流体力学研究领域,提供一种上呼吸道凹凸榫卯装配的实验装置及该装置建模方法,所述装置包括上呼吸道实验模型主体,所述上呼吸道实验模型主体包括多个分块模型,所述多个分块模型通过榫卯接口结构拼接；所述上呼吸道实验模型主体包括至少三个测量仪器接口。与现有技术相比防止了3D打印内部支撑的生成,并且切口处采用榫卯结构进行拼接,使得物理模型具有防漏气、易拆卸等优点；模型与器材的接口采用凹型接口,确保了测量仪器连接处的稳定性与密封性,消除了测量过程中由仪器晃动带来的异常数据。另外,实验装置使用志愿者的真实呼吸作为驱动。</td>   <td>1.一种上呼吸道凹凸榫卯装配的实验装置,其特征在于,包括上呼吸道实验模型主体,所述上呼吸道实验模型主体包括多个分块模型,所述分块模型内含多个上呼吸道腔体,且分块模型的外缘与腔体边缘上设有凹凸接口结构,相连的两个分块模型之间通过凹凸接口结构拼接；所述上呼吸道实验模型主体包括至少三个测量仪器接口；所述凹凸接口结构为榫卯接口结构。</td>   <td>G06T17/00;G06T19/20;G09B23/34</td>  </tr>        <tr>   <td>中国专利</td>   <td>         蔡铭;              王海波;                   杨炜俊       </td>   <td>中山大学</td>   <td>一种应用于大区域交通噪声计算的优化方法</td>   <td>广东省</td>   <td>CN106407929B</td>   <td>2019-09-20</td>   <td>本发明公开一种应用于大区域交通噪声计算的优化方法,该方法可大大提高城市大区域交通噪声计算算法的效率。具体过程为：首先根据接收点到声源点之间的距离自动选取所需计算的交通源,忽略距离接收点较远的交通源；然后采用智能化自动划分网格的方法,在靠近交通源的地方划分比较密集的计算网格,在远离交通源的地方划分比较稀疏的计算网格；最后通过计算目标快速索引的方式,根据道路节点和建筑物角点自动识别计算区域并智能分区,将计算目标(交通源和建筑物)所在及周围共9个方块作为新的计算区域,并在新区域内进行交通噪声计算。本发明应用于大区域下交通噪声的传播衰减计算,可有效的解决大区域交通噪声计算时间效率过低的问题。</td>   <td>1.一种应用于大区域交通噪声计算的优化方法,其特征在于,包含以下步骤：(a)交通源自动筛选,基于接收点到交通源的距离自动筛选出需要计算的交通源；(b)智能化网格划分,以交通源为中心,在半径r内的计算区域划分密集的计算网格,在半径r外的计算区域划分稀疏的计算网格；(c)计算目标快速索引,根据道路节点和建筑物角点自动识别计算区域并智能分区,将以目标为中心的9方格计算区域作为新的计算区域,并在新的计算区域内进行交通噪声计算；其中所述目标是指交通源或建筑物；所述步骤(b)中,以交通源为中心,对所计算区域的面积和网格数量进行判断,当计算区域面积小于或等于r*r时,在计算区域划分密集的计算网格,即按照预设的步长或预设的数量的均匀划分计算网格,以进行交通噪声的计算；所述步骤(b)中,所计算区域面积大于r*r时或计算网格数量大于所预设的个数时,在计算区域划分稀疏集的计算网格；在计算区域划分稀疏集的计算网格的过程为：对计算区域进行分块,令每个分块内有m*m个接收点,然后针对每一分块进行判断,若分块内无交通源通过则进行网格的稀疏,只计算m/2*m/2个接收点的噪声值；若分块内有交通源通过,则依然计算m*m个接收点的噪声值；所述步骤(c)中,根据所有道路结点和建筑物角点的地理坐标自动识别出计算区域的范围,该计算区域是指由左下角坐标及右上角坐标构成的矩形区域,若计算区域的面积大于S’时,则进行计算目标快速索引。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郭开华;                   朱琳琳       </td>   <td>中山大学</td>   <td>一种基于贝叶斯网络的LNG安全风险评估系统</td>   <td>广东省</td>   <td>CN110245856A</td>   <td>2019-09-17</td>   <td>本发明涉及风险分析技术领域,具体为一种基于贝叶斯网络的LNG安全风险评估系统,包括：陆上LNG安全事故分析模块,用于分析陆上LNG危险源所有可能的风险因素,并分析计算其可能引发的火灾安全事故；水上LNG安全事故分析模块,用于分析计算水上LNG危险源所有可能的风险因素,并分析计算其可能引发的火灾安全事故；事故后果计算模块,用于事故后果的计算；风险分析模块,用于个人风险与社会风险的分析计算,并绘制出整体个人风险分布图和社会风险F-N图。本发明可以对陆上和水上LNG固定危险源与移动危险源进行定量风险分析,为LNG相关设施安全、LNG装卸作业操作与LNG船舶航运交通提供科学参考,对相关部门对LNG安全事故的预防与控制具有重要指导意义。</td>   <td>1.一种基于贝叶斯网络的LNG安全风险评估系统,其特征在于,包括：陆上LNG安全事故分析模块,用于分析陆上LNG危险源所有可能的风险因素,并分析计算其可能引发的火灾安全事故；水上LNG安全事故分析模块,用于分析水上LNG危险源所有可能的风险因素,并分析计算其可能引发的火灾安全事故；事故后果计算模块,用于事故严重后果的计算；风险分析模块,用于个人风险与社会风险的分析计算,并绘制出整体个人风险分布图与社会F-N图。</td>   <td>G06Q10/06;G06Q50/26;G06N7/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         唐晓颖;                   杨浦城       </td>   <td>佛山市顺德区中山大学研究院;中山大学</td>   <td>一种基于LDDMM曲线匹配的人脸情感识别方法</td>   <td>广东省</td>   <td>CN107563292B</td>   <td>2019-09-10</td>   <td>本发明提供的方法利用LDDMM曲线匹配所具有的能够反映物体几何形变的特点来对人脸的表情实行精确的识别,因而能够实现精确的情感识别。同时,本发明提供的方法在进行LDDMM曲线匹配时,是分别通过被识别样本的中性表情轮廓的曲线与被识别样本的有特征表情轮廓的曲线、被识别样本的有特征表情轮廓的曲线与每种类型表情的平均有特征表情轮廓G的曲线进行匹配的,然后利用两种匹配所产生的微分同胚映射来进行特征的提取。两种特征的提取及融合使得本发明提供的方法能够充分挖掘被识别样本、识别样本之间的联系并用于后续的识别,因此,本发明提供的方法能够实现对情感的精确识别。</td>   <td>1.一种基于LDDMM曲线匹配的人脸情感识别方法,其特征在于：包括以下步骤：S1.构建识别样本集：为每个样本构建图像序列,所述图像序列包括了由中性表情变化到有特征表情的多帧图像；S2.对图像序列中的每帧图像进行特征点的提取,然后将每帧图像提取的特征点根据其所在的脸部区域进行划分,将划分的各个脸部区域内的特征点连接起来,得到一条曲线；所述每帧图像的表情轮廓通过所有脸部区域的曲线进行表征；S3.对于所有的图像序列中的中性表情轮廓,将其标记为N<Sub>i</Sub>,i＝1,2,…,n,i表示图像序列的序号；以N<Sub>1</Sub>作为参考,对其他所有的中性表情轮廓进行普氏变换,得到变换后的中性表情轮廓N′<Sub>j</Sub>,j＝2,…,n；S4.将N<Sub>1</Sub>和N′<Sub>2</Sub>、N′<Sub>3</Sub>、…、N′<Sub>n</Sub>进行求平均处理,得到识别样本集的平均中性表情轮廓M；S5.对于所有的图像序列中的中性表情轮廓和有特征表情轮廓,以平均中性表情轮廓M进行普氏变换,完成预处理；S6.对于每种类型表情的所有图像序列中的有特征表情轮廓进行求平均处理,得到该类型表情的平均有特征表情轮廓G；S7.对于被识别样本的图像序列,通过步骤S2的方法得到表征其每帧图像的表情轮廓的曲线；然后将被识别样本的中性表情轮廓和有特征表情轮廓以平均中性表情轮廓M进行普氏变换,完成预处理；S8.将被识别样本的中性表情轮廓的曲线与被识别样本的有特征表情轮廓的曲线进行LDDMM曲线匹配,得到一个微分同胚映射<Image id="icf0001" he="69" wi="81" file="FDA0002112545920000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>q表示表情轮廓的曲线号；S9.将被识别样本的有特征表情轮廓的曲线与每种类型表情的平均有特征表情轮廓G的曲线进行LDDMM曲线匹配,得到一个微分同胚映射<Image id="icf0002" he="62" wi="103" file="FDA0002112545920000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>k表示表情的类别号；S10.分别从<Image id="icf0003" he="71" wi="182" file="FDA0002112545920000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>中进行特征的提取；S11.使用线性组合的方法对分别从<Image id="icf0004" he="71" wi="182" file="FDA0002112545920000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>中提取的特征进行融合；S12.使用SVM分类器进行学习识别,输出被识别样本所属的表情类型,进而实现情感识别。</td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              梅岭;                   陈军       </td>   <td>中山大学</td>   <td>一种基于MATV特征的人脸活体检测方法</td>   <td>广东省</td>   <td>CN106228129B</td>   <td>2019-09-10</td>   <td>本发明涉及基于MATV特征的人脸活体检测方法,包括步骤：读取并分解训练集视频,提取每一帧的人脸区域并归一化做输入,采用基于幅度-角度变分法的MATV光流场特征,用统计直方图对特征进行编码得到MATV直方图特征,最后将MATV直方图特征输入SVM分类器进行训练,建立SVM模型；对于测试集的人脸视频帧序列,对每一帧进行人脸检测,然后提取其中的人脸区域并归一化得到测试集人脸帧样本,计算人脸帧样本的MATV直方图特征,最后将其输入训练好的SVM模型,预测出人脸活体检测的结果。本发明在传统的基于幅度特征的光流方法基础上,加入了角度特征,考虑了光流场的方向特性,从而得到更多活体和非活体人脸的运动细节,提高了人脸活体检测的准确率。</td>   <td>1.一种基于MATV特征的人脸活体检测方法,其特征在于,包括下述步骤：S1、训练阶段：读取并分解训练集视频,提取每一帧的人脸区域并归一化做输入,采用基于幅度-角度变分法的MATV光流场特征,用统计直方图对特征进行编码得到MATV直方图特征,最后将MATV直方图特征输入SVM分类器进行训练,建立SVM模型；S2、测试阶段：对于测试集的人脸视频帧序列,对每一帧进行人脸检测,然后提取其中的人脸区域并归一化得到测试集人脸帧样本,计算人脸帧样本的MATV直方图特征,最后将其输入训练好的SVM模型,预测出人脸活体检测的结果；步骤S1包括以下过程：S11、对训练样本人脸图像的每个像素点依次计算MATV光流场特征,MATV光流场特征采用基于L1范数的变分法光流运动估计模型,采用数值优化和prima-dual加速的方法求出光流场的最终稳定收敛解(u,v)；S12、计算训练样本人脸图像的每个像素点的幅度和角度联合特征(mag,θ)；S13、计算MATV光流场的幅度和角度二维联合直方图H(x,y)：将幅值特征按大小分为C<Sub>X</Sub>个组距依次编码,得到幅度直方图分布；将角度特征按照大小分为C<Sub>Y</Sub>个方向组距依次编码,得到角度直方图分布,从而得到二维直方图特征H<Sub>2D</Sub>(x,y),x轴表示幅度,y轴表示角度；S14、将二维直方图H<Sub>2D</Sub>(x,y)降维,得到一维直方图H<Sub>1D</Sub>(x)；S15、建立SVM模型：SVM分类器对输入训练集的样本人脸图像的MATV特征进行训练,采用LIBSVM工具,利用径向基核函数RBF作为训练模型；将训练集得到的所有k个直方图特征构成训练集特征矩阵<Image id="icf0001" he="238" wi="455" file="FDA0002045385660000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>得到SVM模型；步骤S11所述光流场的最终稳定收敛解(u,v)的计算方式为：横轴光流场u的计算如下两式：          <Image id="icf0002" he="143" wi="656" file="FDA0002045385660000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0003" he="204" wi="700" file="FDA0002045385660000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中λ是稳定系数,<Image id="icf0004" he="172" wi="335" file="FDA0002045385660000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>▽u<Sup>(k)</Sup>是第k次迭代解u<Sup>(k)</Sup>的梯度；纵轴光流场v的计算方法与横轴光流场u的相同。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         路崇;              谭洪舟;              吴华灵;              陆许明;              陈凡;                   李浪兴       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学花都产业科技研究院</td>   <td>一种大规模数字集成电路时钟网格分布方法</td>   <td>广东省</td>   <td>CN106446366B</td>   <td>2019-09-10</td>   <td>本发明提供一种大规模数字集成电路时钟网格分布方法,该方法将电路划分为若干个物理相邻的时钟域,使每个时钟域与周围四个时钟域相邻；之后在每个时钟域内设置一个延迟可调的延迟补偿单元,并在每两个相邻的时钟域之间设置一个相位检测单元；对各个时钟域之间的相位关系进行探测,得到向量f(N,S,W,E)代表一个时钟域与其四个相邻时钟域的相位关系对比结果,并得到向量g(N,S,W,E)表示时钟域与其相邻其他时钟域之间的相位差在时域上是否大于预先设定的阈值g；利用得到的两个向量通过最小邻域偏斜算法对延迟补偿单元进行调整,使得每两个相邻时钟区域之间的相位差都将小于g就完成电路的网格分布。</td>   <td>1.一种大规模数字集成电路时钟网格分布方法,其特征在于,包括以下步骤：S1：将电路划分为若干个物理相邻的时钟域,每个时钟域与周围四个时钟域相邻；S2：在每个时钟域内设置一个延迟可调的延迟补偿单元CU,并在每两个相邻的时钟域之间设置一个相位检测单元PDU；S3：对各个时钟域之间的相位关系使用PDU进行探测,并获得相位关系结果,以向量f(N,S,W,E)代表一个时钟域与其四个相邻时钟域的相位关系对比结果；S4：相位检测单元PDU探测时钟域与其相邻其他时钟域之间的相位差在时域上是否大于预先设定的阈值t,以g(N,S,W,E)进行表示；S5：利用向量f(N,S,W,E)和向量g(N,S,W,E)通过最小邻域偏斜算法对延迟补偿单元CU进行调整,使得每两个相邻时钟区域之间的相位差都将小于t就完成电路的网格分布,具体过程是：所述延迟补偿单元CU由控制逻辑单元、移位寄存器和可变延迟路径单元构成,控制逻辑单元根据时钟域接收到的向量f(N,S,W,E)和g(N,S,W,E)对移位控制器进行操作,移位控制器的位数为d,调节步长为s,其中s&lt;t。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              何炜雄;                   陈颖聪       </td>   <td>中山大学</td>   <td>一种基于不对称变换的行人再标识方法</td>   <td>广东省</td>   <td>CN105956606B</td>   <td>2019-09-10</td>   <td>本发明公开了一种基于不对称变换的行人再标识方法,包括步骤：提取训练所用的行人图像集的LOMO特征构成特征矩阵,构建每一张训练所用的测试图像期望的稀疏重构向量；构建需要优化的整体的目标函数；采用交替优化的框架队所述目标函数进行优化,当目标函数收敛后,得到非对称变换矩阵T<Sub>A</Sub>和T<Sub>B</Sub>；在测试阶段,使用变换后的图像集中的图像特征来对变换后的测试集中的图像特征进行重构；使用重构系数向量的大小关系对图像集中的图像进行排序。本发明提高了稀疏重构的可靠性,提高了行人再标识的准确性和鲁棒性；解决了行人再标识问题中遮挡情况带来的困扰；且提高了行人再标识的准确性。</td>   <td>1.一种基于不对称变换的行人再标识方法,其特征在于,包括步骤：S1：提取训练所用的行人图像集的LOMO特征构成特征矩阵G和测试集特征矩阵P,对于每一个行人的所有图像特征进行求平均操作,得到表示该行人的特征；S2：构建每一张训练所用的测试图像期望的稀疏重构向量；S3：得到所有测试图像期望的稀疏重构向量后,构建需要优化的整体的目标函数；S4：采用交替优化的框架对所述目标函数进行优化,当目标函数收敛后,得到非对称变换矩阵T<Sub>A</Sub>和T<Sub>B</Sub>；S5：在测试阶段,将图像集中的图像使用非对称变换矩阵T<Sub>B</Sub>进行变换,将测试集中的图像使用非对称变换矩阵T<Sub>A</Sub>进行变换,并使用变换后的图像集中的图像特征来对变换后的测试集中的图像特征进行重构；S6：使用重构系数向量的大小关系对图像集中的图像进行排序；其中,步骤S2构建每一张训练所用的测试图像期望的稀疏重构向量包括步骤：对于训练用的测试图像特征,在训练用的图像集平均特征寻找与其类标一样的特征并记录其位置,构造一个长度与图像集特征数目相等的全0向量,并将该向量中对应于所述类标一样的特征的位置设置为1；步骤S3中目标函数的构建具体步骤为：假设训练所用的行人图像集中测试图像数量为n,对于测试图像p<Sub>k</Sub>构造的期望的稀疏重构向量<Image id="icf0001" he="71" wi="94" file="FDA0002087389480000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>待优化不对称的变换矩阵分别为T<Sub>A</Sub>和T<Sub>B</Sub>,其中T<Sub>A</Sub>和T<Sub>B</Sub>初始为单位阵,构造的目标函数为,          <Image id="icf0002" he="67" wi="700" file="FDA0002087389480000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0003" he="186" wi="700" file="FDA0002087389480000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0004" he="56" wi="700" file="FDA0002087389480000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中<Image id="icf0005" he="78" wi="42" file="FDA0002087389480000025.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是对于第i个样本期望的稀疏重构向量,S＝[s<Sub>1</Sub>,s<Sub>2</Sub>,...s<Sub>n</Sub>]是所有待优化的重构系数向量组成的矩阵。</td>   <td>G06K9/62;G06K9/46</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林思宜;                   吴维刚       </td>   <td>中山大学</td>   <td>一种区块链支付通道网络的路径选择方法</td>   <td>广东省</td>   <td>CN110223055A</td>   <td>2019-09-10</td>   <td>本发明公开了一种区块链支付通道网络的路径选择方法,具体步骤包括如下：步骤一：通过路径选择方法寻找多条路径；步骤二：计算交易前每条路径上每个支付通道的直观倾斜系数和优化倾斜系数；步骤三：计算交易后每条路径上每个支付通道的直观倾斜系数和优化倾斜系数；步骤四：计算交易前后支付通道的倾斜优化程度,并选择最优路径。本发明提供了一种区块链支付通道网络的路径选择方法,在支付通道网络中,为任意节点寻找最优的交易路径,不仅改善支付通道网络中的资金倾斜现象,而且允许节点主动调节通道内的资金分配,突破交易瓶颈。</td>   <td>1.一种区块链支付通道网络的路径选择方法,其特征在于,具体步骤包括如下：步骤一：通过路径选择方法寻找多条可行路径；步骤二：计算交易前每条路径上每个支付通道的直观倾斜系数和优化倾斜系数；步骤三：计算交易后每条路径上每个支付通道的直观倾斜系数和优化倾斜系数；步骤四：计算交易前后支付通道的倾斜优化程度,并选择最优路径。</td>   <td>G06Q20/08;G06Q20/38</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;                   肖翔       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于混合池化策略的深度卷积特征的动作识别方法</td>   <td>广东省</td>   <td>CN106650674B</td>   <td>2019-09-10</td>   <td>本发明公开一种基于混合池化策略的深度卷积特征的动作识别方法,包括：1)对输入视频每一帧采用空间流深度网络模型,得到每帧的表观特征；对视频中每连续10帧采用时间流深度网络模型,提取视频的运动特征；2)对空间流和时间流深度网络的最后一层卷积层输出的深度卷积图采用时间滤波器池化方法得到对应的特征表示,采用主成分分析方法进行降维得到第一描述子特征；对空间流和时间流深度网络的最后一层卷积层输出的深度卷积图采用时空金字塔池化方法得到对应的特征表示,用主成分分析方法进行降维得到第二描述子特征；3)将步骤2)得到的第一、二描述子特征级联起来,形成输入视频的特征描述子,并采用线性支持向量机进行特征分类,得到识别准确率。</td>   <td>1.一种基于混合池化策略的深度卷积特征的动作识别方法,其特征在于,包括以下步骤：(1)输入待识别的视频,对输入视频的每一帧,利用空间流深度网络模型得到每帧的表观特征；同时对输入视频的每连续M帧,利用时间流深度网络模型得到运动特征；其中空间流深度网络模型和时间流深度网络模型均包括5个卷积层,3个池化层,以及3个全连接层；(2)对空间流深度网络模型和时间流深度网络模型得到的最后一层卷积层输出的深度卷积图采用时间滤波器池化方法得到对应的特征表示,采用不同长度间隔的时间序列,以获取视频的全局和局部运动,并采用主成分分析方法对特征进行降维,得到第一描述子特征；同时,对空间流深度网络模型和时间流深度网络模型得到的最后一层卷积层输出的深度卷积图采用时空金字塔池化方法得到对应的特征表示,采用4层的时空金字塔结构来获取深度特征图中的局部信息,并对于目标和几何变形具有鲁棒性；同样的也采用主成分分析进行特征降维,得到第二描述子特征；(3)对步骤(2)提取的第一、二描述子特征级联起来,形成该视频最终的向量表示；采用支持向量机(SVM)进行特征分类,最终输出分类结果,获取视频的动作识别结果；所述步骤(2)中,选取空间流深度网络和时间流深度网络的最后一层卷积层输出的卷积图来进行时间滤波器池化的操作,具体是对特征图采用4种不同时间间隔的滤波器(1,4,8,16)来分析深度特征在时间域的运动,其中时间间隔1对应的是整个视频范围内的时间运动也即全局运动,而时间间隔16对应的是最大尺度下的局部时间运动；对于每个不同的时间间隔,深度特征在整个视频时间范围内都会被分割成多个时间片,对每个时间片内的特征我们同时采用最大池化和求和池化方法获取该时间片内最具代表性的特征,并将这两种池化结果串联起来表示在该时间片内的运动；然后对整个时间滤波器池化后得到的视频特征进行PCA降维；所述步骤(2)中,选取空间流深度网络和时间流深度网络的最后一层卷积层输出的多通道卷积图来进行时空金字塔池化的操作,具体是对卷积图采用4层时空金字塔结构(1×1×1,2×2×2,3×3×3,4×4×4),其中第一层(1×1×1)对应的是整个时间和空间范围内的特征图,而第4层(4×4×4)对应的是最大尺度下的局部时空特征块；因此通过时空金字塔结构得到特征图位于不同时空尺度下的局部块；对每个局部时空块采用最大池化方法,计算时空块中的最大值作为该局部块的特征表示；由于每个通道上的特征图提取了不同的图像/视频信息,故将所有通道上的特征图中同一时空位置的局部块的特征串联起来,形成该局部时空块的多通道特征描述子；最后将视频内所有时空块特征级联起来,形成视频的特征表示；然后对整个时空金字塔池化后得到的视频特征进行PCA降维。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;                   陈靖       </td>   <td>中山大学</td>   <td>一种彩色素描模拟方法</td>   <td>广东省</td>   <td>CN106652009B</td>   <td>2019-09-10</td>   <td>本发明提供的彩色素描模拟方法在线积分卷积生成黑白素描的基础上,通过研究彩色素描的绘制特点,结合双色调映射技术实现了基于色彩定制的彩色素描绘制。该策略首先对图像进行区域分割,而后按照上下叠加的方式,用一个主色调和一个副色调进行色彩融合来表现每个分割的区域。每个特定的区域可以自动地、也可以由用户交互地指定颜色集。此外,本发明所提供的方法通过增设纸肌理,使得能够将真实的素描纸肌理添加到计算机生成的图像当中,使得最终的效果更接近真实的绘画。本发明在对图像进行分割方面采用的是基于k-means聚类的分割方法,在保证分割效率的同时还使得分割结果更好地适应彩色素描的绘制。</td>   <td>1.一种彩色素描模拟方法,其特征在于：包括以下步骤：S1.使用K-means聚类分割方法对图像进行分割,将图像分割成若干个区域；S2.以图像所分割区域原来的颜色为依据,为每个区域指定主色调和副色调；各个分割区域所确定的主色调和副色调联结在一起形成图像的主色调层和副色调层；S3.依据随机赋值算法对主色调层和副色调层进行赋值处理,分别生成主色调层和副色调层的噪声图像；S4.对每个区域围绕其中心建立边长为m的窗口,将该窗口转换到频域；然后将频域分成几个不同的角度区间,并计算每个角度区间的能量值和总体均值；将计算获得的最大能量值与总体均值进行比较,若两者之间的比例大于所设定的阈值,则判定该区域的纹理走向与最大能量值所对应的角度垂直；S5.对主色调层和副色调层噪声图像中的每个像素点,以其为中心沿其所属区域的纹理走向的正、反两个方向查找与其相邻的n个像素点以形成流线,利用卷积核对流线上各个像素点的噪声值进行卷积计算,并将计算得到的结果赋值给主色调层或副色调层噪声图像中的原像素点；S6.使用霓虹变换的方法对图像进行处理,得到素描轮廓；S7.将步骤S5得到的主色调层和副色调层噪声图像按照上下次序使用双色调映射的方法进行融合,得到彩色素描的纹理图,在纹理图的基础上叠加素描轮廓,即可得到彩色素描的效果；所述步骤S2中,为每个区域指定主色调和副色调的具体过程如下：设区域的主色调为c,即基本色库中的颜色c<Sub>i</Sub>与c的颜色差异d<Sub>i</Sub>按照公式定义如下：          <Image id="icf0001" he="65" wi="700" file="FDA0002056389910000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,r、g、b分别代表主色调c在RGB色彩空间下的三原色分量,r<Sub>i</Sub>、g<Sub>i</Sub>、b<Sub>i</Sub>分别代表颜色c<Sub>i</Sub>在RGB色彩空间下的三原色分量,i、j、k则为依照人眼的生理特性设置的三个权值；通过以上方式依次算出基本色库中的颜色与区域的主色调c的颜色差异,然后选取d<Sub>i</Sub>最小时的颜色c<Sub>i</Sub>作为该区域的主色调,确定主色调后,再通过以上方法从基本色库剩余的颜色中选取另一种颜色作为区域的副色调。</td>   <td>G06T15/02;G06T7/11;G06T7/49;G06T7/60</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王金桥;              招继恩;              马援;              唐明;              张海;              段绪海;                   谭大伦       </td>   <td>杰创智能科技股份有限公司;中山大学</td>   <td>一种高精度锚点匹配策略的人脸检测方法</td>   <td>广东省</td>   <td>CN110197113A</td>   <td>2019-09-03</td>   <td>本发明涉及一种高精度锚点匹配策略的人脸检测方法,包括构建人脸识别系统,设定识别模型,图像识别及构建识别数据库及人脸识别等四个步骤。本发明有效的提高了人脸识别作业的工作效率和精度,并极大的提高了人脸识别作业使用灵活性和通用性,有效满足不同使用场合的需要,并具有良好的数据处理能力、数据通讯能力及数据管理能力,极大的提高了人脸识别作业的可靠性。</td>   <td>1.一种高精度锚点匹配策略的人脸检测方法,其特征在于：所述的高精度锚点匹配策略的人脸检测方法包括以下步骤：S1,构建人脸识别系统,根据人脸识别作业需要,首先构建包括CCD摄像机、承载座、转台机构、数据采集控制电路、通讯网络及基于云计算基础的人脸识别服务器,其中CCD摄像机至少一个,均安装在承载座上,且CCD摄像机通过转台机构与承载座连接,然后将承载座固定到指定工作位置上,此外另将数据采集控制电路安装在承载座内,并分别与CCD摄像机、转台机构及通讯网络相互连接,所述通讯网络与云计算基础的识别服务器间建立数据连接,从而构成人脸识别系统；S2,设定识别模型,完成S1步骤后,在基于云计算基础的识别服务器录入基于锚点计算为基础的人脸识计算算法和基于“四邻域连通”图像预处理策略；S3,图像识别及构建识别数据库,完成S2步骤后,首先由基于云计算基础的识别服务器通过通讯网络一方面从第三方平台中下载先用人脸识别数据,并保存在基于云计算基础的识别服务器中,另一方面由CCD摄像机直接对目标人群进行面部图像信息采集,并保存至基于云计算基础的识别服务器中,共同构成人脸识别训练数据库,然后由基于云计算基础的识别服务器基于S2步骤中的“四邻域连通”图像预处理策略,对人脸识别训练数据库中的面部图像信息进行滤波、图像差分、二值化处理,并在二值化后,对物基础视频图像进行连通域分析,在连通域上按照“四邻域连通”对基础视频图像中的人体进行分离并标记,并统计各区域面积,最后由基于云计算基础的识别服务器基于S2步骤中的基于锚点计算为基础的人脸识计算算法,分别为各人脸识别训练数据库中的面部图像划分的连通域范围内进行锚点计算和定位,并对划分面域中分布的锚点数量和位置进行统计,从而完成识别数据库建设；S4,人脸识别,在完成S3步骤后,即可进行人脸识别作业,在人脸识别作业时,首先由S1步骤中CCD摄像机对待识别人员面部采集至少3张图像信息,然后将采集的图像信息通过通讯网络发送至云计算基础的识别服务器在内的人脸识别服务器中,由云计算基础的识别服务器在内的人脸识别服务器按照S3步骤,对新采集的图像信息分别进行图像识别,然后对识别后的图像信息首先根据S2步骤中的“四邻域连通”图像预处理策略进行划分,然后根据S2步骤中的基于锚点计算为基础的人脸识计算算法对各划分区域内进行锚点锚点数量和位置进行统计,完成图像识别,最后将完成图像识别后的图像连通域划分信息及锚点数量和分布信息与S3步骤中识别数据库中存储数据进行比对,并在新识别图像信息于识别数据库建中相关图像新型存储信息相似度达到80％以上时,完成人脸识别,低于80％时则再次识别,并在连续识别3—5次后相似度均低于80％时,则向及基于云计算基础的人脸识别服务器发送信息拓展请求,并在信息拓展请求审批通过后再返回S3步骤进行图像信息识别并保存至识别数据库中。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         冯定云;              李星;              张天托;              周宇麒;                   柴象飞       </td>   <td>中山大学附属第三医院</td>   <td>一种基于影像组学对糖皮质激素疗效的预测和验证方法</td>   <td>广东省</td>   <td>CN110197236A</td>   <td>2019-09-03</td>   <td>本发明提供了一种基于影像组学对糖皮质激素疗效的预测和验证方法,包括如下步骤：步骤S1、采集使用糖皮质激素治疗结缔组织病相关间质性肺病患者的原始医学影像,并将获得的原始医学图像分成训练组和验证组；步骤S2、对原始医学影像进行量化处理,获得影像组学特征数据；步骤S3、建立训练组中影像组学特征的预测模型,并在验证组中进行测试验证；步骤S4、确立采用糖皮质激素有效的临床特征和影像组学特征标签,并在验证组中进行验证。本发明基于影像组学,对糖皮质激素疗效进行预测并验证,以识别对糖皮质激素治疗敏感的患者,为医生对结缔组织相关间质性肺病患者的诊断和治疗方案提供具体的参考依据,以有效地对患者进行治疗,提高治疗的成功率。</td>   <td>1.一种基于影像组学对糖皮质激素疗效的预测和验证方法,其特征在于,包括如下步骤：步骤S1、采集使用糖皮质激素治疗结缔组织病相关间质性肺病患者的原始医学影像,并将获得的原始医学图像分成训练组和验证组；步骤S2、对原始医学影像进行量化处理,获得影像组学特征数据；步骤S3、建立训练组中影像组学特征的预测模型,并在验证组中进行测试验证；步骤S4、确立采用糖皮质激素有效的临床特征和影像组学特征标签,并在验证组中进行验证。</td>   <td>G06K9/62;G06T7/45;A61B6/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;              刘稳;                   黄捷       </td>   <td>中山大学</td>   <td>基于深度学习子网络特征提取的视网膜OCT图像分类方法</td>   <td>广东省</td>   <td>CN110188820A</td>   <td>2019-08-30</td>   <td>本发明涉及计算机视觉技术,为基于深度学习子网络特征提取的视网膜OCT图像分类方法,对计算机辅助诊断提供算法支持,计算资源消耗小,训练速度快,准确率高；包括步骤：准备视网膜OCT图像数据；构建多个不同的深度学习模型子网络；对视网膜OCT图像数据用所构建的不同的深度学习模型子网络进行特征提取,将所提取的特征输入随机森林分类器进行训练和分类,并对训练和分类的结果进行评估筛选,获得准确率较高的几个模型；利用所评估筛选出来的模型,采用特征连接或多数投票的分类方法对视网膜OCT图像进行分类。</td>   <td>1.基于深度学习子网络特征提取的视网膜OCT图像分类方法,其特征在于,包括以下步骤：S1、准备视网膜OCT图像数据；S2、构建多个不同的深度学习模型子网络；S3、对视网膜OCT图像数据用步骤S2所构建的不同的深度学习模型子网络进行特征提取,将所提取的特征输入随机森林分类器进行训练和分类,并对训练和分类的结果进行评估筛选,获得准确率较高的几个模型；S4、利用步骤S3所评估筛选出来的模型,采用特征连接或多数投票的分类方法对视网膜OCT图像进行分类。</td>   <td>G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄诚;              谢逸;              高健;                   彭桢       </td>   <td>四川大学;中山大学</td>   <td>一种动静态结合的Docker容器内容安全性检测方法和装置</td>   <td>四川省</td>   <td>CN110187955A</td>   <td>2019-08-30</td>   <td>本申请涉及计算机技术领域,提供了一种动静态结合的Docker容器内容安全性检测方法和装置。所述方法应用于服务器,所述方法包括：获取目标Docker容器的原始镜像,所述目标Docker容器为待检测的Docker容器；对所述原始镜像进行漏洞检测,得到静态检测结果；获取所述目标Docker容器在运行时产生的日志；从所述日志中提取出所述目标Docker容器在运行时访问的外部IP地址和请求DNS解析的域名；利用恶意IP情报数据对提取出的外部IP地址进行恶意性检测,得到第一动态检测结果；利用恶意域名情报数据对提取出的域名进行恶意性检测,得到第二动态检测结果；将提取出的域名输入DGA域名识别模型,以确定提取出的域名是否为DGA域名,得到第三动态检测结果。</td>   <td>1.一种动静态结合的Docker容器内容安全性检测方法,其特征在于,所述方法应用于服务器,所述方法包括：获取目标Docker容器的原始镜像,所述目标Docker容器为待检测的Docker容器；对所述原始镜像进行漏洞检测,得到静态检测结果；获取所述目标Docker容器在运行时产生的日志；从所述日志中提取出所述目标Docker容器在运行时访问的外部IP地址和请求DNS解析的域名；利用恶意IP情报数据对提取出的外部IP地址进行恶意性检测,得到第一动态检测结果；利用恶意域名情报数据对提取出的域名进行恶意性检测,得到第二动态检测结果；将提取出的域名输入DGA域名识别模型,以确定提取出的域名是否为DGA域名,得到第三动态检测结果。</td>   <td>G06F9/455</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈森海;                   周晓聪       </td>   <td>中山大学</td>   <td>一种基于定值到达分析方法的Java空指针分析系统</td>   <td>广东省</td>   <td>CN110188029A</td>   <td>2019-08-30</td>   <td>本发明涉及软件静态分析领域,更具体的,涉及一种基于定值到达分析方法的Java空指针分析系统。包括项目模块、过滤器模块、源码模块、控制台模块、空指针分析模块、结果展示模块；本发明将能够帮助开发人员在开发过程中及时发现源码中存在空指针异常的怀疑源码行,通过工具能够直接展示和跳转到疑似异常源码行,给予开发人员更加方便和直观的查看源码和分析,从而能够对应的源码进行修改,对源码结构进一步完善和增强源码的易读性,形成更好的源码风格,以及开发出更完善的软件系统。</td>   <td>1.一种基于定值到达分析方法的Java空指针分析系统,其特征在于,包括项目模块、过滤器模块、源码模块、控制台模块、空指针分析模块、结果展示模块；所述的项目模块通过java.swing.JTree以树形结构展示每个项目结构,通过继承javax.swing.tree.TreeNode来展示和保存包、类、方法和属性,java.util.ArrayList类保存项目源码和分析结果数据；所述的过滤器模块利用java,io.FIle类保存和读取硬盘上过滤器配置信息,并将配置信息转换成自定义的配置信息实体类；所述的源码模块通过java.swing.JTextPane控件展示源码文件信息和行号；所述的控制台模块基于java.swing.TextArea控件,能够将分析过程输出到工具界面；所述的空指针分析模块基于定值到达分析方法,实现对源码扫描和空指针的分析；所述的结果展示模块将通过java.swing.JTextPane以HTML格式展示空指针的分析结果。</td>   <td>G06F11/36</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;              庄耿行;                   孟浩       </td>   <td>中山大学</td>   <td>一种针对自动驾驶场景的激光雷达线束分布调整优化方法</td>   <td>广东省</td>   <td>CN110175344A</td>   <td>2019-08-27</td>   <td>本发明涉及激光雷达传感器及自动驾驶领域,更具体地,涉及一种针对自动驾驶场景的激光雷达线束分布调整优化方法。本发明首先根据指定的优化线束数量,建立激光雷达线束模型。之后,根据环境感知任务的要求,设定优化参数,进行粗优化。最后,对粗优化结果进行细优化调优,得到优化后的激光线束分布。得到的优化激光线束分布可以在驾驶模拟器等虚拟环境中测试及验证其在环境感知任务的效果提升。与现行常用多线激光雷达均匀的线束分布相比,该方法对特定的目标检测任务具有针对性,使得优化后的传感器在目标检测具有检测准确率高,范围大的特点。</td>   <td>1.一种针对自动驾驶场景的激光雷达线束分布调整优化方法,其特征在于,包括以下步骤：S1.构建激光雷达传感器线束模型：根据多线激光雷达传感器的旋转结构,定义激光雷达旋转的扫描水平转角变量；根据多线激光雷达垂直方向多发射器的结构,定义发射通道垂直仰角分布的变量；S2.针对不同距离下的线束分布目标,构建优化目标评价函数：针对短距离,建立线束发散的子目标函数；针对中远距离下的检测,建立线束聚焦的子目标函数；针对激光雷达特定的安装位置,建立避免盲区的子目标函数；S3.根据感知任务,调整优化参数进行粗优化：根据实际自动驾驶场景环境感知的目标检测任务,设定兴趣区域内的关注点、以及各个子目标函数的权重,进行粗优化；S4.根据粗优化的结果,进行细优化调优,得到优化的雷达线束分布；细优化首先设定优化的步长,再针对粗优化的结果作为初始估计,以细优化步长进行细微调整优化,得到最后评价最优结果；S5.对优化后的激光雷达进行仿真测试与验证。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郭英;              杨荣骞;              谢杨洁;                   龚瑾       </td>   <td>中山大学附属第三医院</td>   <td>颅骨缺损结构的重建方法、装置及电子设备</td>   <td>广东省</td>   <td>CN110176066A</td>   <td>2019-08-27</td>   <td>本发明提供了一种颅骨缺损结构的重建方法、装置及电子设备,涉及颅骨修复技术领域,该方法包括：获取术前完整颅骨的三维图像信息、术中缺损颅骨的缺损边缘轮廓信息和主视角方向；根据主视角方向、预设的旋转方向和预设的投影平面,对三维图像信息中的第一点集和缺损边缘轮廓信息中的第二点集均做旋转和投影操作,得到第一平面点集和第二平面点集；根据第一平面点集和第二平面点集,获取缺损颅骨对应的目标点集；对目标点集进行模型结构的三维重建,得到目标颅骨缺损结构。本发明中不需要医生手动分割颅骨缺损结构,减少了手动操作带来的误差,提高了重建颅骨缺损结构的准确度。</td>   <td>1.一种颅骨缺损结构的重建方法,其特征在于,所述方法包括：获取术前完整颅骨的三维图像信息、术中缺损颅骨的缺损边缘轮廓信息和主视角方向；其中,所述三维图像信息包括由形成所述完整颅骨的各个点的坐标构成的第一点集,所述缺损边缘轮廓信息包括由形成所述缺损颅骨的缺损边缘轮廓的各个点的坐标构成的第二点集,所述主视角方向与所述缺损边缘轮廓的拟合平面的法向量平行；根据所述主视角方向、预设的旋转方向和预设的投影平面,对所述第一点集和所述第二点集均做旋转和投影操作,得到所述三维图像信息对应的第一平面点集和所述缺损边缘轮廓信息对应的第二平面点集；其中,所述旋转方向与所述投影平面垂直；根据所述第一平面点集和所述第二平面点集,获取所述缺损颅骨对应的目标点集；对所述目标点集进行模型结构的三维重建,得到目标颅骨缺损结构。</td>   <td>G06T17/00;G06T19/20;G06T7/00;G06T7/13;G06T7/181</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘镇;              张志龙;              明伟华;              张国豪;              周翠英;                   欧阳进武       </td>   <td>中山大学</td>   <td>一种基于钻孔数据进行机器学习的地层序列模拟方法</td>   <td>广东省</td>   <td>CN110176070A</td>   <td>2019-08-27</td>   <td>本发明公开了一种基于钻孔数据进行机器学习的地层序列模拟方法,该方法包括：数据归一化、地层序列填充、地层编码、建立地层类型序列模型、建立地层层厚序列模型、地层序列模型；该方法利用python语言,在Pytorch深度学习框架下进行,特别涉及三维地质建模过程中地层层序的建立,适用于在地层三维建模过程中进行地层序列模型的开发与验证。能够较为准确地判断相应位置的地层信息,同时该方法不依赖于数据假设与专家经验等主观因素,通过与实际钻孔数据结果对比表明,上述模型具有较好的可行性,可应用于地质信息化研究与工程规划、设计建造等方面。</td>   <td>1.一种基于钻孔数据进行机器学习的地层序列模拟方法,属于机器学习与地质学的交叉领域,其特征是：将钻孔地层数据处理为地层类型序列与地层层厚序列,利用循环神经网络建立地层类型序列模型,采用序列-序列架构建立地层层厚序列模型,组成完整地层序列模型。</td>   <td>G06T17/05;G06F17/50;G06N20/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李冠彬;              谢园;              王巨宏;                   黄婷婷       </td>   <td>中山大学;腾讯科技(深圳)有限公司</td>   <td>显著特征检测方法和装置</td>   <td>广东省</td>   <td>CN110163196A</td>   <td>2019-08-23</td>   <td>本申请涉及一种显著特征检测方法和装置,所述方法包括：获取视频帧序列,视频帧序列包括目标帧和多个参考帧；计算目标帧与各参考帧之间的光流图；通过第一神经网络模型,对各光流图进行编码,分别得到各参考帧对应的图像变换特征；通过第二神经网络模型,根据各图像变换特征对目标帧的图像特征进行编码,得到目标帧的协同编码图像特征；将协同编码图像特征输入像素级别分类器进行检测,输出目标帧的显著图。本申请提供的方案可以提升对视频进行显著特征检测的准确性。</td>   <td>1.一种显著特征检测方法,包括：获取视频帧序列,所述视频帧序列包括目标帧和多个参考帧；计算所述目标帧与各所述参考帧之间的光流图；通过第一神经网络模型,对各所述光流图进行编码,分别得到各所述参考帧对应的图像变换特征；通过第二神经网络模型,根据各所述图像变换特征对所述目标帧的图像特征进行编码,得到所述目标帧的协同编码图像特征；将所述协同编码图像特征输入像素级别分类器进行检测,输出所述目标帧的显著图。</td>   <td>G06K9/32;G06K9/00;G06T7/269</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              邓铭豪;              陈荣军;              谢舜道;              何彦东;              苏宏雄;              朱雄泳;                   曾衍瀚       </td>   <td>中山大学</td>   <td>一种基于暗通道先验的图像去雾改进方法</td>   <td>广东省</td>   <td>CN110148093A</td>   <td>2019-08-20</td>   <td>本发明提供了一种基于暗通道先验的图像去雾改进方法,包括以下步骤：输入雾天图像,得到雾天图像的暗通道图像和亮通道图像；利用四叉树搜索方法得到大气光估计值,结合亮通道图像和大气光估计值得到大气光图像；计算得到透射率图像,结合大气光图像对透射率图像进行阈值处理；对透射率图像进行精细化处理,输出去雾的复原图像。本发明提供一种基于暗通道先验的图像去雾改进方法,避免出现无法存在白色或无法适应天空域的雾天图像问题,且将透射率进行阈值处理,有效改进了天空域部分的透射率过低导致的颜色失真问题提高了图像的亮度和对比度,减少图像的信息熵的丢失；且该方法计算过程简单,具有实时性。</td>   <td>1.一种基于暗通道先验的图像去雾改进方法,其特征在于,包括以下步骤：S1：输入雾天图像,得到雾天图像的暗通道图像和亮通道图像；S2：利用四叉树搜索方法得到大气光估计值,结合亮通道图像和大气光估计值得到大气光图像；S3：计算得到透射率图像,结合大气光图像对透射率图像进行阈值处理；S4：对透射率图像进行精细化处理,输出去雾的复原图像。</td>   <td>G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄斐铨;                   宋嵘       </td>   <td>中山大学</td>   <td>喉咙识别方法、装置、系统、存储介质以及设备</td>   <td>广东省</td>   <td>CN110148124A</td>   <td>2019-08-20</td>   <td>本发明涉及一种喉咙识别方法、装置、系统、存储介质以及设备,其中的方法包括：获取待识别的喉咙图像；将待识别的喉咙图像输入至第一喉咙识别模型中进行识别,并从待识别的喉咙图像中确定出喉咙图像；第一喉咙识别模型为通过喉咙样本图像和非喉咙样本图像进行训练后建立的模型；将喉咙图像输入至第二喉咙识别模型中进行识别,获得喉咙的状态分值；第二喉咙识别模型为通过喉咙样本图像以及对应的喉咙的状态分值进行训练后建立的模型。本公开发现通过拍摄获取待识别的喉咙图像,再通过第一喉咙识别模型和第二喉咙识别模型对待识别的喉咙图像进行识别,从而获得喉咙的状态分值,进而使用户无需依赖他人和专业人群,即可方便快捷地获悉喉咙的状态。</td>   <td>1.一种喉咙识别方法,其特征在于,包括如下步骤：获取待识别的喉咙图像；将待识别的喉咙图像输入至第一喉咙识别模型中进行识别,并从待识别的喉咙图像中确定出喉咙图像；其中,所述第一喉咙识别模型为通过喉咙样本图像和非喉咙样本图像进行训练后建立的模型；将喉咙图像输入至第二喉咙识别模型中进行识别,获得喉咙的状态分值；其中,所述第二喉咙识别模型为通过喉咙样本图像以及对应的喉咙的状态分值进行训练后建立的模型。</td>   <td>G06T7/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘永红;              廖文苑;                   陈泳钊       </td>   <td>中山大学</td>   <td>基于交通运行数据和劣化率的轻型汽车排放速率计算方法</td>   <td>广东省</td>   <td>CN106446398B</td>   <td>2019-08-20</td>   <td>本发明提供一种基于交通运行数据和劣化率的轻型汽车排放速率计算方法,该方法通过以VSP-v所定义的运行模式作为排放表征参数来建立了机动车MOVES排放模型,之后对MOVES排放模型进行多次输入输出测试得到中国的轻型汽油车的基础排放速率库；然后再对轻型汽油车进行劣化规律分析,建立车龄与污染物浓度的拟合方程,计算轻型汽油车的劣化率；最后利用机动车劣化规律中所得结论,对建立的轻型汽油车的基础排放速率库进行校准与修正得到修正后的排放速率,该方法对优化道路运行工况、降低机动车排放量、提高城市空气质量具有重要的意义,对我国机动车污染控制措施的制定与实施也具有一定的借鉴意义。</td>   <td>1.一种基于交通运行数据和劣化率的轻型汽车排放速率计算方法,其特征在于,包括以下步骤：S1：以VSP-v所定义的运行模式作为排放表征参数来建立MOVES排放模型,对MOVES排放模型进行多次输入输出测试得到轻型汽油车的基础排放速率库；S2：对轻型汽油车进行劣化规律分析,建立车龄与污染物浓度的拟合方程,计算轻型汽油车的劣化率；S3：利用机动车劣化规律中所得结论,对建立的轻型汽油车的基础排放速率库进行校准与修正得到修正后的排放速率；所述步骤S1中以VSP-v所定义的运行模式作为排放表征参数的过程如下：采用机动车比功率VSP与速度v两个参数共同描述机动车的瞬时运行工况,对机动车比功率VSP与速度v的组合命名为运行模式OpMode,根据机动车比功率VSP与速度v的取值范围将机动车的运行工况共划分为23种运行模式,即有23个不同的机动车比功率VSP与速度v的取值组如下表：          <Image id="icf0001" he="674" wi="700" file="FDA0001989674900000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0002" he="202" wi="700" file="FDA0001989674900000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,可将23种运行模式分成五类：刹车模式为OpMode 0、怠速模式为OpMode 1、低速运行模式为OpMode 11～16、中速运行模式为OpMode 21～30,高速运行模式为OpMode 33～40；a<Sub>t</Sub>为t时刻的加速度,m/s<Sup>2</Sup>；v<Sub>t</Sub>为t时刻的瞬时速度,km/h；P<Sub>t</Sub>为t时刻的机动车比功率VSP,kW/t；所述步骤S1中得到轻型汽油车的基础排放速率库的具体过程如下：S11：利用中国轻型排放标准中规定的车辆型式认证Ⅰ型试验工况“ECE15+EUDC”作为运动参数输入MOVES模型；S12：将轻型车排放标准中规定的测试条件输入MOVES模型作为仿真计算的条件；S13：对MOVES模型中各个年份的新车进行排放仿真测试,从而获得MOVES模型中各个年份新车的排放因子值；S14：将各个排放因子值与中国排放标准限值进行比较,得到MOVES模型中与中国不同排放标准车辆所对应的新车年份,并根据该新车年份在MOVES模型中得出与中国排放标准车型相接近的排放速率库。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李恺;              彭安基;                   印鉴       </td>   <td>中山大学;广州中大南沙科技创新产业园有限公司</td>   <td>基于混合型平衡二阶可逆二维细胞自动机图像加密方法</td>   <td>广东省</td>   <td>CN106056527B</td>   <td>2019-08-20</td>   <td>本发明提供一种基于混合型平衡二阶可逆二维细胞自动机图像加密方法,该方法对需加密图像I的RGB三个通道进行像素置乱得到一个N×24M的0-1矩阵I’；将N×24M的0-1矩阵I’矩阵分成若干8×16的0-1矩阵分组；构造混合型平衡二阶可逆二维细胞自动机对每个矩阵分组进行加密迭代；将加密后的分组组合成为加密后图像。</td>   <td>1.一种基于混合型平衡二阶可逆二维细胞自动机图像加密方法,其特征在于,包括以下步骤：S1：对需加密图像I的RGB三个通道进行像素置乱得到一个N×24M的0-1矩阵I’；S2：将N×24M的0-1矩阵I’矩阵分成若干8×16的0-1矩阵分组；S3：构造混合型平衡二阶可逆二维细胞自动机对每个矩阵分组进行加密迭代；S4：将加密后的分组组合成为加密后图像；所述步骤S1的具体过程如下：使用8bit表示一个颜色通道中的一个像素值,将大小为N×M图像I按R通道-G通道-B通道的顺序对像素值进行排列,直至图像I的像素字节序列的末端,得到一个N×24M的0-1矩阵I’；所述步骤S2的具体过程如下：将N×24M的0-1矩阵I’分成若干组128bit的8×16的0-1矩阵,得到一个矩阵分组集合PB＝{PB<Sub>1</Sub>,PB<Sub>2</Sub>,···,PB<Sub>k</Sub>},并给每个分组一个32bit的编号n<Sub>i</Sub>；所述步骤S3的具体过程如下：S31：将分组矩阵PB<Sub>i</Sub>分为两个子块<Image id="icf0001" he="72" wi="58" file="FDA0001969761150000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0002" he="72" wi="85" file="FDA0001969761150000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>每个子块是一个8×8的0-1矩阵,将<Image id="icf0003" he="69" wi="65" file="FDA0001969761150000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0004" he="72" wi="56" file="FDA0001969761150000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>两个子块作为细胞自动机的两个初始构造；S32：随机生成128bit串作为加密的密钥,每个分组的密钥<Image id="icf0005" he="65" wi="409" file="FDA0001969761150000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>S33：随机挑选出符合平衡性的64个细胞自动机规则集合S＝{R<Sub>1</Sub>,R<Sub>2</Sub>,...R<Sub>64</Sub>}；S34：将K<Sub>i</Sub>左移8bit,取低位的64bit为分组的子密钥SK<Sub>i</Sub>,更新子块<Image id="icf0006" he="74" wi="63" file="FDA0001969761150000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0007" he="70" wi="80" file="FDA0001969761150000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>如果64bit的SK<Sub>i</Sub>对应细胞的值为0,则该细胞使用R<Sub>i</Sub>∈S进行迭代；如果为1,则使用R<Sub>i</Sub>的补规则进行迭代；S35：对步骤S34重复迭代L次得到<Image id="icf0008" he="76" wi="95" file="FDA0001969761150000018.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0009" he="76" wi="90" file="FDA0001969761150000019.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>令<Image id="icf0010" he="77" wi="282" file="FDA00019697611500000110.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>得到分组矩阵PB<Sub>i</Sub>的加密分组矩阵CB<Sub>i</Sub>,进而得到加密分组矩阵集合CB＝{CB<Sub>1</Sub>,CB<Sub>2</Sub>,···,CB<Sub>k</Sub>}。</td>   <td>G06T1/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              刘颜;              陈荣军;              谢舜道;              苏宏雄;              谢子豪;              朱雄泳;                   曾衍瀚       </td>   <td>中山大学</td>   <td>一种基于区块链技术的农作物种子安全溯源系统</td>   <td>广东省</td>   <td>CN110135860A</td>   <td>2019-08-16</td>   <td>本发明提供了一种基于区块链技术的农作物种子安全溯源系统,包括服务层、业务层、接口层和应用层；其中：服务层为所述业务层提供网络和数据处理服务；业务层提供系统的业务逻辑功能；接口层提供所述业务层和所述应用层的中间消息服务；应用层提供人机交互服务,与所述业务层信息交互；服务层包括区块链网络,通过所述区块链网络将区块链返回所述应用层。本发明提供一种基于区块链技术的农作物种子安全溯源系统,通过区块链网络保证链上记录的数据的安全性、完整性以及真实性,实现了防篡改、信息共享、强化溯源系统管理等功能,在提升农作物种子产品质量的同时,也提升农作物种子的市场竞争力和提升企业品牌形象。</td>   <td>1.一种基于区块链技术的农作物种子安全溯源系统,其特征在于：包括服务层、业务层、接口层和应用层；其中：所述服务层为所述业务层提供网络和数据处理服务；所述业务层提供系统的业务逻辑功能；所述接口层提供所述业务层和所述应用层的中间消息服务；所述应用层提供人机交互服务,与所述业务层信息交互；所述服务层包括区块链网络,通过所述区块链网络将区块链返回所述应用层。</td>   <td>G06Q30/00;G06Q50/02;G06Q40/04;G06F9/54;G06F21/62;G06F21/64</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林小拉;                   冯昭凯       </td>   <td>中山大学</td>   <td>一种基于内部注意力的跨语言纯文本反讽识别方法</td>   <td>广东省</td>   <td>CN110134962A</td>   <td>2019-08-16</td>   <td>本发明提供一种基于内部注意力的跨语言纯文本反讽识别方法,该方法采用了内部注意力机制,内部注意力机制使得句子关注词汇两两之间的关系权值,可以有效的表明反讽分类与词汇对权重的关系,此模型显著的提升了反讽识别的效果；利用丰富语料的英语反讽标注数据作为训练数据,把反讽知识迁移到汉语特征空间上,该方法可以显著提高反讽识别的效果。</td>   <td>1.一种基于内部注意力的跨语言纯文本反讽识别方法,其特征在于,包括以下步骤：S1：把源语言的带标签的反讽文本数据集翻译成目标语言的反讽文本数据集；S2：分别对步骤S1中的源语言带标签的反讽文本和其目标语言反讽文本进行注意力向量提取,得到源语言注意力向量和目标语言意力向量；S3：分别对步骤S1中的源语言带标签的反讽文本和其目标语言反讽文本进行LSTM的隐向量表达,得到源语言隐藏向量和目标语言隐藏向量,将源语言注意力向量和源语言隐藏向量进行拼接得到源语言带注意力的隐藏向量,将目标语言注意力向量和目标语言隐藏向量进行拼接得到目标语言带注意力的隐藏向量；S4：将源语言带注意力的隐藏向量和目标语言带注意力的隐藏向量通过各自的ReLU网络得到源语言文本表示向量和目标语言文本表示向量；S5：将源语言文本和目标语言通过线性分类器,得到源语言文本预测标签和目标语言文本预测标签；S6：通过源语言文本所带的标签和其预测标签得到源语言文本的反讽识别的标签误差损失函数,通过目标语言文本所带的标签和其预测标签得到目标语言文本的反讽识别的标签误差损失函数,将两者加起来得到总的误差损失函数；S7：对S4步骤得到的源语言文本和目标语言文本进行距离约束的得到距离损失函数；S8：结合S6的总误差损失函数和S7得到的距离损失函数得到总的损失函数；S9：将步骤S1中的源语言带标签的反讽文本和其目标语言反讽文本进行步骤S2-S8的多次迭代,得到最终模型；其中,反讽文本数据集表示具有反语和讽刺修辞手法的文本。</td>   <td>G06F17/27;G06F17/28;G06F16/35;G06F16/33</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘红梅;              何李域;                   卢伟       </td>   <td>中山大学</td>   <td>一种基于扰动失真和像素选择的二值图像隐写方法</td>   <td>广东省</td>   <td>CN107133991B</td>   <td>2019-08-13</td>   <td>本发明提供一种基于扰动失真和像素选择的二值图像隐写方法,通过更合适的扰动度量方法计算二值图像像素点翻转带来的失真情况,通过STC编码嵌入秘密信息,提高秘密信息的嵌入容量,得到载密图像,最后通过像素选择策略对载密图像进行后处理,得到用于信道传输的二值隐写图像。本发明得到的二值隐写图像相比以前的方法,视觉质量更高,安全性、抗检测能力更强。本发明考虑翻转像素点之间存在相互影响,还提出一种像素选择的后处理策略,得到视觉质量更高,更安全的二值隐写图像。本发明可用于信息安全特别是二值图像秘密信息传输方面。</td>   <td>1.一种基于扰动失真和像素选择的二值图像隐写方法,其特征在于,包括以下步骤：11)假设输入的载体图像为X,计算载体图像的扰动失真分数图,记为D；12)把载体图像X和对应的扰动失真分数图D分块,分成大小为L×L的不重叠的载体图像块和扰动失真分数块,同时把秘密信息m分成长度为n的不重叠的秘密信息块；13)丢掉载体图像中所有的全黑或者全白的像素块以及对应的扰动失真分数块,选择所有非全黑或者全白的像素块以及对应的扰动失真分数块,分别记为X’和D’；14)采用一个随机序列S去置乱X’和D’,对于第i个载体图像块及其对应的扰动失真分数块,进一步将其分成l×l的超像素和对应的超像素扰动失真分数块,每一个超像素的值由超像素中黑色像素点的奇偶性决定,超像素的扰动失真分数块由超像素中最低分的扰动失真分数块来代替,通过这个方法,能得到载体向量vector_x,通过STC编码方法嵌入第i个秘密信息段,得到载密向量vector_y,对比vector_x和vector_y,得到需要翻转的超像素的位置；15)对于每一个需要翻转的超像素,翻转其最低分的像素点；16)重复14)和15)步,直到所有的秘密信息都被成功嵌入；17)使用随机序列S去逆置乱步骤16)得到的已成功嵌入秘密信息的载体图像,得到载密图像Y；18)对于载密图像Y,通过像素选择策略,得到视觉质量更高且更安全的二值隐写图像Y’；其中,计算二值图像的扰动失真分数图包括以下步骤：21)找到图像中所有边缘位置像素点,标记这部分像素点的位置为P＝{pi},i＝0,1,2...k,k为能找到的所有边缘像素点的数量；22)使用扰动度量方法计算每个像素点的扰动失真分数,得到对应的扰动失真分数图,记为T；23)从1到k遍历pi,找到T中所有位置为pi的像素点,对这部分像素点的扰动失真分数乘以一个系数t,得到新的扰动失真分数图T”；24)取分数图D的最低分为min_score,截断新的扰动失真分数图T”,使其小于min_score的分数都置为min_score；25)在信息嵌入步骤,分别用T和T”的块进行选择,选择能得到视觉质量更好的隐写图像的扰动失真分数图块来进行嵌入；其中,二值图像隐写中像素选择策略的具体实现方式为：31)对STC编码后得到的载密二值图像Y,对比原载体图像X,得到所有需要翻转的像素点的位置,记为C＝{ci},i＝1,2,...,k’,k’为需要翻转的像素点的总数量；32)使用扰动度量方法计算载密二值图像Y的扰动失真分数图,记为Dy；33)从i＝1到k’遍历ci,找到与像素点ci来自同一超像素的其他像素点,在Dy中判断ci是否是这些像素点中的最低分像素点,若是,则不处理,若不是,则找到最低分像素点所在的位置,记为plow,分别翻转ci和plow,分别计算图像视觉质量,选择翻转后视觉质量更好的像素点ci或者plow进行翻转。</td>   <td>G06T9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王凯;                   李惠敏       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>采用单一器件实现显示驱动和指纹图像采集的方法</td>   <td>广东省</td>   <td>CN106295540B</td>   <td>2019-08-13</td>   <td>本发明提供的显示驱动和指纹图像采集的方法,采用单一器件即可实现显示驱动和指纹图像采集,因此使得显示屏的生产装配过程与现有技术相比得到了简化。</td>   <td>1.一种采用单一器件实现显示驱动和指纹图像采集的方法,在于使用双栅极光电薄膜晶体管进行显示驱动和指纹图像采集,其特征在于：双栅极光电薄膜晶体管的源极与电容C<Sub>LC</Sub>的一端连接,电容C<Sub>LC</Sub>的另一端称之为共电极,所述双栅极光电薄膜晶体管包括有两个工作模式,分别为显示驱动模式和指纹图像采集模式,具体分别如下：(1)显示驱动模式当触摸启动开关检测不到触摸动作时,双栅极光电薄膜晶体管工作在显示驱动模式,其包括以下步骤：a)写入阶段：使双栅极光电薄膜晶体管的暗栅极设置为正电压,若双栅极光电薄膜晶体管上一个状态处于负极性驱动状态,则向双栅极光电薄膜晶体管的漏极施加正偏压,并使双栅极光电薄膜晶体管的光栅极、源极由负电压充电至正电压,并向共电极施加负电压；若双栅极光电薄膜晶体管上一个状态处于正极性驱动状态,则向双栅极光电薄膜晶体管的漏极施加负偏压,并使双栅极光电薄膜晶体管的光栅极、源极由正电压放电至负电压,并向共电极施加正电压；此时显示信号通过双栅极光电薄膜晶体管的漏极、源极输送至显示单元,显示信号对显示单元进行驱动；b)保持阶段：完成写入操作后,向双栅极光电薄膜晶体管的暗栅极施加负偏压,并使光栅极、源极、共电极的电压保持不变；c)调变阶段：保持阶段持续一定时间后进入调变阶段,调变阶段对共电极电压进行提高或降低,为显示单元下一状态的驱动做准备,并使双栅极光电薄膜晶体管暗栅极、漏极的偏压保持不变；(2)指纹图像采集模式当触摸启动开关检测到触摸动作时,双栅极光电薄膜晶体管工作在指纹图像采集模式,这个模式包括以下步骤：d)复位阶段：双栅极光电薄膜晶体管的暗栅极设置为负电压,光栅极、源极、漏极和共电极设置为正偏压,对光栅极进行复位；e)收集阶段：完成复位后,使双栅极光电薄膜晶体管的暗栅极、漏极、光栅极和源极设置为负偏压,双栅极光电薄膜晶体管完全关闭,并使共电极保持正偏压,从手指反射进来的光被光栅极吸收,激发光生载流子的产生并保存在光栅极内；f)读取阶段：完成收集阶段后,向双栅极光电薄膜晶体管的暗栅极、漏极施加正偏压,并向共电极、光栅极、源极施加负偏压,使得保存在光栅极的光生载流子能够通过双栅极光电薄膜晶体管的源极输出。</td>   <td>G06K9/00;G09G3/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马争鸣;              光毓;                   刘希       </td>   <td>中山大学</td>   <td>一种基于样本选择的域自适应降维方法</td>   <td>广东省</td>   <td>CN110110739A</td>   <td>2019-08-09</td>   <td>本发明涉及机器学习中域自适应相关问题,提出了一种基于样本选择的域自适应降维算法。为了减少源域和目标域之间的数据分布差异,学习一个降维矩阵,把源域和目标域数据投影到一个低维子空间中。首先在源域和目标域样本中寻找一个子集,该子集中的样本是对衡量域间数据分布差异比较重要的样本,把这些样本叫做地标样本,利用地标样本匹配域间差异。在对投影矩阵进行优化时,本发明把投影矩阵看做是格雷斯曼流形上的点,把在欧式空间中的约束优化问题转化为格雷斯曼流形上的无约束优化问题,利用格雷斯曼流形上的共轭梯度法求解投影矩阵。交替优化投影矩阵和地标样本直至达到最大迭代次数,使域间差异达到最小。</td>   <td>1.一种基于样本选择的域自适应降维方法,其特征在于：A.令<Image id="icf0001" he="107" wi="612" file="FDA0002005180540000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0002" he="106" wi="599" file="FDA0002005180540000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>分别表示源域和目标域数据样本,其中N<Sub>s</Sub>表示源域样本个数,N<Sub>t</Sub>表示目标域样本个数,D为高维空间的维度；X<Sub>s</Sub>和X<Sub>t</Sub>具有不同且相关的数据分布；学习投影矩阵W,把源域和目标域数据共同映射到一个低维的潜在子空间,通过匹配降维后源域和目标域之间的类条件概率分布和边缘概率分布差异,得到源域和目标域数据在低维子空间中相应的数据表示<Image id="icf0003" he="96" wi="700" file="FDA0002005180540000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0004" he="96" wi="700" file="FDA0002005180540000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中d(d＜＜D)为低维空间的维度；用最大均值差异(MaximumMeanDiscrepancy简称MMD)测量域间的分布差异,源域和目标域间的边缘概率分布差异记为MMD<Sub>u</Sub>(Y<Sub>s</Sub>,Y<Sub>t</Sub>)；为目标域数据添加伪标签,源域和目标域间类条件概率分布差异为MMD<Sub>c</Sub>(Y<Sub>s</Sub>,Y<Sub>t</Sub>),最小化源域和目标域之间的边缘概率分布和类条件概率分布差异；B.用MMD衡量域间差异时,并不是用源域和目标域中所有样本数据,而是在源域和目标域分别定义指示向量α和β,用于选择部分样本作为地标样本,地标样本指那些对衡量域间差异比较重要的关键样本；选择地标样本后,再用MMD衡量域间边缘概率分布差异D<Sub>u</Sub>(α,β,W,Y<Sub>s</Sub>,Y<Sub>t</Sub>),类条件概率分布差异D<Sub>C</Sub>(α,β,W,Y<Sub>s</Sub>,Y<Sub>t</Sub>),为使得所选取的样本不至于使得样本类别失衡,加入类均衡的限制条件；得到目标函数：          <Image id="icf0005" he="89" wi="700" file="FDA0002005180540000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0006" he="106" wi="700" file="FDA0002005180540000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        对指示向量α、β及W交替进行优化,对α、β的求解转化为二次规划问题,对W的求解转化为格雷斯曼流形上的优化问题,利用格雷斯曼流形上的共轭梯度法求解；得到最终的W后,对于源域和目标域的输入数据,得到降维表示Y<Sub>s</Sub>和Y<Sub>t</Sub>。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         梁小丹;              高一鸣;              吴洋鑫;                   林倞       </td>   <td>中山大学</td>   <td>一种基于图迁移学习的通用人体解析框架及其解析方法</td>   <td>广东省</td>   <td>CN110111337A</td>   <td>2019-08-09</td>   <td>本发明公开了一种基于图迁移学习的通用人体解析框架及其解析方法,该框架包括：图像特征提取单元,用于利用卷积神经网络对人体图像提取图像特征；图内推理单元,用于将提取的图像特征映射到一个高层的图卷积神经网络中,其中的语义结点和边由数据集中类别以及类别间的关系决定,全局信息通过图内推理进行传播并重新映射以增强视觉特征的可区分性；图间迁移单元,用于从源图网络中提取相关的语义信息到目标图网络,以将不同数据集的标签联系起来,并利用不同标签集合之间的多种图的特征迁移,来加强目标图网络的类别特征。</td>   <td>1.一种基于图迁移学习的通用人体解析框架,包括：图像特征提取单元,用于利用卷积神经网络对人体图像提取图像特征；图内推理单元,用于将提取的图像特征映射到一个高层的图卷积神经网络中,其中的语义结点和边由数据集中类别以及类别间的关系决定,全局信息通过图内推理进行传播并重新映射以增强视觉特征的可区分性；图间迁移单元,用于从源图网络中提取相关的语义信息到目标图网络,以将不同数据集的标签联系起来,并利用来自不同数据集的不同标签集合之间的多种图的特征迁移,来加强目标图网络的类别特征。</td>   <td>G06T7/11;G06K9/62;G06K9/66;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         詹宜巨;              吕律;              蔡庆玲;                   唐承佩       </td>   <td>中山大学</td>   <td>一种基于头部姿态的眼睛注视视角测定方法</td>   <td>广东省</td>   <td>CN106529409B</td>   <td>2019-08-09</td>   <td>本发明公开了一种基于头部姿态的眼睛注视视角测定方法,其不需添加LED光源,仅采用一个摄像头、一个点激光器和布置在竖向实体标定平面上的标定点,即可完成训练过程,获得脸部模型数据库、三维头部姿态数据库和回归模型,再采用同一个摄像头获得用户在当前时刻的脸部图像,采用训练过程得到的数据通过实时测定过程,即可实时计算出用户的眼睛在当前时刻的眼睛注视视角,因此,本发明能够实现对用户眼睛注视视角进行测定,其对硬件的依赖小,能够有效的扩展眼睛注视视角测定的应用范围。</td>   <td>1.一种基于头部姿态的眼睛注视视角测定方法,包含训练过程和实时测定过程；所述的训练过程,包括：步骤1-1、原始数据获取步骤,包括：步骤1-1-1,在用户的头上佩戴一个朝向所述用户前方的点激光器(Laser),并在所述用户的前方安放一个摄像头、在所述用户前方的竖向实体标定平面(W)上设置多个标定点(Point),其中,所述用户的头部(Head)完全落在所述摄像头的拍摄范围之内,所述用户所在位置距离所述竖向实体标定平面(W)的距离、每一个所述标定点(Point)与用户的相对位置以及由所述距离和相对位置决定的用户二维头部姿态均为已知的预设值,该预设的二维头部姿态记为训练用二维头部姿态,所述训练用二维头部姿态以欧拉角表示,包括所述用户头部绕一个三维直角坐标系Y轴转动的Yaw旋转角和绕所述三维直角坐标系Z轴转动的Pitch旋转角,并且,所述用户头部绕所述三维直角坐标系X轴转动的欧拉角为Roll旋转角；步骤1-1-2,令所述用户在所述所在位置通过转动头部,使得所述点激光器(Laser)所发出的激光点分别落在所述各个标定点(Point)上,以用所述摄像头拍摄所述激光点落在每一个所述标定点(Point)时所述用户的脸部图像,并记为训练用脸部图像,其中,所述各个标定点(Point)中包含有正中标定点,当所述激光点落在正中标定点上时,所述激光器(Laser)的指向垂直于所述竖向实体标定平面(W),即所述用户头部的Yaw旋转角和Pitch旋转角均为0,将所述摄像头在所述激光点落在正中标定点上时拍摄到的训练用脸部图像记为正中训练用脸部图像；步骤1-2、将每一幅所述训练用脸部图像作为受处理脸部图像按以下步骤1-2-1至步骤1-2-5的方法进行处理,得到所述各幅训练用脸部图像的二维脸部特征,并保存在脸部模型数据库中,其中,所述二维脸部特征包含二维脸部特征像素点、眼部特征像素点、角膜缘特征像素点和瞳孔中心特征像素点；步骤1-2-1、对受处理脸部图像中的人脸进行定位；步骤1-2-2、用主动形状模型基于其默认的参数对所述受处理脸部图像中被定位出的人脸进行处理,得到所述受处理脸部图像中人脸的二维脸部特征像素点的坐标；步骤1-2-3、从所述受处理脸部图像的二维脸部特征像素点中提取出包围所述用户双眼眼部的特征像素点,记为眼部特征像素点,其中,所述用户的每一只眼睛对应有六个所述眼部特征像素点,位于中间位置的四个所述眼部特征像素点根据相互之间的相对位置分别记为左上方特征像素点、右上方特征像素点、右下方特征像素点和左下方特征像素点,其余两个所述眼部特征像素点根据相对位置分别记为内眼角特征像素点和外眼角特征像素点；步骤1-2-4、对所述受处理脸部图像位于所述用户每一只眼睛所对应眼部特征像素点围成区域内的图像区域进行处理,分别定位出所述受处理脸部图像中位于所述用户双眼角膜缘上的特征像素点,记为角膜缘特征像素点；步骤1-2-5、对所述受处理脸部图像的用户每一只眼睛对应的角膜缘特征像素点进行曲线拟合,得到所述受处理脸部图像中用户每一只眼睛对应的角膜缘曲线及该角膜缘曲线的中心像素点,将所述角膜缘曲线的中心像素点记为瞳孔中心特征像素点；步骤1-3、回归模型建立步骤,包括：步骤1-3-1、用所述主动形状模型基于所述脸部模型数据库计算每一所述训练用二维头部姿态所对应的用户头部的Roll旋转角,将所述训练用二维头部姿态及其对应的Roll旋转角记为训练用三维头部姿态,保存在三维头部姿态数据库中；并且,所述主动形状模型在训练过程即计算所述各个二维头部姿态所对应Roll旋转角的过程中自动对其默认的参数进行更新,使得主动形状模型的默认参数向适配于所述摄像头的参数变化,将所述主动形状模型完成所述训练过程后更新得到的参数记为适配参数；其中,在所述训练过程中计算得到的所述正中训练用脸部图像与用户本人的缩放比例记为s'；步骤1-3-2、将每一个所述训练用三维头部姿态及其对应的二维脸部特征作为输入,建立三维头部姿态与二维脸部特征的回归模型,并保存在所述三维头部姿态数据库中；所述的实时测定过程,包括：步骤2-1、用所述摄像头拍摄所述用户在当前时刻的脸部图像,并记为测定用脸部图像,其中,所述用户位于其头部(Head)能够完全落在所述摄像头拍摄范围之内的任意位置；步骤2-2、将所述测定用脸部图像作为受处理脸部图像按所述步骤1-2-1至步骤1-2-5的方法进行处理,并且,按所述步骤1-2-2的方法进行处理时,所述主动形状模型基于所述步骤1-3-1得到的适配参数对受处理脸部图像即所述测定用脸部图像中被定位出的人脸进行处理,得到所述测定用脸部图像的二维脸部特征、所述测定用脸部图像中人脸所对应的三维脸部特征点的坐标和测定用脸部图像与用户本人的缩放比例s,其中,所述二维脸部特征包含二维脸部特征像素点、眼部特征像素点、角膜缘特征像素点和瞳孔中心特征像素点；步骤2-4、将所述测定用脸部图像的二维脸部特征作为步骤1-3-2所建立回归模型的输入,计算出所述测定用脸部图像的三维头部姿态,即所述用户在拍摄该测定用脸部图像时的欧拉角,包括Yaw旋转角、Pitch旋转角和Roll旋转角,记为三维头部姿态欧拉角(φ<Sub>Roll</Sub>,φ<Sub>Yaw</Sub>,φ<Sub>Pitch</Sub>)；步骤2-5、用户的眼睛注视视角实时计算步骤,包括：步骤2-5-1、建立眼部模型,即：将所述用户的一只真实眼球视为一个球体,并将所述真实眼球按照所述步骤2-2得到的缩放比例s进行缩放后得到的球体记为图像眼球,其中,所述真实眼球的半径预设为R<Sub>0</Sub>、眼球中心至外眼角特征点与内眼角特征点所在直线的距离预设为L<Sub>0</Sub>,所述图像眼球的球心记为o；并且,将所述步骤2-2得到的三维脸部特征点中对应所述图像眼球的外眼角特征点、内眼角特征点和瞳孔中心特征点依次记为e1、e2和p,将所述外眼角特征点e1与内眼角特征点e2的中点记为m,所述球心o在所述外眼角特征点e1与内眼角特征点e2的连线上的垂足记为n；所述球心o与瞳孔中心特征点p之间的距离即为所述图像眼球的半径R,所述球心o到所述垂足n的方向即为所述用户在拍摄所述测定用脸部图像时的头部姿态方向V1,该头部姿态方向V1用欧拉角表示即为所述步骤2-4得到的三维头部姿态欧拉角(φ<Sub>Roll</Sub>,φ<Sub>Yaw</Sub>,φ<Sub>Pitch</Sub>),所述球心o到所述瞳孔中心特征点p的方向即为所述用户在拍摄所述测定用脸部图像时的眼睛注视方向V2,该眼睛注视方向V2用欧拉角表示即为所述用户的眼睛注视视角,记为(ω<Sub>Roll</Sub>,ω<Sub>Yaw</Sub>,ω<Sub>Pitch</Sub>)；步骤2-5-2、以所述测定用脸部图像的二维脸部特征、三维脸部特征点的坐标和三维头部姿态作为所述眼部模型的输入,按以下公式七至公式十一计算出所述用户在当前时刻即拍摄所述测定用脸部图像时的眼睛注视视角(ω<Sub>Roll</Sub>,ω<Sub>Yaw</Sub>,ω<Sub>Pitch</Sub>)：          <Image id="icf0001" he="61" wi="700" file="FDA0002017178460000041.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0002" he="61" wi="700" file="FDA0002017178460000042.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0003" he="91" wi="700" file="FDA0002017178460000043.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0004" he="41" wi="700" file="FDA0002017178460000044.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0005" he="69" wi="700" file="FDA0002017178460000045.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,L为所述球心o与垂足n之间的距离,T为所述中点m与垂足n之间的距离,s'为所述正中训练用脸部图像与用户本人的缩放比例,(p<Sub>x</Sub>,p<Sub>y</Sub>,p<Sub>z</Sub>)、(e1<Sub>x</Sub>,e1<Sub>y</Sub>,e1<Sub>z</Sub>)和(e2<Sub>x</Sub>,e2<Sub>y</Sub>,e2<Sub>z</Sub>)依次为所述步骤2-2得到的三维脸部特征点中对应所述图像眼球的瞳孔中心特征点p、外眼角特征点e1和内眼角特征点e2的坐标,(m<Sub>x</Sub>,m<Sub>y</Sub>,m<Sub>z</Sub>)为所述中点m的坐标,(o<Sub>x</Sub>,o<Sub>y</Sub>,o<Sub>z</Sub>)为所述球心o的坐标。</td>   <td>G06K9/00;A61B3/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              陆瑞智;                   谢晓华       </td>   <td>中山大学</td>   <td>基于双流卷积神经网络的双目图像快速目标检测方法</td>   <td>广东省</td>   <td>CN110110793A</td>   <td>2019-08-09</td>   <td>本发明公开了一种基于双流卷积神经网络的双目图像快速目标检测方法,包括步骤：对双目摄像头进行标定,得到标定参数；根据标定参数对训练图像进行校正,训练隐式深度语义挖掘网络用于在双目图像上隐式地学习深度语义信息,训练多模态特征混合检测网络；将隐式深度语义挖掘网络输出的特征与多模态特征混合检测网络的特征通过通道串联的方式结合在一起,便组成双流卷积神经网络,利用训练图像训练双流卷积神经网络；通过双目摄像头获取测试图像,并对其进行校正,将校正后的图像输入到上述双流卷积神经网络中进行目标检测,得到目标检测结果。本发明可以综合利用RGB和深度语义信息的互补性,具有效率高、目标检测结果更准确的优点。</td>   <td>1.基于双流卷积神经网络的双目图像快速目标检测方法,其特征在于,包括步骤：(1)对双目摄像头进行标定,得到标定参数；(2)根据标定参数对训练图像进行校正,训练隐式深度语义挖掘网络用于在双目图像上隐式地学习深度语义信息,训练多模态特征混合检测网络；将隐式深度语义挖掘网络输出的特征与多模态特征混合检测网络的特征通过通道串联的方式结合在一起,便组成双流卷积神经网络,利用训练图像训练双流卷积神经网络；(3)通过双目摄像头获取测试图像,并对其进行校正,将校正后的图像输入到上述双流卷积神经网络中进行目标检测,得到目标检测结果。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李中华;                   林柏顶       </td>   <td>中山大学</td>   <td>基于RFID和极限学习机的冷链产品质量预测及智能配送方法</td>   <td>广东省</td>   <td>CN110110909A</td>   <td>2019-08-09</td>   <td>本发明涉及物联网技术领域,提出一种基于RFID和极限学习机的冷链产品质量预测及智能配送方法,包括以下步骤：在冷链产品进行配送路线规划前,采用RFID传感器标签采集冷链产品的环境数据并作为极限学习机的输入,通过极限学习机训练与预测产品的质量指数；以配送总成本最小为目标根据质量指数通过路径规划算法对配送路线进行规划,输出原始配送方案并进行配送；在配送过程中定时通过RFID识别传感器标签对冷链产品的环境数据进行采集,通过极限学习机预测当前质量指数,并判断：若当前质量指数大于或等于报警阈值,则按原始配送方案继续配送；若当前质量指数小于报警阈值,则发出报警信息并采取动态调度策略对配送方案进行优化规划。</td>   <td>1.一种基于RFID和极限学习机的冷链产品质量预测及智能配送方法,其特征在于,包括以下步骤：S1：在冷链产品进行配送路线规划前,采用射频识别传感器标签采集冷链产品的环境数据；S2：将采集的环境数据作为极限学习机的输入,通过极限学习机训练与预测产品的质量指数C；S3：以配送总成本最小为目标根据质量指数C通过路径规划算法对配送路线进行规划,输出原始配送方案,然后根据输出的原始配送方案对冷链产品进行配送；S4：在配送过程中定时通过射频识别传感器标签对冷链产品的环境数据进行采集,通过极限学习机预测冷链产品当前的质量指数C′,然后判断当前的质量指数C′与预设的报警阈值的大小关系：若当前的质量指数C′大于或等于预设的报警阈值,则按照原始配送方案继续进行配送；若当前的质量指数C′小于预设的报警阈值,则发出警报信息,采取动态调度策略对配送方案进行优化规划。</td>   <td>G06Q10/04;G06Q10/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张小虎;              林彬;              王扬洋;              叶雪辀;              甘叔玮;                   杨夏       </td>   <td>中山大学</td>   <td>芯片气泡的检测方法、装置及存储介质</td>   <td>广东省</td>   <td>CN110097542A</td>   <td>2019-08-06</td>   <td>本发明公开了一种芯片气泡的检测方法,包括以下步骤：根据获取待测芯片的结构信息进行多层结构建模,再获取芯片图像,基于芯片结构轮廓对图像进行多区自动分割,识别出各目标区域,对所述芯片图像进行图像二值化处理,得到二值化图像,识别所述二值化图像中各目标区域的灰度值非零的像素点,并根据所述灰度值非零的像素点确定各目标区域的气泡轮廓,对所有区域的气泡进行相应的气泡大小以及气泡位置的统计。本发明还公开了一种芯片气泡的检测装置以及计算机可读存储介质。本发明提高了芯片气泡检测的效率。</td>   <td>1.一种芯片气泡的检测方法,其特征在于,所述芯片气泡的检测方法包括以下步骤：获取芯片图像,所述芯片图像通过拍摄待检测芯片得到；对所述芯片图像进行图像二值化处理,得到二值化图像；根据所述二值化图像中灰度值非零的像素点,确定所述芯片图像中的气泡。</td>   <td>G06T7/00;G06T3/60;G06T7/11</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              肖翔;              张伟;                   顾建权       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于密集轨迹的动作识别方法</td>   <td>广东省</td>   <td>CN105956517B</td>   <td>2019-08-02</td>   <td>本发明公开一种基于密集轨迹的动作识别方法,包括：1)对输入视频每一帧的用密集采样法获取密集点,并对密集点在光流域中进行跟踪,形成该视频的轨迹；2)对轨迹进行筛选,提取位于中心区域的轨迹视作前景的轨迹,将区域外的轨迹视作背景轨迹进行删除；3)提取轨迹的形状特征,梯度方向直方图特征,光流直方图特征,运动边缘直方图特征,以及运动邻域特征；4)对每一种特征分别采用增强型局部级联描述子向量方法进行特征表示,得到3)中五种特征的向量表示,将这五种特征向量级联起来,得到最终这个视频的中层表示；5)采用支持向量机进行特征分类,得到识别准确率。</td>   <td>1.一种基于密集轨迹的动作识别方法,其特征在于,包括以下步骤：(1)输入待识别的视频,对输入视频的每一帧,利用密集采样得到密集的抽样点,对抽样点进行跟踪形成密集轨迹；(2)筛选出视频帧中心区域的轨迹作为前景轨迹,区域以外的轨迹视作背景轨迹予以删除；(3)提取视频帧中心区域内的前景轨迹的五种描述子特征：形状特征(TS),梯度方向直方图特征(HOG),光流直方图特征(HOF),运动边缘直方图特征(MBH)以及轨迹运动邻域特征(TMNF)；(4)对步骤(3)提取的每一种描述子特征分别采用增强型局部级联描述子向量方法(IVLAD)进行特征建模,得到每一种描述子特征的向量表示,然后将这五种特征向量级联起来,形成该视频最终的向量表示；(5)采用支持向量机(SVM)进行特征分类,最终输出分类结果,获取视频的动作识别结果,在YouTube人体行为数据集上实现了91.4％的准确率；其中,步骤(4)中,描述子特征表示为{x<Sub>1</Sub>,...x<Sub>i</Sub>,...x<Sub>d</Sub>},d表示该描述子特征的总维数,对描述子特征{x<Sub>1</Sub>,...x<Sub>i</Sub>,...x<Sub>d</Sub>},假设利用k-means聚类已经学习得到的码本(codebook)和k个中心(c<Sub>1</Sub>,...c<Sub>j</Sub>,...c<Sub>k</Sub>),采用增强型局部级联描述子向量方法(IVLAD)对特征进行特征建模,具体过程如下：(5-1)描述子特征的每一维x<Sub>i</Sub>赋给码本(codebook)中离该描述子特征最近的聚类中心c<Sub>j</Sub>,然后得到量化后的索引：NN(x<Sub>i</Sub>)＝arg min<Sub>j</Sub>||x<Sub>i</Sub>-c<Sub>j</Sub>||(5-2)将所有离聚类中心c<Sub>j</Sub>最近的描述子特征,计算这些描述子特征和中心c<Sub>j</Sub>的差的累积和并且进行归一化,获得子向量：          <Image id="icf0001" he="204" wi="591" file="FDA0002025985050000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        (5-3)将各子向量u<Sup>j</Sup>连接起来,得到k×d维的向量表示u＝[u<Sup>1</Sup>...u<Sup>k</Sup>]；(5-4)对向量u先采用“power-law”归一化处理,即：v<Sub>j</Sub>＝|v<Sub>j</Sub>|<Sup>α</Sup>×sign(v<Sub>j</Sub>),j＝1...k×d,α＝0.2随后用2范数归一化(L2-normalized),最终得到该描述子特征的IVLAD表示。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李定林;              华丕龙;              彭鹏;              陈满;              江淑文;              刘广立;              韩正洋;              石江健;                   张昊英       </td>   <td>南方电网调峰调频发电有限公司;中山大学</td>   <td>海岛环境下的植被调查方法、装置、系统、设备和介质</td>   <td>广东省</td>   <td>CN110084120A</td>   <td>2019-08-02</td>   <td>发明公开了海岛环境下的植被调查方法、装置、系统、设备和介质、针对于需要进行植被调查的海岛,首先获取无人机航拍的多张二维地形图像,对海岛植被地形进行三维重建,得到海岛的三维地形图；然后根据无人机拍摄指定区域的植物图像时的位置坐标,确定出无人机拍摄的植物图像所属指定区域位于三维地形图中的位置；针对无人机拍摄到的指定区域的多张植物图像,选取存在植物叶片的图像,并且将其中的植物叶片分割出来,得到植物叶片目标；将上述获取到的植物叶片目标与植物叶片图像数据库进行对比,根据对比结果识别到物种信息。本发明能够快速以及准确的实现海岛环境植被调查的优点,为海岛上的植被资源勘察提供了一种快速安全的方法。</td>   <td>1.一种海岛环境下的植被调查方法,其特征在于,步骤如下：针对于需要进行植被调查的海岛,获取无人机航拍的多张二维地形图像,根据无人机每次航拍到的二维地形图像以及无人机每次航拍二维地形图像时的位置坐标对海岛植被地形进行三维重建,得到海岛的三维地形图；获取无人机拍摄到的指定区域的多张植物图像,根据无人机拍摄指定区域的植物图像时的位置坐标,确定出无人机拍摄的植物图像所属指定区域位于三维地形图中的位置；针对于无人机拍摄到的指定区域的每张植物图像,首先对其进行切分处理,将其中的各棵植物提取出来,得到仅包括一棵植物的一张或多张植物图；针对于上述获取到的仅包括一棵植物的植物图像,将其中的植物叶片分割出来,得到植物叶片目标图像；将上述获取到的植物叶片目标图像与植物叶片图像数据库进行对比,根据对比结果识别到物种信息。</td>   <td>G06K9/00;G06K9/40;G06K9/62;G06T7/155</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              叶灵昶;                   王青       </td>   <td>中山大学</td>   <td>一种人体骨骼关键点的快速检测方法及系统</td>   <td>广东省</td>   <td>CN110084161A</td>   <td>2019-08-02</td>   <td>本发明公开了一种人体骨骼关键点的快速检测方法及系统,所述方法包括如下步骤：步骤S1,构建并训练一卷积神经网络,以通过所述卷积神经网络利用多尺寸图像特征信息来检测图像中人体的各个部位；步骤S2,获取一二维彩色图；步骤S3,将所述二维彩色图输入至步骤S1中经训练好的卷积神经网络中,利用多尺寸图像特征信息来检测图像中人体的各个部位,并将该些检测到的部位聚合起来以形成单人的骨骼点集合,完成关键点的检测,本发明通过设计轻量化的卷积神经网络结构,可减少运算时间,实现在智能相机上实现人体骨骼关键点的检测。</td>   <td>1.一种人体骨骼关键点的快速检测方法,包括如下步骤：步骤S1,构建并训练一卷积神经网络,以通过所述卷积神经网络利用多尺寸图像特征信息来检测图像中人体的各个部位；步骤S2,获取一二维彩色图；步骤S3,将所述二维彩色图输入至步骤S1中经训练好的卷积神经网络中,利用多尺寸图像特征信息来检测图像中人体的各个部位,并将该些检测到的部位聚合起来以形成单人的骨骼点集合,完成关键点的检测。</td>   <td>G06K9/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              惠晓璐;              陈添水;              许慕欣;                   王青       </td>   <td>中山大学</td>   <td>一种基于特定语义的图表示学习框架及其多标签分类方法</td>   <td>广东省</td>   <td>CN110084296A</td>   <td>2019-08-02</td>   <td>本发明公开了一种基于特定语义的图表示学习框架及其多标签分类方法,该框架包括：语义结耦模块,用于利用卷积神经网络对输入图像提取图像特征,将图像特征与语义特征相结合,并引入注意机制,利用语义特征引导图像特征权重的学习,并作用于图像特征,得到新的特征向量；语义交互模块,用于先通过构建知识图谱统计数据集中类别共存的关联性来构建大型知识图谱,再利用一个门图网络来对知识图谱进行特征表达,迭代的更新知识图谱得到知识图谱的特征表示；知识嵌入表达模块,用于将所述语义交互模块知识表达学习到的特征表示与所述语义结耦模块提取的图像特征学习相结合,以实现多标签分类。</td>   <td>1.一种基于特定语义的图表示学习框架,包括：语义结耦模块,用于利用卷积神经网络对输入图像提取图像特征,将图像特征与语义特征相结合,并引入注意机制,利用语义特征引导图像特征权重的学习,并作用于图像特征,得到新的特征向量；语义交互模块,用于先通过构建知识图谱统计数据集中类别共存的关联性来构建大型知识图谱,再利用一个门图网络来对知识图谱进行特征表达,迭代的更新知识图谱得到知识图谱的特征表示；知识嵌入表达模块,用于将所述语义交互模块知识表达学习到的特征表示与所述语义结耦模块提取的图像特征学习相结合,以实现多标签分类。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李中华;                   林柏涛       </td>   <td>中山大学</td>   <td>一种基于非监督学习和多属性决策的分区间供应商选择方法</td>   <td>广东省</td>   <td>CN110084483A</td>   <td>2019-08-02</td>   <td>本发明涉及一种基于非监督学习和多属性决策的分区间供应商选择方法。当潜在供应商数据集S属于大样本区间时,本方法首先采用非监督学习方法将潜在供应商聚类成若干类,再借助多属性决策方法对所有供应商类进行排序,从而选出优质供应商类,剔除一般供应商类。上述过程不断迭代,直到潜在供应商数据集S减少到小样本区间时,再通过多属性决策方法对潜在供应商进行最终排序与选择。当潜在供应商数据集S属于小样本区间时,本方法直接采用多属性决策方法对潜在供应商进行排序与选择。本方法可以便捷有效地实现从任意规模的潜在供应商数据集中筛选出预期数量的合作供应商,对大数据时代背景下企业的供应链管理决策提供有力支持。</td>   <td>1.一种基于非监督学习和多属性决策的分区间供应商选择方法,其特征在于,包括以下步骤：步骤S1：选取供应商评价指标,结合供应商评价指标获取潜在供应商的评价数据；步骤S2：对潜在供应商数据集S进行预处理；步骤S3：根据行业背景和企业的实际经营情况,设定一个规模阈值X,通过比较X与潜在供应商数据集S的规模大小|S|来判断S的区间属性,即若|S|&gt;X,则潜在供应商数据集S属于大样本区间；若|S|&lt;X,则潜在供应商数据集S属于小样本区间。若潜在供应商数据集S属于大样本区间,则执行步骤S4；若潜在供应商数据集S属于小样本区间,则执行步骤S6；步骤S4：采用非监督学习方法对潜在供应商数据集S进行聚类分析,获得聚类结果；步骤S5：根据聚类结果得到的不同供应商类,采用多属性决策方法对供应商类进行排序并筛选,重复该步骤直至潜在供应商数据集S减少到小样本区间；步骤S6：采用多属性决策方法对潜在供应商进行最终排序,选择出预期数量的合作供应商。</td>   <td>G06Q10/06;G06Q10/08;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苏琬棋;              陈志广;              瞿毅力;              邓楚富;              卢宇彤;              肖侬;                   王莹       </td>   <td>中山大学</td>   <td>一种基于生成对抗网络的多域图像转换方法与系统</td>   <td>广东省</td>   <td>CN110084863A</td>   <td>2019-08-02</td>   <td>本发明公开了一种基于生成对抗网络的多域图像转换方法与系统,本发明的多域图像转换方法包括输入指定的X、Y两个模态的原图x、原图y；在重建训练部分针对原图x、原图y分别进行编、解压,分别得到原图特征、重建图、重建特征,并进行特征和图的模态鉴别对抗学习；循环训练部分基于前文的原图特征交换模态的编码器生成重建图、重建图特征以及循环重建图,并再次进行特征和图的模态鉴别对抗学习,最终将循环重建图输出。本发明采用半监督学习方法,既可以利用已有的标签数据也可以使用无标签数据,本发明能够实现多向的多域图像转换而不限于单向域转换或双向的二域转换,对域的数量没有限制,能解决图像风格迁移和医学图像多模态转换等问题。</td>   <td>1.一种基于生成对抗网络的多域图像转换方法,其特征在于实施步骤包括：1)输入指定的X、Y两个模态的原图x、原图y；2)将原图x进行X模态编码得到第一原图特征code_x,将第一原图特征code_x进行X模态解码得到第一重建图x',将第一重建图x'进行X模态编码得到第一重建特征code_x'；将原图y进行Y模态编码得到第二原图特征code_y,将第二原图特征code_y进行Y模态解码得到第二重建图y',将第二重建图y'进行Y模态编码得到第二重建特征code_y'；3)将第一原图特征code_x、第一重建特征code_x'进行特征鉴别,将第二原图特征code_y、第二重建特征code_y'进行特征鉴别对抗学习；将原图x、第一重建图x'进行X模态鉴别对抗学习,将原图y、第二重建图y'进行Y模态鉴别对抗学习；4)将第一原图特征code_x进行Y模态解码得到第三重建图y”,将第三重建图y”进行Y模态编码得到第三重建图特征code_y”,将第三重建图特征code_y”进行X模态解码得到第一循环重建图x”'；将第二原图特征code_y进行X模态解码得到第四重建图x”,将第四重建图x”进行X模态编码得到第四重建图特征code_x”,将第四重建图特征code_x”进行Y模态解码得到第二循环重建图y”'；5)将原图x、第四重建图x”进行X模态鉴别对抗学习,将原图y、第三重建图y”进行Y模态鉴别对抗学习；将第一原图特征code_x、第三重建图特征code_y”进行特征鉴别对抗学习,将第二原图特征code_y、第四重建图特征code_x”进行特征鉴别对抗学习；6)计算系统网络总体损失；7)对系统网络总体损失求导,开启反向传播每个损失函数反向逐层计算出各层参数的梯度值,然后根据各层参数梯度更新这些参数,完成本轮迭代,且所述第一循环重建图x”'以及第二循环重建图y”'构成本轮迭代的输出结果。</td>   <td>G06T9/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         曾坤;              区炳坚;                   周凡       </td>   <td>中山大学</td>   <td>一种基于生成式对抗网络的人像卡通化方法</td>   <td>广东省</td>   <td>CN110070483A</td>   <td>2019-07-30</td>   <td>本发明公开了一种基于生成式对抗网络的人像卡通化方法。本发明通过生成式网络将人脸与背景分割、将人脸转化为卡通的人脸、将背景转化为卡通背景,之后合成得到卡通图,并用判别式网络进行判别；然后通过损失函数对生成式网络与判别式网络进行训练；最后把待处理人脸图像输入训练好的生成式网络即可生成对应的卡通图像。本发明有助于根据输入的人脸图片全自动的生成人像卡通图片,或者根据用户输入人脸图像,给出推荐的卡通化方案,供用户选择或修改,节省用户选取素材拼接的时间。</td>   <td>1.一种基于生成式对抗网络的人像卡通化方法,其特征在于,所述方法包括：步骤一,获取人脸数据训练集和卡通数据训练集；步骤二,对人脸数据训练集进行预处理,得到头发掩模、脸部掩模以及五官掩模；步骤三,构建生成式网络,将人脸数据训练集中的人脸图像转化为卡通图像,以及将卡通数据训练集中的卡通图像转化为人脸图像；步骤四,构建判别式网络,对转化而成的卡通图像以及转化而成的人脸图像分别进行判别；步骤五,根据步骤二生成的掩模、步骤三生成式网络生成的人脸和卡通图像、步骤四得到的判别结果,计算损失函数值并优化生成式网络和判别式网络；步骤六,重复步骤三到步骤五,循环迭代多轮,得到训练好的卡通图像生成式网络；步骤七,把待处理的人脸图像输入到最终得到的卡通生成式网络,则可得到对应的具有个人特色的卡通图像。</td>   <td>G06T3/00;G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         田茜;              郑慧诚;                   王腾       </td>   <td>中山大学</td>   <td>一种结合边界分布与纠正的事件提名方法</td>   <td>广东省</td>   <td>CN110059584A</td>   <td>2019-07-26</td>   <td>本发明提供的一种结合边界分布与纠正的事件提名方法,通过构建起点分布网络、终点分布网络和边界循环修正网络形成事件提名网络；通过构建事件提名网络损失函数对事件提名网络进行训练更新；利用训练更新后的事件提名网络对视频事件进行提名预测；所述起点分布网络、终点分布网络用于事件提名预测；所述边界循环修正网络用于产生预测的事件提名的偏置信息,对事件提名边界进行修正。本发明提供的一种结合边界分布与纠正的事件提名方法,结合了真实视频中的事件起止点分布规律产生拟合真实事件分布的事件提名,并利用循环修正网络对事件提名的边界进行修正,从而得到更加符合现实事件且使事件的边界更加精确的事件提名。</td>   <td>1.一种结合边界分布与纠正的事件提名方法,其特征在于：通过构建起点分布网络、终点分布网络和边界循环修正网络形成事件提名网络；通过构建事件提名网络损失函数对事件提名网络进行训练更新；利用训练更新后的事件提名网络对视频事件进行提名预测；所述起点分布网络、终点分布网络用于事件提名预测；所述边界循环修正网络用于产生预测的事件提名的偏置信息,对事件提名边界进行修正。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         毛莉;                   李中华       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院</td>   <td>WSN充电服务站点设定方法及充电设备行驶路径规划方法</td>   <td>广东省</td>   <td>CN110059848A</td>   <td>2019-07-26</td>   <td>本发明公开了一种WSN充电服务站点设定方法及充电设备行驶路径规划方法,通过若干个大小相等的图形将WSN划分成若干个区域,将每个传感器节点分别对应到所属的区域中,并在每个区域中根据传感器节点的位置选取锚节点,利用锚节点作为充电服务站点的设定基准,可以简单、合理地确定充电服务站点的最优位置；而通过人工免疫模型,筛选出移动无线充电设备完成能量补充任务所需总耗能最小的行驶路径,有效地节约了能量资源；另一方面,由于人工免疫模型的多样性和快速的收敛速度,有利于提高行驶路径的优化准确性以及获取效率。</td>   <td>1.一种WSN充电服务站点设定方法,其特征在于,包括：利用若干个大小相等的图形将WSN划分成若干个区域；将每个传感器节点分别对应到所属的区域中；在每个区域中根据传感器节点的位置选取锚节点；将充电服务站点设定在与各个锚节点的距离之和最小的位置。</td>   <td>G06Q10/04;G06Q50/06;G06N3/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈创荣;              成慧;                   范正平       </td>   <td>中山大学</td>   <td>一种基于3D卷积神经网络的双目视差计算方法</td>   <td>广东省</td>   <td>CN110060290A</td>   <td>2019-07-26</td>   <td>本发明涉及一种基于3D卷积神经网络的双目视差计算方法。包括：S1.根据定义的多尺度特征提取方法对输入的左右视图分别进行特征提取；S2.堆叠左右图相应视差对应位置的特征得到4D的cost volume；S3.使用3D CNN子网络进行代价聚合,得到视差值的对数似然估计,并且上采样到原图分辨率,得到每个像素的可能视差值的对数似然估计,进行对数归一化操作得到新的对数似然估计；S4.计算设置的真实分布；S5.进行反向传播训练；S6.得到每个像素的视差对数似然分布后,转换成概率得到视差概率分布；S7.找到对应最大概率的视差值,S8.由前述左右视差值和视差概率分布,得到归一化概率分布；S9.通过加权平均操作得到每个像素视差的最终估计值。本发明可以有效提高视差计算的精度。</td>   <td>1.一种基于3D卷积神经网络的双目视差计算方法,其特征在于,包括以下步骤：S1.构建一种用于多尺度特征提取网络结构,根据该结构定义一种多尺度特征提取方法；S2.根据S1步骤提供的特征提取网络方法,对输入的左右两幅图像分别进行特征提取,得到的特征设为F<Sub>1</Sub>、F<Sub>2</Sub>；S3.根据提取得到的左右图特征F<Sub>1</Sub>和F<Sub>2</Sub>,堆叠左右图相应视差对应位置的特征得到4D的cost volume；S4.基于构建的4D cost volume,使用3D CNN子网络进行代价聚合,得到视差值的对数似然估计,并且上采样到原图分辨率,得到每个像素的可能视差值的对数似然估计,进行对数归一化操作得到新的对数似然估计,定义为L；S5.根据训练数据的视差真实值,计算设置的真实分布；S6.根据对数似然估计和真实分布,计算交叉熵,得到loss,利用该loss进行反向传播训练；S7.局部推断：得到每个像素的视差对数似然分布后,转换成概率得到视差概率分布P<Sub>i</Sub>；S8.基于得到的视差概率分布P<Sub>i</Sub>,找到对应最大概率的视差值,设为d<Sub>max</Sub>；S9.由前述左右视差值和视差概率分布,得到归一化概率分布<Image id="icf0001" he="74" wi="60" file="FDA0001995638180000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>S10.通过加权平均操作得到每个像素视差的最终估计值<Image id="icf0002" he="77" wi="73" file="FDA0001995638180000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image></td>   <td>G06T7/55;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         闫琦;              杨锐;                   黄继武       </td>   <td>深圳大学;中山大学</td>   <td>一种检测并定位语音片段内的平滑处理的方法</td>   <td>广东省</td>   <td>CN110060703A</td>   <td>2019-07-26</td>   <td>本发明公开了一种检测并定位语音片段内的平滑处理的方法,包括以下步骤：S1.选定平滑滤波器；S2.选取原始语音,提取原始语音集,并通过所述滤波器处理成训练语音集；S3.从原始语音和训练语音集提取特征集；S4.将原始语音的特征集和训练语音集的特征集各筛选出样本,采用分类器训练出SVM分类器模型；S5.选取待测语音,将待测语音进行分帧,对每一帧信号都提取待测语音特征集；S6.使用步骤S4的SVM分类器模型对待测语音特征集进行分类,判断信号是否经过平滑处理,如果是,则定位平滑处理所在的位置。本发明的优点在于,本发明提出的方法比现有同类的检测方法明显具有更高的检测率,可以作为判别数字语音是否被平滑处理的一种高成功率的方法。</td>   <td>1.一种检测并定位语音片段内的平滑处理的方法,其特征在于,包括以下步骤：S1.选定平滑滤波器；S2.选取原始语音,提取原始语音集,并通过所述滤波器处理成训练语音集；S3.从所述原始语音和训练语音集提取特征集；S4.将所述原始语音的特征集和训练语音集的特征集各筛选出样本,采用分类器训练出SVM分类器模型；S5.选取待测语音,将所述待测语音进行分帧,对每一帧信号都提取待测语音特征集；S6.使用所述步骤S4的SVM分类器模型对待测语音特征集进行分类,判断信号是否经过平滑处理,如果是,则定位平滑处理所在的位置。</td>   <td>G10L25/54;G10L25/51</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈旭游;              王若梅;              周凡;              陈小燕;                   林格       </td>   <td>中山大学</td>   <td>基于MPM的血液凝固模拟方法及系统</td>   <td>广东省</td>   <td>CN109271696B</td>   <td>2019-07-23</td>   <td>本发明公开了一种基于MPM的血液凝固模拟方法,包括：S1,初始化通用数据；S2,将血液离散化成粒子和网格；S3,根据物质点法模拟血液凝固。本发明还公开了一种基于MPM的血液凝固模拟方法系统。采用本发明,可基于MPM框架,根据凝固的特点结合连续介质力学以及流体力学N-S方程,并且根据血液凝固速度设计拉梅系数函数,来精确地模拟血液凝固过程。</td>   <td>1.一种基于MPM的血液凝固模拟方法,其特征在于,包括：S1,初始化通用数据；S2,将血液离散化成粒子和网格；S3,根据物质点法模拟血液凝固；所述步骤S3包括：S31,初始化粒子数据；S32,将粒子的数据映射到网格点上,并计算粒子的密度和体积；S33,进行网格点上的计算；其中,所述步骤S33包括：S331,计算网格点的所受的力；S332,根据所受的力更新网格点的速度临时值,具体地,当血液为流体时,采用N-S方程更新网格点的速度临时值；当血液为固体时,采用连续介质力学更新网格点的速度临时值；S333,处理网格点的碰撞,并更新网格点的速度临时值；S334,更新速度临时值为网格点的最终速度；S34,更新粒子变形梯度；S35,更新粒子的速度；S36,处理粒子的碰撞；S37,更新粒子的位置。</td>   <td>G06F17/50;G06T13/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周凡;              戴熹;              林淑金;                   苏卓       </td>   <td>中山大学</td>   <td>基于PBF的流体凝固模拟方法及系统</td>   <td>广东省</td>   <td>CN109344450B</td>   <td>2019-07-23</td>   <td>本发明公开了一种基于PBF的流体凝固模拟方法,包括：S1,初始化数据；S2,粒子固定半径邻域搜索；S3,更新粒子的凝固状态；S4,将速度衰减因子作用于粒子,计算粒子的位置中间量；S5,计算粒子密度及运动约束量；S6,计算粒子的位置变化量,并进行防穿透修正；S7,更新粒子速度及位置,并处理粒子与边界碰撞问题；S8,计算人造粘性和涡流约束。本发明还公开了一种基于PBF的流体凝固模拟系统。本发明,可通过不同的粒子速度衰减因子来模拟不同的流体模拟的快慢,还可通过位置变化量修正粒子穿透现象,在GPU的支持下实现更大规模更有效率的流体凝固模拟。</td>   <td>1.一种基于PBF的流体凝固模拟方法,其特征在于,包括：S1,初始化数据；S2,粒子固定半径邻域搜索；具体地,根据流体运动空间的包围盒大小以及平滑半径长度将包围盒划分成大小均匀的网长方体网格,根据粒子的位置,计算出粒子所在网格格子的索引,并为每个格子维护一个其所包含的粒子的索引表,从而对于每个粒子,查找器固定半径内的邻域粒子只需要至多在27个网格格子内寻找,即每个粒子本身所在格子以及与该格子紧密邻接的另外至多26个格子,在这27个格子内,对于每个粒子而言,与其距离小于等于平滑核半径的粒子即为其邻域粒子；S3,更新粒子的凝固状态；粒子的凝固状态分为三种,分别为：非凝固状态、在凝固状态和已凝固状态；若粒子的凝固状态为“非凝固状态”,计算凝固发生值Th<Sub>i</Sub>,当Th<Sub>i</Sub>的值达到指定阈值时,更新粒子的凝固状态为“在凝固状态”,其中,<Image id="icf0001" he="129" wi="383" file="FDA0002035635630000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>W<Sub>ij</Sub>＝W(x<Sub>ij</Sub>,h),x<Sub>ij</Sub>和h分别表示粒子i到粒子j的位移和固定的核平滑半径,m、ρ<Sub>0</Sub>、Th<Sub>j</Sub>分别表示粒子的质量、初始密度和凝固发生值；若粒子的凝固状态为在凝固状态,计算速度衰减因子,当速度衰减因子为0时,更新粒子的状态为已凝固状态；S4,将速度衰减因子作用于粒子,计算粒子的位置中间量；具体地,所述步骤S4包括：S41,计算粒子所受合力、加速度及当前速度；S42,根据粒子的凝固状态计算出粒子的速度衰减因子,并将速度衰减因子作用于粒子,计算出粒子的位置中间量；S5,计算粒子密度及运动约束量；S6,计算粒子的位置变化量,并进行防穿透修正；S7,更新粒子速度及位置,并处理粒子与边界碰撞问题；S8,计算人造粘性和涡流约束；所述步骤S2到S8,通过设置合适的CUDA线程块和每块线程数,以使得每个粒子的每步计算都可以分配到GPU的一个线程上单独执行,从而实现整个模拟算法的并行化计算。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         袁宝煜;                   郑伟诗       </td>   <td>中山大学</td>   <td>基于多层特征学习的行人属性识别系统及方法</td>   <td>广东省</td>   <td>CN110046550A</td>   <td>2019-07-23</td>   <td>本发明公开了一种基于多层特征学习的行人属性识别系统及方法,该系统包括自下到上特征提取模块、自上到下特征融合模块、特征预测模块、多层预测融合模块和测试模块,该方法具体步骤为：自下到上逐层处理图片得到多层特征；自上到下逐层融合相邻层的特征,较高一层得到的特征图压缩通道,并与上一层上采样后的特征图进行特征融合和通道降维,输出当前层特征；融合后特征和提取的最上层特征,经最大池化层、全连接层后得到不同层级的初步预测结果；将不同层级的初步预测结果叠加,并对每层预测的每个属性对应赋予权重值,得到最终的预测结果；提取图片对应预测结果,计算各个指标的结果。本发明针对融合后的特征得到的预测值,对每个属性学习一组特定的权重,让每个属性各自更好地利用多层特征来得到更好的识别效果。</td>   <td>1.一种基于多层特征学习的行人属性识别系统,其特征在于,包括行人属性识别网络训练模块和测试模块,所述行人属性识别网络训练模块包括自下到上特征提取模块、自上到下特征融合模块、特征预测模块和多层预测融合模块,所述自下到上特征提取模块：采用卷积神经网络逐层提取图片特征得到多层特征,所述自上到下特征融合模块：用于逐层融合相邻层的特征,较高一层得到的三维特征图先通过第一卷积层来压缩通道数,保持通道数与上一层上采样后的特征图一致,通过相加的操作进行特征融合,融合后的特征通过第二卷积层,继续进行通道数降维,得到当前层特征；特征预测模块：根据从自下到上特征提取模块的最后一层的特征、以及自上到下特征融合模块得到的融合特征,每一层特征经过最大池化层和全连接层得到初始向量,通过Sigmoid函数进行激活后得到每个属性对应的预测概率,最后得到不同层级的初步预测结果；多层预测融合模块：将特征预测模块的初步预测结果进行向量叠加,每组预测结果的每个属性赋予权重,得到最终的预测结果；测试模块：利用训练好的行人属性识别网络,提取图片对应的最终的预测结果,然后算出各个指标的结果。</td>   <td>G06K9/00;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              张伟;                   程凤雯       </td>   <td>中山大学</td>   <td>一种基于时间感知结构的视频动作识别方法</td>   <td>广东省</td>   <td>CN110046568A</td>   <td>2019-07-23</td>   <td>本发明涉及一种基于时间感知结构的视频动作识别方法,本发明基于全分组三维卷积的时间感知结构设计,使用少量参数对多尺度的时间信息进行了建模,是一种十分高效的时间建模结构。并且本发明设置了监听流网络,增加了模型收敛的速度,在时间感知结构的基础上,能够进一步提高模型的识别准确率,降低了对预训练数据集的依赖,针对特定小规模问题可以快速实现部署；并且对不同时间尺度的行为具有鲁棒性。</td>   <td>1.一种基于时间感知结构的视频动作识别方法,其特征在于,包括以下步骤：步骤S1：对原始视频数据进行稀疏采样,等间隔地对视频抽取n帧,对视频帧经过数据增广处理后作为第一二维卷积神经网络的输入帧数据；步骤S2：利用第一二维卷积神经网络对原始视频的各个输入帧数据分别进行处理,得到对背景、尺度和光照的变化具有鲁棒性的深度特征,并形成特征图t；步骤S3：将第二二维卷积神经网络进行训练,将训练好的第二二维卷积神经网络作为监听流网络,将第一二维卷积网络中的一部分卷积层输出的深度特征经过压缩后作为监听流网络的输入信息；步骤S4：利用多尺度时间感知结构对特征图t的多个尺度上的时间信息进行建模,得到各个卷积分支含有时间维度的特征图t<Sub>1</Sub>；步骤S5：使用时间维度的最大化池化操作去除特征图t<Sub>1</Sub>时间维度上的冗余性信息,再使用第三二维卷积网络进一步对特征图t<Sub>1</Sub>中的时空特征进行提取,得到最终的视频描述向量；步骤S6：将最终的视频描述向量经过全连接层之后输出各个类别的概率对数值；步骤S7：将监听流网络的输入信息输入到训练好的监听流网络进行特征提取,得到特征图t<Sub>2</Sub>,使用全局池化将特征图t<Sub>2</Sub>压缩成一个特征向量,经过全连接层之后输出各个类别的概率对数值；步骤S8：对最终的视频描述向量输出的各个类别的概率对数值以及监听流网络输出的各个类别的概率对数值进行归一化处理,获得最终各个动作类别的概率,概率最大动作类别的即为网络识别的最终结果。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘冶;              印鉴;              桂进军;              吕梦瑶;              傅自豪;              李宏浩;              刘春鹏;              岑卓;                   陈志良       </td>   <td>中山大学;广州赫炎大数据科技有限公司</td>   <td>一种基于地理位置和人脸特征的签到方法及系统</td>   <td>广东省</td>   <td>CN110046870A</td>   <td>2019-07-23</td>   <td>本发明涉及一种基于地理位置和人脸特征的签到方法及系统,通过获取用户签到时间、签到时刻所处的经度、纬度和人脸特征信息,通过对上述信息进行审核得到签到审核结果,并将签到审核结果返回客户端。相对于现有技术,本发明极大地提高了签到效率和准确性。</td>   <td>1.一种基于地理位置和人脸特征的签到方法,其特征在于,包括以下步骤：获取用户签到时间、签到时刻所处的经度、纬度和人脸特征信息；判断用户是否处于签到设定时间内,得到签到时间审核结果；调用半正矢模型计算出用户签到地点与签到规则预设地点的距离,判断用户是否在签到约束范围内,得到地理信息审核结果；调用通过对预训练的深度神经网络迁移学习得到的人脸识别模型,对接收到的人脸信息进行特征提取,并与数据库中的注册时的特征进行匹配,得到人脸信息审核结果；根据签到时间审核结果、地理信息审核结果和人脸信息审核结果得到签到审核结果并返回客户端。</td>   <td>G06Q10/10;G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         袁陶希;              郑慧诚;                   吕怡静       </td>   <td>中山大学</td>   <td>基于时域分段及特征差分的动作识别方法</td>   <td>广东省</td>   <td>CN110032942A</td>   <td>2019-07-19</td>   <td>本发明公开了一种基于时域分段及特征差分的动作识别方法,包括：S1.将训练集中的动作视频等间隔分成多个片段,并在每个片段中随机提取一帧RGB图像以及光流图像；S2.构建双流网络；S3.将所有RGB图像以及光流图像分别对应输入双流网络进行训练；S4.将目标动作视频输入训练好的双流网络进行动作识别,并将其中所有网络流得到的结果进行融合从而得到动作视频的识别结果。通过将动作视频在时域上进行分段,整合动作视频中不同时段的特征,并进行特征差分融合得到动作视频的差分融合特征,从而有效提取长时动态信息；同时对双流网络中的空间流特征以及时间流特征进行时空相关融合,在保留原有时空信息的同时,进一步提取具有时空一致性的重要局部信息。</td>   <td>1.基于时域分段及特征差分的动作识别方法,其特征在于,包括以下步骤：S1.将训练集中的动作视频等间隔分成多个片段,并在每个片段中随机提取一帧RGB图像以及光流图像；S2.构建双流网络,其包括空间流网络和时间流网络,空间流网络的输入为RGB图像；时间流网络的输入为光流图像；S3.将所有RGB图像以及光流图像分别对应输入双流网络进行训练,具体步骤如下：S31.分别利用所述双流网络中的空间流网络和时间流网络对所有RGB图像以及光流图像进行特征提取,得到训练集中动作视频的空间流特征及时间流特征；S32.对动作视频的空间流特征进行差分融合操作,得到差分融合特征；将得到的差分融合特征与原始的空间流特征进行串联操作得到空间流融合特征；S33.利用空间流融合特征及时间流特征对双流网络进行训练,并将双流网络中所有网络流得到的结果进行融合从而得到动作视频的识别结果；S4.将目标动作视频输入训练好的双流网络进行动作识别,并将其中所有网络流得到的结果进行融合从而得到动作视频的识别结果。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李熙莹;              袁敏贤;              江倩殷;              罗东华;                   吕硕       </td>   <td>中山大学;广东方纬科技有限公司</td>   <td>一种车辆类型的精细识别方法及系统</td>   <td>广东省</td>   <td>CN105787466B</td>   <td>2019-07-16</td>   <td>本发明公开了一种车辆类型的精细识别方法及系统,方法包括：对获取的原始车辆图像进行灰度化和标准化处理,得到标准化图像；计算标准化图像每个像素点的梯度和方向；根据计算的梯度和方向对标准化图像进行方向梯度直方图特征提取和局部线性约束编码,得到标准化图像的编码向量；根据得到的编码向量采用权值空间金字塔对局部线性约束编码后的标准化图像进行处理,得到车辆图像的最终表达向量,所述车辆图像的最终表达向量包含有车辆图像的位置信息和语义信息；将车辆图像的最终表达向量送入预先训练好的线性支持向量机分类器进行车辆类型识别。本发明具有准确率高、复杂度低、鲁棒性强和细节特征丰富的优点,可广泛应用于图像处理领域。</td>   <td>1.一种车辆类型的精细识别方法,其特征在于：包括以下步骤：S1、对获取的原始车辆图像进行灰度化和标准化处理,得到标准化图像；S2、计算标准化图像每个像素点的梯度和方向；S3、根据计算的梯度和方向对标准化图像进行方向梯度直方图特征提取和局部线性约束编码,得到标准化图像的编码向量；S4、根据得到的编码向量采用权值空间金字塔对局部线性约束编码后的标准化图像进行处理,得到车辆图像的最终表达向量,所述车辆图像的最终表达向量包含有车辆图像的位置信息和语义信息；S5、将车辆图像的最终表达向量送入预先训练好的线性支持向量机分类器进行车辆类型识别；所述步骤S3包括：S31、将标准化图像划分若干个大小相同的图像块,并根据计算的梯度和方向获取各个图像块的方向梯度直方图特征向量以及整幅标准化图像的方向梯度直方图特征集合的表达式；S32、采用局部线性约束编码的方法对标准化图像的方向梯度直方图特征向量进行编码,得到标准化图像的编码向量,所述标准化图像的编码向量包含有标准化图像的语义信息；所述步骤S4包括：S41、对局部线性约束编码后的标准化图像分别进行一等分、四等分以及十六等分,得到三个层次的21个子区域,所述三个层次中子区域的数量分别是1、4以及16个；S42、对三个层次的每个子区域的编码向量进行最大池化操作,得到每个子区域的初步表达向量,所述最大池化操作所采用的公式为：P<Sub>e</Sub>＝max{c<Sub>e1</Sub>,c<Sub>e2</Sub>,…,c<Sub>eN</Sub>},其中,P<Sub>e</Sub>为池化表达向量P的第e个元素,c<Sub>eN</Sub>为第N个编码向量中的第e个元素,e＝1,2,…,L,L为编码向量的维数；S43、根据每个子区域的初步表达向量计算每个子区域的最终表达向量,所述每个子区域的最终表达向量计算公式为：F<Sub>g</Sub>＝W<Sub>g</Sub>*V<Sub>g</Sub>；其中,V<Sub>g</Sub>为第g个子区域的初步表达向量,F<Sub>g</Sub>为第g个子区域的最终表达向量,W<Sub>g</Sub>为第g个子区域的权重,g＝1,2,…,21；所述子区域的编号g的编号规则为：第一个层次的1个子区域编号g＝1,第二个层次的4个子区域的编号g按自左向右以及自上而下的顺序依次为2,3,4,5；第三个层次的16个子区域的编号g按自左向右以及自上而下的顺序依次为6,7,…,21；S44、把每个子区域的最终表达向量按顺序连接起来组成车辆图像的最终表达向量；所述21个子区域的权重集合W＝{w|w＝W<Sub>g</Sub>}＝{1,1,1,2,2,2,2,2,2,2,2,2,2,5,5,5,5,5,5,5,5}。</td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郭雪梅;              张玮嘉;                   谢泳伦       </td>   <td>中山大学</td>   <td>基于条件变分自编码器的人体活动识别系统及方法</td>   <td>广东省</td>   <td>CN110020623A</td>   <td>2019-07-16</td>   <td>本发明公开了一种基于条件变分自编码器的人体活动识别系统及方法,该方法包括获取原始时间序列：通过传感器获取采样样本,多个采样样本构成原始时间序列；构建批数据：通过随机序列起始点的数据增强方式构建批数据,得到构造好的传感器批数据X和对应的活动标签批数据Y；训练条件变分自编码器模型：批数据输入到模型中,通过损失函数和反向传播算法训练模型；预测人体活动：将传感器批数据X作为测试数据,输入到训练好的变分自编码器模型中,批数据输入变分自编码器模型得到最终的预测活动标签。本发明以一个采样样本为单位预测其对应的活动标签,具有实时活动识别的能力,能够对同类样本的相关性进行建模,从而提升识别准确率。</td>   <td>1.一种基于条件变分自编码器的人体活动识别方法,其特征在于,包括下述步骤：S1：获取原始时间序列：通过传感器获取采样样本,多个采样样本构成原始时间序列；S2：构建批数据：通过随机序列起始点的数据增强方式构建批数据,包括传感器批数据X、对应的活动标签批数据Y；S3：训练条件变分自编码器模型：构造好的传感器批数据X和对应的活动标签批数据Y输入到条件变分自编码器模型当中,条件变分自编码器采用神经网络的反向传播框架进行训练,通过设定的损失函数和Adam优化算法训练模型,所述损失函数基于交叉熵函数计算得到；S4：预测人体活动：步骤S2构造的传感器批数据X作为测试数据,输入到步骤S3中训练好的变分自编码器模型中,得到最终的预测活动标签。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         高成英;                   汤梦玥       </td>   <td>中山大学</td>   <td>一种基于图像的彩铅画风格绘制方法</td>   <td>广东省</td>   <td>CN107085859B</td>   <td>2019-07-12</td>   <td>本发明提供了一种基于图像的彩铅画风格绘制方法,其步骤包括：获取输入图像的边缘图像；用具有不同长度的笔画作为卷积核,对边缘图像进行自适应卷积,生成铅笔轮廓线图像；对输入图像进行颜色滤波处理,获得彩铅底色图像；对彩铅底色图像进行伽马校正,获得增强了暗部细节的彩铅色调图像；对增强了暗部细节的彩铅色调图像进行纹理渲染,获得彩铅纹理图像；将铅笔轮廓线图像与彩铅纹理图像结合,得到最终结果。与现有技术相比,本方法得到的彩铅画结果,无论从轮廓线、色调还是纹理方面,都与现实中由画家创作的彩铅画效果更加接近,且具有较高的时间效率。</td>   <td>1.一种基于图像的彩铅画风格绘制方法,其特征在于：分别获取输入图像的铅笔轮廓线图像以及输入图像的彩铅纹理图像,再将所述铅笔轮廓线图像以及所述彩铅纹理图结合,得到最终的结果,其中：所述铅笔轮廓线图像的获取包括以下步骤：步骤1,先使用低通滤波器对输入图像进行滤波,而后使用FDoG算子提取出输入图像的边缘图像；步骤2,用若干线段作为卷积核,对边缘图像进行自适应卷积,生成铅笔轮廓线图像S；所述彩铅纹理图像的获取包括以下步骤3～步骤5：步骤3,对输入图像I<Sub>1</Sub>使用滤波器进行滤波,得到图像m(x,y),接着将输入图像I<Sub>1</Sub>转换到YUV颜色空间,取Y通道,得<Image id="icf0001" he="85" wi="178" file="FDA0002046479200000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>用以下公式对<Image id="icf0002" he="86" wi="148" file="FDA0002046479200000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>进行处理：          <Image id="icf0003" he="148" wi="387" file="FDA0002046479200000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        再将<Image id="icf0004" he="87" wi="150" file="FDA0002046479200000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>映射回RGB颜色空间,然后令输入图像I<Sub>1</Sub>和<Image id="icf0005" he="87" wi="149" file="FDA0002046479200000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>进行平均,获得彩铅底色图像I<Sub>p</Sub>；步骤4,对彩铅底色图像I<Sub>p</Sub>在YUV颜色空间的Y通道上进行伽马校正,然后使用滤波器进行平滑,获得增强了暗部细节的彩铅色调图像I；步骤5,对增强了暗部细节的彩铅色调图像进行纹理渲染,获得彩铅纹理图像I<Sub>tex</Sub>；步骤6,将彩铅纹理图像I<Sub>tex</Sub>与铅笔轮廓线图像S利用以下公式进行融合,其中·<Sup>S</Sup>和·<Sup>L</Sup>分别指HSL颜色空间的S通道和L通道：          <Image id="icf0006" he="83" wi="691" file="FDA0002046479200000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0007" he="83" wi="549" file="FDA0002046479200000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        得到最终结果T。</td>   <td>G06T11/00;G06T11/40;G06T11/60</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李潇焓;              倪江群;              张东;                   苏文康       </td>   <td>中山大学</td>   <td>一种JPEG非对称数字图像隐写方法</td>   <td>广东省</td>   <td>CN110009547A</td>   <td>2019-07-12</td>   <td>本发明涉及一种JPEG非对称数字图像隐写方法,通过获取载体图像,将载体图像划分两个交织子图X<Sub>1</Sub>X<Sub>2</Sub>；计算载体图像的初始化失真代价值和初始化嵌入修改图R<Sub>1</Sub>、R<Sub>2</Sub>；优化R<Sub>1</Sub>中每个DCT块的块内嵌入修改情况,更新子图X<Sub>1</Sub>每个元素的代价值,获取新载密子图Y′<Sub>1</Sub>和新的嵌入修改图R′<Sub>1</Sub>；构造四邻域模型,结合嵌入修改图R′<Sub>1</Sub>和BBC策略更新X<Sub>2</Sub>中每个元素的代价值,并获取新载密子图Y′<Sub>2</Sub>；将载密子图Y′<Sub>1</Sub>和载密子图Y′<Sub>2</Sub>进行合并,得到完整的载密图像Y′,再将其送入隐写分析器检测提出的JPEG非对称隐写算法的安全性能。本发明所提供的一种JPEG非对称数字图像隐写方法,构建了新的代价值更新方法,有效提高算法的安全性能高,通过四邻域模型的建立,实现了水平和垂直两个方向+1/-1失真代价值的同时更新,收敛速度块。</td>   <td>1.一种JPEG非对称数字图像隐写方法,其特征在于,包括以下步骤：S1：获取一张JPEG格式的载体图像X,将载体图像划分两个交织子图X<Sub>1</Sub>、X<Sub>2</Sub>；S2：基于JPEG图像对称隐写算法获取载体图像的初始化失真代价值C<Sub>ori</Sub>和初始化嵌入修改图R<Sub>1</Sub>、R<Sub>2</Sub>；S3：优化R<Sub>1</Sub>中每个DCT块的块内嵌入修改情况,更新子图X<Sub>1</Sub>每个元素的代价值,通过模拟仿真嵌入获取新载密子图Y<Sub>1</Sub>'和新的嵌入修改图R<Sub>1</Sub>'；S4：构造四邻域模型,结合嵌入修改图R<Sub>1</Sub>'和BBC策略更新X<Sub>2</Sub>中每个元素的代价值,并通过模拟仿真嵌入获取新载密子图Y<Sub>2</Sub>'；S5：将载密子图Y<Sub>1</Sub>'和载密子图Y<Sub>2</Sub>'进行合并,得到完整的载密图像Y',再将其送入隐写分析器,检测提出的JPEG非对称隐写算法的安全性能。</td>   <td>G06T1/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              刘付康;              朱雄泳;              陈荣军;                   陆许明       </td>   <td>中山大学</td>   <td>一种亮度、色彩自适应与细节丰富的低动态范围图像逆向生成高动态范围图像的方法</td>   <td>广东省</td>   <td>CN110009574A</td>   <td>2019-07-12</td>   <td>本发明公开了一种亮度、色彩自适应与细节丰富的低动态范围图像逆向生成高动态范围图像的方法,本发明方法首先对输入的低动态范围图像亮度灰度图全局细节增强；再由直方图裁剪与补偿、亮度与标准差估算模型获得输入图像到中间高动态范围图像的亮度直方图全局映射曲线,其中由最大熵亮度估算方法自适应选出最优输出中间高动态范围图像亮度；然后对中间高动态范围图像灰度图利用高效滤波器滤波去除映射产生的伪影得到输出高动态范围图像亮度；最后合并色彩通道并对色彩进行自适应校正,生成的高动态范围图像很好的体现了真实场景。本发明能将输入图像映射到高动态范围图像,输出的高动态范围图像亮度与色彩自适应,细节重现,视觉效果真实震撼。</td>   <td>1.一种亮度、色彩自适应与细节丰富的低动态范围图像逆向生成高动态范围图像的方法,其特征在于,包括：a)对输入图像亮度灰度图进行全局细节增强,计算输入图像亮度灰度图细节增强后的对数,利用对数转换初步压缩原始场景中的亮度；其中所述图像为低动态范围图像或高动态范围图像；b)对全局细节增强的输入图像亮度对数进行直方图统计,计算其平均值与标准差,对直方图进行分段裁剪与补偿；c)由亮度与标准差估算模型计算映射到中间高动态范围图像的平均亮度与标准差,从而求解输入图像到中间高动态范围图像的亮度直方图全局映射曲线,其中由最大熵亮度估算方法自适应估算出最优输出中间高动态范围图像亮度；d)利用高效滤波器对中间高动态范围图像亮度滤波去除映射产生的伪影得到输出高动态范围图像亮度；e)将输入图像色彩通道映射到对应输出高动态范围图像色彩通道并进行自适应校正,合并色彩空间获得输出高动态范围图像。</td>   <td>G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄以华;                   谭骏朗       </td>   <td>中山大学</td>   <td>一种基于FPGA的色调映射系统</td>   <td>广东省</td>   <td>CN110009577A</td>   <td>2019-07-12</td>   <td>本发明涉及图像处理领域,更具体的,涉及一种基于FPGA的实时色调映射算法系统,本发明针对输入视频特定的分辨率,结合其权重值固定不变的特点,将原来需要进行大量计算的权重值使用了预先计算的方法来减低整个算法的计算量,在需要使用权重值时直接在存储器中读取即可,提高整个系统的吞吐率,满足实时映射需求。同时,本发明整个硬件系统的结构采用全局流水,局部并行的方式进行数据映射,整个系统各个模块间都以流水形式进行数据处理,在模块中以并行方式进行处理,极大地提高了整个系统的吞吐率,满足实时映射的需求。</td>   <td>1.一种基于FPGA的色调映射系统,其特征在于,包括有分块映射模块、图像融合模块以及细节增强模块,所述的分块映射模块和图像融合模块相连接,所述的图像融合模块与细节增强模块相连接；在分块映射模块中,通过使用前一帧图像中计算出来的统计信息,实现N块映射图片并行映射；在图像融合模块中,通过提前计算出融合图片的权重值并储存到RAM上,避免权重值的大量计算,当图像融合模块接收到分块映射模块传入的数据后,图像融合模块分别读取对应图像的权值并进行卷积融合操作；细节增强模块负责对融合后图像进行细节增强,得到最终输出映射后的图像。</td>   <td>G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李明;                   蔡炜城       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>基于先验知识规整协方差的概率线性鉴别说话人识别方法</td>   <td>广东省</td>   <td>CN105139856B</td>   <td>2019-07-09</td>   <td>本发明公开一种基于先验知识规整协方差的概率线性鉴别说话人识别方法,该能够根据任意关于训练语音的有用信息去规整概率线性鉴别分析模型的协方差假设以及迭代过程,最终训练出更具有区分性、更能反映真实情况的概率线性鉴别分析模型。同时,引入两个规整系数使得模型可调,能针对各类不同的规整信息进行自适应达到最优。采用本发明训练得出的模型比传统模型在相同数据集上得出的说话人识别评测效果有明显提升,在国际权威说话人识别评测数据库中能使等错误率(EER)和最小检测错误代价(norm minDCF)相对下降10%-20%。</td>   <td>1.一种基于先验知识规整协方差的概率线性鉴别说话人识别方法,其特征在于利用训练语音的有效已知信息去规整概率线性鉴别分析模型的协方差假设以及迭代过程,包括以下步骤：1)采集每条训练语音固有的物理信息或者主客观评分信息,记作信息d<Sub>ij</Sub>,下标i、j表示该信息属于第i个说话人的第j条训练语音；2)用信息d<Sub>ij</Sub>对概率线性鉴别分析模型中刻画残差项的协方差矩阵进行规整,所述规整方法如下：<Image id="icf0001" he="134" wi="345" file="FDA0001962139890000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中∑是全局的协方差矩阵,u和v是规整系数,通过不断调整找到最优取值,<Image id="icf0002" he="129" wi="164" file="FDA0001962139890000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>整体构成一个规整项,其将全局的协方差矩阵映射为针对每条训练语音自适应的独立项；3)利用规整后的协方差矩阵,获取第i个说话人的身份向量ivector的平均的条件分布；          <Image id="icf0003" he="274" wi="700" file="FDA0001962139890000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,F<Sub>i</Sub>表示第i个训练说话人的所有身份向量ivector的平均向量,β<Sub>i</Sub>是第i个说话人的低维说话人向量,是一个隐含变量,等式右边<Image id="icf0004" he="325" wi="486" file="FDA0001962139890000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示一个高斯分布,这个高斯分布的方差是<Image id="icf0005" he="158" wi="251" file="FDA0001962139890000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>它的均值向量是φβ<Sub>i</Sub>,φ是说话人空间矩阵,M<Sub>i</Sub>是第i个训练说话人的语音总条数；根据贝叶斯公式,得出隐含变量β<Sub>i</Sub>在给定平均向量F<Sub>i</Sub>下的后验概率,均值向量为：          <Image id="icf0006" he="193" wi="700" file="FDA0001962139890000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,I为单位矩阵,χ<Sub>i</Sub>为第i个人的所有身份向量ivector的加和向量；根据EM算法,得出已知后验概率P(β<Sub>i</Sub>|F<Sub>i</Sub>)的均值向量E(β<Sub>i</Sub>)下的每次说话人空间矩阵φ以及协方差矩阵∑的更新公式如下：          <Image id="icf0007" he="119" wi="700" file="FDA0001962139890000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0008" he="177" wi="700" file="FDA0001962139890000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        通过交替更新E(β<Sub>i</Sub>)和φ、∑的值迭代直至收敛,取得最优的φ和∑值,完成说话人识别中的概率线性鉴别分析模型的训练,得到训练好的概率线性鉴别分析模型,其中η<Sub>ij</Sub>为第i个说话人的第j条训练语音数据的ivector向量,T表示训练数据中全体说话人的总数量；4)采用由步骤3)得到的训练好的概率线性鉴别分析模型对待鉴别的语音进行鉴别。</td>   <td>G10L17/02;G10L17/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              潘瑜;              曹向前;                   胡伟鹏       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于分层网络流图的多目标跟踪方法</td>   <td>广东省</td>   <td>CN106067179B</td>   <td>2019-07-09</td>   <td>本发明提供一种基于分层网络流图的多目标跟踪方法,该方法通过分层的网络流图模型,充分利用了轨迹的整体特征信息,将整个视频等分成若干个时间段作为第一层,在每个时间段建立网络流图,计算相邻目标间的关联值,用<I>K</I>条最短路径优化算法计算各时间内目标间的匹配关系,再逐步合并相邻的视频段形成新的一层,得到多目标跟踪的结果,减少了计算时间,提高了目标跟踪准确率。</td>   <td>1.一种基于分层网络流图的多目标跟踪方法,其特征在于,包括以下步骤：S1：采用DPM算法在视频的每帧中检测出运动目标,并保留每个运动目标的检测准确率；S2：将整个视频等分成若干个时间段作为第一层,在每个时间段建立网络流图,构建分层的网络流图模型；S3：计算第一层各时间段的网络流图中相邻目标的关联值,通过K条最短路径优化算法计算各关联值得到时间段内的匹配关系,确定当前层各目标的运动轨迹；S4：将固定数量的相邻时间段合并得到若干个较长时间段,形成新的一层,计算当前层各时间段网络流图中相邻轨迹的关联值；S5：在当前层各时间段内用K条最短路径优化算法计算各关联值,确定合并后的时间段中各目标的运动轨迹,重复S4、S5直至确定整个视频的运动轨迹。</td>   <td>G06T7/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;              白大伟;              吴江旭;                   梁津铨       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于时序的RFID主动防冲突方法</td>   <td>广东省</td>   <td>CN106503600B</td>   <td>2019-07-09</td>   <td>本发明提供一种基于时序的RFID主动防冲突方法,该方法采用RFID卡自动发送方式,RFID读写器在成功接收到一个RFID卡的响应命令ATQC后,可以选择发送WTRD命令,使所有卡片进入Ready状态,停止其他卡片的发送,然后选中刚才接到响应命令ATQC代表的那张RFID卡进行操作,也可以不发送WTRD命令,继续接收卡片数据,完成对射频场内所有卡的寻卡；在RFID读写器读多张RFID卡时RFID读写器只存储有效的ATQC,而且RFID读写器在发送读卡REQC命令后,在接收到有效的相应命令ATQC前都不需要再发送命令了,节约了发送命令的时间,同时也节约了发送和接收的数据量节约了存储空间。</td>   <td>1.一种基于时序的RFID主动防冲突方法,其特征在于,包括以下步骤：S1：RFID读写器向其射频范围内的若干RFID卡发送读卡命令REQC,其中读卡命令REQC包括RFID卡片类型AFI、槽数N；S2：RFID卡接收到读卡命令REQC后判断其中的卡片类型AFI与自身是否相同,若相同,就判断读卡命令REQC中的槽数N的值,当判断出的槽数N的值为1时,RFID卡向RFID读写器发送响应命令ATQC,当判断出的槽数N的值不为1时,RFID卡在槽数N规定的取值范围内产生随机数M,其中,在接到下一个读卡命令REQC命令前,槽数N的值不变；S3：RFID卡判断是否接收到数据,若没有接收到数据,比较随机数M是否等于I,若M不等于I,RFID卡重新判断是否接收到数据；若M等于I,RFID卡向RFID读写器发送响应命令ATQC,其中,I为RFID卡上的计数器,每与随机数M对比一次I内的数值就会加1,I的初始值为0；若接收到数据,RFID卡判断接收到的数据是读写器发送的命令还是其他RFID卡发送的命令,若为读写器发送的命令则执行该命令,若为其他RFID卡发送的命令,则等待TD时间后,RFID卡重新判断是否接收到数据,其中,响应响应命令ATQC中包括RFID卡的序列号PUPI；S4：RFID读写器接收响应命令ATQC,同时判断是否发生冲突,若发生冲突,将RFID读写器中CT位置1；若未发生冲突,则RFID读写器接收完整的响应命令ATQC；S5：RFID读写器再向其射频范围内的若干RFID发送选卡命令ATTRIC并选中与选卡命令ATTRIC中的序列号PUPI相同的RFID卡,被选中的RFID卡再完成选卡命令ATTRIC后,向RFID读写器发送响应命令REQRIC,RFID读写器接到响应命令REQRIC后,表示卡片已经完成选卡命令ATTRIC规定的操作,此时RFID读写器对该RFID卡发送停止命令HLTC,使该RFID卡进入停止状态。</td>   <td>G06K7/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         吴蓉;              刘济科;              吕中荣;                   余敏立       </td>   <td>中山大学</td>   <td>含非比例结构阻尼框架-剪切结构损伤识别方法及系统</td>   <td>广东省</td>   <td>CN109992883A</td>   <td>2019-07-09</td>   <td>本发明公开了一种含非比例结构阻尼框架-剪切结构损伤识别方法及系统。所述损伤识别方法利用类模态将振动微分方程在频域上解耦,得到含非比例结构阻尼系统的模态参数识别方程,使用Levenberg-Marquardt算法对模态参数识别方程进行迭代求解,得到模态参数识别值；进而结合模态参数的灵敏度分析,使用Trust-Region-Reflective算法对损伤识别问题进行优化求解,得到损伤参数的识别值。采用本发明所提供的含非比例结构阻尼框架-剪切结构损伤识别方法及系统能够有效识别结构模态参数和结构损伤参数。</td>   <td>1.一种含非比例结构阻尼框架-剪切结构损伤识别方法,其特征在于,包括：获取无损的含非比例结构阻尼框架-剪切结构的无损参数；所述无损层参数包括无损质量、无损刚度以及无损阻尼；根据所述无损参数建立所述含非比例结构阻尼框架-剪切结构在频域下的振动方程；利用类模态将所述振动方程对应的特征值问题进行解耦,确定无损结构模态参数；所述无损结构模态参数包括特征值、正则复模态矩阵以及正则类模态矩阵；对所述无损的含非比例结构阻尼框架-剪切结构进行多组不同角频率的简谐激励,获取所述含非比例结构阻尼框架-剪切结构损伤后的损伤振动数据；根据所述无损结构模态参数以及所述损伤振动数据确定损伤后的含非比例结构阻尼框架-剪切结构的模态参数识别方程；利用莱文贝格-马夸特Levenberg-Marquardt算法求解所述模态参数识别方程,确定损伤结构模态参数；结合所述损伤结构模态参数的灵敏度分析,以所述损伤结构模态参数作为损伤检测参数,利用信赖域反射Trust-Region-Reflective算法确定损伤结构损伤参数。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;                   叫洁宁       </td>   <td>中山大学</td>   <td>基于超分辨图像生成的低分辨率行人重识别系统和方法</td>   <td>广东省</td>   <td>CN109993072A</td>   <td>2019-07-09</td>   <td>本发明公开了一种基于超分辨图像生成的低分辨率行人重识别系统和方法,该系统包括行人属性引导的超分辨图像生成网络模型和行人重识别网络模型,该方法步骤为：选取高、低分辨率图像样本及行人属性向量；训练超分辨率图像生成网络模型；训练行人重识别网络模型；联合训练超分辨图像生成网络模型和行人重识别网络模型；将低分辨率行人图像测试集与对应的行人属性向量输入到联合训练后的超分辨图像生成网络模型和行人重识别网络模型提取行人图像特征；计算行人图像特征的余弦相似度,根据余弦相似度得到不同分辨率的行人图像匹配的结果,本发明实现低分辨率图像细节恢复,同时扩大了网络的容量,提高低分辨率行人重识别的效果。</td>   <td>1.一种基于超分辨图像生成的低分辨率行人重识别系统,其特征在于,包括行人属性引导的超分辨图像生成网络模型和行人重识别网络模型,所述行人属性引导的超分辨图像生成网络模型包括生成器和判别器,生成器包括第一生成器G<Sub>h→l</Sub>和第二生成器G<Sub>l→h</Sub>,判别器包括第一判别器D<Sub>h→l</Sub>,第二判别器D<Sub>l→h</Sub>；所述第一生成器G<Sub>h→l</Sub>用于将高分辨率图像生成低分辨率图像,第二生成器G<Sub>l→h</Sub>用于将低分辨率图像和行人属性向量生成高分辨率图像；所述第一判别器D<Sub>h→l</Sub>用于判定低分辨率图像的真假,第二判别器D<Sub>l→h</Sub>用于判定高分辨率图像以及属性的真假；所述行人属性引导的超分辨图像生成网络模型训练后生成超分辨率行人图像；所述行人重识别网络模型包括行人重识别网络和分类器,所述行人重识别网络用于提取图像中的行人特征,所述分类器用于将超分辨行人图像和高分辨率的行人图像中的真实行人身份区别。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         佘焕波;              田翔;                   张亚东       </td>   <td>华南理工大学;中山大学附属第一医院</td>   <td>一种基于深度学习的白鼠行为分类方法</td>   <td>广东省</td>   <td>CN109993076A</td>   <td>2019-07-09</td>   <td>本发明提供一种基于深度学习的白鼠行为分类方法,包括以下步骤：步骤S1：采集白鼠的行为视频数据并进行预处理,作为训练数据；步骤S2：构建深度卷积神经网络模型；步骤S3：利用训练数据对深度卷积神经网络模型进行训练；步骤S4：利用训练好的深度卷积神经网络模型对录制好的白鼠行为视频进行分类。本发明基于深度学习的白鼠行为分类方法,相比其他基于深度学习的行为分类方法,可更好地提取白鼠动作时序信息,从而提高了白鼠行为分类的准确率。</td>   <td>1.一种基于深度学习的白鼠行为分类方法,其特征在于：包括以下步骤：步骤S1：采集白鼠的行为视频数据并进行预处理,作为训练数据；步骤S2：构建深度卷积神经网络模型；步骤S3：利用训练数据对深度卷积神经网络模型进行训练；步骤S4：利用训练好的深度卷积神经网络模型对录制好的白鼠行为视频进行分类。</td>   <td>G06K9/00;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡继华;              袁均良;              王浩远;                   张力越       </td>   <td>中山大学</td>   <td>一种基于HOG和SVM分类器的道路交叉口车辆检测方法</td>   <td>广东省</td>   <td>CN109993134A</td>   <td>2019-07-09</td>   <td>本发明涉及一种基于HOG和SVM分类器的道路交叉口车辆检测方法,所述检测识别方法如下：将获得的交叉口视频进行稳态处理,标定出稳态处理后的目标区域,并对图形中的图像进行HOG特征提取,用已经训练好的SVM分类器对图像进行检测分类,并不断旋转图像目标区域,当旋转次数达到5次时,检验识别率是否满足要求,如不满足,将误检测的难例作为负样本输出,重新进行SVM训练,如满足要求,将获得的目标数据进行旋转逆处理得到原中心坐标,输出中心坐标及图像,完成对目标的检测。本发明能够解决道路交叉口多方向车辆的目标检测,且有效处理车辆重叠、路口渠化设施、建筑物、绿化等影响因素的干扰,提高检测效率。</td>   <td>1.一种基于HOG和SVM分类器的道路交叉口车辆检测方法,其特征在于,包括以下步骤：步骤S1.对已有的正样本及负样本图像提取HOG特征,对提取后的HOG特征进行SVM训练,得到SVM分类器；步骤S2.获取待检测的交通场景视频,在待检测的交通场景视频中获取某帧图像,并对图像进行稳态处理；步骤S3.对经过稳态处理后的图像标定出图像中的目标区域,并将图像分成多个图像块进行HOG特征提取；步骤S4.通过图像提取出的HOG特征,利用步骤S1中的SVM分类器对图像进行检测分类；步骤S5.将图像中的目标区域以一定的角度旋转,规定最大旋转次数为M,旋转次数为m,m的初始值为1,每旋转一次,令m＝m+1,并对图像的目标区域进行HOG特征提取,使用SVM分类器进行分类检测；步骤S6.判断m是否大于M；(1)若m&gt;M,判断识别率是否达到期望,若没有达到,将识别出的难例作为负样本输出,重新放入到SVM训练器进行训练；若识别率达到期望,将获得的目标区域进行旋转逆处理得到原中心坐标,输出中心坐标及图像,并判断是否有大于车辆像素点的异常矩形框,若有,则将异常矩形框排除；(2)若m≤M,则返回到步骤S4。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑子彬;                   李阳       </td>   <td>中山大学</td>   <td>一种基于上市公司公告摘要的自动提取方法</td>   <td>广东省</td>   <td>CN106227722B</td>   <td>2019-07-05</td>   <td>本发明涉及一种基于上市公司公告摘要的自动提取方法,包括以下步骤：S1：从证券交易所中爬取上市公司公告文档形成公告文档数据库；S2：采用word2vec模型,从文本语料得到词向量；S3：计算句子之间相似度,构建句子图模型；S4：计算句子的权重；S5：根据句子位置调整句子权重矩阵；S6:选择权重最大且无冗余的句子组成摘要。基于上市公司公告摘要的自动提取技术,为金融市场的投资者提供准确且可读性较高的摘要文档,帮助投资者更短时间理解以及更好的做出投资判断,同时为量化基金公司提供重要的指标。</td>   <td>1.一种基于上市公司公告摘要的自动提取方法,其特征在于包括以下步骤：S1：从证券交易所中爬取上市公司公告文档形成公告文档数据库；S2：采用word2vec模型,从文本语料得到词向量；S3：计算句子之间相似度,构建句子图模型；S4：计算句子的权重；S5：根据句子位置调整句子权重矩阵；S6：选择权重最大且无冗余的句子组成摘要；句子相似度主要通过三个维度计算得到：第一个维度计算句子与句子之间的相似度,采用词向量的余弦关系来表示：          <Image id="icf0001" he="179" wi="595" file="FDA0001972453050000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中<Image id="icf0002" he="90" wi="135" file="FDA0001972453050000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>分别代表句子si,sj的特征词向量集合,均是经过word2vec训练得到的,为了保证句子向量的长度是相同的,我们采用添加停用词和删掉不重要的词的方式；第二个维度计算句子与标题的相似度,通过如下公式获得：          <Image id="icf0003" he="138" wi="700" file="FDA0001972453050000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中|p<Sub>0</Sub>|,|p<Sub>j</Sub>|分别代表句子p<Sub>0</Sub>,p<Sub>j</Sub>中特征词的个数,w<Sub>k</Sub>代表第k个特征词；第三个维度计算句子与候选关键术语的相似度,通过如下公式获得：          <Image id="icf0004" he="142" wi="700" file="FDA0001972453050000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中|p<Sub>j</Sub>|,|q|分别代表句子p<Sub>j</Sub>,q中特征词的个数,w<Sub>k</Sub>代表第k个特征词；将每一个句子作为图模型的顶点,而上述计算得到句子与句子之间的相似度作为图模型中节点句子与节点句子之间的权重,构建的图模型为一个有权无向图。</td>   <td>G06F17/27</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              吴炆芳;              朱雄泳;              陈荣军;                   陆许明       </td>   <td>中山大学</td>   <td>基于NSCT和PCNN的压缩感知域内的高动态范围图像融合方法</td>   <td>广东省</td>   <td>CN109978802A</td>   <td>2019-07-05</td>   <td>本发明公开了一种基于NSCT和PCNN的压缩感知域内的高动态范围图像融合方法,包括如下步骤：1)对输入多曝光图像序列进行归一化处理；2)对归一化处理后的多曝光图像序列使用相机响应函数进行辐射校准；3)对辐射校准后的多曝光图像序列进行NSCT变换得到各图像的低、高频系数；4)对各图像的高频系数进行压缩采样；5)计算各图像低、高频系数的空间频率作为PCNN网络的激励信号,通过最大值决策进行融合；6)对融合后的高频系数进行压缩感知重构；7)对融合后的低、高频系数进行NSCT逆变换,得到目标高动态范围图像。利用NSCT、PCNN和压缩感知方法完成对多曝光的低动态范围图像序列在压缩感知重构的同时融合,最后生成一幅包含丰富细节的高动态范围图像。</td>   <td>1.一种基于NSCT和PCNN的压缩感知域内的高动态范围图像融合方法,其特征在于,包括有如下步骤：1)对输入多曝光图像序列进行归一化处理；2)对归一化处理后的多曝光图像序列使用相机响应函数进行辐射校准；3)对辐射校准后的多曝光图像序列进行NSCT变换得到各图像的低、高频系数；4)对各图像的高频系数进行压缩采样；5)计算各图像低、高频系数的空间频率作为PCNN网络的激励信号,通过最大值决策进行融合；6)对融合后的高频系数进行压缩感知重构；7)对融合后的低、高频系数进行NSCT逆变换,得到目标高动态范围图像。</td>   <td>G06T5/50;G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         卢宇彤;              蔡婷婷;              郑馥丹;              王莹;              邓楚富;                   陈志广       </td>   <td>中山大学</td>   <td>用于行人重识别的掩膜池化模型训练和行人重识别方法</td>   <td>广东省</td>   <td>CN109977798A</td>   <td>2019-07-05</td>   <td>本发明涉及一种用于行人重识别的掩膜池化模型训练和行人重识别方法,包括S1.获取锚图像a、正样本图像p、负样本图像n；S2.将a、p、n以及a、p、n对应的掩膜分别输入掩膜池化模型中,得到对应的三维张量T<Sub>a</Sub>、T<Sub>p</Sub>、T<Sub>n</Sub>；S3.对T<Sub>a</Sub>、T<Sub>p</Sub>、T<Sub>n</Sub>分别进行池化操作、卷积操作,得到对应的H<Sub>a</Sub>、H<Sub>p</Sub>、H<Sub>n</Sub>；S4.将H<Sub>a</Sub>、H<Sub>p</Sub>、H<Sub>n</Sub>分别输入分类器,得到对应的预测结果R<Sub>a</Sub>、R<Sub>p</Sub>、R<Sub>n</Sub>；S5.根据预测结果R<Sub>a</Sub>、R<Sub>p</Sub>、R<Sub>n</Sub>计算损失值；S6.根据损失值训练掩膜池化模型。本发明可以增强图像中的非背景信息,学习到图像最关键的特征。</td>   <td>1.一种用于行人重识别的掩膜池化模型训练方法,其特征在于,包括：S1.获取锚图像a、正样本图像p、负样本图像n；S2.将a、p、n以及a、p、n对应的掩膜分别输入掩膜池化模型中,得到对应的张量T<Sub>a</Sub>、T<Sub>p</Sub>、T<Sub>n</Sub>；S3.对T<Sub>a</Sub>、T<Sub>p</Sub>、T<Sub>n</Sub>分别进行池化操作、卷积操作,得到对应的张量H<Sub>a</Sub>、H<Sub>p</Sub>、H<Sub>n</Sub>；S4.将H<Sub>a</Sub>、H<Sub>p</Sub>、H<Sub>n</Sub>分别输入分类器,得到对应的预测结果R<Sub>a</Sub>、R<Sub>p</Sub>、R<Sub>n</Sub>；S5.根据预测结果R<Sub>a</Sub>、R<Sub>p</Sub>、R<Sub>n</Sub>计算损失值；S6.根据损失值训练掩膜池化模型。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         叶志豪;              刘冶;              桂进军;              李宏浩;                   印鉴       </td>   <td>中山大学;广州赫炎大数据科技有限公司</td>   <td>一种多层次自然语言反垃圾文本方法及系统</td>   <td>广东省</td>   <td>CN109977416A</td>   <td>2019-07-05</td>   <td>本发明涉及一种多层次自然语言反垃圾文本方法及系统,通过获得待识别文本的敏感词识别结果、敏感词变形体识别结果,并结合对待识别文本进行文本分类得到其为垃圾文本的预判概率,基于对所述敏感词识别结果、敏感词变形体识别结果和预判概率进行综合评判,得出所述待识别文本为垃圾文本的最终概率。本发明能高效地识别垃圾文本,能避免垃圾文本对互联网健康交流环境的不利影响,有较高的稳健性,可广泛地适用于社交、评论等互联网产品。</td>   <td>1.一种多层次自然语言反垃圾文本方法,其特征在于,包括以下步骤：接收待识别文本；基于原始敏感词库,对所述待识别文本进行原始敏感词的匹配,识别出所述待识别文本中的原始敏感词,输出敏感词识别结果；其中,所述原始敏感词库包括原始敏感词；基于敏感词变形体库,对所述待识别文本进行敏感词变形体的匹配,并对匹配到的疑似词汇进行语义分析,验证所述疑似词汇是否属于敏感词,输出敏感词变形体识别结果；其中,所述敏感词变形体库根据所述原始敏感词库建立,所述敏感词变形体库包括所述原始敏感词对应的敏感词变形体；对所述待识别文本进行文本分类,得出所述待识别文本为垃圾文本的预判概率；对所述敏感词识别结果、敏感词变形体识别结果和预判概率进行加权计算,得出所述待识别文本为垃圾文本的最终概率。</td>   <td>G06F17/27;G06F16/33;G06F16/35;G06F16/903</td>  </tr>        <tr>   <td>中国专利</td>   <td>         邓倩倩;              王若梅;              周凡;              林格;                   陈湘萍       </td>   <td>中山大学</td>   <td>基于控制网格变形的部件网格融合方法及系统</td>   <td>广东省</td>   <td>CN109308732B</td>   <td>2019-07-02</td>   <td>本发明公开了一种基于控制网格变形的部件网格融合方法,包括：S1,对源网格和目标网格的融合边界进行预处理；S2,生成源网格与目标网格的控制网格；S3,对控制网格进行变形及融合处理；S4,对变形及融合后的控制网格进行渗透处理。本发明还公开了一种基于控制网格变形的部件网格融合系统。采用本发明,可有效处理具有任意拓扑结构的融合边界的网格之间的融合问题,并且在保持网格几何特征的基础上自适应的变形。</td>   <td>1.一种基于控制网格变形的部件网格融合方法,其特征在于,包括：S1,对源网格和目标网格的融合边界进行预处理；具体地,所述步骤S1包括：根据目标网格与源网格的顶点分布及长度,分别对目标网格与源网格的融合边进行插值,以使目标网格与源网格的融合边上的边界点一一对应；S2,生成源网格与目标网格的控制网格；具体地,所述步骤S2包括：分别根据目标网格与源网格的融合边上的边界点构建控制网格；其中,控制网格的构建方法如下：S21,假设服装部件网格的融合边组成的集合为I＝{P<Sub>i</Sub>,E<Sub>j</Sub>},源网格上连续的边界点组成的集合为P<Sub>i</Sub>,边界点作为融合过程的约束点,融合边的集合为E<Sub>j</Sub>；S22,根据公式<Image id="icf0001" he="294" wi="405" file="FDA0002022817240000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>构造双棱锥,具体地,以P<Sub>i</Sub>作为棱锥的底边上的顶点,在P<Sub>i</Sub>围成的多边形的中心上沿着法线的方向长度为R的地方插入两个顶点v<Sub>k</Sub>分别作为双棱锥的两个顶点,M,N分别为边界边的数目以及边界点的数目；S23,根据公式<Image id="icf0002" he="134" wi="429" file="FDA0002022817240000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>计算边界点的平均法向n<Sub>k</Sub>,其中,k＝1,2分别代表法向的方向以及法向的反方向；S3,对控制网格进行变形及融合处理；具体地,所述步骤S3包括：S31,计算控制网格上的每个顶点的均值坐标；S32,根据均值坐标对控制网格进行变形及融合；S4,对变形及融合后的控制网格进行渗透处理。</td>   <td>G06T15/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              麦思杰;              邢宋隆;                   陈志鸿       </td>   <td>中山大学</td>   <td>基于度量学习和元学习的小样本和零样本图像分类方法</td>   <td>广东省</td>   <td>CN109961089A</td>   <td>2019-07-02</td>   <td>本发明涉及计算机视觉识别和迁移学习领域,提出一种基于度量学习和元学习的小样本和零样本图像分类方法,包括以下步骤：构建训练数据集和目标任务数据集；从训练数据集选取支撑集和测试集；将测试集和支撑集的样本分别输入特征提取网络得到特征向量；将测试集和支撑集的特征向量依次输入特征关注模块和距离度量模块中,计算测试集样本与支撑集样本的类别相似度,并利用损失函数对各个模块的参数进行更新；重复上述步骤至各个模块网络的参数收敛,完成各模块的训练；将从目标任务数据集中的待测图片和训练图片依次通过特征提取网络、特征关注模块和距离度量模块,输出与测试集类别相似度最高的类别标签,即为待测图片的分类结果。</td>   <td>1.基于度量学习和元学习的小样本和零样本图像分类方法,其特征在于,包括以下步骤：S1.收集生活场景图像,经过人工分类构建训练数据集和目标任务数据集；S2.从训练数据集中随机抽取不同类别的若干张训练图片或语义属性作为样本组成支撑集,从所选取的类别中抽取若干张不重复的训练图片作为样本组成测试集；S3.将测试集样本输入特征提取网络f<Sub>θ</Sub>,将支撑集样本输入特征提取网络g<Sub>θ</Sub>中输出得到对应的特征向量f(x)和g(x)；S4.将测试集样本和支撑集样本对应的特征向量f(x)和g(x)分别输入特征关注模块中,输出对应的关注后的特征向量f′(x)和g′(x)；S5.将测试集样本和支撑集样本对应的关注后的特征向量f′(x)和g′(x)分别输入距离度量模块中,计算测试集样本与支撑集样本的类别相似度,并利用损失函数通过梯度反向传播算法更新各个模块的参数；S6.重复步骤S2～S5,直到各个模块或网络的参数收敛；S7.将目标任务数据集中的待测图片输入训练后的特征提取网络f<Sub>θ</Sub>,将目标任务中的所有训练图片或语义属性输入训练后的特征提取网络g<Sub>θ</Sub>,然后将输出的特征向量依次通过训练后的特征关注模块和距离度量模块,最终输出与待测图片类别相似度最高的类别标签,即为待测图片的识别分类结果。</td>   <td>G06K9/62;G06K9/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              陈炳成;              赖兆荣;                   王青       </td>   <td>中山大学</td>   <td>一种基于核回归的多时段资产组合选择方法及系统</td>   <td>广东省</td>   <td>CN109948831A</td>   <td>2019-06-28</td>   <td>本发明公开了一种基于核回归的多时段资产组合选择方法及系统,该方法包括：步骤S1,初始化；步骤S2,从数据集中读取各资产在当前时刻的窗口长度的历史价格,计算资产价格比向量；步骤S3,计算当前时刻的资产总值；步骤S4,通过核回归估计下一时刻的资产价格向量,并计算下一时刻的资产价格比向量估计值；步骤S5,计算各个资产的平均价格比,并对下一时刻的资产价格比向量估计值进行归一化；步骤S6,计算下一时刻的增长因子估计值,若该值小于预设阈值,则将归一化后的资产价格比向量估计值作为步长更新资产组合的权重向量；步骤S7,计算与更新后的资产组合的权重向量距离最短且满足资产组合前提条件的权重向量；步骤S8,返回步骤S2,直至运行至最终时刻。</td>   <td>1.一种基于核回归的多时段资产组合选择方法,包括如下步骤：步骤S1,对资产组合的权重向量进行平均初始化,读取资产价格的数据集S,选择核函数,并设置核函数窗口长度为w；步骤S2,从所述数据集S中读取各资产在当前时刻的窗口长度为w的历史价格,并计算当前时刻的资产价格比向量；步骤S3,计算当前时刻的资产总值；步骤S4,通过采用所述核函数的核回归计算公式估计下一时刻的资产价格向量,从而得到下一时刻的资产价格比向量估计值；步骤S5,计算各个资产的平均价格比,并用该平均价格比对下一时刻的资产价格比向量估计值进行归一化；步骤S6,计算下一时刻的增长因子估计值,若该增长因子估计值小于一个预先设定的阈值,则将归一化后的资产价格比向量估计值作为步长来更新资产组合的权重向量；步骤S7,计算与更新后的资产组合的权重向量距离最短且满足资产组合前提条件的权重向量；步骤S8,返回步骤S2,直至运行至最终时刻。</td>   <td>G06Q10/04;G06Q10/06;G06Q40/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈川;              林志伟;                   郑子彬       </td>   <td>中山大学</td>   <td>一种基于投资关系网络的重要节点挖掘方法及装置</td>   <td>广东省</td>   <td>CN109949164A</td>   <td>2019-06-28</td>   <td>本发明公开一种基于投资关系网络的重要节点挖掘方法及装置,该装置用于实现本方法,本方法包括建立基于投资行为的投资关系网络；以鲁棒性指标为衡量节点重要性排序的优劣R,定义优劣R；将优劣R中删除节点的当前网络最大连通规模值反向为添加节点的当前网络最大连通规模值,采用反向生成网络法BGN计算该节点i的cost[i]添加入网节点的当前网络最大连通规模值,对优劣R求最小排序,挖掘排序序列前k个节点为重要节点集。本发明采用反向生成网络算法挖掘重要节点,更具新颖性、高精度性及高效率性。</td>   <td>1.一种基于投资关系网络的重要节点挖掘方法,其特征在于,包括如下步骤：S10将参与投资行为的对象抽象为节点,对对象间存在交易的节点连边,生成投资关系的网络；S20以鲁棒性指标为衡量节点重要性排序的优劣R,定义p为节点的移除节点比,σ为删除预设p比例的节点之后当前网络中最大连通集团的规模值,优劣R公式为：<Image id="icf0001" he="141" wi="345" file="FDA0002010964950000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中,n为网络节点个数,<Image id="icf0002" he="142" wi="138" file="FDA0002010964950000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为删除比例为<Image id="icf0003" he="118" wi="132" file="FDA0002010964950000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>的节点之后当前网络的最大连通规模值,以p为横坐标,σ为纵坐标,得优劣R的曲线,鲁棒性指标为该曲线下的面积；S30将<Image id="icf0004" he="142" wi="135" file="FDA0002010964950000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>反向为添加入网节点i的当前网络最大连通规模值cost[i],采用反向生成网络BGN算法计算该节点i的cost[i],对优劣R求最小排序,挖掘排序序列前k个节点为重要节点集。</td>   <td>G06Q40/06;G06F16/2458;G06F16/2457</td>  </tr>        <tr>   <td>中国专利</td>   <td>         温木奇;              谢明森;              黄国燕;                   万海       </td>   <td>中山大学;广州云晫信息科技有限公司</td>   <td>一种教育云平台中的大数据任务调度方法</td>   <td>广东省</td>   <td>CN109947532A</td>   <td>2019-06-28</td>   <td>本发明为教育云平台中的大数据任务调度方法,根据云平台总体资源、核心大数据处理任务优先级及其资源需求量、预期任务运行时间进行任务调度；若任务请求不是大数据任务,直接调度任务；若是核心任务则预测系统资源及运行时间；根据各项任务参数在数据库中添加相应的任务实例；根据调度方法将任务添加到任务队列中,获取运行优先级最高的任务；当云平台资源满足任务资源需求时按需分配资源,若当前使用资源未超过最大可用资源将任务分发到相应受培训者的大数据集群中；否则重新添加到任务队列中。能防止过多用户同时通过其已获得分配的虚拟机集群高并发运行大数据任务,从而支持机构利用有限物理机资源为每个受培训者提供有效的大数据实验环境。</td>   <td>1.一种教育云平台中的大数据任务调度方法,其特征在于,大数据任务调度方法运行在基于云计算技术的教育云平台,根据云平台的总体资源、核心大数据处理任务优先级及其资源需求量、预期任务运行时间进行任务调度；所述任务调度步骤包括：1)受培训者提交任务；2)系统自动拦截任务请求,并判断任务是否为大数据任务；若不是大数据任务,系统直接调度任务；否则,进入下一步骤；3)判断任务是否为教师建议的核心任务,若是核心任务,则预测核心任务需要的系统资源及运行时间；否则,将任务的运行优先级设为最低；4)根据任务代码、数据源、任务名称、任务运行参数,在数据库中添加相应的任务实例；5)根据调度方法将任务添加到任务队列中；6)根据调度方法从任务队列中获取运行优先级最高的任务；7)当云平台的资源满足任务的资源需求时,进入下一步骤；否则,将任务重新添加到任务队列中；8)按任务的资源需求分配资源后,如果用户当前使用的资源没有超过用户最大可用资源时,将任务分发到相应受培训者的大数据集群中；否则,将任务重新添加到任务队列中。</td>   <td>G06F9/455;G06F9/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         余顺争;                   曾旺       </td>   <td>中山大学</td>   <td>一种基于SDN的云安全功能调度系统</td>   <td>广东省</td>   <td>CN109947534A</td>   <td>2019-06-28</td>   <td>本发明公开一种基于SDN的云安全功能调度系统,系统包括云安全虚拟编排模块、云安全虚拟功能模块、云安全虚拟载荷模块、云安全实现模块和虚拟资源服务端模块。该系统基于云环境下的计算资源虚拟化和网络虚拟化,定义了从单个安全功能的部署到一整套安全编排的虚拟化过程,为用户在不同网络环境下实现统一的安全功能调度建立基础。系统在实现层面利用SDN架构控制与数据的分离,因而具有简单灵活和易扩展等特点。</td>   <td>1.一种基于SDN的云安全功能调度系统,其特征在于,包括云安全虚拟编排模块、云安全虚拟功能模块、云安全虚拟载荷模块、云安全实现模块以及虚拟资源服务端模块；所述的云安全虚拟编排模块用于实现对安全编排的定义,调用云安全虚拟功能模块完成对安全编排的整体操作；所述的云安全虚拟功能模块用于建立安全配置服务器和调用云安全虚拟载荷模块完成对单个安全功能的自动配置和部署；所述的云安全虚拟载荷模块用于定义不同类型的虚拟安全载荷并实现自动化部署；所述的云安全实现模块用于接收并处理对于安全载荷的部署命令；所述的虚拟资源服务端模块用于对单个宿主机内部虚拟的虚拟机、docker和OVS的虚拟资源进行管理。</td>   <td>G06F9/455;H04L29/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨晓杏;              温武少;                   李鑫       </td>   <td>中山大学</td>   <td>一种软件缺陷预测的改进排序学习方法</td>   <td>广东省</td>   <td>CN109947652A</td>   <td>2019-06-28</td>   <td>本发明为软件缺陷预测的改进排序学习方法,包括步骤：从已知缺陷个数的源代码文件中提取度量元和相应的缺陷个数,作为训练数据；对训练数据进行预处理；利用预处理过的训练数据构造预测模型,使用多目标优化算法同时优化模型的排序性能、回归性能以及模型复杂度,得到一组模型参数,然后根据实际需要选择合适的模型参数,或者先根据实际需求对模型的排序性能、回归性能以及模型复杂度赋予相应的权值,然后使用单目标优化算法求得模型参数；利用训练得到的模型分析测试数据,得到相应软件模块的缺陷信息。本发明能优化模型的排序性能、回归性能以及模型复杂度,更好适应不同应用场景下的不同需求。</td>   <td>1.一种软件缺陷预测的改进排序学习方法,其特征在于,包括以下步骤：步骤1、从已知缺陷个数的源代码文件中提取度量元和相应的缺陷个数,作为训练数据；步骤2、对训练数据进行预处理,包括对重复样本、不一致样本、缺失数据的处理；步骤3、利用预处理过的训练数据构造预测模型,使用多目标优化算法同时优化模型的排序性能、回归性能以及模型复杂度,得到一组模型参数,然后根据实际需要选择合适的模型参数；或者先根据实际需求对模型的排序性能、回归性能以及模型复杂度赋予相应的权值,然后使用单目标优化算法求得模型参数；步骤4、输入测试数据,利用训练得到的模型分析测试数据,得到相应软件模块的缺陷信息。</td>   <td>G06F11/36;G06F11/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         罗洁;                   张亮军       </td>   <td>中山大学</td>   <td>一种基于磁共振T1增强图像的脑血管图像分割方法</td>   <td>广东省</td>   <td>CN109949322A</td>   <td>2019-06-28</td>   <td>本发明涉及医学影像处理技术领域,涉及一种基于T1增强图像的脑血管图像分割方法,该方法包括：1.对头部磁共振T1增强图像数据进行预处理；2.对脑血管进行自动分割,可利用交互式方法对自动分割结果进行修正；3.根据分割结果自动标记出动脉和静脉；4.交互式对血管进行颜色渲染。本发明利用T1增强图像数据对脑血管进行全体或局部提取,并区分出其中的动脉和静脉,最后进行三维重构,允许用户交互式的对提取过程以及展示效果进行修改,全面的展示出脑血管三维分布状态。</td>   <td>1.一种基于T1增强图像的脑血管图像分割方法,其特征在于,包括以下步骤：S1.对头部磁共振T1增强图像数据进行预处理；S2.对预处理后的脑血管图像进行自动分割,利用交互式方法对自动分割结果进行修正,得到修正分割结果；S3.根据分割结果自动标记出动脉和静脉；S4.对血管用不同颜色进行渲染。</td>   <td>G06T7/11;G06T7/187;G06T17/20;G06T15/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         江明;              吴承刚;              李正鹏;                   彭时玉       </td>   <td>中山大学</td>   <td>一种基于圆形投影的可见光成像定位方法</td>   <td>广东省</td>   <td>CN109949367A</td>   <td>2019-06-28</td>   <td>本发明提供了一种基于圆形投影的可见光成像定位方法。该方法通过对圆形LED灯或配有圆形灯罩的LED灯的成像透视投影的建模和分析,根据圆形投影性质获取LED灯中心在摄像头坐标系统(Camera Coordinate System,CCS)的位置坐标并结合LED灯中心在世界坐标系统(World Coordinate System,WCS)和CCS坐标变换的关系来实现精准定位。该方法只需要拍摄一幅包含两个圆形LED灯的图像,并利用倾斜传感器测出精准的滚转角和俯仰角,即可实现高精度定位。该方法克服了现有技术实施时LED灯具中心与LED成像中心无法对应而引起定位不准,或需要同时检测到数量较多的LED灯具才能进行定位,又或者受限于不精确的方位角测量结果等缺点,具有较好的实际应用价值。</td>   <td>1.一种基于圆形投影的可见光成像定位方法,其特征在于,包括以下步骤：步骤S1：移动终端通过其摄像头对LED灯进行拍照,获得至少有两个LED灯具的RGB图像,再将该RGB图像转为灰度图；步骤S2：移动终端从灰度图解调出各LED灯的ID信息,将该信息与其本地存储的LED-ID数据库的信息进行比对,找到与该ID信息相对应的第i盏LED灯中心的世界坐标系统WCS坐标<Image id="icf0001" he="96" wi="569" file="FDA0001991635870000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>圆形灯罩实体的半径L<Sub>i</Sub>,i＝1,2,...,M,以及LED灯的姿态特征参数集&lt;Δα<Sub>i</Sub>,Δβ<Sub>i</Sub>,0&gt;,i＝1,2,K,M,其中Δα<Sub>i</Sub>表示第i个圆形LED平面绕WCS的x<Sub>w</Sub>轴的旋转角度,Δβ<Sub>i</Sub>表示第i个圆形LED平面绕WCS的y<Sub>w</Sub>轴的旋转角度；步骤S3：对步骤S1获取的灰度图像采用边缘提取技术,获取每个LED灯像的边缘像素点集合,记为S<Sub>i</Sub>,i＝1,2,K,M；步骤S4：利用每个LED灯像的边缘像素点集合进行椭圆拟合,获取每个LED灯像的椭圆参数集合；步骤S5：根据椭圆参数集合以及圆形投影性质,获取每个LED灯中心在摄像头坐标系统CCS的位置坐标；步骤S6：移动终端通过其内置的倾斜传感器测量出移动终端的滚转角α和俯仰角β；步骤S7：通过步骤S2求得的LED灯中心的WCS坐标,圆形灯罩实体的半径,LED灯的姿态特征参数集；以及步骤S6求得的滚转角α和俯仰角β,利用LED灯中心在WCS和CCS的坐标变换关系来获取移动终端的位置。</td>   <td>G06T7/73;G06T7/66</td>  </tr>        <tr>   <td>中国专利</td>   <td>         徐瑞华;              徐国良;              李超峰;                   骆卉妍       </td>   <td>中山大学肿瘤防治中心</td>   <td>一种上消化道内镜图像的诊断方法及装置</td>   <td>广东省</td>   <td>CN109949275A</td>   <td>2019-06-28</td>   <td>本发明实施例公开了一种上消化道内镜图像的诊断方法及装置。该方法包括：对含有肿瘤信息的上消化道内镜图像样本进行肿瘤轮廓标记；以标记完成的含有肿瘤信息的上消化道内镜图像样本和不含肿瘤信息的上消化道内镜图像样本为依据,训练获得上消化道内镜诊断模型；将待测上消化道内镜图像输入上消化道内镜诊断模型,获得待测上消化道内镜图像的诊断结果；诊断结果包括第一分类结果、第二分类结果和语义分割结果中的一种或多种结果；其中,第一分类结果用于描述待测上消化道内镜图像是否为上消化道内镜图像,第二分类结果用于描述待测上消化道内镜图像是否含有肿瘤信息。实施本发明实施例,能够进行一次性多任务输出,进而提高识别准确率。</td>   <td>1.一种上消化道内镜图像的诊断方法,其特征在于,包括：对含有肿瘤信息的上消化道内镜图像样本进行肿瘤轮廓标记；以标记完成的含有肿瘤信息的上消化道内镜图像样本和不含肿瘤信息的上消化道内镜图像样本为依据,训练获得上消化道内镜诊断模型；将待测上消化道内镜图像输入所述上消化道内镜诊断模型,获得所述待测上消化道内镜图像的诊断结果；所述诊断结果包括第一分类结果、第二分类结果和语义分割结果中的一种或多种结果；其中,所述第一分类结果用于描述所述待测上消化道内镜图像是否为上消化道内镜图像,所述第二分类结果用于描述所述待测上消化道内镜图像是否含有肿瘤信息。</td>   <td>G06T7/00;G06T7/10;G06K9/34;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄土琛;                   付琪镔       </td>   <td>中山大学</td>   <td>一种基于拉格朗日插值的脉冲幅度获取方法</td>   <td>广东省</td>   <td>CN109948223A</td>   <td>2019-06-28</td>   <td>本发明公开了一种基于拉格朗日插值的脉冲幅度获取方法,采样脉冲信号波形峰值附近多个连续数据点的时刻和幅度,从幅度最大的第n个数据点开始,以Y<Sub>n</Sub>作为起始峰值Y<Sub>peak</Sub>,向前指定时间间隔Δt1,代入所述拉格朗日插值函数式,得到(T<Sub>peak</Sub>-Δt1)时刻数据点的幅度Y<Sub>t-</Sub>；向后指定时间间隔Δt2,代入所述拉格朗日插值函数式,得到(T<Sub>peak</Sub>+Δt2)时刻数据点的幅度Y<Sub>t+</Sub>；比较Y<Sub>t-</Sub>、Y<Sub>peak</Sub>和Y<Sub>t+</Sub>之间的大小：若Y<Sub>peak</Sub>最大,则获得脉冲信号波形的峰值；由于采用了拉格朗日插值算法重建脉冲信号幅度,只需采用低速率的模数转换器,使得电路的结构更加简单,既不需要采用峰值保持电路等额外的硬件电路,也不需要搭配可编程逻辑器件或者数字信号处理器来进行数据处理,成本低、功耗小。</td>   <td>1.一种基于拉格朗日插值的脉冲幅度获取方法,其特征在于,包括以下步骤：A、采样脉冲信号波形峰值附近至少3个连续数据点的时刻和幅度,且这些数据点的幅度根据其时间顺序同时满足从小到大再到小的关系；B、将所述数据点的时刻和幅度代入拉格朗日插值函数式<Image id="icf0001" he="61" wi="248" file="DEST_PATH_IMAGE001.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>中,得到新的时刻T对应的幅度Y(T)；C、从幅度最大的第n个数据点开始,以Y<Sub>n</Sub>作为起始峰值Y<Sub>peak</Sub> ,向前指定时间间隔Δt1,代入所述拉格朗日插值函数式,得到(T<Sub>peak</Sub>-Δt1)时刻数据点的幅度Y<Sub>t-</Sub> ；同时,向后指定时间间隔Δt2,代入所述拉格朗日插值函数式,得到(T<Sub>peak</Sub>+Δt2)时刻数据点的幅度Y<Sub>t+</Sub> ；D、比较Y<Sub>t-</Sub> 、Y<Sub>peak</Sub>和Y<Sub>t+</Sub> 之间的大小：若Y<Sub>peak</Sub>最大,则获得脉冲信号波形的峰值并结束；若Y<Sub>peak</Sub> ＜Y<Sub>t-</Sub> ,则进入步骤E之后结束；若Y<Sub>t-</Sub> ＜Y<Sub>peak</Sub> ＜Y<Sub>t+</Sub> ,则进入步骤F之后结束；E、以Y<Sub>t-</Sub> 作为新的峰值Y<Sub>peak</Sub> ,继续向前指定时间间隔Δt1,代入所述拉格朗日插值函数式,得到(T<Sub>peak</Sub>-Δt1)时刻数据点的幅度Y<Sub>t-</Sub> ,并比较Y<Sub>peak</Sub>和Y<Sub>t-</Sub>的大小,直到当Y<Sub>peak</Sub> ≥Y<Sub>t-</Sub>时,获得脉冲信号波形的峰值；F、以Y<Sub>t+</Sub> 作为新的峰值Y<Sub>peak</Sub> ,继续向后指定时间间隔Δt2,代入所述拉格朗日插值函数式,得到(T<Sub>peak</Sub>+Δt2)时刻数据点的幅度Y<Sub>t+</Sub> ,并比较Y<Sub>peak</Sub>和Y<Sub>t+</Sub>的大小,直到当Y<Sub>peak</Sub> ≥Y<Sub>t+</Sub>时,获得脉冲信号波形的峰值。</td>   <td>G06F17/50;G06F17/15</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              陈绿然;              严志伟;                   李烨       </td>   <td>中山大学</td>   <td>一种基于判别性区域挖掘的目标检测方法</td>   <td>广东省</td>   <td>CN109948628A</td>   <td>2019-06-28</td>   <td>本发明提供一种基于判别性区域挖掘的目标检测方法,通过特征提取网络进行特征提取,获取特征流；构建LDRM模块,将特征流进行局部的判别性特征学习；构建CDRM模块,对局部的判别性特征学习后的特征流进行上下文判别特征学习；构建特征流s<Sub>3</Sub>,采用检测器对特征流s<Sub>3</Sub>的特征图进行检测,得到最终的检测结果。本发明提供的一种基于判别性区域挖掘的目标检测方法,基于感受野的特征图产生判别性区域的特征表达,避免了从原图中提取判别性区域,再进行特征提取而引入的大量计算量,保证该方法以高的效率进行目标检测,防止受到表观相似的前景目标和背景区域的干扰；通过将生成的判别性区域特征与候选目标的特征进行融合,结合不同感受野的特征,优化特征表达。</td>   <td>1.一种基于判别性区域挖掘的目标检测方法,其特征在于,包括以下步骤：S1：通过特征提取网络进行特征提取,获取特征流s<Sub>1</Sub>、s<Sub>2</Sub>；S2：构建局部判别性区域挖掘LDRM模块,将特征流s<Sub>1</Sub>、s<Sub>2</Sub>进行局部的判别性特征学习；S3：构建基于上下文判别性区域挖掘CDRM模块,对局部的判别性特征学习后的特征流s<Sub>1</Sub>、s<Sub>2</Sub>进行上下文判别特征学习；S4：构建特征流s<Sub>3</Sub>,采用检测器对特征流s<Sub>3</Sub>的特征图进行检测,得到最终的检测结果。</td>   <td>G06K9/46;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         成慧;              杨凯;              吴华栋;                   张东       </td>   <td>中山大学</td>   <td>基于图像输入的多智能体跨模态深度确定性策略梯度训练方法</td>   <td>广东省</td>   <td>CN109948642A</td>   <td>2019-06-28</td>   <td>本发明涉及一种基于图像输入的多智能体跨模态深度确定性策略梯度训练方法；首先构建在仿真平台中的机械臂训练环境；之后构造两个利用不同模态输入的导师(teacher)和学徒(student)智能体；然后基于深度确定性策略梯度算法,训练导师的actor模块与critic模块和学徒的actor模块,最终实现基于图像输入的跨模态深度强化学习机械臂训练算法；在总体训练完成的时候,就能够只使用学徒的演员网络,接受高维度的图像输入,输出能够完成任务的动作,并且这样的方法很适合迁移到真实环境中,由于真实环境无法提供全状态模态的信息,但是图像模态的信息较为容易获得,所以当训练好学徒的演员网络之后,就可以抛弃全状态模态信息的需求,直接利用图像输入获得比较好的输出策略。</td>   <td>1.一种基于图像输入的多智能体跨模态深度确定性策略梯度训练方法,其特征在于,包括以下步骤：S1.搭建仿真器中的实验平台,定义交互物体与机械臂类型,定义机械臂控制任务的最终目标与奖惩规则,明确双智能体的状态空间和动作空间；S2.基于深度确定性策略梯度算法,为两组智能体：teacher和student建立决定行动的actor模块与评判反馈的critic模块,两种模块都基于深度神经网络搭建,并随机初始化网络参数；S3.利用仿真环境中容易直接读取的全状态信息结合深度确定性策略梯度预先训练导师智能体的actor和critic模块,该训练过程包括智能体对环境的探索和智能体利用探索收集到的数据对actor和critic模块进行更新；S4.利用训练好的导师智能体,指导学徒智能体actor模块的训练,该过程包括：学徒智能体对环境的单独探索和学徒智能体利用探索收集到的数据以及导师智能体给予的梯度指导耦合优化actor模块,同时利用学徒智能体的训练数据以极小学习率优化导师的actor与critic模块；S5.重复步骤S4,直到智能体的决策满足优化终止条件。</td>   <td>G06K9/62;G06K9/66;G06N3/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              庄景宇;                   王青       </td>   <td>中山大学</td>   <td>一种基于元对抗学习的多目标域适应迁移方法及系统</td>   <td>广东省</td>   <td>CN109948648A</td>   <td>2019-06-28</td>   <td>本发明公开了一种基于元对抗学习的多目标域适应迁移方法及系统,该方法包括：步骤S1,获取带标记的源域数据集以及无标记的多个目标域数据集,使用所有目标域数据进行通过元学习者进行聚类,得到伪子目标域类别标签；步骤S2,使用源域数据、多目标域数据与伪子目标域类别标签进行对抗,更新目标模型的表示网络,源-目标域判别器和子目标域判别器；步骤S3,判断是否达到更新周期,若达到更新周期,则进行步骤S4,若没有达到,则进行步骤S5；步骤S4,将所有目标域数据与当前网络中产生的目标域特征数据通过元学习者进行聚类,更新所述伪子目标域类别标签；步骤S5,返回步骤S2,直至模型收敛或达到最大迭代次数时停止训练。</td>   <td>1.一种基于元对抗学习的多目标域适应迁移方法,包括如下步骤：步骤S1,获取带标记的源域数据集以及无标记的多个目标域数据集,使用所有目标域数据进行通过元学习者进行聚类,得到伪子目标域类别标签；步骤S2,使用源域数据、多目标域数据与伪子目标域类别标签进行对抗,更新目标模型的表示网络,源-目标域判别器和子目标域判别器；步骤S3,判断是否达到更新周期,若达到更新周期,则进行步骤S4,若没有达到,则进行步骤S5；步骤S4,将所有目标域数据与当前网络中产生的目标域特征数据通过元学习者进行聚类,更新所述伪子目标域类别标签；步骤S5,返回步骤S2,直至模型收敛或达到最大迭代次数时停止训练。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周虹君;              陈佩;              郑慧诚;                   沈伟       </td>   <td>中山大学</td>   <td>基于语义分割和孪生神经网络的无人机图像变化检测方法</td>   <td>广东省</td>   <td>CN109934166A</td>   <td>2019-06-25</td>   <td>本发明属于多时相无人机图像变化检测技术领域,更具体地,涉及基于语义分割和孪生神经网络的无人机图像变化检测方法。包括以下步骤：S1.扩展数据集并划分数据集；S2.搭建基于语义分割框架DeeplabV3和孪生网络结合的深度神经网络模型；S3.利用训练数据集训练基于DeeplabV3的孪生神经网络模型；S4.基于测试数据集和训练后的模型,验证训练结果。本发明结合了语义分割的思想,并利用孪生网络权值共享特点,有利于提取有现实含义的特征,考虑像素间的语义关系和变化区域的多尺度问题,并解决噪声敏感、变化检测精度较低等问题,提高差异图的质量和鲁棒性。</td>   <td>1.一种基于语义分割和孪生神经网络的无人机图像变化检测方法,其特征在于,包括以下步骤：S1.扩展数据集并划分数据集；S2.搭建基于语义分割框架DeeplabV3和孪生网络结合的深度神经网络模型；S3.利用训练数据集训练基于DeeplabV3的孪生神经网络模型；S4.基于测试数据集和训练后的模型,验证训练结果。</td>   <td>G06K9/00;G06K9/62;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         招继恩;              龙飞;              胡建国;              杨焕;              朱勇杰;                   王国良       </td>   <td>杰创智能科技股份有限公司;中山大学</td>   <td>一种基于深度学习的多尺度转换目标检测算法</td>   <td>广东省</td>   <td>CN109934236A</td>   <td>2019-06-25</td>   <td>本发明公开了一种基于深度学习的多尺度转换目标检测算法,其特征在于,包括以下步骤：S1,采用基础网络进行特征提取；S2,采用多尺度转换模块,对于卷积网络生成的特征进行处理；S3,采用目标定位和分类模块,对目标进行精准定位以及对输出目标分类。本发明的算法改善了整个网络的信息流和梯度,使得更容易进行训练。每一层都可以直接访问损失函数和原始输入信号的梯度,从而实现隐式的深度监控,这有助于更深入地训练网络架构。</td>   <td>1.一种基于深度学习的多尺度转换目标检测算法,其特征在于,包括以下步骤：S1,采用基础网络进行特征提取；基础网络作为一种网络体系结构,将所有具有匹配的映射特征大小的层直接连接在一起,每个层从前面所有层获取额外的输入,并将自己的映射特征传递给后面所有层；不在特性被传递到下一个网络层之前通过求和来组合它们,而是通过连接这些特性来进行组合；S2,采用多尺度转换模块,对于卷积网络生成的特征进行处理；采用多尺度转换模块将不同分辨率的映射特征的预测结果结合起来,在上述的网络体系结构中,最后一层的输出是具有不同尺寸大小的映射特征,且具有高维度的通道数；且通过所述网络体系结构,将底层特征直接转移到网络的顶部,得到了强语义特征,网络顶部的映射特征既有底层的细节信息,又有高层的语义信息,从而提高了目标定位和分类的性能；在多尺度转换模块中,一方面采用平均池化来获取低分辨率的映射特征,另一方面对于高分辨率的映射特征,采用将高纬度的通道数转换为更高分辨率的映射特征；S3,采用目标定位和分类模块,对目标进行精准定位以及对输出目标分类；是由目标定位子网和目标分类子网组成模块,通过定锚机制,为在多尺度转换模块中获得的每个映射特征的每个像素点配备上一组若干个尺寸的默认锚框。</td>   <td>G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王青;              赵惠;              陈添水;              陈日全;                   林倞       </td>   <td>中山大学</td>   <td>一种知识驱动参数传播模型及其少样本学习方法</td>   <td>广东省</td>   <td>CN109934261A</td>   <td>2019-06-25</td>   <td>本发明公开了一种知识驱动参数传播模型及其少样本学习方法,该模型包括：特征提取模块,用于在基础类别样本组成的数据集上对特征提取器进行训练,并利用训练好的特征提取器提取基础类别和只有少量样本的新类别的样本的特征；图神经网络模块,用于引入类别之间的关系作为先验知识,利用知识图表示类别之间的先验关系,并集成该知识图利用图神经网络通过图形式传播迭代更新分类器参数；分类预测模块,用于利用提取出的特征和更新后的分类器参数得到分类结果,本发明可提供提高少样本分类的精度及泛化能力。</td>   <td>1.一种知识驱动参数传播模型,包括：特征提取模块,用于在基础类别样本组成的数据集上对特征提取器进行训练,并利用训练好的特征提取器提取基础类别和只有少量样本的新类别的样本的特征；图神经网络模块,用于引入类别之间的关系作为先验知识,利用知识图表示类别之间的先验关系,并集成该知识图利用图神经网络通过图形式传播迭代更新分类器参数。分类预测模块,用于利用提取出的特征和更新后的分类器参数得到分类结果。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         任毅;              陈翔;                   张琳       </td>   <td>中山大学</td>   <td>基于用户APP使用行为的个体经济消费能力预测方法</td>   <td>广东省</td>   <td>CN109934623A</td>   <td>2019-06-25</td>   <td>本发明公开了一种基于用户APP使用行为的个体经济消费能力预测方法,该方法将电信用户蜂窝数据与机器学习算法相结合,进而对用户个体的经济消费能力进行预测研究,属于大数据领域。该方法主要包括四个步骤：获取来自运营商提供的用户App使用记录数据集和用户职业信息数据集；然后,对用户职业信息数据集进行数据预处理,即对用户根据职业划分经济消费能力等级；然后,进行特征提取,从用户App使用记录数据集中提取用户的轨迹特征、用户App使用特征；最后,使用分类算法处理用户特征,建立个体经济消费能力预测模型进行预测。该发明可以预测个体经济消费能力,从而为运营商业务流量的精细化运营和定价策略提供一种有效的参考模型。</td>   <td>1.一种基于用户APP使用行为的个体经济消费能力预测方法,其特征在于,所述的方法包括下列步骤：S1、获取来自运营商提供的用户App使用记录数据集和用户职业信息数据集；S2、对用户职业信息数据集进行数据预处理,根据用户职业划分经济消费能力等级；S3、提取用户特征,从用户App使用记录数据集中提取用户的轨迹特征、用户App使用特征；S4、使用机器学习的分类算法处理用户特征,建立个体经济消费能力预测模型进行预测。</td>   <td>G06Q30/02;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄俊文;                   倪江群       </td>   <td>中山大学</td>   <td>基于卷积神经网络的JPEG图像隐写分析方法</td>   <td>广东省</td>   <td>CN109934761A</td>   <td>2019-06-25</td>   <td>本发明公开了一种基于卷积神经网络的JPEG图像隐写分析方法,包括以下步骤：S1.构建针对JPEG图像隐写分析的卷积神经网络；S2.准备数据集；S3.初始化卷积神经网络；S4.训练卷积神经网络；S5.利用训练好的卷积神经网络对待检测图像进行隐写分析,计算出分类概率向量从而判定待检测图像是否为载密图像。本发明通过将JPEG图像隐写分析相关的领域知识内嵌到网络结构中,针对JPEG图像隐写分析设计了卷积神经网络结构及相关的参数配置；同时为网络训练引入参数增量约束机制,提高了卷积神经网络的性能,解决了现有的图像隐写分析技术中分类准确率不够高以及无法在低负载情况下直接训练网络等问题。</td>   <td>1.基于卷积神经网络的JPEG图像隐写分析方法,其特征在于,包括以下步骤：S1.构建针对JPEG图像隐写分析的卷积神经网络：包括预处理部分和深度网络部分；所述预处理部分用于预处理JPEG图像,所述深度网络部分用于提取JPEG图像隐写分析特征并进行隐写分析；S2.准备数据集：将原始图像按设定比例随机划分为训练集、验证集、测试集的载体图像,并使用隐写算法对所有载体图像进行隐写生成等量的载密图像；根据隐写算法计算所有载体图像的修改概率矩阵β,然后计算所有载体图像与载密图像相应的L<Sub>1</Sub>范数嵌入失真矩阵t(β)；S3.初始化卷积神经网络：对于卷积神经网络中预处理部分的卷积层,使用高通滤波器对其卷积核进行初始化,并采用截断线性单元作为激活函数；对于除预处理部分的卷积层以外的其他卷积层均使用msra方式进行初始化；S4.训练卷积神经网络：使用AdaDelta算法在步骤S2所述的训练集中对卷积神经网络进行训练,通过验证集对训练中的网络进行验证并通过迭代更新参数直至网络收敛；使用测试集测试网络性能；S5.利用训练好的卷积神经网络对待检测图像进行隐写分析,计算出分类概率向量从而判定待检测图像是否为载密图像。</td>   <td>G06T1/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖蔚;                   周晓聪       </td>   <td>中山大学</td>   <td>一种软件易变性预测模型的构建方法</td>   <td>广东省</td>   <td>CN106528428B</td>   <td>2019-06-25</td>   <td>本发明提供一种软件易变性预测模型的构建方法,该方法使用相似度来定义软件易变性,符合软件外部属性,如需求、功能变化时,体现在源代码上的变化,使用了目前所能定义的大部分度量,得到的度量信息更为全面,并对这些原始度量使用特征提取与选择的技术,提高了模型的计算性能,减少了因特征过多造成的信息冗余。即能够使用更多的度量信息,又能够避免过多的特征而影响模型性能。</td>   <td>1.一种软件易变性预测模型的构建方法,其特征在于,包括以下步骤：S1：以面向对象软件系统中的类为研究对象,提取软件源代码的结构信息；S2：通过S1中提取到的源代码结构信息,计算面向对象软件度量,包括规模类度量、耦合度度量、内聚度度量、继承类度量；S3：通过对比在软件演化中不同版本所对应的类的变化情况,通过类相似度来定义并解析出类的易变性信息,该易变性信息包括分类标签；S4：对S2中计算得到的面向对象软件度量进行特征提取和选择,分别分析规模类、内聚度、耦合度、继承类度量与软件易变性的相关性,选择相关度较高的度量组成度量集合,对该度量集合进行主成分分析进行数据降维,去除冗余信息,得到一组新的特征集合,以该组特征集合作为前馈多层感知器算法的输入即前馈神经网络的输入层,使用收集到的训练数据集,选择特定的参数,训练得到软件易变性预测模型；S5：使用S3中得到的分类标签与S4中得到的特征集合,生成软件易变性预测模型的训练集,使用前馈多层感知器算法训练出预测模型。</td>   <td>G06F11/36</td>  </tr>        <tr>   <td>中国专利</td>   <td>         徐萌;              蔡敏;              程芷兰;              陈宣亦;              李佩;              王毅豪;              谭汝铿;                   李自立       </td>   <td>中山大学附属口腔医院</td>   <td>一种基于Unity 3D的三维牙体模型构建方法及系统</td>   <td>广东省</td>   <td>CN109920047A</td>   <td>2019-06-21</td>   <td>本发明公开了一种基于Unity 3D的三维牙体模型构建方法及系统,该方法包括如下步骤：步骤S1,扫描牙体获取图像数据,并对牙体图像数据进行图像处理,通过三维重建技术根据牙体图像对牙体进行三维重建,得到牙体的三维模型；步骤S2,制作牙磨片,并通过采集各牙磨片的镜下结构建立数字化牙磨片图库；步骤S3,利用Unity 3D,于所述Unity 3D中导入牙体三维模型,调整所述牙体三维模型,得到最终的三维牙体模型,根据所述最终的三维牙体模型,以实现牙体的任意旋转、缩放并显示具体结构信息,并提供牙体内外结构,通过本发明,可解决实验室缺乏牙体模型以及学生难以收集牙位齐全、结构典型的牙体的问题。</td>   <td>1.一种基于Unity 3D的三维牙体模型构建方法,包括如下步骤：步骤S1,扫描牙体获取图像数据,并对牙体图像数据进行图像处理,通过三维重建技术根据牙体图像对牙体进行三维重建,得到牙体的三维模型；步骤S2,制作牙磨片,并通过采集各牙磨片的镜下结构建立数字化牙磨片图库；步骤S3,利用Unity 3D,于所述Unity 3D中导入牙体三维模型及其对应牙磨片图像,调整并实现所述牙体三维模型,得到最终的三维牙体模型,根据所述最终的三维牙体模型,以实现牙体的任意旋转、缩放并显示具体结构信息,并提供牙体内外结构。</td>   <td>G06T17/00;G06T7/136</td>  </tr>        <tr>   <td>中国专利</td>   <td>         徐瑞华;              骆卉妍;              李超峰;                   徐国良       </td>   <td>中山大学肿瘤防治中心</td>   <td>消化道内镜影像中肿瘤的多尺度检测方法及装置</td>   <td>广东省</td>   <td>CN109919212A</td>   <td>2019-06-21</td>   <td>本发明实施例公开了一种消化道内镜影像中肿瘤的多尺度检测方法及装置。该方法包括：对各个消化道内镜影像样本进行肿瘤影像标记,并根据预设的肿瘤性质等级,对被标记为肿瘤影像的消化道内镜影像样本进行肿瘤性质等级标记；以进行肿瘤性质等级标记后的消化道内镜影像样本为依据,训练得到消化道内镜影像检测模型；获取待检测内镜影像；根据消化道内镜影像检测模型,获得待检测内镜影像的检测结果,检测结果包括第一分类结果、第二分类结果和多尺度肿瘤区域检测结果中的一种或多种信息。实施本发明实施例,能够降低标记成本,同时实现对消化道内镜影像中的肿瘤区域进行精确定位。</td>   <td>1.一种消化道内镜影像中肿瘤的多尺度检测方法,其特征在于,包括：对各个消化道内镜影像样本进行肿瘤影像标记,并根据预设的肿瘤性质等级,对被标记为肿瘤影像的消化道内镜影像样本进行肿瘤性质等级标记；以进行所述肿瘤性质等级标记后的消化道内镜影像样本为依据,训练得到消化道内镜影像检测模型；获取待检测内镜影像；根据所述消化道内镜影像检测模型,获得所述待检测内镜影像的检测结果,所述检测结果包括第一分类结果、第二分类结果和多尺度肿瘤区域检测结果中的一种或多种信息；其中,所述第一分类结果用于描述所述待检测内镜影像是否为消化道内镜影像,所述第二分类结果用于描述消化道内镜影像正常、良性病变或恶性病变。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         吴嘉婧;              赖戎;              郑子彬;                   张桔       </td>   <td>中山大学</td>   <td>基于电力系统节点重要性的耦合方法</td>   <td>广东省</td>   <td>CN109919801A</td>   <td>2019-06-21</td>   <td>本发明公开基于电力系统节点重要性的耦合方法及装置,该装置用于实现该方法,本方法包括基于电力系统模型创建级联失效模型；对模型中的电力网络实施级联失效攻击,在保证用电节点获取常规输入功率下,动态调节每个供电节点的实际输出功率,在动态调节中获取每个供电节点在每个稳态下的输出功率,分别计算出节点影响力和节点脆弱性,两者加权求和获得节点d<Sub>0</Sub>的重要性,基于节点的重要性从小到小排序生成新的电力网络,将新的电力网络与通信网络进行异配耦合,生成新的电力系统模型。本发明通过直流潮流模型计算得到节点重要性得到更准确的仿真结果,再将新的电力网络与通信网络进行异配耦合,提高电力系统网络在攻击下的鲁棒性。</td>   <td>1.基于电力系统节点重要性的耦合方法,其特征在于,包括如下步骤：S10将电力系统模型中的发电机节点视为供电节点,将消费节点视为用电节点,节点间电传输线路视为节点间的连线,由此抽象成电力网络；将电力系统通信网络中的通信节点按邻居节点数从小到大排序生成通信网络,由电力网络、通信网络和节点的供电或用电属性创建级联失效模型；S20初始化级联失效模型中所有节点的物理信息,对电力网络进行级联失效攻击,在保证用电节点获取常规输入功率下,动态调节每个供电节点的实际输出功率,在动态调节中获取每个供电节点在每个稳态下的输出功率；S30根据每个供电节点在每个稳态下的输出功率,使用直流潮流模型计算电力网络每条线路对应的功率值,获取电力网络在每个稳态下的网络负载和；假设电力网络中共有节点数为n,选取任一节点d<Sub>0</Sub>,若节点d<Sub>0</Sub>为被攻击节点,计算任一线路均不存在负荷超载时的网络负载失效比例ΔP(d<Sub>0</Sub>),视ΔP(d<Sub>0</Sub>)为节点d<Sub>0</Sub>的影响力NI(d<Sub>0</Sub>)；若节点d<Sub>0</Sub>为非攻击节点,则根据其他节点d被攻击时,节点d<Sub>0</Sub>状态情况计算节点d<Sub>0</Sub>的存活比例,其视为节点d<Sub>0</Sub>的脆弱性；对节点d<Sub>0</Sub>的影响力NI(d<Sub>0</Sub>)和节点d<Sub>0</Sub>的脆弱性NV(d<Sub>0</Sub>)加权求和获得节点d<Sub>0</Sub>的重要性,将n个节点d<Sub>0</Sub>按其重要性从小到大排序生成新的电力网络；S40将新的电力网络与通信网络进行异配耦合DIS,生成新的电力信息物理融合系统。</td>   <td>G06Q50/06;H04L12/24</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘芳;              马东辉;              黎燊;                   梁家越       </td>   <td>中山大学</td>   <td>一种基于边缘计算的自适应城市寻人方法</td>   <td>广东省</td>   <td>CN109919033A</td>   <td>2019-06-21</td>   <td>本发明公开了一种基于边缘计算的自适应城市寻人方法,所述方法包括以下步骤：将监控视频转化为视频帧,选择上传的视频帧,过滤冗余视频帧,寻找待上传视频帧,并将寻找到的视频帧上传至边缘服务器；对上传的视频帧行人进行行人检测,将检测到的一定比例的行人进行行人重识别,输出行人的特征；边缘服务器定时将识别到的和寻找目标相似度最高的K个行人上传至云服务器；利用云服务器上的行人重识别模块对剩余比例的行人进行重识别,云服务器和边缘服务器的识别结果结合,并定时将识别到的与寻找目标相似度最高的K个行人返回终端处理。本发明克服了集中式计算量大、能耗高、响应时间长的缺陷,通过过滤冗余的视频帧降低计算量,同时降低了计算能耗和响应时间。</td>   <td>1.一种基于边缘计算的自适应城市寻人方法,其特征在于,所述方法包括以下步骤：S1：通过边缘节点将监控视频转化为视频帧,选定开始上传至边缘服务器的视频帧为第f<Sub>o</Sub>帧,设置视频帧之间的相似度阈值,将第f<Sub>o</Sub>帧之后的视频帧分别与第f<Sub>o</Sub>帧计算相似度,寻找第一个相似度小于相似度阈值的视频帧,并将寻找到的视频帧放入待上传队列Trigger_frame中并按顺序将视频帧上传至边缘服务器,同时将寻找到的视频帧作为上一次上传视频帧,继续寻找当前待上传视频帧；S2：将上传至边缘服务器的视频帧输入行人检测模块进行行人检测,并输出检测到的行人,将检测到的行人中占比为n的行人输入至边缘服务器的行人重识别模块进行行人重识别,并输出行人的特征；同时将检测到的行人中占比为m的行人上传至云服务器；边缘服务器和云服务器的重识别任务的比例值按照两者之间的带宽来分配；边缘服务器根据自身剩余资源和行人重识别模块所占资源自动调整行人重识别模块的数目；边缘服务器定时将识别到的和寻找目标相似度最高的K个行人上传至云服务器；S3：利用云服务器上的行人重识别模块对剩余占比为m的行人进行重识别,云服务器根据自身剩余资源和行人重识别模块所占资源自动调整行人重识别模块的数目；云服务器的识别结果和边缘服务器的识别结果结合,并定时将识别到的与寻找目标相似度最高的K个行人返回终端处理。</td>   <td>G06K9/00;H04L29/08;H04N7/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              张培熙;                   谢晓华       </td>   <td>中山大学</td>   <td>一种具有光照鲁棒性的行人再识别方法</td>   <td>广东省</td>   <td>CN109919073A</td>   <td>2019-06-21</td>   <td>本发明公开了一种具有光照鲁棒性的行人再识别方法,其包括以下步骤：获取具有光照差异的行人再识别数据集,从中选取训练样本,将训练样本划分为正常光照图片和昏暗光照图片；初始化一个深度卷积神经网络netA,用正常光照图片对其进行训练；初始化一个与netA结构完全相同的深度卷积神经网络netB,用昏暗光照图片对其进行训练；合并两种光照的数据集,同时对netA和netB进行协同训练,使两个网络均收敛；测试netA和netB各自的性能,选择性能较高的网络作为最终模型。本发明在不增加最终模型参数的情况,通过双网络的协同学习,增强行人再识别网络对昏暗图片的识别能力,提高模型对于光照的鲁棒性。</td>   <td>1.一种具有光照鲁棒性的行人再识别方法,其特征在于,包括步骤：步骤S1：获取具有光照差异的行人再识别数据集,从中选取训练样本,将训练样本划分为正常光照图片和昏暗光照图片；步骤S2：初始化一个深度卷积神经网络netA,用正常光照图片对其进行训练；初始化一个与netA结构完全相同的深度卷积神经网络netB,用昏暗光照图片对其进行训练；步骤S3：合并两种光照的数据集,同时对netA和netB进行协同训练,使两个网络均收敛；步骤S4：测试netA和netB各自的性能,选择性能较高的网络作为最终模型。</td>   <td>G06K9/00;G06K9/20;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              谢晓华;                   雷云       </td>   <td>中山大学</td>   <td>基于光场相机的人脸活体检测方法</td>   <td>广东省</td>   <td>CN105975926B</td>   <td>2019-06-21</td>   <td>本发明公开了一种基于光场相机的人脸活体检测方法,包括：(1)光场相机原始数据的提取与解码；(2)从光场数据中提取面部不同深度的重对焦图像；(3)从重对焦图像中提取聚焦特征和功率谱特征,连接两种特征作为该人脸图像的综合特征；(4)对采集到的所有光场图像提取上述综合特征,将训练集的综合特征输入支持向量机训练,得到可用于人脸活体检测的模型；然后将待检测的图像的综合特征输入训练好的模型,得到检测结果。本发明利用光场相机一次拍照后可获得不同深度重对焦图像的特点,从中提取人脸重对焦变化特征来进行人脸活体检测,对比传统的人脸活体检测方法,本发明检测结果准确,检测效率更高,更为简单便利,符合实际应用的要求。</td>   <td>1.一种基于光场相机的人脸活体检测方法,其特征在于,包括下述步骤：(1)光场相机原始数据的提取与解码,通过微透镜中心标定、Bayer格式解码、六边形-正交格点校正的步骤从Lytro光场相机中提取原始光场数据并进行色彩校正；(2)通过数字重对焦,从光场数据中提取出两张对焦在面部不同深度的重对焦图像；(3)综合两张不同深度的重对焦图像,分别提取聚焦特征和功率谱特征,将两种特征连接在一起作为人脸图像的综合特征；(4)对采集到的所有光场图像提取上述综合特征,将训练集的综合特征输入支持向量机训练,支持向量机模型选择径向基函数,得到可用于人脸活体检测的模型,然后将待检测的图像的综合特征输入训练好的模型,得到检测结果；所述步骤(3)中,提取聚焦特征的具体方法为：(3-1-1)采用Nayar提出的方法从两幅重对焦图像中提取改进的拉普拉斯能量和特征,记为SML<Sub>1</Sub>和SML<Sub>2</Sub>；(3-1-2)计算差异变化值矩阵DIF＝SML<Sub>1</Sub>-SML<Sub>2</Sub>；(3-1-3)把矩阵DIF按列相加得到SumDIF；(3-1-4)通过二次方程式y＝ax<Sup>2</Sup>+bx+c曲线拟合SumDIF；(3-1-5)计算拟合的曲线y＝ax<Sup>2</Sup>+bx+c的系数A＝[a b c]<Sup>T</Sup>即为聚焦特征F<Sub>R</Sub>。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王美琴;              虞志益;                   黄立文       </td>   <td>中山大学</td>   <td>一种行人重识别方法、系统、装置和存储介质</td>   <td>广东省</td>   <td>CN109902662A</td>   <td>2019-06-18</td>   <td>本发明公开了一种行人重识别方法、系统、装置和存储介质,其中方法包括以下步骤：将无标记的待检测数据集输入预设的特征提取模型,并提取待检测数据集的特征空间；对特征空间进行降维处理后,获得特征空间的稀疏表示；采用预设的聚类公式对稀疏表示进行聚类处理后,获得带标记的聚类结果；对聚类结果进行选择后,获得筛选后的分类结果；将分类结果输入预设的卷积神经网络进行训练优化；重复以上步骤,直到分类结果收敛,并获得行人重识别结果。本发明提出了一个易于实施的无监督学习的深度学习框架,把自步学习嵌入到无监督学习的过程中,实现了把无监督方法整合到深度学习框架当中,且该框架结构容易实施,可广泛应用于计算机视觉技术领域。</td>   <td>1.一种行人重识别方法,其特征在于,包括以下步骤：S1、将无标记的待检测数据集输入预设的特征提取模型,并提取待检测数据集的特征空间；S2、对特征空间进行降维处理后,获得特征空间的稀疏表示；S3、采用预设的聚类公式对稀疏表示进行聚类处理后,获得带标记的聚类结果；S4、对聚类结果进行选择后,获得筛选后的分类结果；S5、将分类结果输入预设的卷积神经网络进行训练优化；S6、重复步骤S1至S5,直到分类结果收敛,并获得行人重识别结果。</td>   <td>G06K9/00;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         江泓谕;                   饶洋辉       </td>   <td>中山大学</td>   <td>一种基于主题鉴别权重和采样式重构的并行化主题模型</td>   <td>广东省</td>   <td>CN109885839A</td>   <td>2019-06-14</td>   <td>本发明涉及人工智能的自然语言处理领域以及数据挖掘领域,更具体地,涉及一种基于主题鉴别权重和采样式重构的并行化主题模型。包括以下步骤：S1.将预处理后的输入文档输入LDA(潜在狄利克雷分配,Latent Dirichlet Allocation)中,推断出主题分布；S2.通过当前每个词的主题分布,计算每个词的主题鉴别权重；S3.采样式重构,继续Gibbs采样。本发明基于TW(Term-Weighting)LDA模型的赋权思想,建立了新的模型SR(Sampling Reconstructed)LDA,针对LDA模型进行文本建模和主题提取时受主题混淆词影响较大的问题,通过计算词汇表中各个词的主题鉴别权重并重构模型参数,降低了主题混淆词的影响。本文模型对所作的矩阵采样式重构和并行加速优化,能得到比TWLDA更好、更稳定的优化效果,并降低了优化所需的时间成本。</td>   <td>1.一种基于主题鉴别权重和采样式重构的并行化主题模型,其特征在于,包括以下步骤：S1.将预处理后的输入文档输入LDA中,推断出主题分布；S2.通过当前每个词的主题分布,计算每个词的主题鉴别权重,计算公式如下：          <Image id="icf0001" he="107" wi="700" file="FDA0001984903350000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,BH(t)表示词语t的熵,K表示主题的总数；<Image id="icf0002" he="129" wi="410" file="FDA0001984903350000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>f(k<Sub>i</Sub>)表示主题k<Sub>i</Sub>中的总词数,f(t,k<Sub>i</Sub>)表示t在主题k<Sub>i</Sub>中的出现次数；S3.采样式重构,继续Gibbs采样：S31.得到每个词的主题鉴别权重之后,我们开始着手对文档-主题矩阵(下称Nmk)和主题-词矩阵(下称Nkt)进行重构；对Nmk的重构公式如下：          <Image id="icf0003" he="214" wi="650" file="FDA0001984903350000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中σ<Sub>t</Sub>表示词t的权重,n<Sub>mkt</Sub>表示在第m篇文档中,属于主题k的词t有多少个；如果t不在第m篇文档中,或不属于主题k,n<Sub>mkt</Sub>均为0；对Nkt的重构也是通过二项式采样<Image id="icf0004" he="87" wi="257" file="FDA0001984903350000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>公式如下：          <Image id="icf0005" he="82" wi="637" file="FDA0001984903350000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        S4.继续进行S3步骤中的Gibbs采样,直到采样次数足够再一次达到收敛为止。</td>   <td>G06F17/27;G06F16/31</td>  </tr>        <tr>   <td>中国专利</td>   <td>         袁媛;              陈逸敏;              牛通;              刘菁;                   尤智扬       </td>   <td>中山大学</td>   <td>城市贫困分布测度方法</td>   <td>广东省</td>   <td>CN109886103A</td>   <td>2019-06-14</td>   <td>本发明公开了一种基于多源数据的城市贫困分布测度方法,其包括：确定城市内部目标区域的物理环境分布特征对应的指标、社会服务水平分布特征对应的指标、经济生活水平分布特征对应的指标；确定目标区域的综合贫困指数GDI；将物理环境分布特征对应的指标、社会服务水平分布特征对应的指标和经济生活水平分布特征对应的指标作为自变量,GDI作为参考变量,使用随机森林算法的机器学习构建预测模型,实现对目标区域贫困分布的测度。与现有技术相比,本发明使用随机森林机器学习的方法计算区域贫困分布测度,能够根据社会经济的实际变动情况进行自我调整,提高了对贫困分布变动情况进行实时测度的准确度。</td>   <td>1.一种基于多源数据的城市贫困分布测度方法,其特征在于：所述方法包括以下步骤：使用多源开放平台获取数据；根据所获取的数据确定目标区域的物理环境分布特征对应的指标、社会服务水平分布特征对应的指标、经济生活水平分布特征对应的指标；根据所获取的数据确定目标区域的综合贫困指数GDI；将物理环境分布特征对应的指标、社会服务水平分布特征对应的指标和经济生活水平分布特征对应的指标作为自变量,GDI作为参考变量,使用随机森林算法的机器学习构建预测模型,实现对目标区域贫困分布的测度。</td>   <td>G06K9/00;G06K9/32;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马争鸣;              张扬;              庄日新;                   刘洁       </td>   <td>中山大学</td>   <td>一种基于对称矩阵空间子空间学习的格拉斯曼流形域自适应方法</td>   <td>广东省</td>   <td>CN109886419A</td>   <td>2019-06-14</td>   <td>本发明涉及机器学习领域的域自适应技术,提出了是一种基于对称矩阵空间子空间学习的格拉斯曼流形域自适应方法。为了减少源域和目标域数据概率分布的差异性,本方法首先建立从格拉斯曼流形到对称矩阵空间的映射,之后将源域和目标域的格拉斯曼流形矩阵数据映射到对称矩阵空间,并在此对称矩阵空间构造子空间。对原始数据在此子空间的投影利用均值相近准则,建立子空间学习的目标函数,并进行目标函数的优化求解获得目标子空间,在目标子空间上实现了原始数据在该目标子空间的投影概率分布相匹配,即原始数据经过从格拉斯曼流形到对称矩阵空间,再从对称矩阵空间到其子空间的两次变换,实现了域自适应。</td>   <td>1.一种基于对称矩阵空间子空间学习的格拉斯曼流形域自适应方法,其主要特征在于：A.源域输入数据<Image id="icf0001" he="91" wi="266" file="FDA0001954614060000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和目标域输入数据<Image id="icf0002" he="93" wi="263" file="FDA0001954614060000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>均为格拉斯曼流形的数据,即每个数据都是D×d维列向量标准正交的矩阵；B.通过映射<Image id="icf0003" he="55" wi="60" file="FDA0001954614060000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>G(D,d)→S<Sub>D</Sub>将格拉斯曼流形数据映射到对称矩阵空间,即对任意的X∈G(D,d),<Image id="icf0004" he="76" wi="396" file="FDA0001954614060000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>S<Sub>D</Sub>表示对称矩阵空间,对称矩阵空间的元素为D×D维对称矩阵；C.利用源域和目标域数据在对称矩阵空间上的映射<Image id="icf0005" he="85" wi="526" file="FDA0001954614060000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>N＝1,...,N<Sub>s</Sub>+N<Sub>t</Sub>的线性组合构造r维子空间S<Sub>r</Sub>,子空间的标准正交基表示为<Image id="icf0006" he="141" wi="381" file="FDA0001954614060000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>i＝1,…,r,子空间由组合系数矩阵W决定,组合系数满足<Image id="icf0007" he="224" wi="616" file="FDA0001954614060000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中I<Sub>r</Sub>为单位矩阵；D.源域和目标域数据经由对称矩阵空间之后均可以投影到S<Sub>r</Sub>,利用源域和目标域数据在此子空间的投影均值相近的原则学习子空间,目标函数为<Image id="icf0008" he="169" wi="560" file="FDA0001954614060000018.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中<Image id="icf0009" he="70" wi="56" file="FDA0001954614060000019.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>与<Image id="icf0010" he="73" wi="60" file="FDA00019546140600000110.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>分别为源域与目标域数据在S<Sub>r</Sub>上的投影坐标。</td>   <td>G06N20/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         招继恩;              朱勇杰;              王国良;              张海;              谭大伦;                   周明       </td>   <td>广州智慧城市发展研究院;中山大学</td>   <td>一种基于注意力机制的行为识别系统</td>   <td>广东省</td>   <td>CN109871777A</td>   <td>2019-06-11</td>   <td>本发明公开了一种基于注意力机制的行为识别系统,由输入、中间Block、输出构成；所述系统整个网络结构基于Inception V3,选择在其中一个Block加入提出的两个Attention Module；其中使用Channel Attention模块来提取通道间依赖,通过使用Spatial Attention来获取空间的依赖。本发明为了克服错误标签和背景信息的影响。使用残差学习将通道注意力和空间注意力结合起来。并使用自我注意作为网络的一部分来获取更长期的时间信息。在模型中,利用了空间和通道的注意力,并且在模块设计中只使用二维通道的注意力。</td>   <td>1.一种基于注意力机制的行为识别系统,其特征在于,由输入、中间Block、输出构成；输入端为RGB图像,其中的Attention Module能任意嵌入在其中一个Block；注意力模块主要分为通道间注意力模块Channel Attention和空间注意力模块；SpatialAttention；视频中的一帧图像输入后,经过前馈运算后,卷积神经网络输出对应行为的类别；所述系统整个网络结构基于Inception V3,选择在其中一个Block加入两个注意力模块Attention Module；其中使用Channel Attention模块提取通道间依赖,通过使用Spatial Attention获取空间的依赖；整体系统通过输入的视频数据切分为图像数据后进行行为识别。</td>   <td>G06K9/00;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马争鸣;              陈映宏;              何健信;                   刘洁       </td>   <td>中山大学</td>   <td>一种多局部线性预测的数据降维方法</td>   <td>广东省</td>   <td>CN109871878A</td>   <td>2019-06-11</td>   <td>本发明涉及机器学习中数据降维领域,是一种流形学习数据降维算法。本发明提出一种基于多局部线性预测的数据降维算法。通过在高维空间根据欧式距离把流形分成一个个局部,由于每个样本点属于多个局部,因此可以在不同的局部对同一个点进行特征保持。局部中的每一个点都可以通过其余点进行拟合系数逼近。该系数就是本发明要保持的特征。局部线性预测,局部同胚预测,局部过零点线性预测都是从不同的角度进行拟合,得到不同的系数预测。同样在低维空间也有不同的角度寻找目标点的坐标和该点不同的预测值,有均值最小化和方差最小化的拟合方式。通过在低维空间选择适合的方式拟合预测点和目标点,得到数据在低维空间的坐标。</td>   <td>1.一种多局部线性预测的线性预测方法,其特征在于该方法的步骤如下：A.计算样本点各样本之间的距离矩阵D,把样本数据集X按照欧氏距离分成一个个局部,记为X<Sub>1</Sub>,L,X<Sub>M</Sub>。<Image id="icf0001" he="142" wi="255" file="FDA0001954610790000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>局部之间存在交集,因此每一个样本点属于不同的局部,多局部的思想即为一个样本点属于不同的局部,每个局部都可以保持该点的特征；B.对每一个局部中的每一个点都计算其余点对它的预测系数w<Sub>i,g</Sub>；C.利用在高维空间计算得到的来自不同局部的预测系数w<Sub>i,g</Sub>重构该点在低维空间的坐标。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘洁;              王鑫;              陈皓青;                   马争鸣       </td>   <td>中山大学</td>   <td>基于逐点扩充以及局部仿射变换的流形学习算法</td>   <td>广东省</td>   <td>CN109871879A</td>   <td>2019-06-11</td>   <td>本发明涉及机器学习领域中的流形学习,基于逐点扩充以及局部仿射变换的流形学习算法。该算法通过选取整个数据集的测地中心的邻域作为一个初始扩充块,用线性算法求出该扩充块的低维表示,迭代的选取与该扩充块最近的数据点加入扩充块,与此同时,局部仿射变换变换用来保持新数据点与扩充块之间的局部几何信息。用这种方式能够提高流形学习算法的执行效率,并且很自然的解决了流形学习算法的一个经常被人批判的out-of-sample问题。</td>   <td>1.一种基于逐点扩充以及局部仿射变换的流形学习算法,其主要特征在于：A.构造输入数据集X的邻域图G,计算出数据集的测地中心x<Sub>c</Sub>；B.取测地中心的邻域构成初始扩充块,记作X<Sub>c</Sub>；C.计算扩充块之外的所有数据点到扩充块的测地距离；D.选取一个与扩充块的测地距离最小的数据点加入扩充块,同时计算出该数据点的低维表示。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马争鸣;              黎伟浚;              吴荟彬;              张国凯;                   刘洁       </td>   <td>中山大学</td>   <td>基于低秩稀疏矩阵分解、局部几何结构保持和类别信息最大统计相关的特征提取方法</td>   <td>广东省</td>   <td>CN109871880A</td>   <td>2019-06-11</td>   <td>本发明涉及模式识别中特征提取相关问题,提出了一种基于低秩稀疏矩阵分解、局部几何结构保持和类别信息最大统计相关的特征提取方法。为了解决现有许多特征提取方法存在的问题,包括鲁棒性不足、没有利用数据的局部几何结构信息、没有利用数据的类别信息,本发明通过对一个总的目标函数的优化求解,实现对数据样本的特征提取,进而可以更有效率地进行下一步的数据处理。这个目标函数由三部分组成,包括低秩稀疏矩阵分解、流形学习以及有监督学习。本发明兼顾这三部分的特点,使特征提取尽可能地达到全局最优,提取出重要的、有判别力的特征,去除冗余信息。</td>   <td>1.一种基于低秩稀疏矩阵分解、局部几何结构保持和类别信息最大统计相关的特征提取方法,其特征在于：A.对输入数据作低秩稀疏矩阵分解,分解为去噪数据矩阵和噪声矩阵,且要求去噪数据矩阵尽可能低秩、噪声矩阵尽可能稀疏,作用是数据清洗、提高鲁棒性；X是数据样本的特征空间,X＝{x<Sub>i</Sub>|x<Sub>i</Sub>∈R<Sup>D</Sup>,i＝1,L,N}是N个样本的训练数据集,<Image id="icf0001" he="63" wi="175" file="FDA0001954684480000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>低秩稀疏矩阵分解的目标函数为：          <Image id="icf0002" he="91" wi="306" file="FDA0001954684480000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        s.t.X＝Z+E其中,参数λ≥0,<Image id="icf0003" he="86" wi="473" file="FDA0001954684480000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>分别是清洗后的数据矩阵、噪声矩阵；||·||<Sub>*</Sub>是指矩阵的核范数,即矩阵奇异值的和；||·||<Sub>1</Sub>是指矩阵的L1范数,即矩阵所有元素的绝对值的和；B.令提取的特征保持数据的局部几何结构信息,作用是揭示数据潜在的拓扑结构,挖掘出蕴含在低维流形中的重要信息；Y＝{y<Sub>n</Sub>|y<Sub>n</Sub>∈R<Sup>d</Sup>,n＝1,L,N}是N个样本的提取的特征,d＝D；保持数据的局部几何结构的目标函数为：          <Image id="icf0004" he="98" wi="299" file="FDA0001954684480000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        s.t.YY<Sup>T</Sup>＝I<Sup>d</Sup>其中,tr(·)是矩阵的迹,L＝[L<Sub>1</Sub> L L<Sub>N</Sub>],<Image id="icf0005" he="101" wi="567" file="FDA0001954684480000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>S<Sub>n</Sub>是选择矩阵,选择出yn的近邻,使得Y<Sub>n</Sub>＝YS<Sub>n</Sub>；I<Sub>K+1</Sub>是K+1维单位矩阵,Θ<Sub>n</Sub>是清洗后的样本数据Z的局部坐标,<Image id="icf0006" he="81" wi="71" file="FDA0001954684480000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是其伪逆；C.令提取的特征与类别信息保持最大统计相关性,作用是利用类别信息,提高提取的特征的判别性；可以采用希尔伯特-施密特独立性准则(HSIC)衡量提取的特征与类别信息的统计相关性；C是样本的类别空间,C是与X对应的类别集合矩阵,<Image id="icf0007" he="78" wi="700" file="FDA0001954684480000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>c是样本的类别总数；令提取的特征与类别信息最大统计相关的目标函数为：          <Image id="icf0008" he="105" wi="700" file="FDA0001954684480000018.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,<Image id="icf0009" he="136" wi="418" file="FDA0001954684480000019.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是中心化矩阵,<Image id="icf0010" he="234" wi="315" file="FDA00019546844800000110.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是N维的全1向量,<Image id="icf0011" he="114" wi="700" file="FDA00019546844800000111.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>k<Sub>C</Sub>和k<Sub>Y</Sub>是两个核函数；D.基于上述三点,总的目标函数如下：          <Image id="icf0012" he="74" wi="700" file="FDA0001954684480000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        s.t.X＝Z+E,Z≥0其中,参数α,β,λ≥0；最终目的是根据训练集X和C,对目标函数优化求解,求得提取的特征Y<Sup>*</Sup>,用于分类任务,如kNN方法,根据分类任务的准确率迭代地调整参数等设置；然后利用这样的目标函数的模型,去掉有监督学习的部分,可以用于预测集数据的特征提取。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赵铜铁钢;                   黄泽青       </td>   <td>中山大学</td>   <td>一种基于log-sinh变换的区域月降水统计频率分析方法</td>   <td>广东省</td>   <td>CN109871584A</td>   <td>2019-06-11</td>   <td>本发明公开了一种基于log-sinh变换的区域月降水统计频率分析方法,通过对原始降水数据进行log-sinh变换得到符合正态分布的降水数据,并基于变换后的数据对降水数据进行平均降水分析、正态分布诊断、生成基础气象学预报和重现期分析。本发明解决了由于原始降水数据偏态分布给区域月降水统计频率分析带来的困难,为快速进行水文气象分析提供了基础。</td>   <td>1.一种基于log-sinh变换的区域月降水统计频率分析方法,其特征在于,包括以下步骤：S1.基于Python平台,使用Python中第三方库Pandas和Numpy中的函数读取目标区域的原始降水数据,并将其存储为浮点型数组；S2.对原始降水数据进行log-sinh变换得到符合正态分布的降水数据,并将所述变换的过程基于C++平台创建变换函数,形成CPP文件和PYD文件；S3.在Python平台上调用所述变换函数,基于所述变换的过程,创建对原始降水数据进行降水分析的函数；使用Matplotlib对降水分析的结果进行可视化,并将所述降水分析以及可视化过程的函数封装成类文件；其中降水分析包括多年平均降水分析、正态分布诊断、生成基础气象学预报和重现期分析；S4.基于Python平台,使用Sys模块中的append函数添加所述类文件所在的文件夹并使用import语句调用该类,然后使用该类处理所述步骤S1中的浮点型数组,得到类对象；再调用类中所述降水分析以及可视化的函数进行分析和可视化操作,得到数据结果和图片结果。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         褚燕燕;              金奥翔;              谭晓军;              梁栋;                   高殿策       </td>   <td>中山大学</td>   <td>一种基于多源信息的智能火灾发展态势识别方法</td>   <td>广东省</td>   <td>CN109871984A</td>   <td>2019-06-11</td>   <td>本发明公开了一种基于多源信息的智能火灾发展态势识别方法,包括步骤：1)拍摄火灾现场的燃烧视频,同时应用温度传感器、CO气体探测器、O<Sub>2</Sub>气体探测器以及亮度测量仪,对探测点的信息进行采集,并通过二总线方式传递至总控制器；2)对每个探测点采集的四种信息进行汇总,然后对探测点位置采集的四种信息进行降噪处理；3)对每个探测点汇总降噪后的信息进行二阶聚类分析,分预聚类和正式聚类两步处理,将获取的信息分簇；4)运用产生的聚类分析结果对火焰热功率等级进行综合评估,并通过用户交互式工具实现数据可视化。本发明能够有效实现了对火灾现场的实时监测、评估,为快速高效的开展救援工作提供了技术指导,避免不必要的人员伤亡以及财产损失。</td>   <td>1.一种基于多源信息的智能火灾发展态势识别方法,其特征在于,该方法能够动态、实时地对火灾现场多源信息进行融合分析,从而实现火灾态势的智能判断,包括以下步骤：1)拍摄火灾现场的燃烧视频,同时应用温度传感器、CO气体探测器、O<Sub>2</Sub>气体探测器以及亮度测量仪,对探测点的信息进行采集,包括温度、CO浓度、O<Sub>2</Sub>浓度以及火焰亮度,并通过二总线方式传递至总控制器；2)对步骤1)中每个探测点采集的四种信息进行汇总,然后对探测点位置采集的四种信息进行降噪处理；3)对步骤2)中每个探测点汇总降噪后的信息进行二阶聚类分析,分预聚类和正式聚类两步处理,将获取的信息分簇；4)运用步骤3)中产生的聚类分析结果对火焰热功率等级进行综合评估,并通过用户交互式工具实现数据可视化。</td>   <td>G06Q10/04;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         蔡庆玲;              裴海军;              何鸿奇;              梁伟霞;                   周毅       </td>   <td>中山大学</td>   <td>一种基于深度卷积生成式对抗网络的甲状腺结节病灶区生成的数据增强方法</td>   <td>广东省</td>   <td>CN109872296A</td>   <td>2019-06-11</td>   <td>本发明公开了一种基于深度卷积生成式对抗网络的甲状腺结节病灶区生成的数据增强方法,该方法将真实病人的甲状腺超声图像按照良性结节和恶性结节进行分类,然后利用深度卷积生成式对抗网络生成病灶区图像,从中挑出生成比较接近真实病灶的图像,与正常人的甲状腺图像相融合,以达到数据增强的目的,这种病灶区生成图像融合的方法提高了增强数据的质量和多样性,使生成的图像最大程度接近真实图像,更具有可信性。</td>   <td>1.一种基于深度卷积生成式对抗网络的甲状腺结节病灶区生成的数据增强方法,其特征是,将真实病人的甲状腺超声图像按照良性结节和恶性结节进行分类,然后利用深度卷积生成式对抗网络生成病灶区图像,从中挑选出生成比较接近真实病灶区的图像,与正常人的甲状腺图像相融合,最终生成病灶在甲状腺不同部位的甲状腺超声图像,具体包括以下步骤：S1：在用XML文件标注出结节的位置和类型之后,将图像上影像科医生对结节位置标注的标签去除；S2：按照结节的类型,把病灶区截图出来然后存放于不同的文件夹；S3：统一所有图像的大小并且用传统的数据增强方法增加图片数量；S4：用深度卷积生成式对抗网络训练生成病灶区图像；S5：挑选生成的病灶区图像；S6：用挑选生成的病灶区图像与正常图像进行融合,最终生成病灶在甲状腺不同部位的甲状腺超声图像。</td>   <td>G06T5/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              黄登;              朱雄泳;              陈荣军;                   李智文       </td>   <td>中山大学;广东顺德中山大学卡内基梅隆大学国际联合研究院</td>   <td>一种图像增强方法</td>   <td>广东省</td>   <td>CN106530237B</td>   <td>2019-06-07</td>   <td>本发明涉及图像处理领域,更具体地,涉及一种图像增强方法。其具体步骤包括：a)对输入图像进行去噪处理得到去噪图像；b)对去噪图像进行边缘提取得到边缘图像；c)对边缘图像进行图像增强处理得到去噪且边缘增强的图像；d)利用亮度可控的直方图均衡方法对去噪图像进行处理得到全局增强图像；e)对c和d步骤所得到的图像进行线性叠加,得到最终的输出图像。本发明通过亮度可控的直方图均衡方法结合UM(Unsharp Masking,反锐化掩膜)算法思想,可以实现输出亮度可以跟进用户需求自动调节,而且通过设定合适的亮度值可以得到一幅对比度明显提升全局增强的输出图像,从而达到图像增强的目的。</td>   <td>1.一种图像增强方法,其特征在于,包括如下步骤：a)对输入图像进行去噪处理得到去噪图像；b)对去噪图像进行边缘提取得到边缘图像；c)对边缘图像进行图像增强处理得到去噪且边缘增强的图像；d)利用亮度可控的直方图均衡方法对去噪图像进行处理得到全局增强图像；e)对c和d步骤所得到的图像进行线性叠加,得到最终的输出图像；步骤d的具体步骤包括：d1)求出去噪图像F(x,y)的直方图,并求得其亮度最大值f<Sub>max</Sub>和最小值f<Sub>min,</Sub>其中,0≤[f<Sub>min</Sub>,f<Sub>max</Sub>]≤255以及平均亮度值μ和标准差σ；d2)根据平均亮度值μ和标准差σ,求出去噪图像F(x,y)直方图的分割点th<Sub>1</Sub>和th<Sub>2</Sub>为          <Image id="icf0001" he="159" wi="382" file="FDA0001994145000000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,w为权值,0≤[th<Sub>1</Sub>,th<Sub>2</Sub>]≤255；d3)根据f<Sub>min</Sub>、f<Sub>max</Sub>、th<Sub>1</Sub>、th<Sub>2</Sub>将去噪图像F(x,y)的直方图分成低、中、高三段,如下          <Image id="icf0002" he="249" wi="638" file="FDA0001994145000000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,h(i)为图像F(x,y)的直方图统计函数,i表示0到255的灰度值；d4)对直方图进行分段剪切与补偿,得到经过裁剪和补偿后的直方图；d5)对图像F(x,y)的直方图进行分段剪切与补偿后,各段直方图的像素总数所占图像F(x,y)的总像素的比例均未变化,即          <Image id="icf0003" he="535" wi="544" file="FDA0001994145000000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,toal代表图像F(x,y)的总像素；r<Sub>1</Sub>、r<Sub>2</Sub>、r<Sub>3</Sub>分别表示去噪图像F(x,y)直方图的低、中、高区域的像素总数占图像F(x,y)的像素总数的比例；d6)计算直方图分段后的低、中、高区域的累积密度函数分别为          <Image id="icf0004" he="536" wi="606" file="FDA0001994145000000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,S<Sub>l</Sub>、S<Sub>m</Sub>、S<Sub>u</Sub>分别为低中高直方图区域内的像素总数,h<Sub>l</Sub>″、h<Sub>m</Sub>″、h<Sub>u</Sub>″为分段剪切与补偿后各个区域的直方图统计函数；d7)假设全局增强的输出图像G(x,y)的直方图的分割点分别为th<Sub>1</Sub>′和th<Sub>2</Sub>′,亮度平均值为μ<Sub>m</Sub>,标准差为σ<Sub>m</Sub>,根据输出图像直方图平均亮度估算模型和输出图像直方图标准差估算模型可分别估算出亮度平均值μ<Sub>m</Sub>标准差σ<Sub>m</Sub>,即μ<Sub>m</Sub>＝0.5[r<Sub>1</Sub>(th<Sub>1</Sub>′-1)+r<Sub>2</Sub>(th<Sub>1</Sub>′+th<Sub>2</Sub>′-1)+r<Sub>3</Sub>(th<Sub>2</Sub>′+255)]          <Image id="icf0005" he="74" wi="700" file="FDA0001994145000000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,th<Sub>2</Sub>′＝th<Sub>1</Sub>′+2σ<Sub>m</Sub>；d8)令模型求出的亮度平均值和设定的亮度平均值mv相等,即σ<Sub>m</Sub>＝mv,其中,mv是用户可自行设定的平均亮度值；d7步骤中的三个方程组是关于th<Sub>1</Sub><Sup>1</Sup>、th<Sub>2</Sub>′和σ<Sub>m</Sub>三个未知数的方程组,通过迭代的方式,计算出输出图像的直方图灰度分割点th<Sub>1</Sub>′和th<Sub>2</Sub>′；d9)定义输出图像的动态范围[0,255]的图像映射曲线函数T为：          <Image id="icf0006" he="133" wi="700" file="FDA0001994145000000031.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,th<Sub>1</Sub>和th<Sub>2</Sub>为自定义的去噪图像F(x,y)的直方图的两个分割点,th<Sub>1</Sub>′和th<Sub>2</Sub>′为输出图像直方图平均亮度估算模型和标准差估算模型计算出来的输出图像的直方图灰度级分割点；d9)根据以上可得,从而求出全局增强的输出图像G(x,y)为G(x,y)＝T(F(x,y))；步骤c的具体步骤包括：c1)对边缘图像进行预处理,得到边缘预处理图像,对边缘预处理图像进行图像增强得到边缘增强图像；c2)对边缘图像进行二值化处理得到二值化图像,对二值化图像进行腐蚀处理得到腐蚀图像；c3)将c1的边缘增强图像和c2的腐蚀图像进行综合处理得到去噪且边缘增强的图像；步骤c1的具体步骤包括：c11)求输入图像I(x,y)的最大和最小灰度值分别为I<Sub>max</Sub>和I<Sub>min</Sub>；c12)对边缘图像进行预处理,得到边缘预处理图像E<Sub>w</Sub>(x,y),即          <Image id="icf0007" he="223" wi="700" file="FDA0001994145000000041.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,E(x,y)为边缘图像,I(x,y)为输入图像；c 13)求出边缘预处理图像E<Sub>w</Sub>(x,y)的灰度最大值max、灰度最小值min以及亮度平均值μ<Sub>0</Sub>、标准差σ<Sub>0</Sub>；c14)求出边缘预处理图像的直方图,然后求出该直方图灰度值小于0的区域的阈值T<Sub>1</Sub>和直方图灰度值大于0的区域的阈值T<Sub>2</Sub>；c15)根据c13和c14中求得的灰度最大值max、灰度最小值min以及图像的阈值T<Sub>1</Sub>和T<Sub>2</Sub>,将边缘预处理图像E<Sub>w</Sub>(x,y)的直方图分成3个区域(min,T<Sub>1</Sub>)、(T<Sub>1</Sub>,T<Sub>2</Sub>)和(T<Sub>2</Sub>,max),然后对该直方图进行基于均值和标准差剪切的分段直方图均衡,得到边缘增强图像E<Sub>e</Sub>(x,y)；步骤c14中阈值是通过Rosin方法求取的；步骤c2和c3的具体步骤包括：c 21)根据Rosin方法求得边缘图像的阈值T<Sub>t</Sub>,然后运用二值化求得二值图像B(x,y)；c22)对二值图像进行形态学腐蚀得到腐蚀图像R(x,y)；c31)结合步骤c1和c2,得到去噪且边缘增强的图像E<Sub>we</Sub>(x,y)：          <Image id="icf0008" he="162" wi="700" file="FDA0001994145000000042.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image></td>   <td>G06T5/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑子彬;                   殷家康       </td>   <td>中山大学</td>   <td>一种基于word2vec的房源词向量训练方法及装置</td>   <td>广东省</td>   <td>CN109858024A</td>   <td>2019-06-07</td>   <td>本发明公开一种基于word2vec的房源词向量训练方法及装置,本装置用于实现本方法,本方法包括制定房源点击行为的训练数据结构；输入训练数据集,构建语料库,且由房源ID及其对应的城市ID生成二元组,统计二元组生成房源词典；在word2vec的skipgram模型中输入有下单标签的房源ID,以滑窗方式获取其正样本；在房源词典的二元组中,从与房源ID对应的同城ID和非同城ID中分别采样其负样本；将其正样本和其负样本一起作为训练样本,使用skipgram模型进行训练,输出房源ID对应的词向量。本发明通过城市ID以及房源ID的对应关系,在训练样本采样中保证了数据的差异性和类别的均衡性,得到了更加优质的词向量。</td>   <td>1.一种基于word2vec的房源词向量训练方法,其特征在于,包括以下步骤：S10将用户的房源点击行为数据集按房源ID分割成若干房源ID的点击序列,一房源ID的点击序列,由若干房源ID的点击序列生成训练数据集,每个训练数据的结构：第一列为当前点击行为的下单或未下单标签,其后列为同一房源ID按时间顺序排列的点击行为数据集,其中后列末尾为同一房源ID的当前点击行为数据；S20输入训练数据集,统计训练数据集生成语料库,在训练数据集中抽取房源ID及其对应城市的同城ID的点击序列生成若干房源ID二元组,统计若干房源ID二元组,生成房源词典；S30在word2vec的skipgram模型中输入有下单标签的房源ID,以滑窗方式获取其正样本；在房源词典的若干房源ID二元组中采样其同城ID的第一负样本；在房源词典中采样其非同城ID的第二负样本；S40将其正样本、第一负样本和第二负样本一起作为训练样本,使用skipgram模型进行训练,输出房源ID对应的词向量。</td>   <td>G06F17/27;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              欧阳柳;              吴卓亮;                   谢晓华       </td>   <td>中山大学</td>   <td>一种融合手工设计描述子和深度特征的人脸姿态估计方法</td>   <td>广东省</td>   <td>CN109858342A</td>   <td>2019-06-07</td>   <td>本发明公开了一种融合手工设计描述子和深度特征的人脸姿态估计方法,常应用于针对安全防范和人脸识别应用的人脸图像质量评测。该方法使用SIFT描述子提取人脸图像的轮廓和局部信息,使用DeepID深度网络提取人脸图像的表观和结构信息,主要包括以下步骤：训练一个提取深度特征的深度神经网络模型,输入一张检测得到的人脸图像,利用该深度网络模型提取所述人脸图像深度特征；提取具有尺度空间不变特性的SIFT特征向量,将该人脸图像的SIFT特征以及深度特征串联,输入到训练好的SVM分类器进行分类,确定该待分类的人脸的姿态类别。本发明能够有效地进行人脸姿态估计,提高姿态估计的准确率。</td>   <td>1.一种融合手工设计描述子和深度特征的人脸姿态估计方法,其特征在于,包括步骤：S1：对图像进行人脸检测；S2：对检测到的人脸图像进行滤波以去除噪声；S3：提取人脸图像的SIFT描述特征,作为手工设计描述子；提取描述人脸的深度特征；S4：对手工设计描述子和深度特征进行有效融合；S5：对融合特征进行训练,生成人脸姿态分类器；S6：利用人脸姿态分类器对图像或视频帧中的人脸姿态进行有效估计。</td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         招继恩;              朱勇杰;              王国良;              张海;              谭大伦;                   周明       </td>   <td>广州智慧城市发展研究院;中山大学</td>   <td>一种自下而上-自上而下的行为识别系统</td>   <td>广东省</td>   <td>CN109858419A</td>   <td>2019-06-07</td>   <td>本发明公开了一种自下而上-自上而下的行为识别系统,包括SBTA模块和STBTA模块；所述SBTA模块和STBTA模块通过自下而上自上而下机制和注意机制对局部特征和全局信息进行编码。本发明的模块可以直接在图像或场景中适当的区域捕获长程依赖；使用最大池和平均池来生成通道统计和空间网格统计；提高其对信息功能的敏感度并选择有用的信息,不仅可以选择聚焦位置,还可以增强该位置对象的不同表示；本发明提出的方法是前馈方式,可以作为一种有效,简单和可解释的方法直接插入到2D/3D CNN中；即使只有STBA和STBTA,在性能上实现了很好的提升。</td>   <td>1.一种自下而上-自上而下的行为识别系统,其特征在于,包括SBTA模块和STBTA模块；所述SBTA模块和STBTA模块通过自下而上自上而下机制和注意机制对局部特征和全局信息进行编码；自下而上自上而下机制为对特征图进行逐层下采样后逐层上采样,通过残差联接保留多尺度学习,并具有科学系参数。</td>   <td>G06K9/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         温武少;              龚江涛;                   秦景辉       </td>   <td>中山大学</td>   <td>基于知识网络精准在线教育系统的多维度信息学情分析方法</td>   <td>广东省</td>   <td>CN109858797A</td>   <td>2019-06-07</td>   <td>本发明属于在线教育学情分析技术领域,涉及基于知识网络精准在线教育系统的多维度信息学情分析方法,基于知识网络精准在线教育,围绕服务对象学习痕迹及其多维度学情信息,对服务对象多维度综合学习评价,推荐个性化教学方案,提出强化学习计划,拓展个性化教学方案,优化个性化学习提升方案。本发明基于全学习痕迹、学习效果以及综合评价预测服务对象未来的学习效果,使得服务对象能够提前掌握学习效果变化的曲线,随时跟踪自己的学习数据；运用神经网络算法推荐个性化教学方案,推荐的个性化教学方案具有针对性；根据综合评价指标聚类和动态识别,使得教师可以根据不同的服务对象群体布置差异化的教学措施,体现了因材施教的教学理念。</td>   <td>1.基于知识网络精准在线教育系统的多维度信息学情分析方法,其特征在于,包括以下步骤：构建知识网络精准在线教育系统；基于知识网络精准在线教育系统,依据服务对象的学习痕迹记录多维度学情信息；基于服务对象已有的学习痕迹及其多维度学情信息,利用知识网络学情综合评估法,形成服务对象多维度综合学习评价；根据教学大纲要求和服务对象多维度综合学习评价,推荐个性化教学方案；根据推荐的个性化教学方案,记录服务对象学习生成的新的学习痕迹及评测结果,通过多维度综合学习评价指标,评估服务对象个性化学习效果；依据服务对象的全学习痕迹和多维度综合学习评价指标,判定服务对象所属类别,提出强化学习计划,拓展个性化教学方案,优化个性化学习提升方案；根据服务对象产生的学习痕迹及多维度学情信息,采用神经网络模型,迭代优化个性化教学方案推荐模型。</td>   <td>G06Q10/06;G06Q50/20;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         田海博;                   刘鲁       </td>   <td>中山大学</td>   <td>一种区块链构造方法</td>   <td>广东省</td>   <td>CN109859044A</td>   <td>2019-06-07</td>   <td>本发明涉及一种基于有向无环图和权益证明的区块链构造方法,涉及客户端、区块链节点两类逻辑主体。区块链节点把时间划分为不同的世代,在每个世代的开始根据节点权益等信息确定该世代的区块链节点列表,客户端节点通过分布式应用DAPP产生应用消息,封装为净交易NT,并通过网络发送给多个区块链节点；之后区块链节点接收NT,封装为区块链交易BT,并通过网络发给其它区块链节点,当区块链节点接收到BT,形成有向无环图,更新不稳定NT的权重证明,当权重证明超过权重门限后,NT稳定,区块链节点通过合约执行稳定NT所含应用消息,更新NT所属账户的状态。该发明能够并行的确认交易,具有较好的安全性和效率。</td>   <td>1.一种区块链构造方法,包括客户端节点、区块链节点两类逻辑节点,所述区块链节点划分为三类：用于发出区块链交易BT的活动区块链节点、不发出区块链交易BT但准备成为活动区块链节点的准区块链节点、和不发出区块链交易也不准备成为活动区块链节点的静默区块链节点；其中,客户端节点包含网络模块、净交易NT模块和分布式应用DAPP模块；区块链节点包含网络模块、NT/BT模块、节点管理模块、合约模块、状态数据库模块、交易数据库/缓存模块；所述网络模块完成网络通信的功能；所述NT模块封装DAPP的应用消息、生成NT；所述DAPP模块提供面向用户的应用；所述净交易NT是客户端节点向区块链节点发送的内容,至少包括应用消息、预付权益数、NT的生成时间、客户端节点的计数器、客户端节点的公钥、客户端的数字签名等；所述状态数据库模块用于存储所有账户的权益、计数器等状态；所述NT/BT模块用于解析并封装NT、生成BT；所述区块链交易BT是一个活动区块链节点向其它区块链节点发出的内容,至少包括NT、2个或多个其它BT的哈希值、该区块链节点的公钥、区块链交易的生成时间,该区块链节点的计数器,该区块链节点的数字签名等；所述节点管理模块用于改变区块链节点的类别,从静默区块链节点成为准区块链节点,从准区块链节点成为活动区块链节点；合约模块用于执行净交易及其中的应用消息,改变状态数据库中账户的内容；合约变量是在区块链节点的合约模块使用的参数；所述交易数据库/缓存模块用于缓存BT和存储BT；所述方法包含如下步骤：S1)区块链节点把时间按照时间周期T划分为不同的时间段,每个时间段称为世代,T是一个大于0的实数；把最开始的时间段作为第一个世代,称为世代0；接下来的第二个世代称为世代1,以此类推；第i+1个世代表示为世代i,i为大于等于1的正整数；在每个世代开始的时间点确定该世代的所有活动区块链节点并形成该世代的活动区块链节点列表,每个世代的活动区块链节点列表的长度有固定的上限NC,所述NC表示一个正整数；区块链节点还在除第一个世代外的每个世代开始的时间点确定该世代之前形成的状态数据库的默克尔根；S2)客户端节点通过DAPP产生应用消息,并通过NT封装应用消息,发送NT给多个当前世代的活动区块链节点；当前世代的活动区块链节点接收NT,判断NT的合法性；如果合法就封装为BT,插入本地的交易数据库,并经由网络发给当前其它区块链节点；S3)区块链节点接收BT,验证BT的合法性,验证不通过放弃处理该BT；否则通过交易数据库和缓存形成区块链交易的有向无环图,采用发送BT的区块链节点的权重来更新其包含的不稳定NT和其直接、间接引用的BT所包含的不稳定NT的权重证明；当权重证明超过权重门限后,NT稳定；S4)区块链节点通过合约模块执行稳定的NT,更新该NT所含公钥对应账户的状态,更新根据合约代码授权更改的账户状态。</td>   <td>G06Q40/04;G06Q20/38</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;              冷绵绵;                   印鉴       </td>   <td>中山大学;广州智海纵横信息科技有限公司</td>   <td>一种基于改进的LBP算子的人群计数方法</td>   <td>广东省</td>   <td>CN106250828B</td>   <td>2019-06-04</td>   <td>本发明公开一种基于改进的LBP算子的人群计数方法,采用基于圆形领域的自适应尺度的旋转不变等价模式的ASLBP算子描述图像的局部纹理特征实现人群计数。采用基于透视归一化图的自适应分块方案,对块提取旋转等价不变的LBP特征算子,特征提取中用灰度变化度确定自适应半径,根据半径确定采样频率,最后对块的归一化的特征描述符,联合BOF特征袋模型,形成场景的特征描述向量；最后用SVR支持向量回归机对图像的特征和场景中的人数之间的映射关系进行回归学习,用训练得到的模型对未知的图像中的人数进行预测。本方法具有良好的实时性,较好的准确度。可用于安防监控等领域。</td>   <td>1.一种基于改进的LBP算子的人群计数方法,其特征在于,包括以下步骤：S1：采集用于回归模型训练的训练图像集；S2：提取训练图像集中每幅图像的自适应尺度的旋转不变局部二值模式算子特征向量,提取的步骤包括：(1)对图像采用基于摄像头透视关系的场景自适应分块处理,将场景划分成子图像块；(2)用灰度变化度确定圆形区域的自适应半径,根据半径确定采样像素点数目；(3)对于每个子图像块,采用自适应尺度的旋转不变等价模式的自适应尺度的旋转不变局部二值模式算子特征算子提取场景信息,形成自适应尺度的旋转不变局部二值模式算子特征直方图；(4)对于每个子图像块的自适应尺度的旋转不变局部二值模式算子特征直方图采用归一化处理；(5)对归一化后特征描述符,联合特征袋模型的BOF,形成特征描述向量F<Sub>lbp</Sub>；S3：回归模型的训练：对于训练图像集,视每一幅图像得到的特征描述向量F<Sub>lbp</Sub>为一个训练样本数据X<Sub>i</Sub>,相应场景的实际人数为当前标签y<Sub>i</Sub>,构建样本数据库并获得回归模型SVR<Sub>model</Sub>；S4：回归模型的估计：对于待估计的图像,提取其场景特征描述向量F<Sub>lbp</Sub>,采用SVR<Sub>model</Sub>进行估计,得到当前场景的人数n<Sub>people</Sub>。</td>   <td>G06K9/00;G06K9/46</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈庆;                   卓汉逵       </td>   <td>中山大学</td>   <td>一种基于强化学习的完形填空型阅读理解分析模型及方法</td>   <td>广东省</td>   <td>CN109840322A</td>   <td>2019-06-04</td>   <td>本发明公开了一种基于强化学习的完形填空型阅读理解分析模型及方法,该模型包括：编码层,将原始文本的单词进行向量化,对单词进行编码,取各句子最后一个单词的隐向量输出作为句子向量,将文本编码成句子向量的序列传递给语句抽取层；语句抽取层,对句子向量选择,将得到的句子作为当前给定文段,对其进行编码；分类层,把每个待填的空位视为一问题,将得到的文段编码和四个候选单词的词向量作为输入,通过多特征分类网络进行计算输出概率；预测层,将上层得到的概率值与语言模型的概率值归一化,得到最终四个选项的概率；输出层,计算上一层得到的概率与实际概率的交叉熵并优化分类网络,将损失值作为延迟奖励对网络进行参数更新。</td>   <td>1.一种基于强化学习的完形填空型阅读理解分析模型,包括：编码层,用于将原始文本的单词进行向量化,然后对单词进行编码,取每个句子最后一个单词的隐向量输出作为句子向量,从而将文本编码成一个句子向量的序列,并将此句子向量的序列传递给语句抽取层；语句抽取层,用于利用语句向量抽取网络对句子向量进行选择,只对一部分的句子进行保留,将得到的句子作为当前新的给定文段,并对获得的文段进行编码；分类层,用于把每一个待填的空位视为一个问题,将上一层得到的文段编码和四个候选单词的词向量作为输入,通过多特征分类网络进行计算,输出四个选项各自的概率；预测层,用于将所述分类层得到的概率值与语言模型的概率值[p<Sub>lA</Sub>,p<Sub>lB</Sub>,p<Sub>lC</Sub>,p<Sub>lD</Sub>]相加并进行归一化,得到最终四个选项的概率输出；输出层,用于计算所述预测层得到的概率与实际概率间的交叉熵,并通过最小化交叉熵优化分类模型,并将损失值作为延迟奖励提供给所述语句向量抽取网络和多特征分类网络进行参数更新。</td>   <td>G06F17/27;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         卞静;              何宇行;                   田臻       </td>   <td>中山大学</td>   <td>一种基于社交网络的消息传播模型的构建方法</td>   <td>广东省</td>   <td>CN106096075B</td>   <td>2019-05-31</td>   <td>本发明公开一种基于社交网络的消息传播模型,是一种基于网络中人群的特殊行以及社交网络中结点的度平均值,对社交网络的消息传播进行建模分析并提出基于结点重要性的谣言免疫策略的方法。在小世界网络中建立的&lt;k&gt;-SLIR消息传播模型的动力学方程；使用本发明的方法能更真实地考虑社交网络中各种状态的结点对消息传播的影响,根据发明中提出的改进的基于结点重要性的谣言传播策略能更好地抑制谣言在社交网络中传播。</td>   <td>1.一种基于社交网络的消息传播模型的构建方法,其特征在于,基于传染病传播SIR模型增添“潜伏结点”,构建SLIR模型；根据不同的网络特性,在社交网络中进行消息传播模型的构建,其中所述社交网络是指任意两个人接触的概率都相等的网络的均匀混合网络；具体为：将社交网络的人群分为四类,用S,L,I,R四种状态来描述易感结点,潜伏结点,感染结点以及免疫结点,其中易感结点代表了目前尚未知晓消息,但是接收到消息后会转变成为潜伏结点；潜伏结点会以不同的概率转化为感染结点以及免疫结点,其具有微弱的传播能力,能够传播消息；感染结点具有很强的传播能力,且会以一定速率转化为免疫结点；免疫结点不再关心传播中的该条消息,不具有传播能力；在社交网络中,一个具有传播能力的结点把消息传播给该网络中的其他结点,做出三条合理的基本假设：1、在均匀混合网络中的结点是均匀混合的,一个具有传播能力的结点把消息传播给网络中任意一个结点的机会均等,任意处于感染态的传播结点可以把消息传递给任意处于易感态的结点；2、在社交网络中总人数不会发生变化；3、潜伏结点以及感染结点都知晓消息,都具有传播性,以[0,1]之间的数值来量化传播性,数值越大,传播性越强,其中传播结点的传播性为1,当传播结点与易感结点接触的时候,易感结点知晓消息的概率为1；在上述3条基本假设下,设定如下的社交网络传播规则：(1)N为社交网络中结点总数,即社交网络中的总人数,一个传播性为1的传播结点,在单位时间内,平均与βN个易感结点接触并使得他们了解到在网络中传播的消息,其中β∈[0,1]；(2)在单位时间内,潜伏结点以概率κ转化为感染结点I,也会以概率δ转化为免疫结点R,其中κ+δ≤1且κ,δ∈[0,1]；(3)在单位时间内,感染结点以速率α转化为免疫结点,其中α∈[0,1]；(4)潜伏结点具有的传播能力远比感染结点弱,使用ε∈[0,1]来表示其传播性；基于上述的假设以及规则,在社交网络中建立的消息传播模型对各个状态的结点进行分析；易感结点S：易感结点与具有传播能力的结点接触时,将会以S/N的概率知晓消息；根据传播规则(1),在Δt时间内,社交网络中的所有感染结点与易感结点接触,使得βN·S/N·Δt·I个易感结点转变为潜伏结点；同时,由于潜伏结点也具有微弱的传播能力,所以在Δt时间内,易感结点与潜伏结点接触,使得ε·βN·S/N·Δt·L个易感结点转化为潜伏结点；因此,关于易感结点的变化率的微分方程如下：          <Image id="icf0001" he="72" wi="700" file="FDA0001810863210000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        潜伏结点L：由于易感结点在接触到具有传播能力的结点时先转化为潜伏结点,所以在Δt时间内,潜伏结点增加的数量为(βSI+εβSL)·Δt；同时,根据规则(2),在Δt时间内,潜伏结点转化为感染结点的个数为κL·Δt,转化为免疫结点的个数为δL·Δt；因此,关于潜伏结点的变化率的微分方程如下：L'＝βS(I+εL)-(κ+δ)L    (2)感染结点I：根据潜伏结点的描述,在Δt时间内,有κL·Δt个潜伏结点转化为感染结点；同时,根据规则(3),有αI·Δt个感染结点转化为免疫结点；因此,关于感染结点的变化率的微分方程如下：I'＝κL-αI    (3)免疫结点R：根据潜伏结点以及感染结点的描述,在Δt时间内,有δL·Δt个潜伏结点转化为免疫结点,有αI·Δt个感染结点转化为免疫结点；因此,关于免疫结点的变化率的微分方程如下：R'＝δL+αI结合(1)～(3)式,采用(4)式对各个状态结点的变化进行描述：          <Image id="icf0002" he="293" wi="700" file="FDA0001810863210000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中S代表社交网络中的易感结点,L代表社交网络中的潜伏结点,I代表社交网络中的感染结点,R代表社交网络中的免疫结点；N代表了社交网络中的总人数,S<Sub>0</Sub>,L<Sub>0</Sub>,I<Sub>0</Sub>,R<Sub>0</Sub>表示的是消息开始传播的初始状态时,易感结点,潜伏结点以及免疫结点的初始数量；β代表了网络中易感节点与感染结点接触的概率,反映的是网络本身的性质；ε代表的是潜伏结点的微弱传播能力；κ是潜伏结点转化为感染结点的概率,δ是潜伏结点转化为免疫节点的概率,其中α,β,ε,κ,δ∈(0,1)且κ+δ≤1。</td>   <td>G06F17/50;G06Q50/00;H04L12/58</td>  </tr>        <tr>   <td>中国专利</td>   <td>         汪飞;              林淑金;              罗笑南;                   周凡       </td>   <td>中山大学</td>   <td>基于保密度和无散度的不可压缩流体的模拟方法及系统</td>   <td>广东省</td>   <td>CN109063375B</td>   <td>2019-05-31</td>   <td>本发明公开了一种基于保密度和无散度的不可压缩流体的模拟方法,包括：S1,初始化粒子信息；S2,计算粒子的中间速度和中间位置；S3,计算粒子的密度；S4,计算粒子的约束因子；S5,计算粒子的位移；S6,更新粒子的位置；S7,重复步骤S4～S6,直到迭代次数达到最大值并且约束因子大于阈值。本发明还公开了一种基于保密度和无散度的不可压缩流体的模拟系统。采用本发明,不但提高了模拟的时间效率,而且流体模拟的分布比先前的方法更加的均匀和稳定。</td>   <td>1.一种基于保密度和无散度的不可压缩流体的模拟方法,其特征在于,包括：S1,初始化粒子信息；S2,计算粒子的中间速度和中间位置；S3,计算粒子的密度；S4,根据公式<Image id="icf0001" he="70" wi="700" file="FDA0001994052360000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>计算粒子的约束因子,其中,ρ<Sub>0</Sub>表示为常量密度,t代表当前的时刻,Δt表示的是迭代的时间步长,<Image id="icf0002" he="68" wi="164" file="FDA0001994052360000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示在时刻t核函数W<Sub>ij</Sub>(t)的偏导数,所述约束因子用于使得所模拟的流体同时达到常密度和无散度的条件；S5,计算粒子的位移；S6,更新粒子的位置；S7,重复步骤S4～S6,直到迭代的约束因子大于阈值；所述步骤S1包括：S11,进行粒子的固定半径邻域搜索,具体地,将粒子所在场景的包围盒均匀划分成若干个边长为2*h的小正方形网格,其中h为粒子的平滑半径,也即固定搜索半径；计算出粒子所在网格,从而每个粒子只需要在27个网格里面搜索邻域粒子即可；S12,初始化粒子的密度ρ<Sub>i</Sub>,其中,<Image id="icf0003" he="135" wi="424" file="FDA0001994052360000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>m<Sub>j</Sub>表示粒子的质量,W表示核平滑函数,x<Sub>ij</Sub>＝x<Sub>i</Sub>-x<Sub>j</Sub>,x<Sub>i</Sub>表示粒子i的位置,x<Sub>j</Sub>表示粒子j的位置。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑永川;              王若梅;              周凡;              林谋广;                   林格       </td>   <td>中山大学</td>   <td>基于体素模型的影像重建四边形网格方法及系统</td>   <td>广东省</td>   <td>CN109285223B</td>   <td>2019-05-31</td>   <td>本发明公开了一种基于体素模型的影像重建四边形网格方法,包括：S1,将CT影像转换为文本格式并进行平滑处理；S2,选取灰度值并使用插值法提取出等值点作为点云数据；S3,根据堆叠立方体的方法构建初始体素模型,并对体素模型进行体素优化,通过深度优先搜索提取出体素模型外表面；S4,计算体素模型外表面每个顶点的法向量,通过领域搜索方法查找与法向量最近的一个点云点作为顶点的映射点；S5,将四边形网格的顶点调整到重心后进行重新映射并对四边形网格进行网格塌缩处理。本发明还公开了一种基于体素模型的影像重建四边形网格系统。采用本发明,可直接从点云重建出四边形网格,并可选取不同的体素立方体大小来控制生成的四边形网格精细程度。</td>   <td>1.一种基于体素模型的影像重建四边形网格方法,其特征在于,包括：S1,CT影像预处理：将CT影像转换为文本格式,并进行平滑处理过滤噪声；S2,提取点云数据：根据要重建的组织选取灰度值,并使用插值法提取出等值点作为点云数据；S3,构建体素模型：根据堆叠立方体的方法构建初始体素模型,并对体素模型进行体素优化,通过深度优先搜索提取出体素模型外表面；S4,顶点映射：计算体素模型外表面每个顶点的法向量,通过领域搜索方法查找与法向量最近的一个点云点作为顶点的映射点；具体地,所述步骤S4包括：S41,根据法向量公式计算体素模型外表面的法向量,并对法向量进行单位化处理；S42,根据点云数据与立方体之间的归属关系构建顶点相邻的立方体并将点云点作为顶点的搜索点集,在搜索点集中提取与法向量最近的点并将与法向量最近的点作为顶点的映射点；S5,网格优化：将四边形网格的顶点调整到重心后进行重新映射,并对四边形网格进行网格塌缩处理；具体地,所述步骤S5包括：S51,根据重心公式<Image id="icf0001" he="225" wi="269" file="FDA0001994631490000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>计算顶点的重心,并重新将重心映射到点云上,其中,G<Sub>P</Sub>为点p要调整到的重心位置,A<Sub>i</Sub>表示点p的邻接区域,G<Sub>i</Sub>为邻接区域的重心；S52,将四边形进行塌缩处理。</td>   <td>G06T17/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         孙颖;              陆遥;                   林丽       </td>   <td>中山大学;中山大学肿瘤防治中心(中山大学附属肿瘤医院、中山大学肿瘤研究所)</td>   <td>一种基于深度语义分割网络的自动识别鼻咽癌原发肿瘤方法</td>   <td>广东省</td>   <td>CN109829885A</td>   <td>2019-05-31</td>   <td>本发明公开了一种基于深度语义分割网络的自动识别鼻咽癌原发肿瘤方法,方法包括：采集患者的磁共振三维图像；对三维图像进行灰度偏差场的纠正处理；对预处理步骤后的三维图像利用改进的直方图匹配算法进行处理；对上述预处理步骤后的三维图像截取ROI区域,并分割为2*2个有重叠的patch作为模型的输入；对多个patch输入已训练的深度语义分割网络进行鼻咽癌原发肿瘤的识别,最终将输出的多个patch的识别结果合并,得到最终的原发肿瘤识别结果。本发明能有效提高输入数据的质量,并学习到高分辨率图像的全局信息和细节信息,结合后处理方法,能有效提高预测的准确度和模型的泛化能力,进而有效提高医疗工作者的工作效率。</td>   <td>1.一种基于深度语义分割网络的自动识别鼻咽癌原发肿瘤方法,其特征在于,包括以下步骤：S1：采集患者的磁共振三维图像,并对所述磁共振三维图像进行初步的数据预处理；S2：对数据预处理后的磁共振三维图像进行灰度偏差场的纠正处理,使同一图像中相同组织内的灰度值更加均匀；S3：利用改进的直方图匹配算法对磁共振三维图像进行处理,通过训练一个灰度映射函数,使图像的直方图与预先挑选的模板图像的直方图进行匹配,使不同图像中相同组织内的灰度值更加相近；S4：截取磁共振三维图像的ROI区域,并分割为2*2个有重叠的patch作为模型的输入；S5：构建并训练深度语义分割网络对原发肿瘤进行识别；S6：将多个patch输入至已训练的深度语义分割网络进行原发肿瘤的识别；S7：将输出的多个patch的识别结果合并,得到最终的原发肿瘤识别结果；S8：利用平均场迭代算法对原发肿瘤识别结果进行后处理。</td>   <td>G06T7/00;G06T7/11</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林谋广;                   周凡       </td>   <td>中山大学</td>   <td>一种智能眼镜的三维图形自适应显示方法</td>   <td>广东省</td>   <td>CN109829974A</td>   <td>2019-05-31</td>   <td>本发明公开了一种智能眼镜的三维图形自适应显示方法。本发明使用服务器对三维模型进行简化,并在简化过程中把简化掉的部分按照设计好的文件格式保存成一系列的细节文件,智能眼镜根据网络与设备性能通过读取从服务器下载的模型细节自适应地快速重构并显示三维图形。本发明可以在达到满足用户实时交互需求的显示帧率的前提下,自适应地为用户提供尽量多的三维模型细节,为用户在网上商店查看商品的三维模型、在虚拟博物馆中查看藏品的三维模型等应用中提供有效的帮助。本发明设计的便于重构的细节文件结构,把大量的图形简化与细节保存计算放到服务器上,使性能低下的智能眼镜能快速进行图形重构,提升用户交互地查看三维模型时的使用体验。</td>   <td>1.一种智能眼镜的三维图形自适应显示方法,其特征在于,所述方法包括：用户从智能眼镜的应用中选取需要查看三维模型的物品；所述智能眼镜的应用把用户所选取的物品的编号传输给服务器；所述服务器根据所述物品编号,调取对应的细节丰富的原始三维模型进行图形简化,并在简化的过程中把简化掉的细节保存成文件,最终形成一个最简模型与一系列有顺序的细节文件；所述智能眼镜的应用从所述服务器下载所述物品的最简模型进行交互显示,以满足用户最基本的交互式查看需求；当网络传输速度以及三维模型显示帧率大于设定的阈值时,所述智能眼镜的应用从所述服务器按顺序下载所述细节文件进行渐进式重构与显示,直到显示帧率刚好小于设定的阈值为止。</td>   <td>G06T17/00;G06T15/08;G02B27/01</td>  </tr>        <tr>   <td>中国专利</td>   <td>         朱俊勇;              李锴莹;              赖剑煌;                   谢晓华       </td>   <td>中山大学</td>   <td>基于循环生成对抗网络的彩色人脸图像光照域归一化的方法</td>   <td>广东省</td>   <td>CN109815893A</td>   <td>2019-05-28</td>   <td>本发明公开了一种基于循环生成对抗网络的彩色人脸图像光照域归一化的方法,包括以下步骤：S1:建立用于彩色人脸图像光照归一化的循环生成对抗网络模型；S2:建立模型的损失函数；S3:进行模型的训练,并在测试集上测试。本发明是对多种光照下的彩色人脸图像进行到指定目标光照域的转换,输入不均匀光照的彩色人脸图片,使用循环生成对抗网络作为模型架构,以目标均匀光照域为目标,实现人脸图像多光照的归一化,归一化的图像不仅可以较好的保持原有人脸的脸部属性特征,还可以很好实现跨数据集迁移。</td>   <td>1.基于循环生成对抗网络的彩色人脸图像光照域归一化的方法,其特征在于,包括以下步骤：S1：建立用于彩色人脸图像光照归一化的循环生成对抗网络模型,该循环生成对抗网络模型包括生成网络和鉴别网络,所述生成网络通过构建生成器生成转换成指定光照域的人脸图片和人脸特征重构图片,所述鉴别网络通过构建鉴别器来鉴别光照域；S2：建立模型的损失函数,使生成对抗网络训练稳定,并且使得生成器在学习目标光照域光照信息时能够比较好保留输入图像的脸部特征；S3：进行模型的训练,将不同光照类别的图像分成不均匀光照域和目标均匀光照域,进行在生成对抗网络中循环训练,并在测试集上测试,并查看生成的人脸图像效果图。</td>   <td>G06K9/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑子彬;                   郑万山       </td>   <td>中山大学</td>   <td>一种基于深度强化学习A3C算法的金融交易方法</td>   <td>广东省</td>   <td>CN109816530A</td>   <td>2019-05-28</td>   <td>本发明公开了一种基于深度强化学习A3C算法的金融交易方法,所述方法包括：构建模拟交易环境,采集历史分钟数据并预处理,得到待输入向量；初始化A3C模型；工作者线程与模拟交易环境的副本交互,并将待执行的动作发送到模拟交易环境的副本；模拟交易环境的副本接跳转到下一个交易状态,计算奖赏值,将下一交易状态和奖赏值发送至智能交易体；交易智能体分别计算工作者线程的策略网络和评估网络的权值梯度并发送至全局网络；全局网络更新权值,重复工作线程的交互直到A3C模型训练完毕；使用全局网络对待执行的交易操作进行选择。本发明通过市场历史数据不断地交互训练模型,得到稳定的交易策略,避免引入大量的专家知识,动态适应环境变化并更新策略。</td>   <td>1.一种基于深度强化学习A3C算法的金融交易方法,其特征在于,所述方法包括以下步骤：S1：构建模拟交易环境,通过模拟交易环境的数据接口采集金融市场的历史分钟数据,将历史分钟数据进行缺省值处理完毕后,再做归一化处理,得到历史分钟数据指标；同时分别计算按分钟的简单移动平均指标和按分钟的指数移动平均指标,将归一化处理后的历史分钟数据指标和按分钟的简单移动平均指标、按分钟的指数移动平均指标组合得到待输入向量；S2：初始化包含在智能交易体中的A3C模型,所述A3C模型包含有一个全局网络和若干工作者线程,所述全局网络的结构和每一个工作者线程的结构均相同；所述工作者线程均包括有评估网络和策略网络,所述评估网络和策略网络共用若干隐含层；所述A3C模型创建工作者线程时,模拟交易环境同时创建模拟交易副本；所述待输入向量通过输入层输入至工作者线程,然后通过若干隐含层处理,隐含层的最终输出分别输入至策略网络和评估网络,策略网络和评估网络分别对应输出策略网络输出向量和评估网络输出向量,所述若干工作者线程输出的策略网络输出向量和评估网络输出向量共同作为全局网络的输入,所述全局网络的输入经过全局网络的处理最终得到全局网络的输出；所述全局网络的权值使用随机初始化,每一个工作者线程的网络权重与全局网络权重同步初始化；S3：每一个工作者线程与一个模拟交易环境的副本进行交互,所述模拟交易环境副本的数目与工作者线程的数目相同；利用工作者线程中策略网络的输出向量和评估网络输出值计算需要执行的交易动作,并将动作发送到模拟交易环境的副本；S4：模拟交易环境的副本接收到交易动作,跳转到下一个交易状态s<Sub>t+1</Sub>,根据交易动作前后的交易状态计算奖赏值r<Sub>t</Sub>,奖赏值计算完成后,模拟交易环境的副本将下一交易状态s<Sub>t+1</Sub>和奖赏值r<Sub>t</Sub>发送到交易智能体；S5：交易智能体接收到s<Sub>t+1</Sub>和r<Sub>t</Sub>,分别计算工作者线程的策略网络权值梯度和评估网络权值梯度,每个工作者线程将各自的策略网络权值梯度和评估网络权值梯度发送至全局网络；S6：全局网络接收到各工作者发送的策略网络权值梯度和评估网络权值梯度后,更新全局网络的网络权值,全局网络权值更新完成后,以s<Sub>t+1</Sub>作为工作者线程的新的输入,重复步骤S3直到达到设定的步数或累计奖赏收敛,此时A3C模型训练完毕；S7：A3C模型训练完毕后,使用全局网络对待执行的交易操作进行选择,模拟交易环境从交易所接收行情数据,计算待输入向量作为全局网络输入,根据全局网络中策略网络输出的最大概率值选取下一目标持仓量,通过ctp自动化交易接口下订单使得当前仓位达到目标仓位。</td>   <td>G06Q40/04;G06Q10/06;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈创荣;                   成慧       </td>   <td>中山大学</td>   <td>一种双目视觉系统高精度且无拖影的视差计算方法</td>   <td>广东省</td>   <td>CN109816710A</td>   <td>2019-05-28</td>   <td>本发明涉及双目视觉系统处理的技术领域,更具体地,涉及一种双目视觉系统高精度且无拖影的视差计算方法。本发明提供的方法用于在自然场景中,利用双目视觉系统计算出精确像素视差,并且不同于已有系统,本发计算得到的场景深度没有普遍存在的过度平滑现象,无需复杂的后处理,能直接用于下游任务。该方法首先对像素的视差进行分布建模,利用卷积神经网络模拟传统方法,并对每个像素点输出一个像素的视差分布。</td>   <td>1.一种双目视觉系统高精度且无拖影的视差计算方法,其特征在于,包括以下步骤：S1.对输入的左右两幅图像,提取特征,得到左右两幅图像在1/4分辨率下特征；S2.根据提取得到的左右图特征,构建4D的代价体V；S3.进行代价聚合,得到视差值的对数似然估计,并上采用到原图分辨率,得到每个像素的可能视差值的对数似然估计；S4.对得到的对数似然估计,每个像素在视差维度上做归一化操作,得到每个像素的视差概率分布；S5.找出概率值最大的峰,做进一步推断,得到更精确的视差概率分布；S6.基于得到的精确视差概率分布,通过加权平均操作得到每个像素视差的最终估计值。</td>   <td>G06T7/55</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈楚城;              张灵敏;                   戴宪华       </td>   <td>中山大学</td>   <td>一种基于深度学习的农作物病害检测算法</td>   <td>广东省</td>   <td>CN109800806A</td>   <td>2019-05-24</td>   <td>本发明涉及一种基于深度学习的农作物病害检测算法,包括：(1)获取训练集和测试集并对训练集进行图像预处理；(2)将训练集输入到具有自适应多尺度的模型中进行训练,该模型连结了两种全局池化层,使用了有差异性学习速率的迁移学习同时使用了Focal loss为损失函数；(3)将测试集图像输入到训练好的检测模型中预测图像的病害类型。本发明的方法相比其他算法使用了差异性学习速率的迁移学习提高了检测性能和泛化能力,同时连结两种全局池化层使得模型保留更多细节信息和具有尺寸自适应性,通过渐进式学习策略加快模型收敛且使用Focal loss缓解类别不平衡的问题,该算法对病状相近的病害具有更好的检测性能。</td>   <td>1.一种基于深度学习的农作物病害检测算法,其特征在于,包括如下步骤：(1)图像采集,利用摄像头对农作物叶子进行拍摄,获取相关的数据集并对图像重命名,如1.jpg,2.jpg,3.jpg,…,M.jpg等,同时标定每张图像中农作物病害种类；(2)图像划分,将图像划分成训练集和测试集两部分,两个部分不存在相同的图像,训练集用来训练农作物病害检测模型,测试集用来评估农作物病害检测模型的性能；(3)图像预处理,统计各类图像的数量并对数量较少的图像进行过采样,对全部图像都进行线上数据增强,包括随机灰度化、左右翻转、上下翻转、对角线翻转、随机裁剪和亮度变化；(4)训练检测模型,将经过图像预处理后的训练集图像通过有差异性学习速率的迁移学习和以Focal Loss为损失函数的自适应多尺度resnet50网络,对得到的预测结果与真实标签进行对比计算分类损失,通过带动量的梯度下降算法更新模型参数；(5)农作物病害图像检测,将测试集中的图像输入到训练好的农作物病害检测模型中,通过前向传播得到该图像的农作物病害类型。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王欣明;              聂瑞华;              赵淦森;                   纪求华       </td>   <td>中山大学</td>   <td>基于马尔可夫逻辑网络的在线活动识别方法及系统</td>   <td>广东省</td>   <td>CN105894017B</td>   <td>2019-05-21</td>   <td>本发明公开了一种基于马尔可夫逻辑网络的在线活动识别方法及系统,方法包括：对所有操作行为进行采集并储存于数据库中,得到行为数据；根据行为数据,对其进行处理并识别出对应的活动结果,得到活动数据；将识别得到的活动数据进行处理并以可视化的方式进行呈现。系统包括：数据采集单元、识别处理单元和数据呈现单元。本发明通过建立活动识别模型能从动作日志中自动识别用户活动,并且有良好扩展性,允许使用者添加逻辑规则,而且采用规则挖掘算法从训练样本集中挖掘出强关联规则作为逻辑规则,有效解决了解决动作与在线活动的对应关系的不确定性,大大提高识别的准确性。本发明可广泛应用于计算机领域中。</td>   <td>1.基于马尔可夫逻辑网络的在线活动识别方法,其特征在于,包括以下步骤：A、对所有操作行为进行采集并储存于数据库中,得到行为数据；B、根据行为数据,对其进行处理并识别出对应的活动结果,得到活动数据；C、将识别得到的活动数据进行处理并以可视化的方式进行呈现；所述步骤B包括：B1、将行为数据导出并进行预处理,得到行为数据集；B2、根据行为数据集,进行活动识别建模,得到活动识别模型；B3、通过活动识别模型对行为数据集进行推理识别,得到活动数据；所述步骤B2包括：B21、根据行为数据集,通过规则发掘算法得出一阶逻辑规则；B22、对行为数据以及其对应的活动进行数据标注；B23、根据标注的数据作为训练数据,进行一阶逻辑规则的权重学习,得到每条规则的权重。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         温武少;                   谢明森       </td>   <td>中山大学</td>   <td>一种批量大数据实验环境的快速自动构建方法</td>   <td>广东省</td>   <td>CN109783198A</td>   <td>2019-05-21</td>   <td>本发明涉及云计算技术领域,具体涉及一种批量大数据实验环境的快速自动构建方法。包括步骤：自动构建大数据集群网络资源池,大数据实验环境为已配置大数据环境的虚拟机集群,大数据集群网络资源池包括批量独立的虚拟网络,每个虚拟网络通过虚拟路由和外部网络进行连接；根据大数据集群模版自动生成大数据集群快照；基于大数据集群快照快速自动构建批量大数据集群。本发明通过上述批量大数据实验环境的快速自动构建方法,可以解决批量大数据实验环境的自动构建速度慢的问题。</td>   <td>1.一种批量大数据实验环境的快速自动构建方法,其特征在于,包括以下步骤：S1、自动构建大数据集群网络资源池；所述大数据实验环境为已配置大数据环境的虚拟机集群,大数据集群网络资源池包括批量独立的虚拟网络,每个虚拟网络通过虚拟路由和外部网络进行连接；S2、根据大数据集群模版自动生成大数据集群快照；S3、基于大数据集群快照快速自动构建批量大数据集群。</td>   <td>G06F9/455;G06F9/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李中华;                   何冠男       </td>   <td>中山大学</td>   <td>RFID移动阅读器防碰撞资源自适应分配与动态再竞争方法</td>   <td>广东省</td>   <td>CN109784115A</td>   <td>2019-05-21</td>   <td>本发明公开了一种RFID移动阅读器防碰撞资源自适应分配与动态再竞争方法,本方法包含识别时间估计阶段、全局通信资源竞争阶段和局部通信资源再竞争配置阶段。本发明从RFID阅读器通信资源最优化分配的角度出发,RFID阅读器根据识别范围内的RFID标签数量情况,获取和释放通信资源,以实现通信资源的高效利用,最大化RFID阅读器网络识别RFID标签效率。</td>   <td>1.RFID移动阅读器防碰撞资源自适应分配与动态再竞争方法,其特征在于,所述方法包括每轮依次执行的三个连续的阶段,分别是识别时间估计阶段、全局通信资源竞争阶段和局部通信资源再竞争阶段；识别时间估计阶段：每个RFID移动阅读器首先对识别范围内的标签数量进行估计,从而估算出识别完识别范围内所有电子标签的总时间；全局通信资源竞争阶段：RFID移动阅读器在轮训服务器广播同步信号的条件下,通过竞争通信资源规则进行资源竞争,竞争资源成功的RFID移动阅读器识别标签,竞争资源失败的RFID移动阅读器进行休眠；局部通信资源再竞争阶段：识别标签的RFID移动阅读器识别完覆盖范围内标签之后,调整再竞争优先级并向邻居RFID移动阅读器发送激活信号,处于休眠状态的RFID移动阅读器根据接收到的激活信号数量,来设定竞争优先级,然后RFID移动阅读器根据竞争优先级进行通信资源的再竞争。</td>   <td>G06K7/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马争鸣;              苏薛;              刘佳;                   刘洁       </td>   <td>中山大学</td>   <td>基于核空间的流形学习非线性逼近的算法</td>   <td>广东省</td>   <td>CN109784498A</td>   <td>2019-05-21</td>   <td>本发明涉及机器学习领域中的数据降维问题,提出了一种基于核空间的流形学习非线性逼近的算法。本方法首先根据给定的高维数据以及选定的流形学习算法,在此流形学习算法下计算高维数据的低维坐标并保存下来；其次根据给定的高维数据张成一个核函数空间,在此空间上定义一个降维函数,使得由此降维函数得到的低维坐标和流形学习算法得到的低维坐标之间的误差最小,从而学习出降维函数的参数；最后对于新给定的高维数据可直接由构造的降维函数来预测其低维坐标。本发明的主要思想是给定高维数据和由流形学习得到的低维坐标,用核函数空间上定义的核函数去逼近训练数据集从高维空间到低维空间的映射关系,进而得到一个新的降维函数。</td>   <td>1.一种基于核空间的流形学习非线性逼近的算法,其主要特征在于：A.计算给定高维数据集X<Sub>t</Sub>的低维坐标Y<Sub>t</Sub>：根据高维数据X<Sub>t</Sub>和选定的流形学习算法ε,计算高维数据X<Sub>t</Sub>在此流形学习算法下的低维坐标Y<Sub>t</Sub>＝ε(X<Sub>t</Sub>)并保存下来；B.在给定的高维数据空间中构造降维函数：根据给定的高维数据X<Sub>t</Sub>张成一个核函数空间<Image id="icf0001" he="144" wi="397" file="FDA0001954684890000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>在此空间上定义一个降维函数<Image id="icf0002" he="131" wi="427" file="FDA0001954684890000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>C.根据给定的高维数据学习降维函数f的参数：用降维函数去逼近给定训练数据集X<Sub>t</Sub>从高维到低维的映射关系,即在核函数空间中最小化降维函数得到的低维坐标和流形学习得到的低维坐标之间的误差,由于流形学习得到的低维坐标与实际低维坐标的误差为零,故目标函数为          <Image id="icf0003" he="131" wi="542" file="FDA0001954684890000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        从而学习到降维函数的参数a；D.对于新给定的高维数据利用步骤C中得到的降维函数f来预测新来数据的低维坐标。</td>   <td>G06N20/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         唐承佩;              王善庆;              胡鹏丽;              李昌镐;                   谭杜康       </td>   <td>中山大学</td>   <td>一种基于深度学习的厂区能耗预测方法及系统</td>   <td>广东省</td>   <td>CN109784532A</td>   <td>2019-05-21</td>   <td>本发明公开了一种基于深度学习的厂区能耗预测方法及系统,所述方法包括将厂区按功能划分为若干个厂区单元,对各厂区单元的若干个目标对象的电能参数数据进行采集,采集频率为3秒/次,对每个厂区单元采集数据进行预处理,依据预处理的数据,运用基于深度学习长短时记忆神经网络的电力负荷预测算法进行厂区能耗预测,该方法能效提高预测的准确度,达到精确预测厂区能耗的目的。</td>   <td>1.一种基于深度学习的厂区能耗预测方法,其特征在于：所述方法包括如下步骤：步骤S1：将厂区按功能划分为若干个厂区单元；步骤S2：对各厂区单元的若干个目标对象的电能参数数据进行采集；步骤S3：对每个厂区单元采集的数据进行预处理；步骤S4：依据预处理的数据,运用深度学习算法进行厂区能耗预测。</td>   <td>G06Q10/04;G06Q50/06;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              陈子轩;              郭春超;                   谢晓华       </td>   <td>中山大学</td>   <td>融合物体表观信息和运动信息的视频运动物体分割方法</td>   <td>广东省</td>   <td>CN109785327A</td>   <td>2019-05-21</td>   <td>本发明公开了一种融合物体表观信息和运动信息的视频运动物体分割方法,该方法首先通过用深度特征提取视频的物体表观信息以及运动信息,然后对深度表观信息和深度运动信息进行交互编码得到表观-运动信息和运动-表观信息,再将两者融合,得到交互编码后的初始分割图；视频序列逐帧均进行分割后得到视频分割序列,构建一个能量方程,以使整个视频分割序列的能量总值最小为目标对能量方程进行优化,从而生成一个视频运动物体分割模型；根据该分割模型对初始分割图进一步分割,得到最终分割结构。本发明方法拥有更为强大的泛化能力,在图像质量以及分割准确率上面都有很大的提升。</td>   <td>1.融合物体表观信息和运动信息的视频运动物体分割方法,其特征在于,包括步骤：(1)通过用深度特征提取视频的物体表观信息以及运动信息；(2)对深度表观信息和深度运动信息进行交互编码得到表观-运动信息和运动-表观信息,再将两者融合,得到交互编码后的初始分割图；(3)视频序列逐帧均进行分割后得到视频分割序列,构建一个能量方程,以使整个视频分割序列的能量总值最小为目标对能量方程进行优化,从而生成一个视频运动物体分割模型；根据该分割模型对初始分割图进一步分割,得到最终分割结构。</td>   <td>G06T7/10;G06T7/50;G06T7/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              肖逢枝;              谢舜道;              陈荣军;              朱雄泳;                   曾衍瀚       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院</td>   <td>一种二维码的二次遍历二值化方法、装置和存储介质</td>   <td>广东省</td>   <td>CN109785353A</td>   <td>2019-05-21</td>   <td>本发明公开了一种二维码的二次遍历二值化方法、装置和存储介质。在获取到原始图像后进行预处理获得输入图像和积分图像,结合积分图像对输入图像进行横向扫描遍历,得出粗定位子图和粗定位参数,再根据粗定位参数对粗定位子图遍历时进行二值化,得出二值化结果图。本发明的方法仅执行了两次遍历,无需对二值图进行对此遍历,大大减少了计算量,加快了计算效率,从而实现了模糊二维码的快速识别。</td>   <td>1.一种二维码的二次遍历二值化方法,其特征在于,包括以下步骤：获取原始图像,对原始图像进行预处理,得出输入图像和对应的积分图像；获取预先设定的扫描宽度,根据积分图像和扫描宽度对所述输入图像进行横向扫描遍历,得出粗定位子图和粗定位参数；根据粗定位参数对所述粗定位子图进行遍历并二值化,得出二值化结果图。</td>   <td>G06T7/136</td>  </tr>        <tr>   <td>中国专利</td>   <td>         卢伟;                   李静伟       </td>   <td>中山大学</td>   <td>一种基于KAZE特征点的图像区域复制粘贴篡改检测方法</td>   <td>广东省</td>   <td>CN106056122B</td>   <td>2019-05-17</td>   <td>本发明提供一种基于KAZE特征点的图像区域复制粘贴篡改检测方法。首先对彩色图像提取KAZE特征点,然后将这些特征点用64维特征向量描述。接下来计算每个特征向量和剩下特征向量之间的欧式距离,利用最近邻距离和次近邻距离之间的比值,找到相似的特征向量,作为匹配对。然后使用SLIC算法对图像进行语义分割,滤除错误的匹配对。通过匹配对在图像中的位置关系,使用迭代的思想,估计篡改区域之间的仿射变换关系,得到仿射矩阵。最后通过仿射矩阵,计算原始图像和变换后图像之间的相关系数图,并且定位篡改区域。本发明使用了一种新型的特征点提取算法,并且使用迭代的方法求仿射矩阵,具有很好的检测准确率。</td>   <td>1.一种基于KAZE特征点的图像区域复制粘贴篡改检测方法,其特征在于,包括以下步骤：S1：KAZE特征点提取：对于待检测的图像,采用加性算子分裂算法AOS算法和可变传导扩散方法来构造非线性尺度空间,然后检测感兴趣特征点,这些特征点在非线性尺度空间上经过尺度归一化后的Hessian矩阵行列式是局部极大值；S2：特征点描述：根据步骤S1得到的特征点,若特征点的尺度参数为σ<Sub>i</Sub>,则搜索半径设为6σ<Sub>i</Sub>；对搜索圈内所有邻点的一阶微分值L<Sub>x</Sub>和L<Sub>y</Sub>通过高斯加权,使得靠近特征点的响应贡献大,而远离特征点的响应贡献小；将这些微分值视作向量空间中的点集,在一个角度为π/3的扇形滑动窗口内对点集进行向量叠加,遍历整个圆形区域；获得最长向量的角度就是主方向；在梯度图像上以特征点为中心取24σ<Sub>i</Sub>×24σ<Sub>i</Sub>的窗口,并将窗口划分为4×4子区域,在每个子区域上进行高斯核加权,然后计算出长度为4的子区域描述向量d<Sub>v</Sub>＝(∑L<Sub>x</Sub>,∑L<Sub>y</Sub>,∑|L<Sub>x</Sub>|,∑|L<Sub>y</Sub>|),再通过另一个大小为4×4的高斯窗口对每个子区域的向量d<Sub>v</Sub>进行加权,最后进行归一化处理,得到4×4×4＝64维的描述向量；S3：特征匹配：对于步骤S2中提取出来的每个特征,计算其与其它所有特征向量之间的欧式距离,并按照从小到大排序；计算最近邻d<Sub>1</Sub>和次近邻d<Sub>2</Sub>之间的比值,如果比值小于0.5,则认为距离为d<Sub>1</Sub>的两个特征匹配；S4：错误匹配对滤除：使用SLIC算法对输入的彩色图像进行语义分割,得到有意义的图像块；统计每个块中匹配特征点的个数N<Sub>point</Sub>,如果N<Sub>point</Sub>小于3,则将块中的特征点连同其匹配点判断为离异点并删除；S5：仿射矩阵估计：任取三对不共线的匹配对,计算它们之间的仿射变换矩阵T<Sub>i</Sub>,并将剩下的匹配点根据T<Sub>i</Sub>进行变换,计算变换前后匹配点对之间的误差；如果误差小于β,则这个矩阵T<Sub>i</Sub>获得一票；将前述步骤迭代多次,每次选出得票数最多的矩阵,直到最后剩下的矩阵票数不超过5为止；S6：可疑区域定位：对于原始图像I,使用步骤S5得到的仿射矩阵进行坐标变换,得到变换后的图像M；计算原始图像与变换后图像相应位置之间的相关系数,得到代表相似度的相关系数图；相关系数的取值范围在[0,1]之间,值越大代表越相似；对于得到的相关系数图进行二值化处理,二值化阈值为0.55；如果相关系数值大于0.55,则认为本位置的点为可疑点,其二值图相应位置的值设为1,否则设为0；最后将得到的二值图进行形态学操作以滤除杂乱点,生成最终的检测结果图。</td>   <td>G06K9/46</td>  </tr>        <tr>   <td>中国专利</td>   <td>         余志;                   赵有婷       </td>   <td>中山大学</td>   <td>一种基于透视变换的角度车牌图像库搭建方法</td>   <td>广东省</td>   <td>CN106169076B</td>   <td>2019-05-14</td>   <td>本发明公开一种基于透视变换的角度车牌图像库搭建方法,其具体实现过程为：1)建立针孔相机成像模型,获得三维车牌四个顶点的坐标值；2)对三维车牌进行空间几何变换,获得变换后的三维车牌四个顶点的坐标值；3)将三维车牌投影变换到二维图像,并获得车牌二维图像四个顶点的坐标值；同时获得正投影的车牌图像四个顶点的坐标值；4)计算二维图像变成不同形状二维图像的投影矩阵H；5)基于所得的投影矩阵H,将正投影的车牌图像变换成不同形状的车牌图像,建立不同成像形状的车牌图像库。本发明提出的建立车牌图像库的方法可以模拟实际的拍摄环境,能够明确成像属性,图像可扩充性强,并且成本低。</td>   <td>1.一种基于透视变换的角度车牌图像库搭建方法,其特征在于,其具体实现过程为：1)建立针孔相机成像模型,获得三维车牌四个顶点的坐标值；2)对三维车牌进行空间几何变换,获得变换后的三维车牌四个顶点的坐标值；3)将三维车牌投影变换到二维车牌图像,并获得二维车牌图像四个顶点的坐标值；同时可手动取点并且根据车牌的标准尺寸获得正投影的车牌图像四个顶点的坐标值,所述正投影的车牌图像是指当车牌平面平行于成像平面,并且车牌上下边缘平行于x轴的一种车牌投影图像；该车牌图像的形状为矩形,并且车牌没有变形；4)计算二维图像变成不同形状二维图像的投影矩阵H；5)基于所得的投影矩阵H,将正投影的车牌图像变换成不同形状的车牌图像,建立不同成像形状的车牌图像库。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄晓;              林嘉良;                   保延翔       </td>   <td>中山大学</td>   <td>一种基于多加速度传感器的瑜伽动作识别方法</td>   <td>广东省</td>   <td>CN109740418A</td>   <td>2019-05-10</td>   <td>本发明公开了一种基于多加速度传感器的瑜伽动作识别方法,步骤包括：通过设置在瑜伽运动者身上的加速度计采集加速度计的X、Y、Z三个方向的加速度数据；加速度数据发送至微处理器,微处理器将加速度数据打包通过无线传输技术发送至客户端；客户端收到数据后对数据进行对齐处理,将对齐后的所有数据排成样本数据矩阵,然后对样本数据矩阵进行预处理,提取表征动作的样本特征矩阵；将输入的样本特征矩阵与存储在数据库中动作标准特征矩阵进行匹配,实现瑜伽动作的识别。本发明采集三维加速度数据,构建标准特征序列,来进行动作的识别分析,参数可靠,简单可行,计算复杂度低,对相关硬件的要求较低,实现成本低。</td>   <td>1.一种基于多加速度传感器的瑜伽动作识别方法,其特征在于,所述方法包括如下步骤：S1：首先在瑜伽运动者的双手手腕、双脚脚腕、胸前、额头佩戴内置加速度计的微型处理器的手环、脚环、胸带、头带,采集加速度计的X、Y、Z三个方向的加速度数据；S2：瑜伽运动者运动时,加速度计感知运动者的加速度变化采集加速度数据发送至微处理器,微处理器将加速度数据打包通过无线传输技术发送至客户端；S3：客户端收到来自不同的加速度计采集的数据,利用同步技术对数据进行对齐,将对齐后的所有数据排成样本数据矩阵,然后对样本数据矩阵进行预处理,提取表征动作的样本特征矩阵；S4：手机客户端后台将样本特征矩阵与存储在数据库中动作标准特征矩阵进行匹配,实现瑜伽动作的识别；所述样本数据矩阵的计算过程如下：采集m个加速度传感器的X-Y-Z三个方向的数据,把方向记为k,k∈{x,y,z},每一个传感器的数据进行时间同步后,用三个向量分别表示三个不同方向的数据,把这三个向量都统称为单向数据向量V,把第i个传感器中k方向的单向数据向量记为Vik,则矩阵Di＝(Vix,Viy,Viz)(i∈[1,m])可以表示第i个传感器中三个方向的数据,把所有矩阵Di都统称为三向数据矩阵D,m个传感器总共形成的m个三向数据矩阵,把所有的三向数据矩阵组合起来,记为矩阵S＝(D1,…,Dm),矩阵S称为样本数据矩阵,样本数据矩阵S代表了一个瑜伽者动作的所有加速度数据；所述样本特征矩阵的计算过程如下：把第i个传感器中k方向的单向数据向量Vik进行处理和特征提取后,得到的向量记为Wik,并统称为单向特征向量W；三向特征矩阵E由单向特征向量W组成,第i个传感器的三向特征矩阵Ei＝(Wix,Wiy,Wiz)(i∈[1,m]),把m个三向特征矩阵全部组合起来,形成样本特征矩阵T＝(E1,…,Em),样本特征矩阵T代表了瑜伽运动者动作的所有加速度特征。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         曹惠茹;              钟嗣东;              黄凯帆;              陈荣杰;                   陈玮       </td>   <td>中山大学南方学院</td>   <td>监护设备及监护轮椅</td>   <td>广东省</td>   <td>CN109740531A</td>   <td>2019-05-10</td>   <td>本申请公开了一种监护设备及监护轮椅,监护设备设置在监护轮椅上,监护设备包括：主控芯片、动作识别单元、传感单元、语音播放单元和通信单元,通过动作识别单元进行动作图像追踪和采集,通过传感单元采集生理信号,通过主控芯片对采集到的动作图像和生理信号进行识别,获得情绪信号后,利用语音播放单元生成语音信号,以及利用通信单元将情绪信号发送至智能终端。与现有技术相比,本发明采用了实时图像追踪,并利用多元信息实现多维度情绪识别后,根据识别到的情绪做出对应措施,克服了表征情绪变化的信息单一且监护效果不理想的问题,进而提高情绪识别的准确率以及监护效果。</td>   <td>1.一种监护设备,适用于配置在轮椅上,其特征在于,包括：主控芯片、动作识别单元、传感单元、语音播放单元和通信单元；所述主控芯片与所述动作识别单元、所述传感单元、所述语音播放单元和所述通信单元连接；所述动作识别单元用于进行动作图像追踪,并将追踪到的所述动作图像发送到所述主控芯片；所述传感单元用于采集人体的生理信号并将所述生理信号发送到所述主控芯片；所述主控芯片用于根据所述动作图像和所述生理信号进行情绪信号识别,并将识别到的所述情绪信号发送到所述语音播放单元和所述通信单元；所述语音播放单元用于根据所述情绪信号生成对应的语音信号；所述通信单元用于将所述情绪信号发送至与所述主控芯片绑定的智能终端。</td>   <td>G06K9/00;G06K9/46;A61B5/0205;G01R19/00;A61G5/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭军;                   张凯华       </td>   <td>中山大学</td>   <td>一种网络新词识别方法</td>   <td>广东省</td>   <td>CN106528523B</td>   <td>2019-05-10</td>   <td>本发明提供的新词识别方法能够对重复串和文章关键词、超链接词、标点符号中间的词这些特殊格式的新词进行识别,因此能够很好地适应于网络新词的特点并将其识别出来,实验证明,本发明提供的新词识别方法能够有效地对网络新词进行识别。</td>   <td>1.一种网络新词识别方法,其特征在于：包括以下步骤：S1.使用网络蜘蛛对网页进行抓取,然后从抓取的网页中提取文本信息,并对提取文本信息进行预处理；S2.将文本信息中前后被空格隔开的候选新词提取出来,然后执行步骤S3；将文本信息中重复出现的候选新词提取出来,执行步骤S7；S3计算候选新词i的字串长度L,判断L是否大于1小于4,若是执行步骤S4,否则执行步骤S5；S4.判断候选新词i是否已经存储在词典中,若是则将候选新词i过滤掉,否则通过人工校对后将候选新词i添加入词典中；S5.判断候选新词i能否被分词词典切分,若是,则将候选新词i过滤掉,否则通过人工校对后将候选新词i添加入词典中；S6.令i＝i+1,然后执行步骤S3；S7.统计文本信息中候选新词j的左邻接词个数m和右邻接词个数n,判断m、n是否分别大于设定的阈值,若是则执行步骤S8,否则将候选新词j过滤掉；S8.统计候选新词j的构词强度,若构词强度大于所设定的阈值,则在通过人工校对后将候选新词j添加入词典中；否则将候选新词j过滤掉；S9.令j＝j+1然后执行步骤S7；所述步骤S8中,统计候选新词j构词强度的具体过程如下：(1)统计词首的构词强度：          <Image id="icf0001" he="142" wi="566" file="FDA0001948910490000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,L是词典的词条总数,Head(x,s)是二值函数,定义如下：          <Image id="icf0002" he="95" wi="700" file="FDA0001948910490000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        (2)统计词中的构词强度：          <Image id="icf0003" he="113" wi="518" file="FDA0001948910490000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0004" he="88" wi="700" file="FDA0001948910490000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        (3)统计词尾的构词强度：          <Image id="icf0005" he="136" wi="525" file="FDA0001948910490000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0006" he="99" wi="700" file="FDA0001948910490000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        则候选新词j构词强度表示为：<Image id="icf0007" he="56" wi="700" file="FDA0001948910490000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>x<Sub>1</Sub>、x<Sub>2</Sub>、x<Sub>i</Sub>表示字串中的字。</td>   <td>G06F17/27;G06F16/953</td>  </tr>        <tr>   <td>中国专利</td>   <td>         温武少;                   刘鹏鹏       </td>   <td>中山大学</td>   <td>一种图形化和容器化的虚拟网络环境构建及批量部署方法</td>   <td>广东省</td>   <td>CN109725986A</td>   <td>2019-05-07</td>   <td>本发明涉及云计算和容器技术领域,为图形化和容器化的虚拟网络环境构建及批量部署方法,包括步骤：基于图形化工具的虚拟网络拓扑图设计与虚拟资源属性配置；模板可用性验证以及容器化的虚拟网络环境批量部署；运行时虚拟网络环境的生命周期管理。本发明简化了虚拟网络环境的构建和管理过程,克服了现有技术配置步骤繁琐、应用场景单一的缺陷,加快了批量虚拟网络环境的部署速度,提高了高校或研究机构进行网络教学和研究的效率。</td>   <td>1.一种图形化和容器化的虚拟网络环境构建及批量部署方法,其特征在于,包括步骤：基于图形化工具的虚拟网络拓扑图设计与虚拟资源属性配置；模板可用性验证以及容器化的虚拟网络环境批量部署；运行时虚拟网络环境的生命周期管理。</td>   <td>G06F9/455;G06F9/48;G06F8/60</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赵军鹏;                   卓汉逵       </td>   <td>中山大学</td>   <td>一种基于GAN和迁移学习的病理图片分类装置及方法</td>   <td>广东省</td>   <td>CN109711426A</td>   <td>2019-05-03</td>   <td>本发明公开了一种基于GAN和迁移学习的病理图片分类装置及方法,该装置包括：one-hot编码单元、映射单元、随机采样单元、生成器模块、共享单元、Rule模块、判别器模块、分类器模块、迁移学习模块、深度学习模块以及最终分类器,实现了针对较小样本量数据集的病理图片分类的目的。</td>   <td>1.一种基于GAN和迁移学习的病理图片分类装置,包括：one-hot编码单元,用于对病理图片进行one-hot编码,将病理图片离散特征的取值扩展到欧式空间；映射单元,用于将所述one-hot编码单元输出的稀疏的one-hot编码映射成为非稀疏的高维表示；随机采样单元,用于对多维高斯分布进行随机采样,获取随机噪音；生成器模块,用于将映射单元的输出和随机采样单元输出的随机噪声拼接在一起,并经过全连接和多轮反卷积以及批量标准化操作后,最终生成和数据集既有图片同维度的分布,记作生成图片；共享单元,为判别器模块和分类器模块共享的网络模块,以在训练时梯度反向传播至生成器模块时,使得生成器模块的参数调整不仅接收到判别器模块真假判断的信息,同时也接收到分类器模块对于病理图片分类的分类信息,最终导致生成器不仅生成更加逼真的图片,且结合所述one-hot编码单元的结果指定生成哪一类别的图片；Rule模块,用于针对病理图片的胞核特征和间质特征进行检测,输出每一个胞核特征的概率值和每一个间质特征的概率值,所述胞核特征包括核重叠,毛玻璃核,核沟,核内包涵体,间质特征包括间质钙化,间质砂粒体形成和纤维组织玻璃样变；判别器模块,与共享单元共同组成生成对抗网络的判别；分类器模块,与所述共享单元组合共同完成病理图片类别分类任务；迁移学习模块,对所述生成器模块生成的图片以及当前数据集既有的图片进行特征提取；深度学习模块,用于整合所述分类器模块的输出和迁移学习模块的输出,利用神经网络根据自适应学习的方法将学习的特征作为依据来检测病理图片的分类；最终分类器,用于对所述深度学习模块和Rule模块输出的判别结果进行加权平均,输出最终的病理图片分类结果。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              王伟轩;              于遨波;              陈志鸿;                   朱蔚中       </td>   <td>中山大学</td>   <td>基于层次化特征关系图构建的图像描述方法</td>   <td>广东省</td>   <td>CN109711464A</td>   <td>2019-05-03</td>   <td>本发明涉及计算机视觉识别领域,提出一种基于层次化特征关系图构建的图像描述方法,包括以下步骤：构建训练数据集；将图像输入区块检测模块中,输出区块视觉信息；将图像输入目标检测模块中,输出目标视觉信息；将图像输入文本检测模块中,输出文本视觉信息；将所述三种视觉信息分别输入描述生成器中,分别构建各类型视觉信息与训练图像坐标信息的关系图,并对所述三种视觉信息进行优化；对所述三种视觉信息进行筛选和融合,得到多模态特征；输入递归神经网络中提取特征信息,预测下一个描述单词至生成完整的描述句子。本发明通过对各类型视觉信息进行优化、筛选以及融合,实现对任意输入测试图像进行描述,能够有效提高图像描述的准确性。</td>   <td>1.基于层次化特征关系图构建的图像描述方法,其特征在于：包括以下步骤：S1：收集训练图像、训练图像的坐标信息以及描述训练图像的参考文本,构建训练数据集；S2：将训练图像输入区块检测模块中,输出区块视觉信息；S3：将训练图像输入目标检测模块中,输出目标视觉信息,并根据目标视觉信息包括的目标框坐标信息与训练数据集中的坐标信息比较,计算分类的损失函数L<Sub>1</Sub>,优化目标检测模块参数；S4：将训练图像输入文本检测模块中,输出文本视觉信息,并根据文本视觉信息包括的检测文本与训练数据集中的参考文本比较,计算分类的损失函数L<Sub>2</Sub>,优化文本检测模块参数；S5：将所述区块视觉信息、目标视觉信息和文本视觉信息分别输入描述生成器中,通过构建所述三种视觉信息分别与训练图像坐标信息的关系图,对所述三种视觉信息分别进行优化；S6：利用关注机制分别对优化后的区块视觉信息、目标视觉信息和文本视觉信息进行视觉信息筛选,再输入多元融合模块中进行特征融合,得到多模态特征；S7：将多模态特征和当前时刻的语义信息输入递归神经网络中提取特征信息,并预测下一个单词,并将所预测的结果和训练数据集中的信息比较,计算分类的损失函数L<Sub>3</Sub>,优化递归神经网络中的参数；S8：重复S2～S7,至损失函数L<Sub>1</Sub>,L<Sub>2</Sub>,L<Sub>3</Sub>收敛至某一指定值；S9：将待图像描述的图片输入区块检测模块、目标检测模块和文本检测模块中,分别获得区块视觉信息、目标视觉信息和文本视觉信息,输入描述生成器中通过构建所述三种视觉信息分别与输入图像的坐标信息之间的关系图对所述三种视觉信息进行优化,再利用关注机制对所述三种视觉信息进行视觉信息筛选,并输入到多元融合模块中进行特征融合,最后输入到递归神经网络预测下一个描述单词至生成完整的描述句子。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈楚城;                   戴宪华       </td>   <td>中山大学</td>   <td>一种基于深度学习的铝材表面缺陷检测算法</td>   <td>广东省</td>   <td>CN109711474A</td>   <td>2019-05-03</td>   <td>本发明涉及一种基于深度学习的铝材表面缺陷检测算法,包括：(1)利用摄像头对铝材表面进行拍摄,获取相关数据集并用labelImg工具对图像进行标注,获取标签信息；(2)将图像划分成训练集和测试集,并对训练集进行数据增强；(3)每次同时输入一张有缺陷图像和无缺陷图像及有缺陷图像的标签信息到网络中进行模型训练；(4)将测试图像输入到训练好的铝材表面缺陷检测模型,获取缺陷的位置和对应的类别。本发明的方法可以有效利用有缺陷图像和无缺陷图像,提高模型的泛化能力和检测精度,通过充分利用候选区域周围的上下文信息进一步提高检测性能,利用软的非极大值抑制算法可以提高对密集小缺陷的检测性能,是一种高效的铝材表面缺陷检测算法。</td>   <td>1.一种基于深度学习的铝材表面缺陷检测算法,其特征在于包括如下步骤：(1)图像采集,利用摄像头对铝材表面进行拍摄,获取相关的图像并对图像重命名,如1.jpg,2.jpg,3.jpg,…,M.jpg等,采用labelImg工具对拍摄的图像进行标注,获取图像中关于缺陷的标签,缺陷的标签包含了缺陷在图像中左上角的坐标(x1,y1),右下角的坐标(x2,y2)和缺陷的类别defectN,其中N表示数字,如1,2,3,…等,特别的,如果拍摄的图像中没有缺陷,我们不会用labelImg进行处理,只记录其类别信息norm；(2)图像划分,将图像划分成训练集和测试集两部分,两个部分不存在相同的图像,训练集用来训练检测模型,测试集用来评估检测模型的性能；(3)图像预处理,包括随机上下翻转、随机左右翻转和随机光照改变等,其中随机上下翻转、随机左右翻转和随机光照改变只针对训练集,特别的,当进行随机上下翻转和随机左右翻转的时候,缺陷的坐标信息也需要做出相应的变化；(4)训练检测模型,将经过图像预处理后的训练集中的图像和标签信息输入到铝材表面缺陷检测模型中进行训练,特别的每次同时输入一张有缺陷的图像和一张无缺陷的图像,获取各图像中缺陷的预测框和类别,并与实际的标签信息中的真实框和类别进行对比,计算出定位误差和分类误差,然后采用多学习任务的方法,利用带动量的梯度下降算法进行训练；(5)检测铝材图像,将测试集中的图像输入到训练好的铝材表面缺陷检测模型中进行检测,获得铝材图像中缺陷的位置和类型。</td>   <td>G06K9/62;G06T7/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>              姚嘉豪       </td>   <td>中山大学</td>   <td>一种基于时序密度聚类的大规模数据群组搜索方法</td>   <td>广东省</td>   <td>CN109711478A</td>   <td>2019-05-03</td>   <td>本发明公开了一种基于时序密度聚类的大规模数据群组搜索方法,基于对线上亿个节点的采集,并对采集到的节点进行初步的预处理,构建原始簇及用来表达节点关系的聚类图,根据代表簇节点的连通关系找到合并节点所在的群组。随着算法的每一轮迭代,计算出不同时刻的节点贡献度得分,根据得分值的高低对节点执行范围查询。在保证最终群组发现的正确性的情况下,采用基于时序的密度聚类方法能很好地提高对大规模数据网络的搜索效率。采用本发明的方案能降低I/O及跨域通信量以满足高能物理数据密集型访问和多样化数据查询需求的关键问题。</td>   <td>1.一种基于时序密度聚类的大规模数据群组搜索方法,其特征在于,包括以下步骤：S1：根据给定的节点,定义节点的三种初始状态和原始簇；所述的初始状态包括初始态、未执行态、已执行态；所述的原始簇是已执行的核心点和已执行的核心点的已知的密度相连的邻居节点的集合；S2：根据原始簇的相互关系,构建出原始簇之间的聚类图,定义不同原始簇的代表之间连通程度为state(a,b),所述的state(a,b)包括三种状态：强连通,弱连通,无连通；所述的a,b均为各自原始簇的代表；S3：根据不同原始簇的代表之间连通程度,找到强连通的分量并进行合并；S4：在合并后的原始簇的节点中选择执行范围查询的节点；S5：执行选择的节点且更新聚类图；S6：对S1中的噪声点进行复核,输出复核的噪声点和聚类好的簇。</td>   <td>G06K9/62;G06F16/901</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘玉葆;                   宁志清       </td>   <td>中山大学</td>   <td>一种基于MapReduce的公共交通出行路径规划索引方法</td>   <td>广东省</td>   <td>CN109711633A</td>   <td>2019-05-03</td>   <td>本发明涉及一种基于MapReduce的公共交通出行路径规划索引方法,具体包括以下步骤：S1.确定时态图G的顶点集V上的全序关系,根据确定的顶点集V的全序关系对时态图G进行子图的划分；S2.对于划分的每个子图,分别使用MapReduce集群中的各个计算节点读取其分区数据,然后通过Map函数计算每个子图的弱规范路径,并将结果以映射形式保存在弱规范路径索引集I中；S3.使用Cleanup函数将弱规范路径索引集I中的每个映射转成键值对；S4.使用Reduce函数将键值对中键等于顶点v<Sub>i</Sub>且顶点v<Sub>i</Sub>是起点的映射加入集合I<Sub>out</Sub>中,把键等于顶点v<Sub>j</Sub>且顶点v<Sub>j</Sub>是终点的映射加入集合I<Sub>in</Sub>中,然后按照分布式时间路径索引的定义,对集合I<Sub>out</Sub>和集合I<Sub>in</Sub>中的映射进行排序,最后得到时态图G的分布式时间路径索引。</td>   <td>1.一种基于MapReduce的公共交通出行路径规划索引方法,其特征在于,应用MapReduce计算框架的Map函数、Cleanup函数和Reduce函数进行分布式时间路径索引的构建,具体包括以下步骤：S1.确定时态图G的顶点集V上的全序关系,根据确定的顶点集V的全序关系对时态图G进行子图的划分；S2.对于划分的每个子图,分别使用MapReduce集群中的各个计算节点读取其分区数据,然后通过Map函数计算每个子图的弱规范路径,并将结果以映射形式保存在弱规范路径索引集I中；S3.使用Cleanup函数将弱规范路径索引集I中的每个映射转成键值对；S4.使用Reduce函数将键值对中键等于顶点v<Sub>i</Sub>且顶点v<Sub>i</Sub>是起点的映射加入集合I<Sub>out</Sub>中,把键等于顶点v<Sub>j</Sub>且顶点v<Sub>j</Sub>是终点的映射加入集合I<Sub>in</Sub>中,然后按照分布式时间路径索引的定义,对集合I<Sub>out</Sub>和集合I<Sub>in</Sub>中的映射进行排序,最后得到时态图G的分布式时间路径索引。</td>   <td>G06Q10/04;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         余志;              廖琼华;                   何兆成       </td>   <td>中山大学</td>   <td>一种基于卡口数据的Paramics精准交通仿真场景构建方法</td>   <td>广东省</td>   <td>CN109711026A</td>   <td>2019-05-03</td>   <td>本发明公开了一种基于卡口数据的Paramics精准交通仿真场景构建方法,该方法通过对Paramics插件二次开发构建精准仿真插件,其包括用于消除Paramics随机发车的缺陷,使得仿真车辆的发车指令符合车辆出行路径表中的发车指令的精准发车模块、用于消除Paramics随机指定车辆出行路径选择行为的缺陷,使得仿真车辆的出行路径选择行为符合车辆出行路径表中的出行路径exit序列的精准出行路径选择行为模块。同时和卡口数据提取处理实现精准交通仿真场景构建,该仿真场景具有精准车辆个体生成和路径选择行为的优点,消除了现有技术中模拟交通生成和出行路径选择行为的随机性,提高了交通仿真精度。</td>   <td>1.一种基于卡口数据的Paramics精准交通仿真场景构建方法,其特征在于：该方法步骤如下：S1：采集目标路网参数信息,利用Paramics软件,建立Paramics仿真路网模型；S2：获取车辆出行路径exit序列,获取发车指令,所述的发车指令包括发车小区、发车时刻和消失小区；结合车辆出行路径exit序列、发车指令建立基于卡口数据的车辆出行路径表；S3：利用Paramics向用户开放的API函数,对Paramics插件进行二次开发获得精准仿真插件；所述精准仿真插件包括精准发车模块、精准出行路径选择行为模块；所述精准发车模块根据车辆出行路径表中的发车指令设置仿真车的发车小区、发车时刻和消失小区；精准发车模块用于消除Paramics随机发车的缺陷,使得仿真车辆的发车小区、发车时刻和消失小区符合车辆出行路径表中的发车指令；所述的精准出行路径选择行为模块用于消除Paramics随机指定车辆出行路径选择行为的缺陷,使得仿真车辆的出行路径选择行为符合车辆出行路径表中的出行路径exit序列；S4：将精准仿真插件编译成动态链接库,设置成Paramics仿真路网加载的插件,保存在步骤S1中的Paramics仿真路网模型的文件夹中,从而实现在Paramics仿真路网模型中能加载精准仿真插件；运行加载精准仿真插件的Paramics路网模型,从而实现构建Paramics精准交通仿真场景。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李宇;              林胜义;                   谭洪舟       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于噪声功率谱Gamma分布统计模型的有音区检测方法</td>   <td>广东省</td>   <td>CN105513614B</td>   <td>2019-05-03</td>   <td>本发明公开了一种基于噪声功率谱Gamma分布统计模型的有音区检测(Voice Activation Detection,VAD)方法,属于语音信号处理技术领域。现有基于统计模型的VAD算法通常不考虑语音统计信息,仅仅利用噪声功率谱(Power Spectral Density,PSD)的统计模型来检测活动语音,常用的噪声PSD统计模型为左右对称的Gaussian模型,不能较好地体现噪声PSD分布的长拖尾特性,不利于处理Babble等非平稳噪声。本发明采用伽玛分布(Gamma Distribution)作为噪声分布统计模型,比高斯分布(Gaussian Distribution)和瑞利分布(Rayleigh Distribution)具有更好的长拖尾特性拟合效果,改进后的VAD算法性能更优。</td>   <td>1.一种基于噪声功率谱Gamma分布统计模型的有音区检测方法,其特征在于,包括以下步骤：1)获取含噪语音的信号z(n),并进行分帧处理,得到第k帧含噪的语音z<Sub>k</Sub>(n)；2)计算第k语音帧频率为f<Sub>l</Sub>时的功率谱密度(PSD)估计值P<Sub>zz,k</Sub>(f<Sub>l</Sub>)；3)对PSD估计值P<Sub>zz,k</Sub>(f<Sub>l</Sub>)进行高通滤波,得到高频带的PSD估计值P′<Sub>ZZ,k</Sub>(f<Sub>l</Sub>)；判断当前语音帧是否为纯噪声,若是,则将第k帧噪声频率为f<Sub>l</Sub>的PSD值P<Sub>vv,k</Sub>(f<Sub>l</Sub>)更新为高频带的PSD估计值P′<Sub>ZZ,k</Sub>(f<Sub>l</Sub>)并跳转到步骤4)；否则,则不更新第k帧噪声频率为f<Sub>l</Sub>的PSD值P<Sub>vv,k</Sub>(f<Sub>l</Sub>)跳转到步骤4)；4)对第k帧噪声频率为f<Sub>l</Sub>的PSD估计值P<Sub>vv,k</Sub>(f<Sub>l</Sub>)求指数平均值<Image id="icf0001" he="80" wi="195" file="FDA0001965301260000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>计算PSD估计值P<Sub>vv,k</Sub>(f<Sub>l</Sub>)的平方再取指数平均得噪声方差值var<Sub>vk</Sub>(f<Sub>l</Sub>)；5)用高频带PSD估计值P′<Sub>ZZ,k</Sub>(f<Sub>l</Sub>)和噪声PSD指数平均值<Image id="icf0002" he="79" wi="170" file="FDA0001965301260000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>计算信噪比(SNR)测量值<Image id="icf0003" he="63" wi="134" file="FDA0001965301260000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>并求其指数平均值<Image id="icf0004" he="63" wi="155" file="FDA0001965301260000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>噪声PSD指数平均值<Image id="icf0005" he="79" wi="171" file="FDA0001965301260000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>结合噪声方差值var<Sub>vk</Sub>(f<Sub>l</Sub>)计算有音区检测(VAD)阈值η<Sub>k</Sub>(f<Sub>l</Sub>),再求其指数平均值<Image id="icf0006" he="64" wi="158" file="FDA0001965301260000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>6)SNR测量值得指数平均<Image id="icf0007" he="63" wi="131" file="FDA0001965301260000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>与VAD阈值的指数平均<Image id="icf0008" he="63" wi="125" file="FDA0001965301260000018.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>进行比较,比较结果通过Hangover方法得出最终的VAD判决；所述步骤2)采用低方差频谱估计的Welch方法来估计语音帧的PSD值,该PSD估计值用于SNR测量值和VAD阈值的计算；所述步骤3)通过对PSD估计值P<Sub>zz,k</Sub>(f<Sub>l</Sub>)进行高通滤波,得到高频带的PSD估计值P′<Sub>ZZ,k</Sub>(f<Sub>l</Sub>)；检测当前VAD值是否为0,若VAD＝0,则判断当前帧为纯噪声并更新噪声PSD估计值P<Sub>vv,k</Sub>(f<Sub>l</Sub>),即将高频PSD估计值P′<Sub>ZZ,k</Sub>(f<Sub>l</Sub>)赋值给噪声PSD估计值P<Sub>vv,k</Sub>(f<Sub>l</Sub>)；若VAD≠0,则不更新噪声PSD估计值P<Sub>vv,k</Sub>(f<Sub>l</Sub>),保留上一次更新的噪声PSD估计值P<Sub>vv,k</Sub>(f<Sub>l</Sub>)；所述步骤6)通过SNR测量值的指数平均<Image id="icf0009" he="63" wi="128" file="FDA0001965301260000019.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>与VAD阈值的指数平均值<Image id="icf0010" he="63" wi="125" file="FDA00019653012600000110.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>进行比较来作出VAD判决,若<Image id="icf0011" he="64" wi="302" file="FDA00019653012600000111.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>则VAD＝1,由此判断该语音帧处于有音区,反之,则VAD＝0,认为该语音帧为纯噪声；依据相邻语音帧之间强相关性,进行VAD阈值判断后串接Hangover方法来降低错误拒绝率；噪声PSD指数平均值<Image id="icf0012" he="79" wi="170" file="FDA0001965301260000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>结合噪声方差值var<Sub>vk</Sub>(f<Sub>l</Sub>)计算有音区检测(VAD)阈值η<Sub>k</Sub>(f<Sub>l</Sub>)的具体过程如下：          <Image id="icf0013" he="143" wi="700" file="FDA0001965301260000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        E[P<Sub>vv,k</Sub>(f<Sub>l</Sub>)]与E[P<Sub>vv,k</Sub>(f<Sub>l</Sub>)]<Sup>2</Sup>分别为噪声功率谱密度的均值与均值平方；式中,gaminv为MATLAB中的伽玛分布CDF逆函数来求阈值,表达式为：X＝gaminv(P,A,B)式(5)的A和B分别为伽玛分布的形状参数和尺度参数；P<Sub>FA</Sub>为纯噪声的虚警概率(也就是将噪声误判成语音的概率),定义如下：          <Image id="icf0014" he="87" wi="700" file="FDA0001965301260000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        var<Sub>vk</Sub>为噪声方差,P<Sub>vv,k</Sub>(f<Sub>l</Sub>)和<Image id="icf0015" he="81" wi="164" file="FDA0001965301260000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>分别为噪声的PSD值及其指数平均值。</td>   <td>G10L25/78</td>  </tr>        <tr>   <td>中国专利</td>   <td>         高连如;              苏远超;              孙旭;              李军;                   张兵       </td>   <td>中国科学院遥感与数字地球研究所;中山大学</td>   <td>一种高光谱图像端元提取方法及系统</td>   <td>北京市</td>   <td>CN105976357B</td>   <td>2019-05-03</td>   <td>本发明公开了一种高光谱图像端元提取方法及系统,将高光谱图像端元提取问题中的优化变量对应人工蜂群算法中的蜜源位置,每个蜜源的优化由适应度函数决定。采蜜蜂搜寻最优蜜源,并在搜索完成后将蜜源信息给跟随蜂,跟随蜂第二次搜寻最优蜜源,并利用选取最优蜜源更新之前获得的最优解,当存在预设时间段内没有更新蜜源的采蜜蜂时,将该采蜜蜂转换为侦查蜂,侦查蜂通过将随机选取蜜源的适应度值与作为最优解的蜜源的适应度值比较,对作为最优解的蜜源进行校验。本发明以人工蜂群算法为基础,将端元提取问题转化为组合优化问题的求解过程,充分发挥人工蜂群算法的优点,并通过侦查蜂对最优解的蜜源进行校验,降低优化过程陷入局部最优解的风险。</td>   <td>1.一种高光谱图像端元提取方法,其特征在于,包括：获取高光谱图像中的候选端元,并确定适应度函数,所述适应度函数的表达式如下：          <Image id="icf0001" he="139" wi="375" file="FDA0001924331570000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,fit(X<Sub>i</Sub>)表示第i个蜜源的适应度函数,X<Sub>i</Sub>表示第i个蜜源,X<Sub>i</Sub>＝(x<Sub>i1</Sub>x<Sub>i2</Sub>......x<Sub>iM</Sub>)′,M为所述高光谱图像中的端元数量,f(X<Sub>i</Sub>)为最优问题的目标函数,所述最优问题的目标函数的表达式如下：          <Image id="icf0002" he="62" wi="700" file="FDA0001924331570000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,u表示惩罚系数,对等式两端起到调节平衡的作用；g(d<Sub>i</Sub>)表示距离项,<Image id="icf0003" he="145" wi="255" file="FDA0001924331570000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>d<Sub>i</Sub>为图像中端元与端元之间的欧氏距离,设<Image id="icf0004" he="85" wi="208" file="FDA0001924331570000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示一个集合,d<Sub>i</Sub>＝min||e<Sub>p</Sub>-e<Sub>q</Sub>||<Image id="icf0005" he="70" wi="177" file="FDA0001924331570000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>e<Sub>p</Sub>∈Q,e<Sub>q</Sub>∈Q,e<Sub>p</Sub>表示第p个端元,e<Sub>q</Sub>表示第q个端元；C<Sup>-1</Sup>为<Image id="icf0006" he="170" wi="517" file="FDA0001924331570000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>的反函数,x<Sub>i</Sub>表示像元,<Image id="icf0007" he="150" wi="401" file="FDA0001924331570000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>像元的限制条件为<Image id="icf0008" he="222" wi="543" file="FDA0001924331570000018.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>e<Sub>k</Sub>表示端元,p<Sub>ik</Sub>为端元在像元中所占的比例,ε<Sub>i</Sub>为误差,M为所述高光谱图像中的端元数量,N为所述高光谱图像中像元的数量,i＝1,…..,N,C<Sub>N ,M </Sub>为搜索空间,<Image id="icf0009" he="51" wi="700" file="FDA0001924331570000019.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>c<Sub>i</Sub>对应第i个端元；RMSE表示均方根误差,<Image id="icf0010" he="95" wi="700" file="FDA00019243315700000110.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>L表示所述高光谱图像的波段数,x<Sub>i</Sub>表示所述高光谱图像中的像元,<Image id="icf0011" he="72" wi="59" file="FDA00019243315700000111.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示所述高光谱图像的反混图像中的像元；初始化参数,包括,采蜜蜂数量m,跟随蜂数量m,最大迭代次数maxIter；在可行解空间中随机产生m个可行解,每一个所述可行解作为一只采蜜蜂对应的蜜源；采蜜蜂随机选择一个蜜源作为当前第一蜜源,根据搜寻函数在所述当前第一蜜源的邻域内搜寻第一新蜜源,利用所述适应度函数计算所述第一新蜜源的适应度值,并选择所述当前第一蜜源和所述第一新蜜源中适应度值大的替换所述当前第一蜜源；采蜜蜂分享蜜源信息给跟随蜂；跟随蜂按照跟随概率选择采蜜蜂,将该采蜜蜂最新的当前第一蜜源作为当前第二蜜源,根据所述搜寻函数在所述当前第二蜜源的邻域内二次搜寻第二新蜜源,利用所述适应度函数计算所述第二新蜜源的适应度值,并选择所述当前第二蜜源和所述第二新蜜源中适应度值大的蜜源替换所述当前第二蜜源；利用跟随蜂选取的蜜源更新之前获得的最优解；判断m个采蜜蜂中在预设时间段内是否有没有更新蜜源的采蜜蜂；如果存在没有更新蜜源的采蜜蜂,则将所述预设时间段内没有更新蜜源的采蜜蜂转换为侦查蜂；侦查蜂在搜索空间中随机选取一个蜜源作为当前第三蜜源,根据所述搜寻函数在所述当前第三蜜源的邻域内搜寻第三新蜜源,利用所述适应度函数计算所述第三新蜜源的适应度值；判断所述第三新蜜源的适应度值是否小于所述第三蜜源的适应度值；如果小于所述第三蜜源的适应度值,则将跟随蜂选取的蜜源作为当前最优解,并判断当前迭代次数是否达到所述最大迭代次数maxIter；如果不小于所述第三蜜源的适应度值,则将该侦查蜂转换为采蜜蜂,继续搜寻新蜜源；如果不存在没有更新蜜源的采蜜蜂,则判断当前迭代次数是否达到所述最大迭代次数maxIter；如果达到所述最大迭代次数maxIter,则输出端元提取结果；如果没有达到所述最大迭代次数maxIter,则采蜜蜂继续搜寻新蜜源。</td>   <td>G06T7/00;G06T7/49;G06N3/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄义成;                   倪江群       </td>   <td>中山大学</td>   <td>一种在特征区域嵌入周期图的抗旋转水印方法</td>   <td>广东省</td>   <td>CN109685711A</td>   <td>2019-04-26</td>   <td>本发明提供一种在特征区域嵌入周期图的抗旋转水印方法,包括水印嵌入方法和水印提取方法,水印嵌入方法通过提取出图像的红色通道分量和蓝色通道分量,对红色通道分量做特征点检测,确定水印待嵌入区域,选择蓝色通道分量进行预处理计算图像嵌入系数JND,生成处理后的蓝色通道分量AII,进而得到彩色水印图像；同理,通过水印提取方法可提取出水印。本发明提供的一种特征区域嵌入周期图的抗旋转水印方法,有效结合了特征点定位的周期图水印算法,更好地抵抗旋转攻击,在旋转角较大时,仍然能够准确定位到嵌有水印的图像区域,从而检测出水印信息,提高水印算法的鲁棒性。</td>   <td>1.一种在特征区域嵌入周期图的抗旋转水印方法,其特征在于：包括水印嵌入方法A和水印提取方法B；其中,所述水印嵌入方法A包括以下步骤：AS1：提取RGB图像的红色通道分量A和蓝色通道分量A；AS2：对红色通道分量A进行Harris特征点检测,计算,确定水印待嵌入区域；AS3：选择蓝色通道分量A作为嵌入水印的宿主图像,对其进行预处理以减少图像自带的随机周期性并计算图像的最小可视差,得到嵌入系数JND；AS4：生成周期模板图,根据确定的待嵌入区域和嵌入系数JND,将旋转后的周期模板图分别嵌入到宿主图像中,生成处理后的蓝色通道分量AII；AS5：将嵌入水印后的蓝色通道分量AII替换原先的蓝色通道分量,即可得到最终的彩色水印图像；所述水印提取方法B包括以下步骤：BS1：提取待测图像的红色通道分量B和蓝色通道分量B；BS2：将红色通道分量B进行Harris特征点检测,确定待检测水印区域；BS3：在蓝色通道分量B对应的n个待测水印区域上,分别进行滤波预处理,提取周期图信号Ipre；BS4：对周期图信号Ipre进行自相关操作,得到图像R<Sub>Ipre</Sub>,并利用高斯滤波器对自相关矩阵图像进行二值化；BS5：利用hough直线检测,将二值化后的每一块待测区域的点连接成线,确定每一块待测区域检测出的直线角度；BS6：将得到的直线角度信息转化为比特位信息,最终通过Hamming解码为水印信息。</td>   <td>G06T1/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭雁红;              谢雪颜;              陈丽婵;              林枫;              刘玲君;              李增宏;              罗钻华;                   郑如远       </td>   <td>佛山市第一人民医院(中山大学附属佛山医院)</td>   <td>一种非接触式静脉图像采集方法及装置</td>   <td>广东省</td>   <td>CN109684926A</td>   <td>2019-04-26</td>   <td>本发明公开了一种非接触式静脉图像采集方法及装置,通过三维立体B超采集含有静脉的身体部位的B超手背、前臂的B超图像,并通过边缘检测算法扫描低频平滑子图得到血管边缘图像,从而获取到清晰的静脉图像,使采集的静脉图像对比度高,可有效地去除噪声,不出现大的失真,改善图像的清晰度,增强静脉图像的对比度,减少了甚至消除了静脉图片中的白噪声。</td>   <td>1.一种非接触式静脉图像采集方法,其特征在于,所述方法包括以下步骤：步骤1,通过B超采集含有静脉的身体部位的B超图像；步骤2,经过小波变换提取B超图像的低频平滑子图；步骤3,通过边缘检测算法扫描低频平滑子图得到血管边缘图像；步骤4,对血管边缘图像进行灰度平滑处理得到静脉区域子图；步骤5,存储静脉区域子图到数据库中。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         丁琪伟;              陈龙;                   轩辕哲       </td>   <td>中山大学</td>   <td>一种基于轻量神经网络的超高速实时目标检测系统及检测方法</td>   <td>广东省</td>   <td>CN109685017A</td>   <td>2019-04-26</td>   <td>本发明涉及无人驾驶的技术领域,更具体地,涉及一种基于轻量神经网络的超高速实时目标检测系统及检测方法。一种基于轻量神经网络的超高速实时目标检测系统,包括视频流数据采集模块、神经网络训练模块、图像处理与检测模块、检测结果反馈模块；本发明操作流程简单、易于实施、成本低、效率高。</td>   <td>1.一种基于轻量神经网络的超高速实时目标检测系统,其特征在于,包括视频流数据采集模块、神经网络训练模块、图像处理与检测模块、检测结果反馈模块；视频流数据采集模块：采集车辆正前方道路的路况图像信息,将采集到的视频流以帧为单位通过数据线传输到图像处理与检测模块,供其进行下一步处理；神经网络训练模块：选定合适的轻量神经网络对需要检测的目标物体使用数据集进行调参,训练,最终生成相应的网络权重模型；利用验证数据集对模型进行评估与选择,并选择出最优的网络权重模型；使用测试数据集来测试选出的网络模型的识别精度,以测试误差作为泛化误差的近似；图像处理与检测模块：在嵌入式硬件平台上预先部署相应的轻量神经网络架构,并将在神经网络训练模块中训练得到的神经网络权重移植到本模块的硬件平台上；将视频流数据采集模块传输过来的图像送入轻量神经网络中,神经网络在权重文件的配置下对该图像进行物体种类识别和边框定位；检测结果将被输入检测结果反馈模块；检测结果反馈模块：在此模块中会将获得的检测结果通过输入输出设备以视频流的形式进行展示。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         秦建强;                   黄方军       </td>   <td>中山大学</td>   <td>一种基于多个二维直方图修改的可逆信息隐藏及提取方法</td>   <td>广东省</td>   <td>CN109671010A</td>   <td>2019-04-23</td>   <td>本发明公开了一种基于多个二维直方图修改的可逆信息隐藏方法,包括S1：将灰度图像以像素对为单位划分为互不相邻的两个集合；S2：利用像素对相邻的像素来对像素对中的像素进行预测,得到预测误差,组成预测误差对；S3：对每一个预测误差对,计算其局部复杂度,再将所有预测误差对的局部复杂度按照从低到高进行排列,分为16个预测误差对个数相同的集合,构造16个二维预测误差直方图；S4：从预先设计的8张映射图中选择一张作为该预测直方图所有预测误差对的映射方式,并根据相应的映射规则进行信息的嵌入。本发明能够高效嵌入信息并能正确地提取出所嵌入信息、无损地恢复原始图像,与在嵌入信息后提高了图像的视觉质量,即峰值信噪比增大。</td>   <td>1.一种基于多个二维直方图修改的可逆信息隐藏方法,其特征在于,应用在灰度图像中,包括以下步骤：S1：将灰度图像中相邻的两个像素作为一个像素对,将灰度图像以像素对为单位划分为互不相邻的两个集合,阴影集合和空白集合；S2：对每一个像素对,利用所述像素对相邻的像素来对所述像素对中的像素进行预测,得到所述像素对中的像素预测值,所述预测值与原始像素值相减得到预测误差,两个相邻的预测误差组成一个预测误差对；S3：对每一个预测误差对,计算其局部复杂度,再将所有预测误差对的局部复杂度按照从低到高进行排列,并确定n个阈值,n为大于2的整数,根据n个阈值将所有预测误差对划分为n+1个预测误差对个数相同的集合,并根据n+1个集合构造n+1个二维预测误差直方图；S4：根据所要嵌入信息的容量,每一集合从预先设计的8张映射图中选择一张作为该预测直方图所有预测误差对的映射方式,并根据相应的映射规则进行信息的嵌入。</td>   <td>G06T1/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄汉栋;              印鉴;                   高静       </td>   <td>中山大学;广东恒电信息科技股份有限公司</td>   <td>一种基于特征提取的深度学习情感分类方法</td>   <td>广东省</td>   <td>CN109670169A</td>   <td>2019-04-23</td>   <td>本发明提供一种基于特征提取的深度学习情感分类方法,该方法通过G网络中的词嵌入层以及双向LSTM的深度学习模型学习到句子的上下文语义特征,能够找到对句子情感极性判断影响比较大的词,提出出来作为有用信息。相应提出对句子极性判断不那么重要的词作为无用信息。通过将有用信息和无用信息输入到C网络中进行学习,使得分类器能够有较高的辨识度。通过在相应数据集上的实验表明,本发明对比之前的情感分类方法,有较大提升。</td>   <td>1.一种基于特征提取的深度学习情感分类方法,其特征在于,包括以下步骤：S1：建立用于生成语义特征以及特征提取的深度学习网络模型G；S2：建立基于有用信息和无用信息的情感分类器C；S3：模型训练与测试。</td>   <td>G06F17/27;G06N3/04;G06N3/08;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         田珂珂;              印鉴;                   高静       </td>   <td>中山大学;广东恒电信息科技股份有限公司</td>   <td>基于迭代膨胀卷积神经网络的病历文本命名实体识别方法</td>   <td>广东省</td>   <td>CN109670179A</td>   <td>2019-04-23</td>   <td>本发明提供一种基于迭代膨胀卷积神经网络的病历文本命名实体识别方法,该方法在医疗电子病历数据集CCKS2017进行命名实体识别,输入一段中文电子病历文本,用迭代膨胀卷积神经网络和条件随机场作为模型架构,用汉字偏旁作为特征,来提取文本中的命名实体,如疾病名称、检查手段等。</td>   <td>1.一种基于迭代膨胀卷积神经网络的病历文本命名实体识别方法,其特征在于,包括以下步骤：S1：建立用于命名实体识别的迭代膨胀卷积神经网络和条件随机场的模型；S2：建立模型的损失函数；S3：进行模型的训练,并在测试集上测试。</td>   <td>G06F17/27;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张耿伟;                   吴维刚       </td>   <td>中山大学</td>   <td>一种基于联盟链网络的去中心化跨集群资源共享方法</td>   <td>广东省</td>   <td>CN109670859A</td>   <td>2019-04-23</td>   <td>本发明提供一种基于联盟链网络的去中心化跨集群资源共享方法,基于联盟链网络中的联盟链网络技术,在集群之间建立一个去中心化的分布式账本,每个集群都有各自的资源额度,限制该集群所能使用其他集群的资源上限,提高集群间的公平性。同时,通过将资源竞价过程中的重要字段写入联盟链网络中来保证交易过程的不可篡改和可追溯,实现去中心化且可信任的资源竞价过程。集群根据资源利用率计算竞价系数,资源需求方选取竞价系数最小的集群。通过这种方式,使得整个集群联邦的资源使用和工作负载更加均衡。</td>   <td>1.一种基于联盟链网络的去中心化跨集群资源共享方法,其特征在于,包括以下步骤：S10.在集群联邦之间建立一个联盟链网络,在联盟链网络中建立一个集群间的分布式账本,记录每个集群的资源额度,集群的角色分为资源需求方与资源提供方,每个集群根据各自的资源利用率在两个角色之间切换；S20.每个集群设置本集群的两个资源利用率参数,期望利用率α和最大利用率β,整个集群联邦设置一个资源最低限额,当集群的资源额度小于最低限额时,则不能作为资源需求方发起竞价,只能作为资源提供方先赚取资源额度,最低限额应不小于零；S30.每个集群实时监控各自集群的资源利用率,当集群的资源利用率大于β时,该集群作为资源需求方。若当前的资源可用额度大于集群联邦限定的最低限额时,将所需的资源描述广播到其他集群,发起竞价；当集群的资源利用率小于等于β时,该集群作为资源提供方,监听其他集群发出的资源请求。当资源需求方的资源可用额度大于最低限额时,资源提供方根据α计算定价参数,通过计算参数参与竞价；资源需求方与资源提供方进行匹配,资源提供方的可用资源满足资源需求方的需求,且资源需求方选取定价参数队列中最小的定价参数并与该定价参数的资源提供方进行匹配,双方进行交易；S40.交易结束后,进行资源费用结算,资源需求方自身的资源额度上支付资源费用给资源提供方,资源提供方获得资源费用。</td>   <td>G06Q30/02;G06Q30/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              邹兵兵;              朱雄泳;              陈荣军;              李智文;                   黄登       </td>   <td>中山大学;中山大学花都产业科技研究院</td>   <td>基于PatchMatch和秩最小化算法合成高动态图像的方法</td>   <td>广东省</td>   <td>CN106504198B</td>   <td>2019-04-23</td>   <td>本发明公开了一种基于PatchMatch和秩最小化算法合成高动态图像的方法,首先,归一化输入图像集,对图像集使用PatchMatch算法实现图像配准；接着,对配准后的图像集使用伽马曲线对图像集进行辐射校准,然后使用秩最小化算法得到批量的对齐图像；最后,将对齐图像集合成得到目标的高动态(high dynamic range,HDR)图像。本发明利用秩最小化和PatchMatch算法的最新研究成果,能够得到有效去除融合后的HDR图像中的伪影和模糊问题。</td>   <td>1.一种基于PatchMatch和秩最小化算法合成高动态图像的方法,其特征在于,它包括：a)对输入图像集进行归一化处理,并使用PatchMatch算法实现图像配准；b)对配准后的图像集使用秩最小化算法得到批量的对齐图像；c)将对齐图像集进行合成得到目标高动态图像；所述步骤a)与步骤b)之间还包括：步骤ab)对配准后的图像集使用伽马曲线对图像集进行辐射校准；所述步骤a)包括：a1)定义输入图像中任意一点的像素值为该点的灰度值M,对灰度值M进行归一化,I表示为归一化之后的灰度值：I＝M/255 ⑴a2)现假设源图像为S,参考图像R,源图像和参考图像合成的图像为L,PatchMatch算法就是一个以参考图像为模板,配准源图像生成图像L的过程；由于PatchMatch算法是处理一对图像,现假设输入图像为I<Sub>1</Sub>...I<Sub>N</Sub>,以N＝5为例,首先令I<Sub>3</Sub>为参考图像R,则I<Sub>3</Sub>和I<Sub>4</Sub>作为其源图像S,然后令I<Sub>2</Sub>和I<Sub>4</Sub>作为参考图像R,I<Sub>1</Sub>和I<Sub>5</Sub>分别作为I<Sub>2</Sub>和I<Sub>4</Sub>对应的源图像S；a3)现定义PatchMatch算法合成图像L的二次函数：          <Image id="icf0001" he="94" wi="700" file="FDA0001901455830000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中τ为灰度映射函数；Ω为图像R和图像S的图像域,i为图像域上的任意一个像素点,n(i)为以i中心p×p的邻域,其中p为邻域的大小,p＝7,故j是邻域n(i)上的像素点,R(i)是图像R上的第i个像素点,S(i+u(j))是图像S上的第(i+u(j))个像素点,其中u(j)表示从图像L上的像素点j映射到图像S的偏移量；α为一个归一化的因子<Image id="icf0002" he="159" wi="687" file="FDA0001901455830000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中w<Sub>τ</Sub>和w<Sub>u</Sub>为一对权重函数,w<Sub>τ</Sub>(i)表示图像R中像素点i映射到图像L的比重,w<Sub>u</Sub>(j)表示图像S中像素点j加上偏移量u(j)映射到图像L的比重；a4)灰度映射函数τ定义如下：          <Image id="icf0003" he="112" wi="700" file="FDA0001901455830000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中灰度映射函数的导数τ′≥0,τ(·)∈[0,1],i为图像域Ω上的像素点,故L(i)表示为图像L上的第i个像素点；使用迭代重加权最小二乘法算法求解灰度映射函数,故灰度映射函数τ的目标函数转化为：          <Image id="icf0004" he="103" wi="700" file="FDA0001901455830000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中求解目标函数τ过程中,τ和权重因子ω更新为：          <Image id="icf0005" he="101" wi="700" file="FDA0001901455830000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0006" he="164" wi="700" file="FDA0001901455830000025.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中n表示第n次迭代,max(·,·)表示为两者当中的最大值,δ＝10<Sup>-10</Sup>；a5)权重函数w<Sub>τ</Sub>定义如下：          <Image id="icf0007" he="120" wi="700" file="FDA0001901455830000031.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中当图像R中图像域上亮度太暗或者太亮,即图像域的像素点的灰度值小于3/255或者大于252/255时,像素点将会被剪切,否则,就不剪切；a6)权重函数w<Sub>μ</Sub>定义如下：          <Image id="icf0008" he="229" wi="700" file="FDA0001901455830000032.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中d(·,·)表示输入参数之间的空间距离；υ<Sub>1</Sub>,υ<Sub>2</Sub>为两个归一化参数,分别取对应空间距离的75百分位数；a7)对于参数x和y,d(x,y)＝||x-y||<Sup>2</Sup>,而对于<Image id="icf0009" he="128" wi="459" file="FDA0001901455830000033.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示为：在图像R和图像S的图像域上的任意一个像素点i,取以i为中心,大小为p×p的邻域,得到图像块<Image id="icf0010" he="87" wi="83" file="FDA0001901455830000034.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>和<Image id="icf0011" he="92" wi="107" file="FDA0001901455830000035.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>然后图像块<Image id="icf0012" he="87" wi="80" file="FDA0001901455830000036.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>经过灰度映射函数得到<Image id="icf0013" he="111" wi="195" file="FDA0001901455830000037.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>而图像块<Image id="icf0014" he="89" wi="76" file="FDA0001901455830000038.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>相对于i平移了u(i)得到<Image id="icf0015" he="104" wi="175" file="FDA0001901455830000039.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>最后求两者的空间距离；同理可得<Image id="icf0016" he="140" wi="534" file="FDA00019014558300000310.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中τ<Sup>-1</Sup>(·)表示灰度映射函数的逆函数；a8)通过上面定义的函数可知,PatchMatch算法实际上就是求解二次函数的过程；输入图像R和图像S,并分别对两幅图像进行向下采样,分别得到图像R和图像S的金字塔图像集,从金字塔顶端向下迭代,求得在对应每层金字塔图像下合成的图像L和灰度映射函数τ,将此结果作为下一次的迭代的初始值,当迭代完成后即可得到最终的配准图像L,依照此方法,即可以得到输入图像I<Sub>1</Sub>...I<Sub>N</Sub>配准后的图像L<Sub>1</Sub>...L<Sub>N</Sub>；所述步骤b)包括：b1)首先先定义矩阵Y<Sub>m×n</Sub>部分奇异值阈值算子<Image id="icf0017" he="73" wi="91" file="FDA0001901455830000041.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>          <Image id="icf0018" he="94" wi="700" file="FDA0001901455830000042.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中β为大于0的常参数；Σ<Sub>Y1</Sub>＝diag(σ<Sub>1</Sub>,0,…,0),Σ<Sub>Y2</Sub>＝diag(0,σ<Sub>2</Sub>,…,σ<Sub>l</Sub>),其中l＝min(m,n),U,V,Σ<Sub>Y</Sub>对应为矩阵Y的奇异值分解矩阵,其中Σ<Sub>Y</Sub>＝Σ<Sub>Y1</Sub>+Σ<Sub>Y2</Sub>；假设向量X,则S<Sub>β</Sub>[X]＝{sign(X)·max(0,|X|-β)}；b2)定义正交映射算子<Image id="icf0019" he="79" wi="163" file="FDA0001901455830000048.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image><Image id="icf0020" he="69" wi="339" file="FDA0001901455830000043.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>假设输入图像B,<Image id="icf0021" he="84" wi="288" file="FDA0001901455830000044.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>那么Y<Sub>ij</Sub>：          <Image id="icf0022" he="156" wi="700" file="FDA0001901455830000045.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中i,j表示图像上第i行、第j列的像素,<Image id="icf0023" he="57" wi="54" file="FDA0001901455830000046.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示为输入图像B的有效像素；b3)输入经PatchMatch算法配准后的图像L,并将所有的输入图像向量化得到矩阵D,现引入低秩矩阵A、噪声矩阵E、拉格朗日乘子矩阵Z和正参数ε变量；b4)初始化变量A<Sub>0</Sub>＝D,E<Sub>0</Sub>＝Z<Sub>0</Sub>＝0,<Image id="icf0024" he="96" wi="357" file="FDA0001901455830000047.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>其中m,n为输入图像L的行和列,ε<Sub>0</Sub>＝1.25/norm(D,'fro'),其中norm(D,'fro')表示为求矩阵D的Frobenius范数,ρ＞1,外迭代次数t＝0,其中A<Sub>0</Sub>表示为矩阵A在第0次外迭代的状态,E<Sub>0</Sub>,Z<Sub>0</Sub>,ε<Sub>0</Sub>亦是如此；b5)初始化内迭代中间变量b<Sub>0</Sub>＝1,G<Sub>0</Sub>＝A<Sub>t</Sub>,A<Sub>t,0</Sub>＝A<Sub>t</Sub>,内迭代次数k＝0,其中b<Sub>0</Sub>,G<Sub>0</Sub>为中间变量b,G在第0次内迭代下状态,A<Sub>t,0</Sub>表示为矩阵A在第t次外迭代,0次内迭代的状态,下面的A<Sub>t,k</Sub>亦是如此；b6)低秩矩阵更新为：          <Image id="icf0025" he="62" wi="700" file="FDA0001901455830000051.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中<Image id="icf0026" he="56" wi="51" file="FDA0001901455830000052.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示为输入图像的掩膜模板,当图像域的灰度值过暗或者过亮,也就是灰度值小于2/255或者大于253/255时,掩膜模板对应的像素点的值取为0,否则取为1；中间变量更新为：          <Image id="icf0027" he="183" wi="555" file="FDA0001901455830000053.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0028" he="138" wi="700" file="FDA0001901455830000054.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        b7)若内循环次数k达到最大内循环次数,此处最大内循环次数为5,或者A<Sub>t,k+1</Sub>已经收敛时,所述收敛是指也就是A<Sub>t,k+1</Sub>和A<Sub>t,k</Sub>已经近似相等,则退出循环,否则k＝k+6,跳转到b6)；b8)内循环迭代结束之后,低秩矩阵更新为：A<Sub>t+1</Sub>＝A<Sub>t,k+1</Sub> ⒁中间变量更新为：          <Image id="icf0029" he="67" wi="700" file="FDA0001901455830000055.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0030" he="75" wi="700" file="FDA0001901455830000056.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        ε<Sub>t+1</Sub>＝ρε<Sub>t</Sub> ⒄b9)若内循环次数t达到最大内循环次数,此处最大内循环次数为50,或者D-A<Sub>t</Sub>-E<Sub>t</Sub>已经收敛时,所述收敛是指A<Sub>t</Sub>+E<Sub>t</Sub>与D已经近似相等,则退出循环,否则t＝t+1,跳转到b5)；b10)内循环迭代结束之后：得到目标低秩矩阵和噪声矩阵：A＝A<Sub>t+1</Sub> ⒅E＝D-A ⒆b11)最后对得到的低秩矩阵和噪声矩阵进行调整m×n大小的图像,即可得到输入图像L对应的低秩图像和噪声图像；所述步骤c)包括：c1)输入对齐后的图像集A,将图像A合成目标HDR图像：          <Image id="icf0031" he="168" wi="656" file="FDA0001901455830000061.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中nImg表示为输入图像的数量,x∈{r,g,b},r,g,b为彩色图像的三个通道；A(x)和H(x)分别输入图像和HDR图像的x通道图像,最后融合H(x)即可得到HDR图像H。</td>   <td>G06T5/00;G06T7/70</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张年崧;              杨嵩毅;              符顺;                   陈翔       </td>   <td>中山大学</td>   <td>基于计算机视觉成像的工业型材几何尺寸自动检测方法</td>   <td>广东省</td>   <td>CN109658402A</td>   <td>2019-04-19</td>   <td>本发明公开了一种基于计算机视觉成像的工业型材几何尺寸自动检测方法,包括步骤：从输入源获取图像,自动检测型材位置并提取感兴趣区；对提取图像进行检测前预处理,改善图片动态范围并得到二值图；分别提取内外轮廓,并做多边形拟合；对于外轮廓,判断多边形顶点处于直线或曲线上,对于内轮廓,判断其为孔或槽；对于外轮廓,根据判断结果计算直线与曲线参数,对于内轮廓,计算孔的位置与半径,槽的位置、长宽与倾斜度；最后,输出检测参数,并将检测结果标注于原图,展现给用户。本发明自动识别图片中工业型材位置,运用计算机视觉和图像处理技术,精确检测输出工业型材的直线、曲线、内部钻孔、锯槽与铣槽各项参数,并将结果标注于原图上。</td>   <td>1.一种基于计算机视觉成像的工业型材几何尺寸自动检测方法,其特征在于,所述的工业型材几何尺寸自动检测方法包括以下步骤：S1、从输入源获取图像,自动检测型材位置并提取感兴趣区域；S2、对提取的图像感兴趣区域进行检测前预处理,改善图像动态范围并得到二值图；S3、分别提取内外轮廓,并做多边形拟合；S4、对于外轮廓,判断多边形顶点处于直线或曲线上,对于内轮廓,判断其为孔或槽；S5、对于外轮廓,根据判断结果计算直线与曲线参数,对于内轮廓,计算孔的位置与半径,槽的位置、长宽与倾斜度；S6、输出检测参数,并将检测结果标注于原图,展现给用户。</td>   <td>G06T7/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;                   冯燊明       </td>   <td>中山大学</td>   <td>基于堆叠沙漏网络的关键特征区域匹配人脸识别方法</td>   <td>广东省</td>   <td>CN109657595A</td>   <td>2019-04-19</td>   <td>本发明涉及计算机视觉识别技术领域,提出一种基于堆叠沙漏网络的关键特征区域匹配人脸识别方法,包括以下步骤：采集训练集,并进行预处理；对输入人脸图片进行预处理；将图片输入堆叠沙漏网络中进行特征提取,输出人脸关键点热图和关键点位置信息；对原图片进行关键区域裁剪,并从训练集中选取三元组；将关键区域进行特征提取,得到特征图F；将特征图F输入嵌入层中得到标签E；根据特征图的L2范数计算三元损失函数,重复上述步骤至三元损失函数收敛；将待识别的人脸图片输入完成训练的堆叠沙漏网络和人脸识别模块中,输出识别的标签E。本发明引入堆叠沙漏网络进行人脸识别,排除非关键区域的影响,有效提高人脸识别效果,具有较强的鲁棒性。</td>   <td>1.基于堆叠沙漏网络的关键特征区域匹配人脸识别方法,其特征在于,包括以下步骤：S1：采集人脸图片作为训练集,并对训练集的图片进行预处理；S2：将训练集的任意一张人脸图片输入堆叠沙漏网络中进行特征提取,输出人脸关键点热图和关键点位置信息；S3：根据所述关键点位置信息对原输入人脸图片进行区域裁剪得到关键区域,并从训练集中选取三元组；S4：将关键区域输入人脸识别模块中的卷积神经网络进行特征提取,得到特征图F；S5：对特征图F求取L2范数,然后通过人脸识别模块中的嵌入层,输出完成识别的人脸图片的标签E；S6：根据L2范数计算三元损失函数,通过梯度下降法对三元损失函数进行优化；S7：重复S2～S6至三元损失函数收敛,完成堆叠沙漏网络和人脸识别模块的训练；S8：将待识别的人脸图片输入堆叠沙漏网络中进行特征提取,裁剪图片的关键区域,然后输入卷积神经网络中进行特征提取,最后通过嵌入层输出识别的人脸图片标签。</td>   <td>G06K9/00;G06K9/46;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈尧钧;              印鉴;                   高静       </td>   <td>中山大学;广东恒电信息科技股份有限公司</td>   <td>一种基于深度学习的抽取式机器阅读理解模型的建立方法</td>   <td>广东省</td>   <td>CN109657246A</td>   <td>2019-04-19</td>   <td>本发明提供一种基于深度学习的抽取式机器阅读理解模型的建立方法,该方法使用卷积代替了广泛应用在机器阅读理解的LSTM、GRU等RNN的变种,不同于RNN当前时刻的计算依赖上一时刻,卷积是可以并行计算的,这使得模型不论训练还是推理速度都优于使用RNN变种的模型；在使用注意力机制捕捉关键信息时,使用到了多头注意力机制,使得对于文章这样的长文本能够捕捉所有相关的信息,进一步提升模型的准确率。</td>   <td>1.一种基于深度学习的抽取式机器阅读理解模型的建立方法,其特征在于,包括以下步骤：S1：对文章和问题的句子进行分词；S2：为每个文章词设置一个精准匹配特征,表示该文章词是否出现在问题中,如果出现则该特征置为1,否则置为0；S3：把单词映射成词表当中对应的词向量,得到每个单词词级别的表示；S4：把单词的每个字母映射成字符表当中对应的字符向量,输入到卷积神经网络训练得到固定大小的向量,得到每个单词字符级别的表示；S5：将文章和问题的每个单词对应的词级别和字符级别的表示拼接在一起,分别输入到两层highway networks中,输出即为文章词和问题词的特征向量表示；S6：将文章和问题的词向量表示分别通过多层卷积进行处理,从而融合每个词周围的上下文信息去调整每个词的表示；S7：将S6得到的文章和问题的词向量表示通过文章-问题注意力机制,得到每个文章词对应相关的问题词表示；S8：将S6得到的文章和问题的词向量表示通过问题-文章多头注意力机制,得到每个问题词对应相关的文章词表示；S9：将S6得到的问题的词向量表示利用注意力机制,得到每个问题词对于整个问句表达的重要性占比,通过这个重要性占比与S8得到的每个问题词对应相关的文章词表示进行加权求和,从而得到一个与问题长度无关的向量,该向量整合了和问题相关的文章词信息；S10：将S6得到的文章词表示、S2得到的每个文章词对应的精准匹配特征、S7得到每个文章词对应相关的问题词向量、S6和S7得到的每个词向量表示对应元素相乘的结果、S6的每个文章词向量表示和S9得到的向量对应元素相乘的结果进行合并,再次输入到多层卷积进行处理,从而融合每个文章词周围的上下文信息和精准匹配特征及结合S7、S9的注意力计算结果去整合每个词的表示；S11：将S10得到的文章词向量表示通过文章-文章多头注意力机制,得到每个文章词在全文范围内的对应相关的上下文表示；S12：将S10得到的文章词表示与S11得到的每个文章词对应相关的上下文表示进行合并,输入到多层卷积进行处理,从而融合每个词周围上下文和全局范围内的上下文去调整每个文章词的表示；S13：将S12得到的文章词表示进行线性变换后,经过softmax函数进行归一化转化为一个概率分布,该分布代表了文章中每个单词是标准答案短语第一个单词的概率,优化该概率分布,作为模型的优化目标之一,使得标准答案短语第一个单词在原文的位置对应的概率相应增大,即通过代表标准答案短语的第一个单词在原文位置的One-hot向量和该概率分布计算交叉熵损失,得到L<Sub>s</Sub>；S14：预测答案短语最后一个单词的位置需要将预测答案短语第一个单词的位置的信息考虑在内,所以对S12得到的文章词表示再一次经过多层卷积进行处理得到新的文章词表示,最后通过线性变换和softmax函数归一化得到一个概率分布,该分布代表了文章每个单词是标准答案短语最后一个单词的概率,优化该概率分布,作为模型的优化目标之一,使得标准答案短语最后一个单词在原文的位置对应的概率相应增大,即通过代表标准答案短语的最后一个单词在原文位置的One-hot向量与该概率分布计算交叉熵损失,得到L<Sub>e</Sub>；S15：将S13和S14的两个优化目标相结合,即把S13的L<Sub>s</Sub>和S14的L<Sub>e</Sub>相加,就得到损失函数,使用基于梯度下降原理的优化器来进行优化训练；S16：取S13和S14两个概率分布最大值的位置,分别对应了模型预测的答案短语的第一个单词和最后一个单词在原文中的位置,这两个位置区间的单词序列即为模型预测的答案短语。</td>   <td>G06F17/27;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖丹宇;                   陈龙       </td>   <td>中山大学</td>   <td>基于半监督学习和细粒度特征学习的分类优化方法</td>   <td>广东省</td>   <td>CN109657697A</td>   <td>2019-04-19</td>   <td>本发明涉及计算机视觉的技术领域,更具体地,涉及基于半监督学习和细粒度特征学习的分类优化方法。本发明是一种利用半监督学习来筛选伪标签数据,并且指定一种细粒度图片特征提取、利用方法来提升神经网络的泛化能力,以提升分类网络其在细粒度分类任务上的表现。包括：通过伪标签数据来扩充数据集；通过SSD对输入图片进行目标检测,对检测结果通过多个尺度进行裁剪,将检测结果和裁剪得到的图像块一起送入分类网络中学习,通过加权输出抑制背景块的影响。本发明还涉及到迁移学习,具体涉及到利用ImageNet数据集上的预训练模型的,冻结卷积层,微调全连接层,极大的节省了训练内存,节省了训练时间。</td>   <td>1.基于半监督学习和细粒度特征学习的分类优化方法,其特征在于,包括以下步骤：S1. 对无标记数据做投票操作,挑选具有一致的结果且不属于混淆类的数据当做伪标签；S2. 用挑选好的伪标签扩充训练集；S3. 选定一张有标记图片,得到检测结果图A；S4. 将上一步得到的图A进行随机裁剪得到图片；S5. 将裁剪得到的图像块和图A一起送进网络去学习；S6. 选定待测试图片,得到检测结果图B；S7. 将图B划分大小,裁剪图像块；S8. 将裁剪得到图像块以及图B一起做预测；S9. 对上一步得到的每张图像块的预测结果,用最大预测概率减去次大预测概率作为他们的权重,累加加权后的概率,取最大概率所在类别当做测试图片的最终结果。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;                   刘晓       </td>   <td>中山大学</td>   <td>一种基于SAE特征可视化学习的人体检测方法</td>   <td>广东省</td>   <td>CN105930793B</td>   <td>2019-04-16</td>   <td>本发明公开了一种基于SAE特征可视化学习的人体检测方法,包括：根据所采集的深度图像通过SAE学习构建的CNN提取图像特征；将所述图像特征可视化为高维抽象图像；对所述高维抽象图像进行第二层SAE学习构建的CNN提取图像特征；将所述图像特征输入已训练的SVM分类器,得到所述深度图像是否包含人体。本发明提出的一种基于SAE特征可视化学习进行人体检测的方法,利用深度图像提取图像特征并可视化特征得到高维图像,通过提取高维图像特征,提高人体检测的正确率,可应用于智能监控以及人机交互系统。</td>   <td>1.一种基于SAE特征可视化学习的人体检测方法,其特征在于,包括下述步骤：(1)训练第一层CNN,将训练集中的图像全部裁剪成固定尺寸的小块,将所述的小块输入到SAE网络中进行参数学习,得到SAE网络的权重和偏置作为第一层CNN网络的权重和偏置,训练样本经过所述第一层CNN网络,得到图像特征；(2)训练第二层CNN,将经过第一层CNN网络后得到的图像特征应用特征可视化技术重构为高维图像,将高维图像全部裁剪成固定尺寸的小块并输入到一个SAE网络中进行参数学习,得到SAE网络的权重和偏置作为第二层CNN网络的权重和偏置；高维图像经过所述第二层CNN网络提取图像高维特征；(3)训练分类器,输入训练集正样本,归一化为固定尺寸,经过步骤(1)所述第一层CNN网络得到图像特征,将图像特征经过特征可视化技术重构得到高维图像,将高维图像经过步骤(2)所述第二层CNN网络得到正样本高维特征；输入训练集负样本,归一化为固定尺寸,经过步骤(1)所述第一层CNN网络得到图像特征,将图像特征经过特征可视化技术重构得到高维图像；将高维图像经过步骤(2)所述第二层CNN网络得到负样本高维特征；最后将所述正样本高维特征和所述负样本高维特征输入SVM训练分类器；(4)检测人体,根据待测深度图像各像素深度值得到待测深度图像深度频率直方图得到深度平面,并经过特征可视化技术重构得到高维图像,使用步骤(3)所述已训练的SVM分类器对待测深度图像的高维特征进行分类,将分类结果中判别为人的候选对象挑选出来,得到最终的人体检测结果。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨达坤;              赖剑煌;                   梅岭       </td>   <td>中山大学</td>   <td>一种基于深度视觉词袋模型的人脸活体检测方法</td>   <td>广东省</td>   <td>CN106203373B</td>   <td>2019-04-16</td>   <td>本发明公开了一种基于深度视觉词袋模型的人脸活体检测方法,该方法包括以下步骤：对于训练集中每一个人脸活体图像,计算其对应的LBP特征；利用深度稀疏自编码网络将LBP特征编码成高级特征；利用训练集的人脸图像的类标对整个深度稀疏自编码网络进行训练,得到更具有区分性的高级特征；将更具有区分性的高级特征输入到LIBSVM进行训练,从而建立SVM模型；将计算的LBP特征输入深度稀疏自编码网络得到高级特征,然后利用建立的SVM模型对其进行分类,得到人脸活体图像的类标。本发明能够得到更有鲁棒性、更有区别性的高级特征,从而对非控条件下的人脸活体检测具有更高的检测率。</td>   <td>1.一种基于深度视觉词袋模型的人脸活体检测方法,其特征在于,包括以下步骤：步骤a、对于训练集中每一个人脸活体图像,计算其对应的LBP特征；步骤b、利用深度稀疏自编码网络将LBP特征编码成高级特征；步骤c、利用训练集的人脸图像的类标对整个深度稀疏自编码网络进行训练,得到更具有区分性的高级特征；步骤d、将步骤c得到更具有区分性的高级特征输入到LIBSVM进行训练,从而建立SVM模型；步骤e、将步骤a计算的LBP特征输入步骤b的深度稀疏自编码网络得到高级特征,然后利用步骤d建立的SVM模型对其进行分类,得到人脸活体图像的类标；所述深度稀疏自编码网络包括四层神经元：一层输入层、一层输出层以及两层隐层,输入层和两层隐层构成了堆叠的两个稀疏自编码网络；每个稀疏自编码网络包括编码和解码过程,编码：对于每个输入向量x＝(x<Sub>1</Sub>,x<Sub>2</Sub>,…,x<Sub>N</Sub>)<Sup>T</Sup>,隐层单元输出为：h＝(h<Sub>1</Sub>,h<Sub>2</Sub>,…,h<Sub>M</Sub>)<Sup>T</Sup>＝f(Wx+b)          <Image id="icf0001" he="115" wi="700" file="FDA0001869198480000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        解码：对于隐层单元向量h＝(h<Sub>1</Sub>,h<Sub>2</Sub>,…,h<Sub>M</Sub>)<Sup>T</Sup>,输出层单元输出为：y＝(y<Sub>1</Sub>,y<Sub>2</Sub>,…,y<Sub>N</Sub>)<Sup>T</Sup>＝g(W'h+b')          <Image id="icf0002" he="123" wi="700" file="FDA0001869198480000021.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中函数f和g都采用sigmoid函数z(x)＝1/(1+exp(-x))；对于稀疏自编码网络的稀疏性,使得所有隐层单元的平均激活值l为0；由于隐层第j个单元的平均激活值为<Image id="icf0003" he="79" wi="406" file="FDA0001869198480000022.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>D是样本个数；对于理想分布l和真实分布l<Sub>j</Sub>的相似度采用KL散度来衡量,其计算公式如下：          <Image id="icf0004" he="121" wi="700" file="FDA0001869198480000023.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        稀疏自编码网络被描述成下面的优化问题：          <Image id="icf0005" he="105" wi="700" file="FDA0001869198480000024.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        根据稀疏自编码网络是一个输入等于输出的网络,即x＝y,无监督训练得到权值W,W'和阈值b,b'；由训练得到的权值和阈值将LBP特征编码成高级特征,即四层深度稀疏自编码网络的第三层的隐层单元输出值。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘丙军;              邱江潮;              谭学志;              彭为;                   杨子博       </td>   <td>中山大学</td>   <td>一种基于贝叶斯理论的天然河道水位流量关系确定方法</td>   <td>广东省</td>   <td>CN109635435A</td>   <td>2019-04-16</td>   <td>本发明涉及水利工程水文测验技术领域,更具体地,涉及一种基于贝叶斯理论的天然河道水位流量关系确定方法。包括以下步骤：基于水流运动规律,通过分析水文测站水力学属性和几何属性,建立水位流量关系模型；考虑测量误差和拟合误差计算实测流量的似然函数；基于贝叶斯理论,根据参数实际物理意义构造参数的先验分布；通过设计的自适应MCMC算法求解参数的后验分布。本发明建立的水位流量关系模型的参数物理意义明确,误差来源清晰,最大化利用了参数已有信息和样本信息,模拟求解效率较高。</td>   <td>1.一种基于贝叶斯理论的天然河道水位流量关系确定方法,其特征在于,包括以下步骤：S1.建立水位流量关系模型：Q＝a(H-b)<Sup>c</Sup>式中,Q为流量,H为水位,a为系数,c为指数,b为零流水位；S2.似然函数计算；水位流量数据为成对出现的离散型随机变量,本发明考虑其为单一对应关系,不考虑复杂绳套关系,且点对相互独立,则似然函数表示参数一定时水位流量点对数据被观测到的概率；由此可建立似然函数的一般表达式：          <Image id="icf0001" he="119" wi="446" file="FDA0001902662240000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,D为水位流量实测数据,<Image id="icf0002" he="72" wi="262" file="FDA0001902662240000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>共有N对,其中<Image id="icf0003" he="71" wi="44" file="FDA0001902662240000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为实测水位,<Image id="icf0004" he="73" wi="55" file="FDA0001902662240000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为实测流量；θ为参数集,θ＝(θ<Sub>RC</Sub>,γ),包括水力学参数θ<Sub>RC</Sub>和误差参数γ,其中水力学参数为幂函数水位流量关系模型中的参数,有θ<Sub>RC</Sub>＝(a,b,c)；S3.先验分布构造；用概率密度函数对参数集θ进行表示,利用实测水位流量数据以外的已有信息确定各参数分布形式；假定各参数彼此独立,则参数集先验分布的联合分布形式可表示为：          <Image id="icf0005" he="111" wi="607" file="FDA0001902662240000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,p(θ)为参数集的联合分布,其中p(a)、p(b)、p(c)为3个水力学参数的先验分布,p(γ<Sub>i</Sub>)为误差参数的先验分布,m为误差参数的数量；S4.后验分布求解；根据S1步骤、S2步骤、S2步骤D中的表达式,基于贝叶斯公式,得到参数集θ后验分布的表达式：          <Image id="icf0006" he="160" wi="630" file="FDA0001902662240000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式中,<Image id="icf0007" he="71" wi="212" file="FDA0001902662240000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为后验分布的概率密度函数；<Image id="icf0008" he="69" wi="206" file="FDA0001902662240000018.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为似然函数；p(θ)为先验分布；<Image id="icf0009" he="91" wi="367" file="FDA0001902662240000019.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为边际分布,与参数无关；最后,通过设计自适应马尔科夫链蒙特卡罗算法,求解参数集θ的后验分布。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              黄毅;              卢心龙;              冼宇乔;                   黄翔星       </td>   <td>中山大学</td>   <td>一种基于三数据集交叉迁移学习的无监督行人再识别方法</td>   <td>广东省</td>   <td>CN109635708A</td>   <td>2019-04-16</td>   <td>本发明公开了一种基于三数据集交叉迁移学习的无监督行人再识别方法,步骤如下：将三个CNN在用于图像分类的大数据集上进行训练,得到三个预训练模型；并在三个有标签的源行人数据集A、B和C上分别进行微调；利用这三个CNN分别提取目标数据集中无标签行人图片的特征,用K-近邻聚类算法对提取到的特征分别进行聚类；筛选出三个模型聚类后靠近聚类中心域的图片样本,并打上拟标签；将三个打上拟标签的样本数据进行交叉轮换加入到另一个源行人数据集中,再对模型进行微调；将一张行人测试图片输入到训练好的三个模型得到三个特征矩阵,并进行最大池化操作,得到测试图片的唯一特征；计算唯一特征与数据库中的图片特征的欧氏距离,距离最小的数据库图片的身份即为本张测试图片的身份。</td>   <td>1.一种基于三数据集交叉迁移学习的无监督行人再识别方法,其特征在于,包括步骤如下：训练时：步骤1：将三个CNN在用于图像分类的大数据集上进行训练,得到三个预训练模型；将这三个预训练的CNN在三个有标签的源行人数据集A、B、C上分别进行微调,使其能有效提取行人特征；步骤2：利用微调后的三个CNN分别提取目标数据集中无标签行人图片的特征,并使用K-近邻聚类算法对提取到的特征分别进行聚类；步骤3：筛选出三个模型聚类后靠近聚类中心域的图片样本,并对这些样本分别打上拟标签；步骤4：将三个模型打上拟标签的样本数据进行交叉轮换加入到另一个源行人数据集中,从而对模型进行微调；重复步骤2～4操作,直到三个模型收敛为止,结束迭代；测试时：步骤5：将上述训练好的三个模型对同一张行人测试图片进行特征提取,得到三个特征矩阵,对这三个特征进行最大池化操作,得到测试图片的唯一特征；步骤6：利用该唯一特征与数据库中的图片特征进行匹配,计算他们之间的欧氏距离,距离最小的库图片身份即为这张测试图片的身份。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;                   陈文东       </td>   <td>中山大学</td>   <td>一种基于显著表情变化区域辅助学习的人脸表情识别方法</td>   <td>广东省</td>   <td>CN109635709A</td>   <td>2019-04-16</td>   <td>本发明公开了一种基于显著表情变化区域辅助学习的人脸表情识别方法,其搭建一个辅助学习网络对人脸表情图像中的显著表情变化区域进行特征提取,将主网络与辅助学习网络的前3层特征提取层的参数进行共享,将辅助学习网络的第四层、第五层特征提取层提取的特征与主网络的第四层、第五层进行特征加权融合,使得主网络结构可以学习到辅助网络中的一些显著表情区域的特征；使用人脸检测和定位算法对人脸表情数据集进行处理得到人脸区域图像,用于对主网络进行训练；将人脸区域图像进行预处理得到具有表情显著变化区域的图像,用于对辅助学习网络进行训练,使得表情识别的主网络可以更加集中注意力在显著表情变化的区域,从而提取更加具有辨识力和鲁棒性的表情特征。</td>   <td>1.一种基于显著表情变化区域辅助学习的人脸表情识别方法,其特征在于：该识别方法如下：S1：搭建一个包括5层特征提取层的主网络用于人脸表情特征的提取,将提取后的高层语义特征输入到全连接层中,再将全连接层输出的特征输入到Softmax分类层中进行表情的分类操作,得到网络判定的表情结果；S2：搭建一个包括5层特征提取层的辅助学习网络用于提取人脸中显著的表情特征,将提取后的高层语义特征输入到全连接层中,再将全连接层输出的特征输入到Softmax分类层中进行表情的分类操作,得到网络判定的表情结果；S3：将主网络和辅助学习网络的前3层特征提取层的参数进行共享；然后将辅助学习网络的第四层和第五层特征提取层的输出特征分别和主网络第四层和第五层特征提取层的输出特征进行加权融合,然后把融合后的特征进行输入到主网络中来继续辅助主网络的高层语义特征的提取工作；S4：主网络、辅助学习网络均采用交叉熵损失函数对网络损失进行判定,根据网络损失判定的结果进行网络的反向传播,调整主网络、辅助学习网络的参数,不断优化主网络、辅助学习网络；S5：使用人脸检测和定位算法对带有人脸表情标签的人脸表情数据集分别提取出每一张图像中相应的人脸区域图像,将其输入主网络中进行训练；同时将人脸区域图像进行预处理得到具有表情显著变化区域的图像,将其输入到辅助学习网络中进行训练；采用交替训练的方式对主网络与辅助学习网络进行训练；S6：将待识别的人脸表情图像输出主网络中,完成人脸表情识别。</td>   <td>G06K9/00;G06K9/62;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              程海杰;                   张权       </td>   <td>中山大学</td>   <td>一种基于非对称度量学习的异构行人再识别方法</td>   <td>广东省</td>   <td>CN109635728A</td>   <td>2019-04-16</td>   <td>本发明公开了一种基于非对称度量学习的异构行人再识别方法,该方法将不同模态下的深度特征进行非对称度量,步骤是：使用两个不共享参数的稀疏自编码器分别将不同模态深度特征投影到共享空间,同时引入全局约束和局部约束去约束不同模态深度特征间的距离,使不同模态特征间的类内距离减小和类间距离增加；将全局约束和局部约束的约束结果作为监督信号反向传播到训练网络中用于修正各个参数。本发明通过缩小不同模态间模态差距,使网络尽可能地忽略模态信息而更加关注身份信息,从而提高行人特征表达力和行人匹配精确度。</td>   <td>1.一种基于非对称度量学习的异构行人再识别方法,其特征在于,包括步骤：在训练模型过程中,输入两种模态下的行人图像,分别提取深度特征；将不同模态下的深度特征进行非对称度量,步骤是：使用两个不共享参数的稀疏自编码器分别将不同模态深度特征投影到共享空间,同时引入全局约束和局部约束去约束不同模态深度特征间的距离,使不同模态特征间的类内距离减小和类间距离增加；将全局约束和局部约束的约束结果作为监督信号反向传播到训练网络中用于修正各个参数；根据深度特征计算全局特征和局部特征的损失,以全局损失、局部损失、以及非对称度量中的全局约束和局部约束之和达到最小化为目标去优化训练模型。</td>   <td>G06K9/00;G06K9/66</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨猛;                   叶林彬       </td>   <td>中山大学</td>   <td>一种基于生成对抗网络的人脸合成方法</td>   <td>广东省</td>   <td>CN109635774A</td>   <td>2019-04-16</td>   <td>本发明在人脸的合成任务上,基于对抗生成网络CycleGAN架构构建多层次稀疏表达的三次转换虚拟生成神经网络TTGAN。TTGAN提出和加入多层次稀疏表达模型和三次转换一致性约束,针对人脸图像对的目标人脸合成上,TTGAN是多个对抗生成网络的协同作用下结果。多层次稀疏表达模型对输入图片中被生成网络不同特征提取层提取的特征,包含对目标图像相关的身份信息进行约束；三次转换一致性约束利用了模型一次循环所产生的三个含有网络状态信息的不同样本,从而引导整体模型两个生成对抗网络相互协作。TTGAN提出的多层次稀疏表达和三次转换一致性约束,进一步增加CycleGAN的图像生成能力,使合成的人脸图像在保持人脸身份信息和表现出更真实性方面都取得更好的结果。</td>   <td>1.一种基于生成对抗深度网络的人脸合成方法,其特征在于,包括构建和训练优化TTGAN模型,所述的TTGAN模型是两个GAN网络通过相互交互构成的,通过多层次稀疏表达模型和三次转换一致性约束构建模型损失项；然后利用训练优化好的TTGAN模型进行人脸合成的步骤,其中训练TTGAN模型的步骤如下：S10.图像输入进TTGAN模型并经过三次领域之间的图像转换；S101.获取一个批次的两个相关领域的对象对,即领域x和目标领域y的图像对,且图像对的人物身份一致；S102.模型的正向循环生成图像过程：输入图像x,经过对抗生成网络GANX的生成器G<Sub>X</Sub>,合成生成图像y′；输入生成图像y′,经过对抗生成网络GANY的生成器G<Sub>Y</Sub>,合成生成图像x′；S103.第三次图像转换：G<Sub>X</Sub>生成器收到G<Sub>Y</Sub>生成器的生成图像x′这个反馈,对该图像进行进一步的转换到y图像的领域,即G<Sub>X</Sub>(G<Sub>Y</Sub>(G<Sub>X</Sub>(x)))～y,合成生成图像y″,<Image id="icf0001" he="99" wi="468" file="FDA0001915874350000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是对生成器G<Sub>X</Sub>的第三次转换约束；S104.对抗生成网络GANX的鉴别器D<Sub>Y</Sub>对未标注的真实图像y和生成图像y′的属性进行判断,判断其为真实图像或生成图像；S105.模型的反向循环生成图像过程：输入图像y,经过GANY生成器G<Sub>Y</Sub>,合成生成图像<Image id="icf0002" he="63" wi="67" file="FDA0001915874350000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>输入生成图像<Image id="icf0003" he="64" wi="69" file="FDA0001915874350000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>经过GANX生成器G<Sub>X</Sub>,合成生成图像<Image id="icf0004" he="71" wi="67" file="FDA0001915874350000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>S106.反向第三次图像转换：G<Sub>Y</Sub>生成器收到G<Sub>X</Sub>生成器的生成图像<Image id="icf0005" he="75" wi="50" file="FDA0001915874350000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>这个反馈,对该图像进行进一步的转换到x图像的领域,即G<Sub>Y</Sub>(G<Sub>X</Sub>(G<Sub>Y</Sub>(y)))～x,合成生成图像<Image id="icf0006" he="65" wi="75" file="FDA0001915874350000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image><Image id="icf0007" he="95" wi="470" file="FDA0001915874350000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是生成器G<Sub>Y</Sub>的第三次转换约束；S107.对抗生成网络GANY的鉴别器D<Sub>X</Sub>对未标注的真实图像x和生成图像<Image id="icf0008" he="47" wi="46" file="FDA0001915874350000018.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>的属性进行判断,判断其为真实图像或生成图像；S20.构建生成器G<Sub>X</Sub>、生成器G<Sub>Y</Sub>的损失项：S201.构建多层次稀疏表达模型中生成图像和目标图像的相似性损失,损失约束作用于生成器G<Sub>X</Sub>和生成器G<Sub>Y</Sub>的最后输出层的输出图像上；L<Sub>MSR1</Sub>(G<Sub>X</Sub>,X,Y)+L<Sub>MSR1</Sub>(G<Sub>Y</Sub>,Y,X)；L<Sub>MSR1</Sub>(G<Sub>X</Sub>,X,Y)对生成器G<Sub>X</Sub>的约束,L<Sub>MSR1</Sub>(G<Sub>Y</Sub>,X,Y)对生成器G<Sub>Y</Sub>的约束；S202.构建多层次稀疏表达模型中生成图像和输入图像的各层特征损失,损失约束作用于生成器G<Sub>X</Sub>和生成器G<Sub>Y</Sub>的编码各个特征提取层的特征上；L<Sub>MSR2</Sub>(G<Sub>X</Sub>,X,Y)+L<Sub>MSR2</Sub>(G<Sub>Y</Sub>,X,Y)S203.构建三次转换过程的一致性约束损失L<Sub>TTC</Sub>(G<Sub>X</Sub>,G<Sub>Y</Sub>),损失约束作用于生成器G<Sub>X</Sub>和生成器G<Sub>Y</Sub>的最后输出层的输出图像上；S204.构建生成对抗网络生成器G<Sub>X</Sub>和生成器G<Sub>Y</Sub>的对抗损失：L<Sub>GAN</Sub>(G<Sub>X</Sub>,D<Sub>Y</Sub>,X,Y)+L<Sub>GAN</Sub>(G<Sub>Y</Sub>,D<Sub>X</Sub>,Y,X),其中对抗损失L<Sub>GAN</Sub>(.)是所有生成式对抗网络的特有损失；对抗损失L<Sub>GAN</Sub>(G<Sub>X</Sub>,D<Sub>Y</Sub>,X,Y)与生成器G<Sub>X</Sub>、鉴别器D<Sub>Y</Sub>、领域图像X和领域图像Y相关；S205.生成器的总损失为各个损失的加权和：V<Sub>TTGAN</Sub>(G<Sub>X</Sub>,G<Sub>Y</Sub>,D<Sub>X</Sub>,D<Sub>Y</Sub>)＝L<Sub>GAN</Sub>(G<Sub>X</Sub>,D<Sub>Y</Sub>,X,Y)+L<Sub>GAN</Sub>(G<Sub>Y</Sub>,D<Sub>X</Sub>,Y,X)+λL<Sub>MSR</Sub>(G<Sub>X</Sub>,X,Y)+λL<Sub>MSR</Sub>(G<Sub>Y</Sub>,Y,X)+γL<Sub>TTC</Sub>(G<Sub>X</Sub>,G<Sub>Y</Sub>)其中的λ,γ为相应项的权重；S30.构建鉴别器和D<Sub>Y</Sub>和鉴别器D<Sub>X</Sub>的损失项：S301.构建生成对抗网络鉴别器D<Sub>Y</Sub>和D<Sub>X</Sub>的对抗损失,损失约束作用于鉴别器D<Sub>Y</Sub>和D<Sub>X</Sub>的最后输出层的输出鉴别结果上；L<Sub>GAN</Sub>(G<Sub>X</Sub>,D<Sub>Y</Sub>,X,Y)+L<Sub>GAN</Sub>(G<Sub>Y</Sub>,D<Sub>X</Sub>,Y,X)；S40.迭代并对抗地训练生成器和鉴别器优化TTGAN模型：S401.通过上述计算的生成器G<Sub>X</Sub>和G<Sub>Y</Sub>的损失项、鉴别器D<Sub>Y</Sub>和D<Sub>X</Sub>的损失项,构建TTGAN的生成器和鉴别器迭代地对抗训练方式,优化调整TTGAN模型的网络参数,S402.通过神经网络的后向传播损失优化生成器和鉴别器的网络参数,直到达到相应的迭代次数或生成器G<Sub>X</Sub>、生成器G<Sub>Y</Sub>的损失项、鉴别器D<Sub>Y</Sub>和鉴别器D<Sub>X</Sub>的损失项达到相应的迭代次数或设定的初始阈值,反之则重复步骤S10到S40；S50.利用训练优化好的TTGAN模型进行人脸合成。</td>   <td>G06K9/00;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林浩添;              吕健;              徐帆;              曾思明;                   李敏       </td>   <td>中山大学中山眼科中心;广西壮族自治区人民医院</td>   <td>一种人工智能眼部图片分析方法、服务器和系统</td>   <td>广东省</td>   <td>CN109636796A</td>   <td>2019-04-16</td>   <td>本发明涉及一种人工智能眼部图片分析方法、服务器和系统,其中方法包括：获取用户有红眼现象的眼部图片；通过眼部定位深度学习模型定位出眼部图片上的眼睛；通过视轴异常深度学习模型筛查眼睛视轴区的异常。本发明利用智能终端和网络技术的普及,并依靠人工智能深度学习的高度敏感性和准确性,对用户眼部图片进行分析,使得眼睛视轴区异常的筛查更准确、更智能、更便捷,有利于提高对视轴区眼病筛查的效率。</td>   <td>1.一种人工智能眼部图片分析方法,其特征在于,包括：获取用户有红眼现象的眼部图片；通过眼部定位深度学习模型定位出眼部图片上的眼睛；通过视轴异常深度学习模型筛查眼睛视轴区的异常。</td>   <td>G06T7/00;A61B3/14;A61B3/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>              庄业广       </td>   <td>中山大学</td>   <td>基于文档嵌入的长文本案件罚金范围分类预测方法及装置</td>   <td>广东省</td>   <td>CN109614606A</td>   <td>2019-04-12</td>   <td>本发明公开一种基于文档嵌入的长文本案件罚金范围分类预测方法,包括：S1：对罚金金额进行离散化处理,并根据罚金金额标记不同的label；S2：进行分词并去停用词处理；S3：将同一label下的判决文书拼接组合成一个文档,计算每个词在不同文档中的TFIDF值,并按照每个词在不同文档中的TFIDF值的方差大小对词进行重要性排序；S4：取排序后top k个词作为关键词,保留是关键词的词；S5：利用过滤后判决文书训练一个doc2vec模型,对应的doc2vec中心向量；S6：计算待预测判决文书doc2vec向量及其与各label的doc2vec中心向量的距离,取距离最邻近的label作为预测的label,得到待预测判决文书的罚金范围。本发明利用机械学习与大数据自动给出案件中责任人所处罚金的范围,提高办案效率,较少人力资源。</td>   <td>1.一种基于文档嵌入的长文本案件罚金范围分类预测方法,其特征在于,包括以下步骤：S1：对已知罚金金额的判决文书的罚金金额进行离散化处理,并根据罚金金额对已知罚金金额的判决文书标记不同的label；S2：对已知罚金金额的判决文书进行分词并去停用词处理；S3：将同一label下的已知罚金金额的判决文书拼接组合成一个文档,计算每个词在不同label下的文档中的TFIDF值,并按照每个词在不同文档中的TFIDF值的方差大小对词进行重要性排序；S4：取排序后top k个词作为关键词,对已知罚金金额的判决文书的词进行过滤,只保留是关键词的词；S5：利用过滤后的已知罚金金额的判决文书训练一个doc2vec模型,按照label计算对应的doc2vec中心向量；S6：对待预测的判决文书,计算其doc2vec向量及其与各label的doc2vec中心向量的欧式距离,取欧式距离最邻近的label作为其预测的label值,得到待预测的判决文书的罚金范围。</td>   <td>G06F17/27;G06K9/62;G06Q50/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         方烜宇;              印鉴;                   高静       </td>   <td>中山大学;广东恒电信息科技股份有限公司</td>   <td>一种融合生成非对抗网络与卷积神经网络的情感分析方法</td>   <td>广东省</td>   <td>CN109614611A</td>   <td>2019-04-12</td>   <td>本发明提供一种融合生成非对抗网络与卷积神经网络的情感分析方法,本发明首先是借鉴传统的生成对抗网络思想,构建了一个生成非对抗网络,用于生成文本的全局语义信息,经过验证,该语义信息具有较好的表征全局的效果。其次对传统卷积神经网络进行改进,使其更适合情感分析任务,并且将生成非对抗网络产生的全局语义信息融合进其中,进行模型训练与测试。过程中使用的数据集为MR与SST-2数据集。</td>   <td>1.一种融合生成非对抗网络与卷积神经网络的情感分析方法,其特征在于,包括以下步骤：S1：建立用于生成全局语义信息的生成非对抗网络；S2：建立通过特征抽取融合局部语义信息的卷积神经网络；S3：通过非对抗网络生成全局信息,结合卷积神经网络提取局部信息,最终进行融合生成非对抗网络与卷积神经网络的情感分析的模型训练与测试。</td>   <td>G06F17/27;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈景宇;              谢晓华;                   郑伟诗       </td>   <td>中山大学</td>   <td>一种基于卷积神经网络模型的嵌入式人群密度估计方法</td>   <td>广东省</td>   <td>CN109614941A</td>   <td>2019-04-12</td>   <td>本发明公开一种基于卷积神经网络模型的嵌入式人群密度估计方法及嵌入式人群密度估计的卷积神经网络模型,本模型用于实现本方法,本方法包括通过3个具有生成人群密度图输出能力的卷积神经分支的结构嵌套,使得模型具有3个运行模式,对训练图像预处理后,训练卷积神经网络模型,输入图像至训练好的卷积神经网络模型,选择三个运行模式的其中之一,输出所选模式对应的人群密度图,对所输出的密度图进行积分操作,获得对图像的总人数估计。本发明的卷积神经网络模型轻量化,准确度高于同量级卷积神经网络模型,部署三个模式可以任意切换,每个模式的速度不同,速度可根据实际情况选择。</td>   <td>1.一种基于卷积神经网络模型的嵌入式人群密度估计方法,其特征在于,包括如下步骤：S10嵌入3个运行模式：通过3个具有生成人群密度图输出能力的卷积神经分支的结构嵌套,使得卷积网络模型具有3个运行模式,其中所述3个模式所使用卷积网络模型的参数由低到高数量逐渐递增且能够进行复用；S20模型训练：对训练图像进行预处理,用激励函数δ(x-x<Sub>i</Sub>)表示图像像素点的标注,生成图像的标记图<Image id="icf0001" he="211" wi="537" file="FDA0001906627190000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>将标记图与高斯核G<Sub>σ</Sub>(x)进行卷积,获得对应的密度图真值F(x)＝H(x)*G<Sub>σ</Sub>(x),x为密度图中的像素,σ表示高斯核G<Sub>σ</Sub>(x)的标准差,使用预处理好的训练数据对所述卷积神经网络模型进行训练,其中使用密度图真值和模型输出密度图之间的欧氏距离作为网络训练的损失函数；S30输入图像至训练好的卷积神经网络模型,根据设备性能和速度要求,选择三个运行模式的其中之一,输出所选模式对应的人群密度图；S40对所输出的密度图进行积分操作,获得对图像的总人数估计。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈梓阳;                   郑伟诗       </td>   <td>中山大学</td>   <td>基于KCF算法的多目标行人跟踪系统及跟踪方法</td>   <td>广东省</td>   <td>CN109615641A</td>   <td>2019-04-12</td>   <td>本发明公开了一种基于KCF算法的多目标行人跟踪系统及跟踪方法,系统包括初始化模块、单目标跟踪KCF模块、跟踪与检测匹配模块、目标移除模块、打印模块和目标新增模块；所述初始化模块,用于初始化所有变量；所述单目标跟踪KCF模块,用于对单个目标进行跟踪；所述跟踪与检测匹配模块,对每个目标的跟踪结果与画面中的检测目标进行匹配；所述目标移除模块,用于判定目标是否已经离开画面；所述打印模块,用于对所述跟踪与检测匹配模块的匹配结果,在图上画出行人的边框以及其id信息；所述目标新增模块,用于判定检测目标是否为新出现的目标。本发明基于单目标跟踪算法KCF设计出一个多目标跟踪的系统框架,实时地提供各个目标的运动轨迹以及id信息。</td>   <td>1.基于KCF算法的多目标行人跟踪系统,其特征在于,包括初始化模块、单目标跟踪KCF模块、跟踪与检测匹配模块、目标移除模块、打印模块和目标新增模块；所述初始化模块,用于初始化所有变量；所述单目标跟踪KCF模块,用于对单个目标进行跟踪；所述跟踪与检测匹配模块,对每个目标的跟踪结果与画面中的检测目标进行匹配,即区分各个目标,将各目标与跟踪轨迹链接；所述目标移除模块,用于判定目标是否已经离开画面；所述打印模块,根据之前的匹配结果,从打印队列中逐个取出检测框与其对应的目标id,并在图中打印出来,即将图中的行人用检测框框出来且添加上其对应id；所述目标新增模块,用于判定检测目标是否为新出现的目标。</td>   <td>G06T7/246</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马争鸣;              车航健;              陈李创凯;                   刘洁       </td>   <td>中山大学</td>   <td>一种基于对称正定矩阵流形切空间子空间学习的描述子局部聚合向量方法</td>   <td>广东省</td>   <td>CN109598311A</td>   <td>2019-04-09</td>   <td>本发明涉及机器学习中描述子局部聚合向量相关问题,提出了一种基于对称正定矩阵流形切空间子空间学习的描述子局部聚合向量方法。已有的局部聚合向量方法大都是在欧式空间上,无法处理对称正定矩阵流形的非线性数据,为此,本方法提出了将训练数据映射到码字的切空间上进行子空间学习,将非线性问题转化为线性问题,然后是在学习的子空间上计算局部聚合向量。在子空间学习阶段,在码字的切空间上根据训练数据的标签信息来学习具有判别性的子空间,让同类别的数据在子空间上尽可能靠近,异类的数据在子空间上尽可能远离。在局部聚合向量计算阶段,将输入图片提取的对称正定矩阵映射到码字切空间的子空间上计算聚合向量。</td>   <td>1.一种基于对称正定矩阵流形切空间子空间学习的描述子局部聚合向量方法,其特征在于：A.训练数据是有标签的对称正定(Symmetric Positive Definite,SPD)矩阵流形数据,将训练数据映射到码字的切空间上,根据训练数据的标签信息,通过同类数据在子空间上距离最小化和异类数据在子空间上距离最大化两个原则在码字的切空间上学习具有判别性的子空间；B.对输入数据进行局部聚合向量的计算；对输入的图片通过计算每个局部区域的描述子协方差矩阵得到SPD矩阵集合,将SPD矩阵映射到码字切空间上,在根据学习到的子空间的标准正交基得到SPD矩阵在码字切空间的子空间的表示,利用这个表示进行聚合向量的计算。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林罗杰;                   马锦华       </td>   <td>中山大学</td>   <td>一种基于深度学习的肺部疾病检测方法</td>   <td>广东省</td>   <td>CN109598719A</td>   <td>2019-04-09</td>   <td>本发明提供一种基于深度学习的肺部疾病检测方法,该方法简化了提取特征的过程,利用的网络结构具有普适性；与现有的深度学习方法相比,本发明加快了模型的训练速度,在较少迭代次数的情况下可以得到较高的准确率。</td>   <td>1.一种基于深度学习的肺部疾病检测方法,其特征在于,包括以下步骤：S1：获取ChestX-ray14数据集并对数据集进行划分；S2：对ChestX-ray14数据集进行预处理；S3：将图像数据输入卷积神经网络提取特征；S4：利用PyTorch训练模型和测试模型；S5：将一张胸部X射线图像输入训练好的模型,输出该图像中肺部常见疾病的概率；其中,ChestX-ray14是由NIH研究院提供的目前规模最大的胸部X射线图像数据集,该数据集包含14种常见的肺部疾病。</td>   <td>G06T7/00;G06N3/08;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨锐;              王刚;                   刘红梅       </td>   <td>中山大学;深圳大学</td>   <td>一种鉴别数字音频AAC格式编解码器的方法</td>   <td>广东省</td>   <td>CN105448299B</td>   <td>2019-04-05</td>   <td>本发明公开一种鉴别数字音频AAC编解码器的方法,是根据不同编解码器对同一音频在压缩时会产生不同编码结果而提出的统计判别方法,属于多媒体信号处理领域。本发明方法依据数字音频在压缩时MDCT系数会发生变化,进而导致Huffman编码的不同,不同编解码器会产生不同的编码结果。本发明方法正是将不同编解码器与同一编解码器的Huffman编码小值的差异组成特征值,再利用SVM分类器分类,对鉴别产生AAC格式的数字音频的编解码器有很好的效果。本发明可以作为鉴别产生AAC格式文件的编解码器的一种有效手段,可以广泛应用在编解码器的鉴别和音频取证方面。</td>   <td>1.一种鉴别数字音频AAC格式编解码器的方法,其特征在于,包括以下步骤：1)数字音频集的构造：11)不同编解码器压缩一次AAC音频集的构造：首先选取无损的WAV格式文件,裁剪成若干t秒钟长度的音频片段,然后分别用不同编解码器以同一码率压缩成AAC格式的音频,取得不同编解码器压缩一次AAC音频集；12)同一编解码器再次压缩音频集的构造：对步骤11)中生成的AAC文件用同一编解码器进行解码得到WAV格式文件,再将它们用同一编解码器分别以相同的码率再次压缩成AAC文件,得到同一编解码器再次压缩的音频集；2)音频集特征提取：对上述得到的两种音频集,按以下方法提取特征：21)对上述两种AAC音频集进行解码,两次解码都用同一编解码器进行,根据AAC标准,每帧提取1024个Huffman编码值；22)统计每个音频片段中所有帧的Huffman编码值为0的个数,±1的个数和以及±2的个数和,然后除以帧数,得到平均每帧中0,±1,±2的个数,将平均每帧中0,±1,±2的个数称作Huffman小值；23)依据步骤22)中的方法,用不同编解码器压缩一次音频片段的Huffman小值减去用同一编解码器再次压缩音频片段的Huffman小值,组成不同编解码器特征值；3)分类器的构造：将步骤2)中得到的不同编解码器特征值利用LibSVM分类器进行训练,得到一个能鉴别分析产生AAC音频信号的编解码器分类器模型Model；4)鉴别待测音频：首先,将待测音频用同一编解码器解码,得到Huffman小值；然后,将解码得到的音频文件再次用同一编解码器压缩成AAC文件,而后再解码,得到另一组Huffman小值；将这两组Huffman小值相减,组成待测音频特征值；最后,利用步骤3)中训练出来的Model进行鉴别；待测音频特征值最接近哪一组编解码器的特征值,则判断产生待测音频的编解码器就是哪一种。</td>   <td>G10L19/22</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              刘峥;              何琛;                   张俊轩       </td>   <td>中山大学</td>   <td>一种基于时空关联神经网络的动作识别方法及其系统</td>   <td>广东省</td>   <td>CN109583334A</td>   <td>2019-04-05</td>   <td>本发明提供一种基于时空关联神经网络的动作识别方法,通过对海量已标注动作类别的视频数据进行训练得到时空关联神经网络。该网络是由三个部分(空间神经网络模块,相邻帧关联神经网络模块,关联模块)组成的动作识别模型。该发明能够有效的提取到视频动作的空间信息以及动作的时间信息,能够更好地实现视频动作时空信息的提取。设计的时空关联神经网络能够实现端到端的学习,在动作识别的准确率和速度上都有很好的表现。</td>   <td>1.一种基于时空关联神经网络的动作识别方法,其特征在于,包括以下步骤：S1：构造与训练空间神经网络模块；S2：构造与训练相邻帧关联神经网络模块,使用关联模块连接空间神经网络模块与相邻帧关联神经网络模块；S3：训练由空间神经网络模块与相邻帧关联神经网络模块复合而成的时空关联神经网络系统；S4：将待测视频输入到训练好的时空关联神经网络系统中进行动作识别。</td>   <td>G06K9/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周小峰;              李奥;              衣杨;              沈金龙;              朱艺;                   顾佳良       </td>   <td>中山大学</td>   <td>一种基于时空信息融合的视频人体行为识别方法</td>   <td>广东省</td>   <td>CN109583335A</td>   <td>2019-04-05</td>   <td>本发明涉及人工智能领域,更具体的,涉及一种基于时空信息融合的视频人体行为识别方法。本发明基于密集光流场结合轨迹的时间显著值提取显著轨迹,并基于底层显著轨迹构造了一种新的中层特征——轨迹组,其在一定程度上刻画了时间维度上的运动信息,弥补了底层轨迹的缺陷,同时构造了轨迹组在时间维度上的前后、远近关系,丰富了轨迹组的时间关系。本发明基于稀疏采样提出了自适应分段采样策略,对视频采样的数目随视频时长而自适应变化,对任意时长的视频都能够提取到富有判别力的空间信息。</td>   <td>1.一种基于时空信息融合的视频人体行为识别方法,其特征在于,包括以下步骤：步骤S1：对视频时间信息进行提取分类,将原始视频进行灰度空间尺度变换,提取显著轨迹；步骤S2：根据显著轨迹的持续时间进行聚类而构建视频中层特征TG；步骤S3：构造TG之间的时间关系；步骤S4：计算TG的特征描述符；步骤S5：采用Fisher编码方法对TG特征进行编码,结合TG以及时间关系作为时间信息视频表示；步骤S6：采用隐结构的支持向量机对视频进行分类；步骤S7：对视频空间信息进行提取分类,采用自适应分段采样策略从视频中进行稀疏采样；步骤S8：对采样所得到的视频帧利用卷积神经网络提取空间特征；步骤S9：根据提取到的特征进行行为视频分类；步骤S10：把根据视频时间信息得到的分类结果和根据视频空间信息得到的分类结果进行平均加权融合,得到最后的视频分类结果。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;                   罗子泉       </td>   <td>中山大学</td>   <td>一种基于深度学习的视频目标检测方法</td>   <td>广东省</td>   <td>CN109583340A</td>   <td>2019-04-05</td>   <td>本发明公开了一种基于深度学习的视频目标检测方法,应用于视频目标检测领域。方法利用卷积神经网络进行图像特征的提取,提出了时间-空间特征提取网络,用于提取视频的空间上下文和时间上下文信息,并将图像特征与时间、空间上下文信息融合,更新骨干网络输出的特征图,最后将所得特征图输入检测网络,得到最终的检测结果,兼顾了目标检测的准确性和实时性。这种方法有效的提升了检测的准确性和实时性。</td>   <td>1.一种基于深度学习的视频目标检测方法,其特征在于：包括以下步骤：S1：归一化训练图像尺寸,以及初始化骨干网络、时间-空间特征提取网络和检测网络的参数；S2：将训练图像数据输入到包含骨干网络、时间-空间特征提取网络以及检测网络的检测器中进行训练并更新检测器参数；S3：将待检测视频输入到检测器进行目标检测并输出最终的预测框以及分类结果。</td>   <td>G06K9/00;G06K9/32;G06T7/269;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         吴昱焜;              李仲泓;              衣杨;              沈金龙;              佘滢;                   朱艺       </td>   <td>中山大学</td>   <td>一种基于时空信息和层级表示的视频人体行为识别方法</td>   <td>广东省</td>   <td>CN109583360A</td>   <td>2019-04-05</td>   <td>本发明涉及人工智能领域,更具体的,涉及一种基于时空信息和层级表示的视频人体行为识别方法。本发明充分利用了视频中的时空信息,层级时空束将视频运动分为几个部分,进而得到视频运动的更高维度的表示。针对传统视频表示方法忽略视频的中高层语义信息,仅仅关注特征出现的次数,利用的只是0阶信息等不足,基于层级时空束的视频表示方法能够有效剪除视频的背景噪声干扰、并且弥补底层特征与高层特征之间的语义鸿沟,可以捕获更高阶更复杂的运动结构信息。层级时空束方法可以在更高的维度上,析取更加复杂和更具表现力的视频表示,能够有效提高视频识别的效果。</td>   <td>1.一种基于时空信息和层级表示的视频人体行为识别方法,其特征在于,包括以下步骤：步骤S1：基于摄像机运动补偿整个视频片段的整体光流,提取前景运动光流,并形成补偿轨迹；步骤S2：通过关键帧选择,过滤得到视频中具有判别力的关键帧；步骤S3：对补偿轨迹采样并训练得到混合高斯模型；步骤S4：选择关键帧得到视频关键帧集合,并结合混合高斯模型对补偿轨迹进行FV编码,形成关键轨迹集；步骤S5：将整个视频进行片段分割与排序模型,将分割后的视频片段执行步骤S1～步骤S4,获得分割后视频片段的层级时空束特征；步骤S6：将以层级时空束作为视频表示,并作为分类器的输入,经过SVM分类之后,得到视频分类标签。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              吴卓亮;              欧阳柳;                   谢晓华       </td>   <td>中山大学</td>   <td>一种多特征融合的人脸图像光照识别方法及系统</td>   <td>广东省</td>   <td>CN109583375A</td>   <td>2019-04-05</td>   <td>本发明公开了一种多特征融合的人脸图像光照识别方法,包括步骤：对每一张不同光照情形下的人脸图像,提取图像的协方差矩阵,计算图像的区域差别特征,二者融合作为图像的统计特征；基于神经网络方法提取人脸图像的深度特征；将所述统计特征和深度特征进行融合,得到融合后特征；对融合后特征进行分类,以实现人脸图像的光照识别。本发明还公开了一种多特征融合的人脸图像光照识别系统。本发明能够实现对不同光照条件下人脸图像的区分,且准确率高、识别速度快、未来技术更新容易。</td>   <td>1.一种多特征融合的人脸图像光照识别方法,其特征在于,包括步骤：对每一张不同光照情形下的人脸图像,提取图像的协方差矩阵,计算图像的区域差别特征,二者融合作为图像的统计特征；基于神经网络方法提取人脸图像的深度特征；将所述统计特征和深度特征进行融合,得到融合后特征；对融合后特征进行分类,以实现人脸图像的光照识别。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;                   陈奕臻       </td>   <td>中山大学</td>   <td>基于特征关注机制的人脸表情识别方法</td>   <td>广东省</td>   <td>CN109583406A</td>   <td>2019-04-05</td>   <td>本发明提出一种基于特征关注机制的人脸表情识别方法,包括以下步骤：收集数据集并进行预处理；输入特征提取模块中,输出特征图并复制；将复制的特征图输入特征关注模块中,输出关注图；将关注图和原特征图进行相乘,得到关注特征图,并计算L1范数损失；将关注特征图输入分类器中输出表情标签,并计算分类损失；计算损失函数,并更新优化模型的参数值；重复上述步骤至模型参数收敛；将待识别图片输入特征提取模块,输出特征图并复制,将复制的特征图输入特征关注模块中,输出关注图,将关注图和原特征图进行相乘,得到关注特征图,最后将关注特征图输入分类器中输出表情标签,即完成表情的识别。本发明具有较强鲁棒性,能有效提升表情识别效果。</td>   <td>1.基于特征关注机制的人脸表情识别方法,其特征在于,包括以下步骤：S1：从公开表情数据集以及真实环境下采集的表情图片中收集数据集,对数据集中的图片进行预处理；S2：将数据集中任意一张图片输入特征提取模块中,输出特征图并对其进行复制；S3：将复制的特征图输入特征关注模块中,输出相应的关注图；S4：将所述关注图和原特征图进行相乘,得到关注特征图,并计算关注图与全零矩阵之间的L1范数距离；S5：将关注特征图输入分类器中输出表情标签,并计算分类损失；S6：计算损失函数,利用随机梯度下降算法更新特征提取模块、特征关注模块和分类器的参数值；S7：重复S2～S6步骤,对特征提取模块、特征关注模块和分类器进行训练及参数值更新至模型参数收敛；S8：将待识别的表情图片输入特征提取模块中,输出特征图并对其进行复制,然后将复制的特征图输入特征关注模块中,输出相应的关注图,将所述关注图和原特征图进行相乘,得到关注特征图,最后将关注特征图输入分类器中输出表情标签,即完成表情的识别。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         云径平;                   王智       </td>   <td>中山大学肿瘤防治中心</td>   <td>病理图片的识别方法及装置</td>   <td>广东省</td>   <td>CN108596882B</td>   <td>2019-04-02</td>   <td>本发明公开了一种病理图片的识别方法及装置,其中该方法包括：获得样本数据,所述样本数据包括正样本和负样本,所述正样本为恶性病变病理图片,所述负样本为正常或良性病变病理图片,所述恶性病变病理图片上标记出病变区域；将所述样本数据划分为训练集和测试集；利用所述训练集对深度神经网络模型进行训练；利用所述测试集对训练好的深度神经网络模型进行测试；根据测试结果对训练好的深度神经网络模型进行参数调整；利用训练好的深度神经网络模型对病理图片进行识别。本发明可以提高病理图片识别的效率和准确率。</td>   <td>1.一种病理图片的识别装置,其特征在于,包括：样本获得模块,用于获得样本数据,所述样本数据包括正样本和负样本,所述正样本为恶性病变病理图片,所述负样本为正常或良性病变病理图片,所述恶性病变病理图片上标记出病变区域,在恶性病变病理图片上标记的病变区域切割出不同尺寸的图片；在正常或良性病变病理图片上随机切割出不同尺寸的图片；样本划分模块,用于将所述样本数据划分为训练集和测试集；模型训练模块,用于利用所述训练集对深度神经网络模型进行训练,进一步用于：将不同尺寸的图片分别输入深度神经网络模型进行训练；模型测试模块,用于利用所述测试集对训练好的深度神经网络模型进行测试,进一步用于：将测试图片的识别结果与测试图片上的病变区域标记进行比对,输出测试结果；模型调整模块,用于根据测试结果对训练好的深度神经网络模型进行参数调整,进一步用于：在测试结果不正确时,将测试图片映射回原始病理图片,在测试图片周围切割出多个相同尺寸的图片,将切割出的图片输入深度神经网络模型进行迭代训练；图片识别模块,用于利用训练好的深度神经网络模型对病理图片进行识别。</td>   <td>G06T7/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         邓豪;                   权小军       </td>   <td>中山大学</td>   <td>一种基于自注意力机制的标点符号预测方法</td>   <td>广东省</td>   <td>CN109558576A</td>   <td>2019-04-02</td>   <td>本发明提供一种基于自注意力机制的标点符号预测方法,包括以下步骤：基于自动语音识别技术进行语音识别,得到无标点符号文本；对无标点符号文本进行处理,得到文本序列；构建标点符号预测模型,将文本序列导入模型中,完成文本序列的标点符号预测。本发明提供的一种基于自注意力机制的标点符号预测方法,通过构建标点符号预测模型,实现了对语音识别文本的标点符号预测,有效缓解了梯度消失的问题,加强了特征传递,有效建立文本长期依赖的关系；同时,相比之前的模型无需额外的参数,有效减少了传递的数据量,降低参数的训练难度。</td>   <td>1.一种基于自注意力机制的标点符号预测方法,其特征在于,包括以下步骤：S1：基于自动语音识别技术进行语音识别,得到无标点符号文本；S2：对无标点符号文本进行处理,得到文本序列；S3：构建标点符号预测模型,将文本序列导入模型中,完成文本序列的标点符号预测。</td>   <td>G06F17/24;G06N3/04;G06N3/08;G10L15/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         尹智毅;              吕中荣;                   刘济科       </td>   <td>中山大学</td>   <td>一种结构损伤识别方法及系统</td>   <td>广东省</td>   <td>CN109558621A</td>   <td>2019-04-02</td>   <td>本发明公开了一种结构损伤识别方法及系统。该方法包括：采用有限元法将所述结构划分为多个单元；以结构的固有频率和模态的相对误差为目标,构造目标函数；根据目标函数,采用改进后的大爆炸算法,确定各所述单元的损伤系数；根据各所述单元的损伤系数确定所述结构的损伤位置和损伤程度。本发明提供的结构损伤识别方法及系统能够准确的实现结构损伤的定位和定量,具有精度高的特点。</td>   <td>1.一种结构损伤识别方法,其特征在于,所述方法包括：采用有限元法将所述结构划分为多个单元；以结构的固有频率和模态的相对误差为目标,构造目标函数；根据目标函数,采用改进后的大爆炸算法,确定各所述单元的损伤系数；根据各所述单元的损伤系数确定所述结构的损伤位置和损伤程度。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              张俊轩;              刘铮;              何琛;                   王伟轩       </td>   <td>中山大学</td>   <td>一种基于运动前景关注及非监督的关键帧提取的动作识别方法</td>   <td>广东省</td>   <td>CN109558811A</td>   <td>2019-04-02</td>   <td>本发明公开了一种基于运动前景关注及非监督的关键帧提取的动作识别方法,步骤包括：方法包括如下步骤：选用预训练好的卷积神经网络作为神经网络模型,构造基于方差统计的视觉关注模型并生成视觉关注权重；利用视觉关注模型提取的视觉关注权重对卷积神经网络的特征进行关注。构造非监督的关键帧提取模型并生成对于每个视频帧的置信度；利用得到的视频帧置信度对视频帧进行筛选,并采用一种随机选取的训练策略训练卷积神经网络；利用光流图像对视频的时间动态信息进行捕获,从而获得更优异的性能。本发明在现有的双流卷积神经网络的基础上,结合基于方差统计的视觉关注机制以及非监督的关键帧提取策略对输入的动作视频进行分类识别。</td>   <td>1.一种基于运动前景关注及非监督的关键帧提取的动作识别方法,其特征在于,所述方法包括如下步骤：S1：选用在图像数据集ImageNet上预训练好的卷积神经网络作为卷积神经网络模型A,将对视频进行密集采样得到一系列视频帧,所述视频帧作为所述神经网络模型A的输入,构建基于方差统计的视觉关注模型并生成视觉关注权重矩阵；S2：利用视觉关注模型提取的视觉关注权重对卷积神经网络模型A的特征进行关注；S3：构造非监督的关键帧提取模型并生成对于每个视频关键帧的置信度,将得到的关键帧的置信度作为所在视频块的置信度；S4：利用得到的不同视频块置信度采取不同的选取概率对步骤S1中采集的视频帧进行筛选,并采用一种随机选取的训练策略训练卷积神经网络模型A,然后将筛选后的视频帧输入到上述随机策略训练好的卷积神经网络模型A得到测试结果A；S5：对步骤S4筛选的所有相邻的视频帧提取其光流运动信息,生成光流图像；将生成的光流图像输入到在ImageNet预训练好的卷积神经网络B中；通过反向传播对网络参数进行更新；将更新后的卷积神经网络B用于测试,得到测试结果B；将测试的结果B与S1-S4步骤中的测试结果A进行结合,得到最终的识别结果。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨猛;                   柯康银       </td>   <td>中山大学</td>   <td>一种基于多特征表示的模式识别方法</td>   <td>广东省</td>   <td>CN109558816A</td>   <td>2019-04-02</td>   <td>本发明涉及人工智能领域,更具体的,涉及一种基于多特征表示的模式识别方法,本发明将系数分离为共享系数与特殊系数,从而将多特征的相似性与特殊性区严格区分开,为充分发掘多特征的相似性与特殊性提供了更有效的条件,并作用于最后的分类器,从而提高识别分类效果；而且本发明引入了相似加权项,使得模型对特征异常点鲁棒。</td>   <td>1.一种基于多特征表示的模式识别方法,其特征在于,包括以下步骤：步骤S1：提出共享和特殊的表示模型,模型表达式如下：          <Image id="icf0001" he="65" wi="700" file="FDA0001869152910000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中K表示特征的个数,τ,λ<Sub>1</Sub>和λ<Sub>2</Sub>是常量参数,<Image id="icf0002" he="69" wi="700" file="1.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示查询样本的第k个特征,<Image id="icf0003" he="59" wi="59" file="FDA00018691529100000114.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为标量,即向量y<Sub>k</Sub>的一个元素,n代表这个特征向量的维度；          <Image id="icf0004" he="65" wi="700" file="FDA0001869152910000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示第k个特征的字典,<Image id="icf0005" he="75" wi="78" file="FDA00018691529100000115.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>为特征向量,n表示向量的维度,m代表第m个训练样本；          <Image id="icf0006" he="67" wi="700" file="FDA0001869152910000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是查询样本对于各个特征字典的一个共同系数,α<Sup>c,m</Sup>表示关于第m个训练样本的共同系数,为标量,c为共同系数的标志；          <Image id="icf0007" he="71" wi="656" file="FDA0001869152910000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是对于第k个特征字典的特殊系数,ω<Sub>k</Sub>为第k个特征的权值；<Image id="icf0008" he="72" wi="94" file="2.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示关于第m个训练样本的第k个特征的特殊系数,s是特殊系数的标志；步骤S2：初始化共享系数α<Sup>c</Sup>,特殊系数<Image id="icf0009" he="52" wi="52" file="FDA0001869152910000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>与权重ω<Sub>k</Sub>,令α<Sup>c</Sup>＝0,<Image id="icf0010" he="54" wi="123" file="FDA0001869152910000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>ω<Sub>k</Sub>＝0；步骤S3：对模型进行交替迭代,更新共享系数α<Sup>c</Sup>,特殊系数<Image id="icf0011" he="56" wi="51" file="FDA0001869152910000018.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>与权重ω<Sub>k</Sub>,直到整个模型收敛到一个局部最小值为止；步骤S4：在求得的共享系数α<Sup>c</Sup>,特殊系数<Image id="icf0012" he="54" wi="51" file="FDA0001869152910000019.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>与权重ω<Sub>k</Sub>的基础上,利用最小重构误差求取测试样本的标签：          <Image id="icf0013" he="67" wi="700" file="FDA00018691529100000110.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,D<Sub>k,j</Sub>是字典D<Sub>k</Sub>中属于j类的子字典,<Image id="icf0014" he="72" wi="60" file="FDA00018691529100000111.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是对应于子字典D<Sub>k,j</Sub>的共享系数,<Image id="icf0015" he="71" wi="92" file="FDA00018691529100000112.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>是对于子字典D<Sub>k,j</Sub>的特殊系数。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王伟;              吕明德;              匡铭;              谢晓燕;              陈立达;              王竹;              梁瑾瑜;                   胡航通       </td>   <td>中山大学附属第一医院</td>   <td>基于超声组学和深度学习的疾病智能化分析方法及系统</td>   <td>广东省</td>   <td>CN109558896A</td>   <td>2019-04-02</td>   <td>本发明公开了一种基于超声组学和深度学习的疾病智能化分析方法及系统,包括：获取病变部位的若干超声数据,得到多模态超声组学数据；将多模态超声组学数据输入深度学习神经网络,根据多模态超声组学数据调整神经元的连接权重、配比卷积和池化层,得到调整后的多模态超声组学数据；利用不同模态下的分类器对调整后的多模态超声组学数据进行分类,并通过判别器得到每个分类的分数后,根据每个分类的分数,获得预后判断、疗效评估和辅助诊断结果。相比于现有利用单模态超声数据进行疾病智能化分析的方法,本发明技术方案根据多模态超声组学数据的特性,从数据和模型设计层面优化深度学习网络,提高疾病智能化分析的准确率和预测价值。</td>   <td>1.一种基于超声组学和深度学习的疾病智能化分析方法,适于在计算设备中执行,其特征在于,至少包括如下步骤：获取病变部位的若干超声数据,得到多模态超声组学数据；将所述多模态超声组学数据输入训练好的深度学习神经网络,并根据所述多模态超声组学数据调整神经元的连接权重、配比卷积和池化层,得到调整后的多模态超声组学数据；利用不同模态下的分类器对调整后的多模态超声组学数据中的每个数据进行分类,得到包含每个分类的所有模态的分类概率；根据判别器给出的模态之间的混淆分数,对所有模态的分类概率进行加权平均处理,得到每个分类的分数；基于临床结局指标和基因组学数据,根据每个分类的分数,采用逻辑回归法计算高风险指标、采用决策树或Adaboost的方法建立分类模型以及采用t检验和pearson/spearman相关性分析,获得预后判断结果、疗效评估结果和辅助诊断结果。</td>   <td>G06K9/62;G06N3/06;G06N3/08;G16H50/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              余伟江;              梁小丹;              龚科;                   王青       </td>   <td>中山大学</td>   <td>一种服装关键点定位系统及其训练、定位方法</td>   <td>广东省</td>   <td>CN109559345A</td>   <td>2019-04-02</td>   <td>本发明公开了一种服装关键点定位系统及其训练、定位方法,该系统包括：基础卷积网络,对输入的服装训练图像或服装测试图像提取卷积特征图；堆叠式层级布局知识推理单元,包括多个层级布局知识推理模块,用于于训练时,结合预定义的服装关键点空间布局关系信息来对提取的卷积特征图进行图-点转换、节点知识推理和点-图转换,利用目标函数对基础卷积网络和层级布局知识推理单元进行协同训练；于测试时,调用层级布局知识推理单元对提取的卷积特征图进行图-点转换、节点知识推理和点-图转换,实现知识推理和特征增强；后处理模块,用于将层级布局知识推理单元输出的卷积特征图转换为特征定位图,基于特征定位图预测和计算出服装关键点的位置。</td>   <td>1.一种服装关键点定位系统,包括：基础卷积网络,用于对输入的服装训练图像或服装测试图像提取卷积特征图；堆叠式层级布局知识推理单元,包括多个层级布局知识推理模块,用于于训练时,结合预定义的服装关键点空间布局关系信息来对提取的卷积特征图进行图-点转换、节点知识推理和点-图转换,利用目标函数对所述基础卷积网络和堆叠式层级布局知识推理单元进行协同训练；于测试时,调用多个层级布局知识推理模块对提取的卷积特征图进行图-点转换、节点知识推理和点-图转换,实现知识推理和特征增强；后处理模块,用于将所述堆叠式层级布局知识推理单元输出的卷积特征图转换为特征定位图,基于生成的特征定位图来预测和计算出服装关键点的位置。</td>   <td>G06T7/73</td>  </tr>        <tr>   <td>中国专利</td>   <td>         蔡志岗;              叶伟洲;              王福娟;                   李佼洋       </td>   <td>中山大学</td>   <td>一种在线高温零件红外图像光谱抽样检测方法</td>   <td>广东省</td>   <td>CN105956591B</td>   <td>2019-03-29</td>   <td>本发明提供一种在线高温零件的红外图像光谱抽样检测方法,是将红外图像识别和选择式红外图像光谱技术融合到在线高温零件的抽样检测中。利用红外图像识别技术,可对生产线上多个高温零件进行识别并自动随机选取；然后利用选择式红外光谱技术,探测所选高温零件或所选位置的发射光谱,并提取每个光谱样品的特征峰波长,对由光谱特征峰波长组成的样品集主成分建模运算,结合光谱分析技术进行高温零件夹杂分析。该方法无须等到高温零件冷却后才检测,而且在生产线上可以对多个红外图像进行识别并自动随机检测,同时利用红外光谱对零件进行夹杂分析,为高温零件的在线抽样检测提供了一种无损、高效、非接触的安全方法。</td>   <td>1.一种在线高温零件的红外图像光谱抽样检测方法,其特征在于,包括：S201.规划生产线零件识别区域,并平分识别区域为若干个搜索窗口；S202.零件未进入识别区域之前,采集背景光图像数据；S203.第一个零件进入识别区域R,采集第一个零件的图像数据；S204.多个零件进入识别区域,获得识别区域内所有零件的图像信息；积分计算各搜索窗口的灰度直方图,并积分计算第一个零件的图像信息和背景光的图像数据作为标准,再求差,得出所有零件滤去背景光的图像数据和第一个零件滤去背景光的图像数据,根据巴氏系数方法在每个搜索窗口中的滤去背景光的图像中匹配出和第一个零件滤去背景光后的图像数据相似度最高的区域作为零件轮廓,并对全部零件轮廓依次进行编号；S205.随机选中一个零件编号,生成该零件轮廓ROI的掩模图,由记录该零件特征发射光谱；S206.判断该零件是否超出该搜索窗口或零件的样品数是否超出采集上限；若都否,则进行下一步S207；若其中一个是,则进行步骤S208；S207.在该搜索窗口内根据该零件的图像数据再次进行匹配,找出该零件的轮廓并继续对此零件进行光谱探测,并形成该零件的特征发射光谱集,再进入S206；S208.去除该零件的编号并且放弃对该零件的采集工作；S209.提取光谱特征峰波长作为主成分样品集；S210.将特征峰波长值与波长数量进行建模分析,判断零件是否夹杂。</td>   <td>G06K9/32;G06K9/46;G06K9/62;G01N21/35</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑子彬;                   马璐       </td>   <td>中山大学</td>   <td>一种基于图向量表征的丰富短文本语义方法及装置</td>   <td>广东省</td>   <td>CN109543176A</td>   <td>2019-03-29</td>   <td>本发明公开一种基于图向量表征的丰富短文本语义方法及装置,该装置用于实现该方法,该方法包括对短文本语料数据进行分词和去停用词的处理；对处理后语料数据进行相邻词的两两相连构成词图；将词图随机游走,由上节点到下节点依次产生序列,待词图的文本链达到指定的文本链长度后停止游走,获取所有节点序列；输入所获取的节点序列至向量化表征模型,对所有节点进行向量化表征；输出所有节点对应的向量表征。本发明通过将短文本中相邻词连边构建成链,不同短文本构成的链之间用关键词相连的方式构建成图,对构建成的词图使用图向量表征算法得到每个节点的向量表征,以便于应用于机器学习模型中。</td>   <td>1.一种基于图向量表征的丰富短文本语义方法,其特征在于,包括如下步骤：S10输入短文本语料数据,对其进行分词和去停用词的处理；S20在处理后的语料数据中将相邻的词两两相连,通过将每个短文本转化为文本链,数个短文本之间通过相同的词相连,由此构成词图；S30将词图随机游走,上节点选取与其相连的一个词作为下一节点,由上节点到下节点依次产生序列,待词图的文本链达到指定的文本链长度后停止游走,获取所有节点序列；S40输入所获取的节点序列至向量化表征模型,对所有节点进行向量化表征；S50输出所有节点对应的向量表征。</td>   <td>G06F17/27</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王甲海;                   宋有伟       </td>   <td>中山大学</td>   <td>一种基于注意力机制的文本情感分析方法</td>   <td>广东省</td>   <td>CN109543180A</td>   <td>2019-03-29</td>   <td>本发明公开一种基于注意力机制的文本情感分析方法,包括如下步骤：一、对文本数据进行预处理；二、构建词表并利用GloVe模型构建词向量；三、利用内在注意力对句向量进行编码,利用交互注意力对目标词向量进行编码,并通过GRU融合编码后的两个向量,平均池化后得到融合表示；四、根据得到的融合表示,通过逐点的前馈网络(FFN)得到上下文向量的抽象特征,再通过全连接与Softmax函数计算情感分类标签的概率分布,得到分类结果；五、将预处理后的语料划分为训练集和测试集,对模型参数进行多次训练,选取分类准确率最高的模型用于情感倾向性分类。本发明的方法仅使用注意力机制对文本建模,并加强了对目标词的理解,使用户可以了解文本中对特定目标词所持有的情感倾向。</td>   <td>1.一种基于注意力机制的文本情感分析方法,其特征在于,包括以下步骤：S1：对文本数据进行预处理,包括分词、去停用词和标点符号；S2：构建词表并利用GloVe模型构建词向量,将本文映射为词向量后作为网络的输入；S3：利用内在注意力对句向量进行编码,利用交互注意力对目标词向量进行编码,并通过GRU融合编码后的两个向量,平均池化后得到融合表示；S4：根据得到的融合表示,通过逐点的前馈计算得到上下文向量的抽象特征,再通过全连接与Softmax函数计算情感分类标签的概率分布,得到分类结果；S5：将预处理后的语料划分为训练集和测试集,对模型参数进行多次训练,选取分类准确率最高的模型用于情感倾向性分类。</td>   <td>G06F17/27;G06F16/35</td>  </tr>        <tr>   <td>中国专利</td>   <td>         任江涛;                   赵泽川       </td>   <td>中山大学</td>   <td>电子病历特征的生成方法、装置及存储介质</td>   <td>广东省</td>   <td>CN109543187A</td>   <td>2019-03-29</td>   <td>本发明公开了一种电子病历特征的生成方法,包括：获取待处理的电子病历文本的分段文本的类别和关联的特征向量,将各个所述特征向量根据关联的分段文本的类别进行归类,获取各类所述特征向量的均值向量,将各类所述特征向量的均值向量进行拼接,得到所述待处理的电子病历文本对应的拼接特征向量。本发明还公开了一种电子病历特征的生成装置和计算机存储介质。本发明结合了电子病历现病史文本的领域知识并对其语义信息进行了深度表示,提供了能准确均衡的表征电子病历文本特征的生成方法。</td>   <td>1.一种电子病历特征的生成方法,其特征在于,所述电子病历特征的生成方法包括以下步骤：获取待处理的电子病历文本的分段文本；获取各个所述分段文本的类别和关联的特征向量；将各个所述特征向量根据关联的分段文本的类别进行归类；获取各类所述特征向量的均值向量；将各类所述特征向量的均值向量进行拼接,得到所述待处理的电子病历文本对应的拼接特征向量。</td>   <td>G06F17/27;G06F16/35;G16H50/70</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李奥;              顾佳良;              衣杨;              朱艺;              周小峰;                   沈金龙       </td>   <td>中山大学</td>   <td>一种基于行为关联度融合特征的视频人体行为识别算法</td>   <td>广东省</td>   <td>CN109543590A</td>   <td>2019-03-29</td>   <td>本发明涉及一种基于行为关联度融合特征的视频人体行为识别算法,本发明在考虑视频帧之间时序关系的基础上,通过分别定义轨迹和视频帧与目标行为之间的关联度,突出了更具表达力的特征在形成视频表示过程中的重要性,并且与基于时间分割网络得到的视频表示进行融合,得到更具判别力的视频表示,有利于更有效地识别自然场景下视频中的人体行为。</td>   <td>1.一种基于行为关联度融合特征的视频人体行为识别算法,其特征在于,包括以下步骤：步骤S1：输入视频,计算视频中轨迹的行为关联度；步骤S2：根据轨迹的行为关联度生成基于轨迹关联度的视频帧表示；步骤S3：根据基于轨迹关联度的视频帧表示计算视频帧的行为关联度；步骤S4：生成基于视频帧行为关联度的视频表示；步骤S5：从视频中提取光流以及将时间分割网络作为特征提取器；步骤S6：生成基于时间分割网络的视频表示；步骤S7：结合基于视频帧行为关联度的视频表示以及基于时间分割网络的视频表示,利用支持向量机进行学习和分类,产生视频相应的动作类别标签。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;                   叶海佳       </td>   <td>中山大学</td>   <td>一种加入注意力机制的人脸识别方法</td>   <td>广东省</td>   <td>CN109543606A</td>   <td>2019-03-29</td>   <td>本发明公开了一种加入注意力机制的人脸识别方法,先用级联的神经网络对数据集进行人脸检测和人脸对齐处理,再构建加入注意力机制的深度神经网络,训练注意力机制网络,最后将测试样本输入训练好注意力机制网络进行人脸识别。本发明采用STN模块构建注意力机制,在深度神经网络的每一个阶段后都输入到不同的STN模块,把STN模块的串联输出结果和深度神经网络的输出结果融合起来,作为输出特征。为了让网络能够自适应地学习到具有判别力的感兴趣区域特征,本发明采用通过STN模块对输入进行仿射变换操作的方法,加强了网络对局部信息的理解与学习,在现有的人脸识别网络上,提高了人脸识别的准确率,增强了识别系统的鲁棒性。</td>   <td>1.一种加入注意力机制的人脸识别方法,其特征在于,包括下述步骤：S1：使用级联的卷积神经网络进行图像预处理,得到对齐的人脸图像；S2：对预处理后的图像进行数据增广,所述数据增广包括随机裁剪和随机翻转操作,经过步骤S1处理后的图像随机裁剪出设定的尺寸区域,以设定的概率对图像进行翻转,最后对图像做白化处理,对于测试样本则直接归一化成设定尺寸的图像,然后进行白化处理,所述设定尺寸与随机裁剪的设定尺寸相同；S3：设置注意力机制模块,用于网络自动学习到具有判别性的人脸块特征,利用注意力机制模块将输入的图像进行卷积操作,然后进行全连接回归输出M个角度值,M为自然数,基于M个角度值构建矩阵,通过矩阵运算提取图像的局部特征；S4：搭建注意力机制网络,采用深度神经网络提取图像特征,并加入注意力机制模块,所述注意力机制网络包括主路和支路,所述主路为图片通过深度神经网络后得到的输出,所述支路为深度神经网络的每个阶段的输出经过不同的注意力机制模块,再依次进行elementwise-add后得到的输出,最后把主路和支路的输出进行特征拼接,得到最终的图像特征图,用于计算损失函数和作为人脸识别的特征；S5：训练注意力机制网络,采用人脸识别损失函数对注意力机制网络进行训练并且保存；S6：提取图像特征,将测试样本输入到训练好的注意力机制网络中,得到优质的图像特征；S7：人脸识别,把提取得到的图像特征用softmax回归方法进行分类,完成测试样本的识别。</td>   <td>G06K9/00;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         朱艺昕;              朱俊勇;                   郑伟诗       </td>   <td>中山大学</td>   <td>一种抗遮挡的服饰关键点检测方法</td>   <td>广东省</td>   <td>CN109543700A</td>   <td>2019-03-29</td>   <td>本发明提出了一种抗遮挡的服饰关键点检测方法,包括金字塔网络的结构设计,空洞卷积模块的应用,和网络框架的训练与测试。本发明突破了手动设计特征与规则的限制,利用金字塔网络中的特征提取模块自动学习融合特征和规则,并且,应用空洞卷积模块进一步提高模型在服饰关键点检测上的稳定性。本发明检测的服饰关键点能很好的适应背景环境复杂以及遮挡的情况,显式的解决了困难关键点的检测问题,同时在训练与测试的复杂度和时间消耗上,有进一步优化。</td>   <td>1.一种抗遮挡的服饰关键点检测方法,其特征在于,包括下述步骤：构建改良后的金字塔深度模型,所述改良后的金字塔深度模型包括特征提取网络以及难点挖掘网络,所述特征提取网络包括五个模块的卷积神经网络,每一模块都由多个子模块组成,所述子模块结构由卷积模板为3×3的卷积层、批归一化层、激活函数为Leaky Relu的激活层、卷积模板为3×3的卷积层、批归一化层、激活函数为Leaky Relu的激活层构成,其输出的特征图和输入的特征图逐元素相加得到下一个模块的输入；所述难点挖掘网络为两个模块的卷积神经网络,每一模块均依次由卷积模板为1×1的卷积层、批归一化层、激活函数为Leaky Relu的激活层、卷积模板为3×3的反卷积层、批归一化层构成,激活函数为Leaky Relu的激活层、卷积模板为1×1的卷积层、批归一化层构成,其输出的特征图和输入在通道上合并得到下一个模块的输入；对改良后的金字塔深度模型进行训练,步骤如下：将服饰图像和对应坐标标签作为特征提取网络的输入,计算特征提取网络的损失函数<Image id="icf0001" he="59" wi="79" file="FDA0001884451150000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>并利用优化器对网络参数进行更新,得到特征提取网络参数；将得到的特征提取网络第四模块的输出作为难点挖掘网络的输入,计算难点挖掘网络损失函数<Image id="icf0002" he="57" wi="83" file="FDA0001884451150000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>并利用优化器对网络参数进行更新,得到难点挖掘网络参数；判别迭代结束条件,最后一次迭代过程求得的特征提取和难点挖掘网络的参数即为最终的网络参数；将服饰图像输入训练好的改良后的金字塔深度模型的特征提取网络中,通过关键点提取和难点挖掘技术,输出最终的服饰关键点。</td>   <td>G06K9/46;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖梓威;                   权小军       </td>   <td>中山大学</td>   <td>一种基于情感分析模型的情感趋势预测方法</td>   <td>广东省</td>   <td>CN109543722A</td>   <td>2019-03-29</td>   <td>本发明提供一种基于情感分析模型的情感趋势预测方法,包括以下步骤：对目标文本进行处理,得到文本序列；构建基于自注意力机制的情感分析模型,将文本序列导入模型中,完成目标文本的情感趋势分析。本发明提供的一种基于情感分析模型的情感趋势预测方法,采用神经网络作为模型基础架构,结合自注意力机制和双向长短期记忆网络,在准确获取每个词的语义信息及位置信息的同时,充分考虑到每个单词本身的注意力分配,从而捕获到文本的长距离依赖关系。</td>   <td>1.一种基于情感分析模型的情感趋势预测方法,其特征在于,包括以下步骤：S1：对目标文本进行处理,得到文本序列；S2：构建基于自注意力机制的情感分析模型,将文本序列导入模型中,完成目标文本的情感趋势分析。</td>   <td>G06K9/62;G06N3/04;G06N3/08;G06F16/35</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              王伟轩;              黄福强;                   张俊轩       </td>   <td>中山大学</td>   <td>基于自适应模糊修复机制的车牌识别方法</td>   <td>广东省</td>   <td>CN109543753A</td>   <td>2019-03-29</td>   <td>本发明涉及计算机视觉识别技术领域,提出一种基于自适应模糊修复机制的车牌识别方法,包括以下步骤：搜索并收集车牌识别数据集的图像,构建训练数据集；利用所述训练数据集单独构建与训练车牌检测模块、车牌分类模块、车牌修复模块和车牌识别模块；将待识别图像输入车牌检测模块中,对其进行目标特征的切割,得到车牌的坐标信息；输入车牌分类模块中对车牌图像是否发生畸变进行判断；输入车牌修复模块中根据图像的畸变类别对车牌图像进行修复；输入车牌识别模块中对图像进行车牌号码识别。本发明能够实现对任意车牌图像的车牌进行检测、修复和识别,能够有效地定位车牌在图像中的位置,修复发生模糊的车牌图像,有效地识别出图像中的车牌号码。</td>   <td>1.基于自适应模糊修复机制的车牌识别方法,其特征在于,包括以下步骤：S1：搜索并收集车牌识别数据集的图像,构建训练数据集；S2：利用所述训练数据集构建与训练车牌检测模块；S3：利用所述训练数据集构建与训练车牌分类模块；S4：利用所述训练数据集构建与训练车牌修复模块；S5：利用所述训练数据集构建与训练车牌识别模块；S6：将待识别的车牌图像输入所述车牌检测模块中,车牌检测模块对车牌图像进行目标特征的切割,得到车牌在图像中的坐标信息；S7：将车牌图像输入所述车牌分类模块中,车牌分类模块对车牌图像是否发生畸变进行判断,若是执行S8步骤,若否执行S9步骤；S8：将车牌图像输入所述车牌修复模块中,车牌修复模块对车牌图像进行修复；S9：将车牌图像输入所述车牌识别模块中,车牌识别模块对车牌图像进行车牌号码识别。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;                   尹靓璐       </td>   <td>中山大学</td>   <td>基于端对端深度学习的目标检测与语义分割的并行方法</td>   <td>广东省</td>   <td>CN109543754A</td>   <td>2019-03-29</td>   <td>本发明提供一种基于端对端深度学习的目标检测与语义分割的并行方法,通过对海量已标注目标检测框和像素级目标分割的图像训练得到一个由目标检测神经网络Darknet-19、目标分割全卷积神经网络FCN组成的模型,成功实现了并行目标检测和目标分割的任务,该发明能够有效地提取图像特征,在保证目标检测和目标分割的基础上,实现实时性的图像处理功能,有较广的应用前景。</td>   <td>1.一种基于端对端深度学习的目标检测与语义分割的并行方法,其特征在于,过程如下：S1：构造与训练深层神经网络Darknet-19；S2：构造与训练全卷积神经网络FCN；S3：使用得到的深层神经网络Darknet-19和全卷积神经网络FCN对输入图像进行目标分类、定位及像素级别的分割。</td>   <td>G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         卢伟;                   刘先进       </td>   <td>中山大学</td>   <td>一种基于频谱分析和差分图像极值点距离分布的JPEG图像下采样因子估计方法</td>   <td>广东省</td>   <td>CN109544502A</td>   <td>2019-03-29</td>   <td>本发明涉及数字图像取证技术领域,更具体地,涉及一种基于频谱分析和差分图像极值点距离分布的JPEG图像下采样因子估计方法。本发明首先计算差分图像的局部极值点,并获取其相邻极值点距离的分布。通过秩统计分析以及大量实验表明原始未压缩图像的差分极值点距离分布服从几何分布,而JPEG图像分布存在周期性峰值。因此该分布可用于检验图像是否存在JPEG块效应并获取其下采样因子的区间估计。对差分图像计算2D傅里叶变换并通过极大值滤波器定位频谱峰值点。结合差分图像极值点距离分布方法和频谱分析方法获取最终下采样因子估计。</td>   <td>1.一种基于频谱分析和差分图像极值点距离分布的JPEG图像下采样因子估计方法,其特征在于,包括以下步骤：S1.对待测图像进行色彩通道选择：如果待测图像是灰度图像则直接进行S2步骤,如果训练图像是彩色图像,则首先选择绿色G通道再进行S2步骤；S2.对S1步骤得到的图像计算其差分的局部极值点,并获取其相邻极值点距离的分布：对差分图像进行局部极值滤波,并计算相邻极值点的距离分布；S3.获取下采样因子区间估计：对经过S2步骤得到的分布图并计算峰值周期T,得到区间估计<Image id="icf0001" he="95" wi="313" file="FDA0001810536710000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>S4.计算差分频谱；对S1步骤得到的图像计算其差分的平方,再计算2D傅里叶变换并得到能量频谱图；S5.获取下采样峰值点：结合S3得到的区间估计和S4的频谱图,选择满足估计区间的最高峰值点；S6.估计下采样因子：S5得到的下采样峰值位置为P,则下采样因子估计值为          <Image id="icf0002" he="96" wi="190" file="1.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>。</td>   <td>G06T7/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王伟;              匡铭;              吕明德;              谢晓燕;              陈立达;              李薇;              陈淑玲;                   徐明       </td>   <td>中山大学附属第一医院</td>   <td>基于深度学习的多模态超声组学分析方法及系统</td>   <td>广东省</td>   <td>CN109544517A</td>   <td>2019-03-29</td>   <td>本发明公开了一种基于深度学习的多模态超声组学分析方法及系统,包括：获取病变部位的若干超声数据,得到多模态超声组学数据；将多模态超声组学数据输入深度学习神经网络,根据多模态超声组学数据调整神经元的连接权重、配比卷积和池化层,得到调整后的多模态超声组学数据；利用不同模态下的分类器对多模态超声组学数据进行分类,并通过判别器得到每个分类的分析结果,即每个分类的分数。相比于现有通过建立单模态医学图像的深度学习模型进行医学图像的分割、分类和识别的方法,本发明技术方案通过训练好的深度学习网络模型对优化后的多模态超声组学数据进行数据分析,提高超声图像获取的准确性,减少工作量。</td>   <td>1.一种基于深度学习的多模态超声组学分析方法,适于在计算设备中执行,其特征在于,至少包括如下步骤：获取病变部位的若干超声数据,得到多模态超声组学数据；将所述多模态超声组学数据输入训练好的深度学习神经网络,并根据所述多模态超声组学数据调整神经元的连接权重、配比卷积和池化层,得到调整后的多模态超声组学数据；利用不同模态下的分类器对调整后的多模态超声组学数据中的每个数据进行分类,得到包含每个分类的所有模态的分类概率；根据判别器给出的模态之间的混淆分数,对所有模态的分类概率进行加权平均处理,得到每个分类的分数。</td>   <td>G06T7/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡凇源;              周晓聪;                   衣杨       </td>   <td>中山大学</td>   <td>空指针检测方法</td>   <td>广东省</td>   <td>CN109522232A</td>   <td>2019-03-26</td>   <td>本发明提供一种空指针检测方法,涉及静态分析技术领域。具体包括：获取待分析程序的控制流图；获取所述控制流图中的所有节点；沿着所有可能的路径,遍历所述控制流图中的所有节点,获取每个所述节点内待检测的变量值与到达条件；遍历至所述控制流图出口节点时,结合所述出口节点的变量的值与到达条件,确定返回的变量值是否为空。相对于现有技术,通过与定值到达条件的结合,可以有效去除掉定值不可能到达的情况,提高了到达定值分析给出的结论准确度不高的问题。</td>   <td>1.一种空指针检测方法,其特征在于,包括：获取待分析程序的控制流图；获取所述控制流图中的所有节点；沿着所有可能的路径,遍历所述控制流图中的所有节点,获取每个所述节点内待检测的变量值与到达条件；遍历至所述控制流图出口节点时,结合所述出口节点的变量的值与到达条件,确定返回的变量值是否为空。</td>   <td>G06F11/36</td>  </tr>        <tr>   <td>中国专利</td>   <td>         任江涛;                   高爽超       </td>   <td>中山大学</td>   <td>文本关系抽取模型的训练方法、装置及可读存储介质</td>   <td>广东省</td>   <td>CN109522557A</td>   <td>2019-03-26</td>   <td>本发明公开了一种文本关系抽取模型的训练方法,包括以下步骤：计算所述字向量与所述实体对的向量之间的位置信息,并根据所述位置信息生成位置向量；将所述字向量与所述位置向量拼接,生成联合字向量；获取所述实体对的向量对应的关系类别向量；根据所述关系类别向量和所述联合字向量计算所述字向量的注意力权重,并根据所述注意力权重和所述联合字向量确定所述训练文本的特征向量,基于所述训练文本的特征向量采用约束损失函数训练所述文本关系抽取模型的参数。本发明还公开了一种文本关系抽取模型的训练装置及计算机可读存储介质。本发明实现了更细粒度下实体关系的识别,改善了模型抽取实体关系的效果。</td>   <td>1.一种文本关系抽取模型的训练方法,其特征在于,所述文本关系抽取模型的训练方法包括以下步骤：获取训练文本中实体对的向量及所述训练文本中每个字的字向量；计算所述字向量与所述实体对的向量之间的位置信息,并根据所述位置信息生成位置向量；将所述字向量与所述位置向量拼接,生成联合字向量；获取所述实体对的向量对应的关系类别向量；根据所述关系类别向量和所述联合字向量计算所述字向量的注意力权重,并根据所述注意力权重和所述联合字向量确定所述训练文本的特征向量；基于所述训练文本的特征向量采用约束损失函数训练所述文本关系抽取模型的参数。</td>   <td>G06F17/27</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;              胡康;                   朱智慧       </td>   <td>中山大学</td>   <td>一种基于小样本学习的动作相似度评估方法</td>   <td>广东省</td>   <td>CN109522850A</td>   <td>2019-03-26</td>   <td>本发明公开了一种基于小样本学习的动作相似度评估方法,该方法步骤包括建立数据预处理模型、训练模型、测试模型,采用人体姿态估计模型提取人体整体骨架运动视频以及各关节点位置,排除了背景干扰,并且对人的动作按照关节点位置拆分,设定采样像素值和采样间隔,截取得到包括人体整体骨架运动视频和以各关节点为中心的关节动作的采样视频,采样视频结合了局部信息与全局信息,通过数据预处理后,再使用改写的三元组损失函数进行训练,最后将视频数据映射至余弦空间,计算余弦距离,输出视频中人的动作整体及各个关节相似程度结果。本发明仅用很少的样本就可以学习到一个很好的动作特征映射模型,进而得到一个很好的动作相似度结果。</td>   <td>1.一种基于小样本学习的动作相似度评估方法,其特征在于,包括下述步骤：建立数据预处理模型：采用人体姿态估计模型提取人体整体骨架运动视频以及各关节点位置；设定采样像素值和采样间隔,截取得到采样视频,采样视频包括人体整体骨架运动采样视频和以各关节点为中心的关节动作采样视频；建立训练模型：选取训练数据,确定模板数据、正类数据、负类数据；将模板数据、正类数据、负类数据分别输入到三维卷积神经网络的特征提取器,得到特征图,再经过全连接层的计算分别得到模板特征向量,正类特征向量,负类特征向量；模板特征向量与正类特征向量对应计算出第一余弦距离；模板特征向量与负类特征向量对应计算出第二余弦距离；采用三元组损失函数训练,更新参数后输出训练好的特征提取器；所述第一余弦距离或第二余弦距离的计算公式如下所述：          <Image id="icf0001" he="151" wi="620" file="FDA0001875271940000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,cos为计算所得余弦值,取值范围为[0,1],余弦值越接近1,表明夹角越接近于0度,两个特征向量越相似,x<Sub>i</Sub>为模板数据的第i个特征向量,y<Sub>i</Sub>正类或负类数据的第i个特征向量；建立测试模型：输入两段待测视频,经过数据预处理模型进行数据预处理；分别输入已经训练好的特征提取器,将得到的特征向量对应计算余弦距离；计算余弦距离的平均值,输出视频中人的动作整体动作的相似度分数。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑子彬;                   曾璇       </td>   <td>中山大学</td>   <td>一种基于多模型堆叠融合预测的方法</td>   <td>广东省</td>   <td>CN109522917A</td>   <td>2019-03-26</td>   <td>本发明公开了一种基于多模型堆叠融合预测的方法,旨在解决强变量缺失的情况下,利用弱变量客观公正评价企业经营状况,预测企业在未来一段时间内的经营风险状况。本发明首先对企业经营行为数据进行数据分析,并对企业经营行为数据进行数据预处理和特征提取；利用提取的特征训练若干单模型并验证模型效果,选择最优的单模型用于堆叠融合；通过多个单模型堆叠融合的方法预测出企业经营风险的概率值。这种方法对企业的经营退出风险有很好的预测能力。</td>   <td>1.一种基于多模型堆叠融合预测的方法,其特征在于：包括以下步骤：S1：从7个单模型中选取5个单模型作为第一层的预测模型；S2：将数据样本分成五份,依次取其中四份作为训练集,一份作为测试集,分别对五个单模型进行预测,其中每个单模型的测试集都不一样；S3：第j个单模型对第i个训练样本的预测结果将作为新的训练集中第i个样本的第j个特征值,并作为新的训练集,其中<Image id="icf0001" he="21" wi="140" file="928077DEST_PATH_IMAGE001.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>S4：第j个单模型对第i个测试样本的预测结果将作为新的测试集中第i个样本的第j个特征值,所有测试结果作为新的测试集输入到二层的单模型中进行测试,其中<Image id="icf0002" he="21" wi="140" file="219381DEST_PATH_IMAGE001.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>S5：对每个单模型设置三个不同的随机种子测试同一模型三次,预测得到的三个结果取算数平均做为该模型的预测值,所有模型预测得到的预测值作为新的测试集；S6：从7个单模型中选取1个单模型作为第二层的预测模型,使用S2中的新的训练集对该单模型进行训练；S7：将S4中的新的测试集预测S6中训练好的单模型,得到的结果即为预测值。</td>   <td>G06K9/62;G06Q10/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;                   黄福强       </td>   <td>中山大学</td>   <td>基于跨尺度特征融合的深度卷积神经网络目标检测方法</td>   <td>广东省</td>   <td>CN109522958A</td>   <td>2019-03-26</td>   <td>本发明公开一种基于跨尺度特征融合的深度卷积神经网络目标检测方法,使用公开目标检测数据集上大量标注了物体类别及边界框信息的图片训练得到一个由三部分(深度卷积神经网络,跨尺度特征融合模块,检测器和分类器)组成的目标检测模型,并实现对任意分辨率输入图片进行处理。该发明首先由深度卷积神经网络的前馈过程生成一系列具有不同分辨率的特征图,然后跨尺度特征融合模块对不同分辨率的特征图进行融合,生成一个更具鲁棒性和辨别性的特征金字塔,最后由检测器对特征金字塔进行检测并由分类器对输出的检测结果进行分类。本发明生成的特征图能有效抑制背景干扰,对有复杂背景信息的输入图片具有很好的表现。</td>   <td>1.一种基于跨尺度特征融合的深度卷积神经网络目标检测方法,其特征在于：包括以下步骤：S1：将待测图片输入到特征提取网络,输出不同分辨率的特征图；S2：将特征提取网络输出的两张特征图输入到跨尺度特征融合模块进行特征融合；S3：对特征融合后的特征图进行噪声抑制、降维、抗混叠处理得到新的特征图,并将处理后的特征图输入至跨尺度特征融合模块中与S1中的另一张输出特征图进行特征融合；S4：将S3得到的特征融合后的特征图进行检测、分类、计算误差并对检测模型进行训练以及参数更新,并利用训练好的模型进行目标检测。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨猛;                   陈家铭       </td>   <td>中山大学</td>   <td>一种基于字典深度学习的半监督图像分类方法</td>   <td>广东省</td>   <td>CN109522961A</td>   <td>2019-03-26</td>   <td>本发明公开了一种基于字典深度学习的半监督图像分类方法,该方法步骤如下：根据深度神经网络的Softmax代价函数构建有标签数据的代价函数L<Sup>l</Sup>(Z<Sup>l</Sup>,Y<Sup>l</Sup>)、无标签数据的代价函数L<Sup>u</Sup>(Z<Sup>u</Sup>,P)：根据有标签数据的代价函数L<Sup>l</Sup>(Z<Sup>l</Sup>,Y<Sup>l</Sup>)、无标签数据的代价函数L<Sup>u</Sup>(Z<Sup>u</Sup>,P)构建总体模型函数；使用交替优化算法以训练总体模型,训练优化过程包括：基于字典学习和Softmax网络信息的联合类别估计、神经网络和无标签类别估计的联合学习。本发明用字典学习与深度神经网络的Softmax分类器结合,强化了深度神经网络对无标签数据的探索能力,极大地提升网络特征学习能力和分类器学习能力。本发明适用于计算机视觉或模式识别领域。</td>   <td>1.一种基于字典深度学习的半监督图像分类方法,其特征在于：该方法步骤如下：S1：构建深度神经网络,根据深度神经网络的Softmax代价函数构建有标签数据的代价函数L<Sup>l</Sup>(Z<Sup>l</Sup>,Y<Sup>l</Sup>)、无标签数据的代价函数L<Sup>u</Sup>(Z<Sup>u</Sup>,P),所述无标签数据的代价函数L<Sup>u</Sup>(Z<Sup>u</Sup>,P)包括基于字典表示和Softmax网络信息的联合预测<Image id="icf0001" he="65" wi="352" file="FDA0001877703070000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>联合类别估计p的熵正则项H(p<Sub>r</Sub>)；S2：根据有标签数据的代价函数L<Sup>l</Sup>(Z<Sup>l</Sup>,Y<Sup>l</Sup>)、无标签数据的代价函数L<Sup>u</Sup>(Z<Sup>u</Sup>,P)构建总体模型函数；S3：使用有标签数据的深度特征表示来构建字典,计算字典表示系数和表示残差；S4：根据步骤S3得到的字典表示残差计算<Image id="icf0002" he="64" wi="358" file="FDA0001877703070000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>根据<Image id="icf0003" he="64" wi="319" file="FDA0001877703070000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>计算H(p<Sub>r</Sub>)；预设熵阈值λ,若H(p<Sub>r</Sub>)大于预设阈值λ,则p<Sub>r</Sub>会被设为0,其监督信号p<Sub>r</Sub>将被阻挡；若H(p<Sub>r</Sub>)小于等于预设阈值,则其监督信号p<Sub>r</Sub>将被传播到深度神经网络,通过反向传播提升深度神经网络性能；S5：根据上述步骤得到无标签数据的代价函数和有标签数据的代价函数,得到总体模型函数,并通过SGD算法,利用总体模型函数对深度神经网络进行训练；S6：将完成训练的深度神经网络对未知的无标签数据进行分类处理,完成分类；其中：(Z<Sup>l</Sup>,Y<Sup>l</Sup>)＝{(z<Sub>1:N</Sub>,y<Sub>1:N</Sub>)}表示有标签数据<Image id="icf0004" he="71" wi="160" file="FDA0001877703070000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>的深度特征；Y<Sup>l</Sup>是有标签数据对应的标签；y<Sub>ji</Sub>＝1表示第j个有标签数据z<Sub>j</Sub>属于第i类；N是有标签数据的样本数；C是类别数；Z<Sup>u</Sup>＝{z<Sub>N+1:N+M</Sub>}表示无标签数据<Image id="icf0005" he="66" wi="61" file="FDA0001877703070000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>的深度特征；P＝{p<Sub>1:M</Sub>}是无标签数据的类别估计；M是无标签数据的样本数；<Image id="icf0006" he="64" wi="50" file="FDA0001877703070000016.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示无标签数据<Image id="icf0007" he="65" wi="57" file="FDA0001877703070000017.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>第r类的深度特征；<Image id="icf0008" he="64" wi="58" file="FDA0001877703070000018.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示无标签数据的Softmax代价函数层前面的全连接层的输出；α表示无标签数据<Image id="icf0009" he="63" wi="58" file="FDA0001877703070000019.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>深度特征在子字典上的编码系数；D<Sub>i</Sub>表示与第i类有关的子字典。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;                   罗小凡       </td>   <td>中山大学</td>   <td>一种基于密集连接卷积神经网络的目标检测方法</td>   <td>广东省</td>   <td>CN109522966A</td>   <td>2019-03-26</td>   <td>本发明公开了一种基于密集连接卷积神经网络的目标检测方法,其为了减小参数量,且提高特征重复利用效果,使用多个密集连接块与转换层交替连接的网络结构代替以往的整体结构,进行特征提取能提取到图像中有判别性的特征映射。全局关注模块融合4种不同感受野的特征图,解决以往单层感受野尺寸相同的问题；同时每条支路的后三个卷积层使得底层的特征图在保证分辨率的前提下拥有足够优秀的特征表达。本发明提出的图像目标检测模型能有效地提取图像的特征,提炼出具有不同尺寸感受野并融合多层次信息的特征图；同时语义信息与空间信息的结合提高了小物体的检测效果；同时整个网络能达到端到端的训练,保持实时检测速度的同时,提高了目标的检测效果。</td>   <td>1.一种基于密集连接卷积神经网络的目标检测方法,其特征在于：该目标检测方法如下：S1：将图像输入到用于特征提取的密集连接卷积神经网络,其包括多个密集连接块以及不同的密集连接块之间进行连接的转换层；S2：将最后一个密集连接块的最后一层卷积神经网络输出的特征图输入特征加权融合模块经过卷积神经网络处理,得到5个感受野不同的特征图；然后对特征图进行特征融合处理,得到4个空间语义信息丰富,感受野不同的特征图,将其输入预测层；S3：将最后一个密集连接块输出的特征图输入全局关注模块中,利用空洞卷积制造不同的感受野,然后分别经过相同寸尺与数量的多个卷积层进行处理,获取不同感受野的特征图,并融合为一个高语义的特征图,最后输入到预测层；S4：预测层对输入的特征图进行处理,同时输出目标边界框信息及分类概率,分别对步骤S2中的5个感受野不同的特征图中的所有尺度的特征映射进行预测,输入的特征映射经过一个卷积神经网络后输出一个大小为S*S*(B*5+C)的向量作为预测结果；其中：B是边界框的个数；5代表每个边界框包含的参数个数,包括边界框的中心坐标偏移值(t<Sub>x</Sub>,t<Sub>y</Sub>),边界框的宽高偏移值(t<Sub>w</Sub>,t<Sub>h</Sub>),以及预测边界框的置信度t<Sub>0</Sub>；C代表目标的类别个数；SxS代表将一张图分为SxS个网格；S5：对密集连接卷积神经网络、全局关注模块、特征加权融合模块、预测层组成的图像目标检测模型进行训练；训练开始时,特征加权融合模块和全局关注模块各层的参数按照Xavier的方式初始化；在训练过程中,通过损失函数,进行反向传导算法对整个网络里所有层中的参数进行微调；S6：将图像输入到完成训练的图像目标检测模型中进行目标检测,完成目标检测。</td>   <td>G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赵铜铁钢;              陈晓宏;              刘丙军;                   林凯荣       </td>   <td>中山大学</td>   <td>一种基于随机游走的季节径流预报因子选择方法</td>   <td>广东省</td>   <td>CN109523054A</td>   <td>2019-03-26</td>   <td>本发明提供了一种基于随机游走的季节径流预报因子选择方法。所述方法将添加备选因子的效果概化为“前进-后退”的随机过程,预报有所改进则前进一步,预报有所变差则后退一步。由此,有效的预报因子将使得随机游走过程显著的前进,显著性水平可以通过高斯逼近来检验。相比而言,无效的预报因子将使得随机游走原地徘徊,甚至后退。实验表明,本发明能较好的挑选出有效预报因子,提高季节径流预报精度,具有相当的应用价值。</td>   <td>1.一种基于随机游走的季节径流预报因子选择方法,其特征在于：包括以下步骤：S1.以未来的季节径流作为预报变量,以前期的大气环流特征量作为备选预报因子,构建季节径流预报数据集；S2.基于已选中预报因子集合,构建“基准模型”进行季节径流预报,并评估其预报精度；初始选择阶段,以前期径流作为单一预报因子,不包含任何大气环流因子；随着选择过程的进行,大气环流预报因子的逐个加入,扩充预报因子集合；S3.在已选中的预报因子集合添加一个新的备选因子,构建“新模型”进行统计预报和评估预报精度；基于随机游走模型,评估这个新的备选因子改进预报的效果；S4.逐一评估所有的备选因子,挑选出最优因子；如果该因子能显著的改进预报,也即其对应的随机游走过程显著前进,则将该因子加入已选中预报因子集合,重复迭代运算步骤S2、S3和S4；如果该因子并不能显著的改进预报,则结束迭代运算,输出已选中预报因子集合。</td>   <td>G06Q10/04;G06Q50/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         吴海航;                   黄方军       </td>   <td>中山大学</td>   <td>基于菱形预测与图像像素排序的可逆信息隐藏及提取方法</td>   <td>广东省</td>   <td>CN109523453A</td>   <td>2019-03-26</td>   <td>本发明公开了一种基于菱形预测与图像像素排序的可逆信息隐藏方法,包括：S11：将图像分为3×3大小的若干图像块,判断图像块中是否存在溢出的像素,得到该图像块的位图LM,同时,选取图像第一行像素,将辅助信息以替换LSB的方式进行嵌入,将被替换的LSB附加在密文后方,一并嵌入；S12：将所有图像块分为黑块和白块,秘密信息分两轮嵌入,其中,黑块和白块各嵌入一半秘密信息的密文；S13：按照图像块像素预测值大小增序排列；S14：计算该图像块的块复杂度S15：计算预测误差,按照嵌入规则进行嵌入,最后将嵌入后的像素放回到像素块对应位置,完成图像块的嵌入。本发明在大部分的图像中,PSNR值都明显优于其它算法,本发明更好的发掘了像素间的内部联系。</td>   <td>1.一种基于菱形预测与图像像素排序的可逆信息隐藏方法,其特征在于,包括以下步骤：S11：将图像分为3×3大小的若干图像块,扫描所有图像块,判断图像块中是否存在溢出的像素,得到该图像块的位图LM,若该图像块的位图LM为1,则该图像块不能用于嵌入,同时,选取图像第一行像素,将辅助信息以替换LSB的方式进行嵌入,将被替换的LSB附加在密文后方,一并嵌入；S12：将所有图像块分为黑块和白块,秘密信息分两轮嵌入,其中,黑块和白块各嵌入一半秘密信息的密文；S13：计算图像块像素的预测值,按照预测值大小增序排列；S14：根据预测值计算该图像块的块复杂度,当块复杂度小于预设的最小复杂度阈值T1时,该图像块可用于信息嵌入；当块复杂度大于阈值T1小于预设的最大复杂度阈值T2时,只在该图像块增序排列后的序列两端进行信息嵌入；当块复杂度大于阈值T2时,不可嵌入信息；S15：计算预测误差,按照嵌入规则进行嵌入,最后将嵌入后的像素放回到像素块对应位置,完成图像块的嵌入；S16：对每一图像块执行步骤S13-S15,直到秘密信息嵌入完成。</td>   <td>G06T1/00;G06F21/60</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;                   黄杨健       </td>   <td>中山大学</td>   <td>一种基于条件生成对抗网络的人脸老化方法</td>   <td>广东省</td>   <td>CN109523463A</td>   <td>2019-03-26</td>   <td>本发明提供了一种基于条件生成对抗网络的人脸自动老化机制,通过对于海量已标注年龄的不同年龄段的图像训练得到一个由四个部分组成的条件生成对抗网络,包括图像生成器G,图像判别器D,年龄估计网络AEN和身份识别网络FRN。其中,G被训练用于生成老化图像,通过输入年轻图像和预设的年龄条件,自动有效地生成年老图像。D用于鉴别生成的年老图像是否为真实图片,能够确保生成的年老图片具备欺骗性。AEN是用于减小生成图像的年龄与预设值的差异,而FRN则是保证生成过程中人像身份的一致性。发明通过对网络结构的设计,使整个网络达到端对端的训练,并且在人脸老化方面有很好的表现,能够生成身份一致,欺骗性强和分辨率高等优点的优质人脸老化图像。</td>   <td>1.一种基于条件生成对抗网络的人脸老化方法,其特征在于,包括以下步骤：S1：搜集人脸数据,并对人脸数据进行预处理；S2：输入预处理后的人脸数据分别训练年龄估计网络AEN和身份识别网络FRN；S3：编码年龄信息,构造图像生成器G和图像判别器D,并将图像生成器G的生成图像分别输入到构造的图像判别器D、训练好的年龄估计网络AEN以及身份识别网络FRN中,计算图像判别器、年龄估计网络、身份识别网络的损失函数并将损失函数进行融合作为生成器G最终的损失函数,从而构造生成对抗网络Age-GAN,训练生成对抗网络Age-GAN；S4：将待测数据输入到生成对抗网络Age-GAN中用于人脸老化。</td>   <td>G06T3/00;G06N3/08;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;              林辉;                   钟圳伟       </td>   <td>中山大学</td>   <td>一种基于单目摄像机的光照估计方法</td>   <td>广东省</td>   <td>CN109523617A</td>   <td>2019-03-26</td>   <td>本发明涉及计算机图形学,为基于单目摄像机的光照估计方法,包括以下步骤：单目摄像机采集RGB图像,作为深度估计的输入；构建用于单目摄像机深度估计的卷积神经网络并进行训练；将RGB图像输入到训练好的卷积神经网络进行深度估计,得到深度预测值,输出深度预测图；将深度预测值进行上采样,使深度预测图的尺寸与RGB图像相匹配,将上采样后的深度预测值作为光照估计的输入；将RGB图像转换到CIELab颜色空间下,其中的亮度通道信息作为光照估计的输入；利用亮度通道信息、深度预测值进行光照估计,将得到真实场景各方向光源信息的球谐函数系数。该方法通过单目摄像机获取真实场景中各方向光源信息,有效提高AR技术中渲染虚拟物体的逼真效果。</td>   <td>1.一种基于单目摄像机的光照估计方法,其特征在于,包括以下步骤：第一步、单目摄像机采集RGB图像,作为深度估计的输入；第二步、构建用于单目摄像机深度估计的卷积神经网络,对卷积神经网络进行训练；将RGB图像输入到训练好的卷积神经网络进行深度估计,得到深度预测值,输出深度预测图；第三步、将深度预测值进行上采样,使得深度预测图的尺寸与RGB图像相匹配,将上采样后的深度预测值作为光照估计的输入；第四步、将单目摄像机采集的RGB图像转换到CIELab颜色空间下,其中的亮度通道信息作为光照估计的输入；利用CIELab颜色空间下的亮度通道信息、深度估计到的深度预测值进行光照估计,将得到真实场景各方向光源信息的球谐函数系数。</td>   <td>G06T15/00;G06T7/593;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李明;              翁时涛;                   王尧       </td>   <td>中山大学;广东顺德中山大学卡内基梅隆大学国际联合研究院</td>   <td>一种自动说话人识别中针对语音欺骗的对抗方法</td>   <td>广东省</td>   <td>CN105139857B</td>   <td>2019-03-22</td>   <td>本发明提出了一种自动说话人识别中针对欺骗技术的对抗方法,该方法是基于多种特征和多个子系统融合方法的反语音欺骗技术。本发明通过融合语音层面音素后验概率串联特征和声音层面MFCC特征或相位层面MFDCC特征,系统的性能得到显著的提升。通过组合已经提出的i-vector子系统和涵盖声音和韵律层信息的OpenSMILE(open Speech and Music Interpretation by Large Space Extraction)基准,进一步提升了系统最终的表现。对于后端模型,使用开发数据,在欺骗性攻击已知的情况下,两级的支持向量机比一级余弦相似度或PLDA评分有更加优异的表现。而在测试数据不可见且欺骗性条件未知的情况下,一级评分方式表现出了更强的鲁棒性。</td>   <td>1.一种自动说话人识别中针对语音欺骗的对抗方法,其特征在于,包括以下步骤：1)音频数据的采集；2)特征提取,提取上述音频数据的四种特征,分别为：OpenSMILE特征,MFCC特征,MFCC-PPP特征和MGDCC-PPP特征,其中,其中,MFCC-PPP特征为音素后验概率串联特征PPP,MGDCC-PPP特征为修正的群延时倒谱系数特征；3)分类判别,对得到的特征采用多种方式进行分类,得到分类结果,其中分类方式包括K近邻分类、余弦相似度评分、PLDA建模和支持向量机SVM；4)得分融合：在评分层面采用加权求和融合的方法来进一步提升性能,融合的权重是通过开发数据集调试出来的。</td>   <td>G10L17/02;G10L17/08;G10L25/24</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈嘉谦;              朱艺;              沈金龙;              顾佳良;              吴昱焜;                   衣杨       </td>   <td>中山大学</td>   <td>一种视频中人体行为识别的方法</td>   <td>广东省</td>   <td>CN109508684A</td>   <td>2019-03-22</td>   <td>本发明涉及人工智能领域,更具体的,涉及一种视频中人体行为识别的方法,本发明的主要算法思想为：首先对输入的视频片段进行预处理,使用六参数仿射模型模拟摄像机运动,通过仿射模型获取运动轨迹；接着对视频基于时间显著性和空间显著性进行关键帧提取；然后对关键帧提取改进密集轨迹,同时选择双流卷积神经网络作为特征提取器,对关键帧提取深度学习特征；对提取出来的特征进行归一化,并基于视频表示的对特征进行融合,融合基于深度特征的视频表示与基于改进密集轨迹特征的视频表示。本发明融合手工设计的改进密集轨迹特征IDT和深度学习特征,可更有效的挖掘两种特征的互补信息与视频中的行为模式,在视频人体行为识别取得更好的效果。</td>   <td>1.一种视频中人体行为识别的方法,其特征在于,具体包括以下步骤：步骤S1：对视频进预处理,获取整个视频所有帧的改进后的密集轨迹；步骤S2：基于时间显著性和空间显著性对视频进行关键帧提取；步骤S3：通过关键帧筛选步骤S1中获得的密集轨迹,对于关键帧的密集轨迹进行保留,去除非关键帧的密集轨迹；步骤S4：在改进后的密集架轨迹的基础上对视频进行基于层次轨迹束的视频表示；步骤S5：提取关键帧的深度学习特征,对视频进行基于深度特征的视频表示；步骤S6：将基于层次轨迹束的视频表示和基于深度特征的视频表示进行融合；步骤S7：对融合后的视频进行SVM分类。</td>   <td>G06K9/00;G06T7/269</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡海峰;              吴永波;                   王伟轩       </td>   <td>中山大学</td>   <td>一种对抗强化的表情识别方法</td>   <td>广东省</td>   <td>CN109508689A</td>   <td>2019-03-22</td>   <td>本发明提出一种对抗强化的表情识别方法,其中的表情识别网络包括特征编码器和特征分类器,生成对抗网络包括特征编码器、特征解码器和鉴别器,首先构建人脸表情和无表情图片集,并对图片集的图片进行姿态归一化处理；将表情图片输入表情识别网络的特征编码器中获得表情特征编码,然后输入到特征分类器中进行分类输出；将无表情图片输入生成对抗网络的特征编码器中进行编码,获得无表情特征编码；将上述两种特征编码和高斯噪声进行串接,得到融合特征编码后输入到特征解码器中,输出新的表情图片；以新的表情图片为负样本,以输入的表情图片为正样本,鉴别器对其类别进行鉴别,重复上述步骤对表情识别网络和生成对抗网络进行交替训练并更新参数。</td>   <td>1.一种对抗强化的表情识别方法,其特征在于,包括表情识别网络和生成对抗网络,其中所述表情识别网络包括特征编码器和特征分类器,所述生成对抗网络包括特征编码器、特征解码器和鉴别器；具体步骤如下：S1：构建人脸无表情图片集和人脸表情图片集,并对人脸无表情图片集和人脸表情图片集中的图片进行姿态归一化处理；S2：将完成处理的人脸表情图片输入表情识别网络的特征编码器中进行编码,获得对应的表情特征编码,并将表情特征编码输入到表情识别网络的特征分类器中进行分类输出；S3：将完成处理的人脸无表情图片输入生成对抗网络的特征编码器中进行编码,获得对应的无表情特征编码；S4：将表情特征编码、无表情特征编码和高斯噪声进行串接,得到融合特征编码,然后输入到生成对抗网络的特征解码器,输出新的人脸表情图片；S5：以所述生成的新的人脸表情图片作为负样本,以输入表情识别网络的人脸表情图片作为正样本,使用生成对抗网络的鉴别器对正负样本图片的类别进行鉴别；S6：重复执行步骤S2～S5对表情识别网络和生成对抗网络进行训练,并基于训练的结果对表情识别网络、生成对抗网络的参数进行更新；S7：将待识别的人脸表情图片输入完成训练的表情识别网络中进行表情识别分类。</td>   <td>G06K9/00;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李仲泓;              吴昱焜;              衣杨;              沈金龙;              佘滢;                   朱艺       </td>   <td>中山大学</td>   <td>一种基于二叉树的人体行为识别方法</td>   <td>广东省</td>   <td>CN109508698A</td>   <td>2019-03-22</td>   <td>本发明公开了一种基于二叉树的人体行为识别方法,应用于计算机视觉领域,旨在于解决现有技术中没有考虑到轨迹之间的相互关系以及对运动部分的特征提取不够细致的问题。本发明首先提取输入视频的综合显著轨迹；然后计算每条轨迹的特征描述符,包括新提出的均衡描述符；再利用谱聚类算法将视频的轨迹分成粒度不一的节点,构造中层语义二叉树；由于经费舍尔向量编码后的特征表示的维度过高,本发明采用子空间随机投影对编码向量进行降维；最终利用线性核的SVM(状态向量机)对特征表示分类,得到视频行为的类别标签。本方法在一定程度上移除背景的干扰,并提高了识别准确度。</td>   <td>1.一种基于二叉树的人体行为识别方法,其特征在于,包括以下步骤：S1：输入视频,对视频帧中的特征点进行采样,对采样后的特征点进行跟踪,生成轨迹,再对轨迹进行筛选；S2：计算筛选后轨迹的显著值,提取出综合显著轨迹；S3：根据求得的综合显著轨迹计算轨迹的特征描述符,用来量化轨迹特征；S4：根据轨迹特征将轨迹进行分类,并利用谱聚类方法将视频的轨迹分类到若干集合中,即分类到二叉树节点中,构造中层语义二叉树；S5：对若干集合内的轨迹进行编码得到编码向量,采用子空间随机投影对编码向量进行降维,并将若干集合的降维后的编码向量进行融合,用来表示一个视频；S6：利用线性核的SVM对视频进行分类,得到视频行为的类别标签并输出结果。</td>   <td>G06K9/00;G06K9/62;G06T7/269</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李熙莹;                   周智豪       </td>   <td>中山大学</td>   <td>一种基于融合特征的车辆重识别方法、系统及装置</td>   <td>广东省</td>   <td>CN109508731A</td>   <td>2019-03-22</td>   <td>本发明公开了一种基于融合特征的车辆重识别方法、系统及装置,其中,方法包括以下步骤：获取待识别车辆的图像后,根据预设的部件检测模型在图像中计算至少两个预设部件的区域信息；根据各区域信息对图像进行特征提取后,将提取到的特征进行串联融合,并获取融合特征；结合融合特征和预设的检索数据库计算欧氏距离,根据计算的欧氏距离进行车辆重识别。本发明有针对性的选取具有代表性的部件,对选取的部件进行部件检测以及特征提取融合,在一定程度上减少了其余无代表性车辆部分所带来的干扰,在相同车辆型号下的不同车辆个体的重识别中,能获取更好的效果,提高了重识别的准确率,可广泛应用于车辆重识别技术领域。</td>   <td>1.一种基于融合特征的车辆重识别方法,其特征在于,包括以下步骤：S1、获取待识别车辆的图像后,根据预设的部件检测模型在图像中计算至少两个预设部件的区域信息；S2、根据各区域信息对图像进行特征提取后,将提取到的特征进行串联融合,并获取融合特征；S3、结合融合特征和预设的检索数据库计算欧氏距离,根据计算的欧氏距离进行车辆重识别。</td>   <td>G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         吴迪;                   杨敏       </td>   <td>中山大学</td>   <td>一种基于用户学习行为的个性化试题推荐方法</td>   <td>广东省</td>   <td>CN109509126A</td>   <td>2019-03-22</td>   <td>本发明公开了一种基于用户学习行为的个性化试题推荐方法,该方法如下：从在线教育平台获取用户历史做题数据、试题与知识点信息；根据用户历史做题数据构造用户-试题得分矩阵R,根据试题与知识点之间的关联构造试题-知识点关联矩阵Q；通过DINA模型构建用户认知诊断模型,得到用户知识点掌握矩阵A；根据矩阵A得到用户-相邻试题集,根据矩阵Q得到试题-相邻试题集,构建备选试题集；对矩阵R非负矩阵分解,得到用户和试题的隐含特征矩阵W和H,求出W和H矩阵的估计值,得到得分预测模型；计算用户潜在作答情况,并将目标用户自己选择难度范围的试题推荐给目标用户。本发明能准确的将适合目标用户的试题推荐给用户。本发明适用于在线教育领域。</td>   <td>1.一种基于用户学习行为的个性化试题推荐方法,其特征在于：所述该个性化试题推荐方法步骤如下：步骤1：从在线教育平台获取用户历史做题数据、试题与知识点信息；步骤2：根据用户历史做题数据构造用户-试题得分矩阵R,根据试题与知识点之间的关联构造试题-知识点关联矩阵Q；基于矩阵R、矩阵Q构建用户认知诊断模型,也即用户知识点掌握矩阵A；步骤3：基于矩阵A得到用户-相邻试题集,用户-相邻试题集表示从矩阵A中得出的相邻用户已做试题而目标用户未做的试题集合；根据矩阵Q得到试题-相邻试题集,试题-相邻试题集表示利用皮尔逊相关系数从矩阵Q得到的相邻试题集合；结合用户-相邻试题集和试题-相邻试题集构建备选试题集；步骤4：对矩阵R进行非负矩阵分解,将R矩阵分解为用户和试题的隐含特征矩阵W、H,其中W∈R<Sup>U×D</Sup>,H∈R<Sup>D×V</Sup>,R＝WH,利用梯度下降法求出W和H矩阵的估计值；其中：U表示总的用户数目；V为总的试题数目；D为某个小于U和V的整数；步骤5：结合矩阵A,计算用户在备选试题集的最终潜在作答情况：η<Sub>uv</Sub>＝ω<Sub>1</Sub>μ<Sub>u</Sub>+ω<Sub>2</Sub>μ<Sub>v</Sub>+ω<Sub>3</Sub>R<Sub>uv</Sub>其中：          <Image id="icf0001" he="174" wi="351" file="FDA0001852693550000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0002" he="175" wi="325" file="FDA0001852693550000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        ω<Sub>1</Sub>+ω<Sub>2</Sub>+ω<Sub>3</Sub>＝1μ<Sub>u</Sub>表示根据认知诊断情况对用户p<Sub>u</Sub>在试题t<Sub>v</Sub>上的得分的预测结果；μ<Sub>v</Sub>表示试题t<Sub>v</Sub>的整体平均分；ω<Sub>1</Sub>、ω<Sub>2</Sub>、ω<Sub>3</Sub>为计算潜在作答情况时的权值,分别表示μ<Sub>u</Sub>、μ<Sub>v</Sub>、R<Sub>uv</Sub>在最终的潜在作答情况中所占的比重；q<Sub>vk</Sub>表示试题t<Sub>v</Sub>对知识点c<Sub>k</Sub>的考察情况,即知识点c<Sub>k</Sub>在试题t<Sub>v</Sub>中所占的权重值；R<Sub>uv</Sub>表示用户p<Sub>u</Sub>在试题t<Sub>v</Sub>上的作答情况；α<Sub>uk</Sub>表示用户p<Sub>u</Sub>对知识点c<Sub>k</Sub>的掌握情况,α<Sub>uk</Sub>＝1表示用户p<Sub>u</Sub>掌握了知识点c<Sub>k</Sub>,α<Sub>uk</Sub>＝0表示用户p<Sub>u</Sub>尚未掌握了知识点c<Sub>k</Sub>；步骤6：结合上一步得到的用户潜在作答情况,从备选试题集当中推荐目标用户自己选择难度范围(d1,d2)的试题给用户。</td>   <td>G06Q50/20;G06Q10/06;G06Q10/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周思宇;              印鉴;                   赖韩江       </td>   <td>中山大学;广东恒电信息科技股份有限公司</td>   <td>一种基于对抗生成网络的与职业相关的人脸老化方法</td>   <td>广东省</td>   <td>CN109509144A</td>   <td>2019-03-22</td>   <td>本发明提供一种基于条件对抗生成网络的与职业相关的人脸老化方法。本发明首先收集一个职业面部老化数据集来研究职业对人脸老化的影响,它包括三个职业类型。其次,本发明提出了一个新的考虑职业信息的基于条件对抗生成网络的人脸老化网络,它学习了不同职业下的人脸的老化过程。本发明的老化过程中考虑了两个方向：个人特征的保留和不同职业所带来的不同的老化特征。本发明通过深度自编码网络,来保持个人的面部特征,使用了条件对抗生成网络来获得不同职业下的老化特征。</td>   <td>1.一种基于条件对抗生成网络的与职业相关的人脸老化方法,其特征在于,包括以下步骤：S1：收集与职业相关的人脸老化图片；S2：训练分类网络区分人脸的不同职业,并根据结果探究职业对人脸老化的影响；S3：训练生成网络来生成不同职业条件下老化的人脸,并通过之前的分类网络验证生成结果的好坏。</td>   <td>G06T3/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈楚城;                   戴宪华       </td>   <td>中山大学</td>   <td>一种针对大分辨率布匹图像中的小瑕疵的高效检验算法</td>   <td>广东省</td>   <td>CN109509187A</td>   <td>2019-03-22</td>   <td>本发明涉及一种针对大分辨率布匹图像中的小瑕疵的高效检验算法,包括：(1)通过摄像头采集图像,然后使用labelImg工具图像进行标注；(2)将处理后图像拆分成训练集和测试集,训练集用来训练检验模型,测试集用来评估检验模型性能；(3)将训练集图像和对应的类别信息位置信息等输入到改进的se-resnext101检验模型中,训练检验模型；(4)采用训练后的检验模型处理测试集中的图像,获取瑕疵的大致位置和对应的类别。本发明的方法可以对单分辨率的输入图像实现多尺度特征图的处理,从而处理多尺度图像块,这样可以适应多种不同大小的瑕疵,大幅度提高检测精度和速度；同时该算法实现在图像分类框架上获取瑕疵大致位置和处理图像中存在多种瑕疵的情况。</td>   <td>1.一种针对大分辨率布匹图像中的小瑕疵的高效检验算法,其特征在于,包括如下步骤：(1)图像采集,利用分辨率为2560*1920的摄像头对布匹图像进行拍摄,获取相关的数据集并对图像重命名,如1.jpg,2.jpg,3.jpg,…,M.jpg等,接着将图像缩放到1024*768大小,并采用label Img工具对拍摄的图像进行标注,获取图像中关于瑕疵的标签,瑕疵的标签包含了瑕疵在图像中左上角的坐标(x1,y1),右下角的坐标(x2,y2)和瑕疵的类别defectN,其中N表示数字,如1,2,3,…等,特别的,如果拍摄的图像中没有瑕疵,我们不会用labelImg进行处理,只记录其类别信息norm；(2)图像划分,将图像划分成训练集和测试集两部分,两个部分不存在相同的图像,训练集用来训练检验模型,测试集用来评估检验模型的性能；(3)图像预处理,包括随机上下翻转、随机左右翻转和随机光照改变等,其中随机上下翻转、随机左右翻转和随机光照改变只针对训练集,特别的,当进行随机上下翻转和随机左右翻转的时候,瑕疵的坐标信息也需要做出相应的变化；(4)训练检验模型,将经过图像预处理后的训练集中的图像和标签信息输入到检验模型中进行训练,检验模型是在se-resnext101的基础上进行改进的,使得网络可以针对单分辨率的输入图像在模型上获取多尺度特征图,通过检验模型的前向传播获取每个特征图上每个特征点的类别概率值,通过Focal Loss函数计算分类损失,利用带动量的梯度下降算法反向传播训练模型；(5)布匹图像检验,将测试集中的图像输入到训练好的检验模型中提取特征并获取多尺度特征图上每个特征点的类别概率值；如果三张特征图中有两张及以上的特征图中所有特征点都判为norm,则认为该图像类别为norm,其他情况则认为图像中存在瑕疵；对于判别为存在瑕疵的图像,我们利用每个特征点都对应原图中的某个图像块,通过特征点的预测类别转换对应图像块的像素值获取相关的热力图,叠加多种特征图对应的热力图获得最终的热力图,由最终的热力图得到瑕疵的大致位置,对瑕疵附近的图像块取概率均值得到该瑕疵的类别,特别的,该算法可以处理图像中存在多瑕疵的情况,并得到各个瑕疵的类别和大致位置。</td>   <td>G06T7/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;              陈航;                   包笛       </td>   <td>中山大学</td>   <td>基于CNN低层语义特征密度图的人群密度估计方法</td>   <td>广东省</td>   <td>CN109492615A</td>   <td>2019-03-19</td>   <td>本发明属于人群分析技术领域,为基于CNN低层语义特征密度图的人群密度估计方法,包括步骤：数据的预处理,根据原图像的行人位置生成密度图；对原图像和密度图进行切片；对原图像进行MCNN多分支特征提取,对各分支特征进行卷积、池化操作后,通过MCNN特征图融合器对各分支特征进行连接得到MCNN连接特征图,并对其进行卷积操作得到初始的MCNN密度图；对原图像进行卷积得到具有低层语义特征图；将低层语义特征图与MCNN多分支特征提取后各分支生成的特征图在通道数这一维度进行连接,得到连接特征图；用若干层卷积层对连接特征图进行解码,生成最终的密度图；对最终密度图的每个像素相加求和,得到图片中的人数。MAE、MSE较低,准确率和稳定性都较高。</td>   <td>1.基于CNN低层语义特征密度图的人群密度估计方法,其特征在于,包括以下步骤：S1、数据的预处理,根据原图像的行人位置生成密度图；S2、对原图像和步骤S1中生成的密度图进行切片；S3、对原图像进行MCNN多分支特征提取,对各分支特征进行卷积、池化操作后,通过MCNN特征图融合器对各分支特征进行连接,得到MCNN连接特征图,对MCNN连接特征图进行卷积操作,得到初始的MCNN密度图；S4、对原图像进行卷积,得到具有低层语义特征图；S5、将低层语义特征图与MCNN多分支特征提取后各分支生成的特征图在通道数这一维度进行连接,完成特征的编码,得到连接特征图；S6、用若干层卷积层对连接特征图进行解码,生成最终的密度图；对得到的最终密度图的每个像素相加求和,得到图片中的人数。</td>   <td>G06K9/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨猛;                   罗文锋       </td>   <td>中山大学</td>   <td>一种基于深度网络的皮肤损伤图片分割方法</td>   <td>广东省</td>   <td>CN109493359A</td>   <td>2019-03-19</td>   <td>本发明涉及人工智能领域,更具体的,涉及一种基于深度网络的皮肤损伤图片分割方法,本发明不进行手工提取皮肤图片特征来进行分割任务,而是使用训练数据去自行学习适合于分割任务的深度卷积特征；本发明的预处理非常简单,只是进行图片像素值的归一化；此外,相比TDLS和Jafari使用引导滤波器的预处理方式解决光照和对比度变化较大的问题,本发明通过数据增强的方式丰富训练数据,让模型自行学习最优的特征表示以进行分割；本发明在真阳性率的指标上超过了现有的方法,而且在GPU和CPU上的运行时间都远低于现有的模型,可以做到实时的皮肤图像分割；本发明还使用了全连接的条件随机场作为后处理方法,可以有效地利用低层次的纹理颜色特征,锐化边缘区域的分割。</td>   <td>1.一种基于深度网络的皮肤损伤图片分割方法,其特征在于,包括以下步骤：步骤S1：对测试图像进行增强和预处理；步骤S2：将预处理后的测试图像输入到的卷积神经网络中进行训练,得到初步的分割结果和概率输出,根据初步的分割结果以及概率输出对卷积神经网络进行参数调整；步骤S3：对训练图像进行增强和预处理；步骤S4：将预处理后的训练图像输入到训练完成的卷积神经网络中进行训练,得到初步的分割结果以及概率输出；步骤S5：将分割结果和概率输出在全连接的条件随机场中进行迭代处理,得到最终的分割结果。</td>   <td>G06T7/143;G06T7/11</td>  </tr>        <tr>   <td>中国专利</td>   <td>         耿立冰;              潘炎;              印鉴;              赖韩江;                   潘文杰       </td>   <td>中山大学;广州中大南沙科技创新产业园有限公司</td>   <td>一种基于极少训练样本的深度图像哈希方法</td>   <td>广东省</td>   <td>CN109472282A</td>   <td>2019-03-15</td>   <td>本发明提供一种基于极少训练样本的深度图像哈希方法,该方法在已有的传统哈希方法的和基于深度学习的哈希方法都是在大量训练样本的前提下来进行设计的,而在真实的生产环境中,得到大量标记训练样本的成本很高,所以在在极少训练样本下,如果能得到一个效果相对较好的图像哈希模型是具有非常大的实用价值的。</td>   <td>1.一种基于极少训练样本的深度图像哈希方法,其特征在于,包括以下步骤：S1：任务定义及数据划分；S2：构建triplet-based通用深度哈希模型；S3：基于通用深度哈希模型构建支持记忆体；S4：通过双向长短期记忆子网络和支持记忆体学习极少样本的特征表示；S5：训练极少样本下的深度图像哈希模型,并对极少样本的测试集进行检索测试。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         任江涛;                   赖超杰       </td>   <td>中山大学</td>   <td>基于时空的警情预测模型的建立方法、装置和存储介质</td>   <td>广东省</td>   <td>CN109472419A</td>   <td>2019-03-15</td>   <td>本发明公开了一种基于时空的警情预测模型的建立方法,所述基于时空的警情预测模型的建立方法包括以下步骤：获取多个目标警情文本,并确定各个所述目标警情文本中警情对应的目标犯罪地点；根据各个所述目标犯罪地点确定犯罪风险区域；根据所述犯罪风险区域对应的各个警情、所述警情的犯罪类型以及所述警情的犯罪时间点,建立所述犯罪风险区域对应的警情预测模型。本发明还公开一种基于时空的警情预测模型的建立装置和存储介质。本发明建立的警情预测模型的警情预测准确性较高。</td>   <td>1.一种基于时空的警情预测模型的建立方法,其特征在于,所述基于时空的警情预测模型的建立方法包括以下步骤：获取多个目标警情文本,并确定各个所述目标警情文本中警情对应的目标犯罪地点；根据各个所述目标犯罪地点确定犯罪风险区域；根据所述犯罪风险区域对应的各个警情、所述警情的犯罪类型以及所述警情的犯罪时间点,建立所述犯罪风险区域对应的警情预测模型。</td>   <td>G06Q10/04;G06Q10/06;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑子彬;                   肖小粤       </td>   <td>中山大学</td>   <td>一种基于多模型堆栈融合的项目风险评级方法及装置</td>   <td>广东省</td>   <td>CN109472462A</td>   <td>2019-03-15</td>   <td>本发明公开一种基于多模型堆栈融合的项目风险评级方法及装置,本装置用实现本方法,包括：数据预处理提取训练集,将训练集拼接成项目文档的语料库；对语料库进行特征构建,构建tf-idf特征、Doc2Vec特征、Word2Vec特征及统计特征,采用两层的多模型堆栈策略,对训练集进行上述特征概率训练,输出项目风险评级结果。本发明通过挖掘项目白皮书中的关键信息,找出与风险评级有关的重要因素,构建一个基于多模型堆栈融合的风险评级模型。训练好的模型能够汇聚过去评级人员的风险评级经验,并且自主学习项目白皮书与评级之间的关联,从而为评级人员提供评级建议,以辅助评级人员,提供评级的准确性和效率。</td>   <td>1.一种基于多模型堆栈融合的项目风险评级方法,其特征在于,包括：S10数据预处理：对输入的项目文档进行预处理,以从项目文档中提取训练集,将训练集拼接成项目文档的语料库；S20特征构建：对语料库进行特征构建,构建tf-idf特征、Doc2Vec特征、Word2Vec特征及统计特征,其中：tf-idf特征为将语料库输入至TF-IDF模型所提取的每个项目文档的tf-idf特征；Doc2Vec特征为采用Doc2Vec方法将预处理后的训练集输入Doc2Vec模型,获得的项目文档的固定长度的特征向量；Word2Vec特征为将语料库输入至Word2Vec模型,获得项目文档的Word2Vec特征；统计特征包括提取项目文档中关键词出现次数的次数统计和对项目文档中的金额的金额统计；S30采用两层的多模型堆栈策略,对训练集进行上述特征概率训练；S40输出项目风险预测评级结果。</td>   <td>G06Q10/06;G06Q10/04;G06K9/62;G06F17/27;G06F17/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         张东;              邓洪汰;                   吴增程       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院</td>   <td>一种基于眼动追踪的在线课程质量的评估方法</td>   <td>广东省</td>   <td>CN109472464A</td>   <td>2019-03-15</td>   <td>本发明公开了一种基于眼动追踪的在线课程质量的评估方法,教学视频开始时,调用摄像头对用户面部进行截图,送入预测模型得到该时刻预测的视点位置,以此来评估教学视频的教学质量,不需要学生和专家的打分,不用耗费人力物力,不存在人为因素,可以客观公正的根据用户的听课状态评估教学结果。</td>   <td>1.一种基于眼动追踪的在线课程质量的评估方法,其特征在于,包括以下步骤：S1：教学视频开始；S2:程序调用前置摄像头开始摄像、获取设备信息；S3：根据设备信息,标记教学重点区域的显示位置；S4：利用大规模眼动数据集构建一个预训练的预测模型；S5：对摄制的面部视频定时截图,然后将截图送入预测模型,得到该时刻预测的视点位置；S6：判断预测的视点是否在教学重点区域内,若是则加入计分项,不是则不加入计分项；S7：判断教学视频是否结束,若没有则重复步骤S2-S6直至教学视频结束,若教学视频结束,则统计计分项并根据计分项进行评估；S8：评估结束。</td>   <td>G06Q10/06;G06Q50/20;G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         任江涛;                   何佳俊       </td>   <td>中山大学</td>   <td>股票价格的预测方法、服务器及存储介质</td>   <td>广东省</td>   <td>CN109472700A</td>   <td>2019-03-15</td>   <td>本发明公开了一种股票价格的预测方法,包括以下步骤：获取目标股票的历史股票数据,并对所述历史股票数据进行预处理；根据所述历史股票数据的时间顺序,对所述预处理后的历史股票数据赋予时间权重,所述时间权重与所述时间顺序成正相关；基于预先构建的深度神经网络模型,根据赋予时间权重后的所述历史股票数据得到所述目标股票的价格预测结果。本发明还公开了一种服务器以及计算机可读存储介质。本发明通过对与价格预测结果的时间点越近的历史股票数据赋予越大的权重,并根据历史股票数据基于深度神经网络模型生成价格预测结果,提高了预测得到的股票价格的准确率。</td>   <td>1.一种股票价格的预测方法,其特征在于,所述股票价格的预测方法包括以下步骤：获取目标股票的历史股票数据,并对所述历史股票数据进行预处理；根据所述历史股票数据的时间顺序,对所述预处理后的历史股票数据赋予时间权重,所述时间权重与所述时间顺序成正相关；基于预先构建的深度神经网络模型,根据赋予时间权重后的所述历史股票数据得到所述目标股票的价格预测结果。</td>   <td>G06Q40/04;G06N3/08;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;              周晓梅;              刘上华;                   李伏清       </td>   <td>中山大学</td>   <td>基于FPGA的4K高清图像实时路面分割方法</td>   <td>广东省</td>   <td>CN109472793A</td>   <td>2019-03-15</td>   <td>本发明涉及自动驾驶的技术领域,更具体地,涉及基于FPGA的4K高清图像实时路面分割方法。本发明可以在保证实时性的同时获得较高的准确率。thermal特征用于使用道路的消失点找到感兴趣的感兴趣区域(ROI),计算出路面区域掩膜(路面区域粗提取)。然后融合RGB特征对粗提取的路面区域进行细分割。我们的RGB-Thermal数据集的实验表明,我们所提出的方法可以在低成本嵌入式设备上准确有效地检测路面,准确度达到了92.01％。</td>   <td>1.基于FPGA的4K高清图像实时路面分割方法,其特征在于,包括以下步骤：S1.检测Thermal特征；S2. 路面粗提取；S3. 路面区域提纯；S4. 结果分析。</td>   <td>G06T7/11;G06T7/136</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄镇业;                   吴贺俊       </td>   <td>中山大学</td>   <td>一种基于无监督学习的单目视觉定位方法</td>   <td>广东省</td>   <td>CN109472830A</td>   <td>2019-03-15</td>   <td>本发明提出一种基于无监督学习的单目视觉定位方法,步骤如下：获取视频流信息并等分切分成图像帧；将相邻的第一和第二图像帧分别输入到位姿估计网络、第一和第二深度估计网络中,获得第一和第二图像帧对应的位姿之间的位姿变换、第一和第二图像帧对应的第一和第二深度图；将第一深度图和位姿变换进行重建得到第二重建图像帧,将第二深度图和位姿变换进行重建得到第一重建图像帧；计算重建误差并以重建误差最小化为目标对位姿估计网络和深度图估计网络进行拟合训练；将拟合训练好的位姿估计网络和深度图估计网络组合的深度神经网络应用在单目视觉定位中。本发明将相邻两帧图像作为输入进行拟合的深度图神经网络,能有效提高定位效果,可扩展性强。</td>   <td>1.一种基于无监督学习的单目视觉定位方法,其特征在于：包括以下步骤：S1：获取视频流信息,将视频流等分切分成图像帧；S2：将任意相邻的第一图像帧和第二图像帧堆叠后输入到位姿估计网络中,获得第一图像帧和第二图像帧对应位姿之间的位姿变换；S3：将所述第一图像帧和第二图像帧堆叠后分别输入到第一深度图估计网络和第二深度图估计网络中得到第一图像帧对应的第一深度图和第二图像帧对应的第二深度图；S4：将所述第一深度图和位姿变换进行重建得到第一重建图像帧,将所述第二深度图和位姿变换进行重建得到第二重建图像帧；S5：根据第一图像帧与第一重建图像帧、第二图像帧与第二重建图像帧计算重建误差L,以重建误差L最小化为目标,使用重建误差L对位姿估计网络、第一深度图估计网络和第二深度图估计网络进行拟合训练；S6：将拟合训练好的位姿估计网络、第一深度图估计网络和第二深度图估计网络组合的深度神经网络应用在单目视觉定位中。</td>   <td>G06T7/80;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈武亚;                   权小军       </td>   <td>中山大学</td>   <td>一种基于门限卷积神经网络的机器阅读理解方法</td>   <td>广东省</td>   <td>CN109460553A</td>   <td>2019-03-12</td>   <td>本发明提供一种基于门限卷积神经网络的机器阅读理解方法,通过构建门限卷积神经网络模型,包括输入层、门限卷积层和答案层；输入层用于编码目标文章,将编码的文章向量序列、问题向量序列和答案向量序列传送给所述门限卷积层；门限卷积层通过交互的方式产生具有高层语义信息的文章,问题,答案表达,并将这些表达传送给答案层；最后由答案层进行推理决策,做出预测；确定目标文章,导入门限卷积神经网络模型中进行机器阅读理解,导出预测结果。本发明提供的一种基于门限卷积神经网络的机器阅读理解方法,有效简化了神经网络模型,大大减少了训练和测试时长,提高了处理效率,提升了用户体验感；保持文本的长期依赖关系,准确预测出答案信息。</td>   <td>1.一种基于门限卷积神经网络的机器阅读理解方法,其特征在于,包括以下步骤：S1：构建门限卷积神经网络模型,包括输入层、门限卷积层和答案层；其中,所述输入层用于编码目标文章,将编码的文章向量序列、问题向量序列和答案向量序列传送给所述门限卷积层；所述门限卷积层通过交互的方式产生具有高层语义信息的文章,问题,答案表达,并将这些表达传送给答案层；最后由所述答案层进行推理决策,做出预测；S2：确定目标文章,导入门限卷积神经网络模型中进行机器阅读理解,导出预测结果。</td>   <td>G06F17/27;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苏航;              李召国;                   张怡       </td>   <td>中山大学深圳研究院</td>   <td>一种基于高分辨率CMOS的车道偏离报警方法</td>   <td>广东省</td>   <td>CN109460742A</td>   <td>2019-03-12</td>   <td>本发明公开了一种基于高分辨率CMOS的车道偏离报警方法。本发明通过使用高分辨率的CMOS图像采集系统,快速准确采集车辆行驶状况,以便准确及时反馈车辆行驶状况；利用FPGA芯片对CMOS采集的信息进行解锁,为图片处理作准备；使用大津法、霍夫变换等算法完成对图像中车道线的识别和提取,便于系统判定是否需要报警；根据条件,判断车身与车道线的相对位置并适时报警,提示驾驶员,保障车辆行驶安全。本发明提出的一种基于高分辨率CMOS的车道偏离报警方法,使用FPGA芯片对视频解锁,节省成本同时又对数据直接加工,提高了整个设计的效率；通过对图像噪声处理,减少实验误差,提高数据的可靠度,提供了一种高速高分辨率CMOS图像采集实现,对路面情况进行了准确快速的监测。</td>   <td>1.一种基于高分辨率CMOS的车道偏离报警方法,其特征在于,所述方法包括：使用高速高分辨率CMOS图像采集系统对道路情况进行采集。使用视频解锁芯片和图像处理芯片对COMS传输的视频和图像进行解锁和传输。通过对解锁后的图像进行二值化、去噪声、边缘检测和直线检测等处理,对车道线进行提取和识别。计算图像处理结果：如果可以识别左右两个车道线,并且车身与车道线的夹角小于等于30度时视为车辆正常行驶不进行报警；如果只能识别单侧车道线,但是车身与单侧车道线的距离加上车身宽度小于常规车道的宽度,视为正常行驶不进行报警；若不符合上述条件则视为非正常情况,立即进行报警。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              江宸瀚;              彭杰锋;              刘凌波;                   王青       </td>   <td>中山大学</td>   <td>一种基于聚焦机制的群体流量预测模型及方法</td>   <td>广东省</td>   <td>CN109460855A</td>   <td>2019-03-12</td>   <td>本发明公开了一种基于聚焦机制的群体流量预测模型及方法,所述模型包括：连续性特征学习模块,用于利用聚焦群体流机(Attentive Crowd Flow Machines,ACFM)对依时间排序的特征图序列学习连续性特征表达,得到连续性特征图；周期性特征学习模块,用于利用聚焦群体流机ACFM对依时间排序的特征图序列学习周期性特征表达,得到周期性特征图；具有时间变化的融合模块,用于引入外部信息引导所述连续性特征图和周期性特征图融合,本发明通过学习数据在时域上变化的动态表示,从而推断出群体流量未来的走向。</td>   <td>1.一种基于聚焦机制的群体流量预测模型,包括：连续性特征学习模块,用于利用聚焦群体流机ACFM对依时间排序的特征图序列学习连续性特征表达,得到连续性特征图；周期性特征学习模块,用于利用聚焦群体流机ACFM对依时间排序的特征图序列学习周期性特征表达,得到周期性特征图；具有时间变化的融合模块,用于引入外部信息引导所述连续性特征图和周期性特征图融合。</td>   <td>G06Q10/04;G06K9/62;G06N3/04;G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林依挺;              潘炎;              印鉴;                   潘文杰       </td>   <td>中山大学;广州中大南沙科技创新产业园有限公司</td>   <td>一种基于ε不敏感对数损失的在线做市方法</td>   <td>广东省</td>   <td>CN109461077A</td>   <td>2019-03-12</td>   <td>本发明公开了一种基于ε不敏感对数损失的在线做市方法,它根据期货分钟级别OHLC数据构建多个候选子策略,同时提出候选子策略的加权收益和理论最优策略的加权收益存在线性关系。本发明以ε不敏感对数损失作为损失函数,使用Follow the Regularized Leader在线学习算法动态更新候选子策略的权重,最后根据权重计算主策略的目标仓位并调整主策略的仓位等于目标仓位。本发明提出以理论最优策略的加权收益作为线性关系的真实值,使得在线学习的优化目标更加明确,同时,本发明提出的以ε不敏感对数损失作为损失函数,能够更好地拟合真实的市场情况。在使用真实数据进行回测试,本发明提出的策略能够获得较好的收益和稳定性,拥有很强的实用性。</td>   <td>1.一种基于ε不敏感对数损失的在线做市方法,其特征在于,包括以下步骤：S1：选择期货品种,获取所选品种的分钟级OHLC数据；S2：构建模拟执行做市策略的候选子策略,初始化候选子策略的权重；S3：读取过去一个周期的OHLC数据,计算候选子策略的市值、现金和持仓,计算理论最优策略的市值,将理论最优策略的收益表示为候选子策略的收益的线性关系；S4:利用在线学习算法,更新候选子策略权重；S5:根据候选子策略权重计算目标仓位,调整主策略的仓位等于目标仓位；S6:候选子策略按照做市方法,根据前一段时间期货收盘价的均值和标准差,模拟发出买单和卖单；每经过一个周期,重复S3～S6,主策略从开始运行到当前时刻的市值之差即为主策略的收益。</td>   <td>G06Q40/04;G06Q40/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         苏航;              李召国;                   张怡       </td>   <td>中山大学深圳研究院</td>   <td>一种基于神经网络算法的智能内后视镜实时去模糊方法</td>   <td>广东省</td>   <td>CN109461131A</td>   <td>2019-03-12</td>   <td>本发明公开了一种基于神经网络算法的智能内后视镜实时去模糊方法。本发明采集道路交通视频产生模糊帧与清晰帧,之后重复训练时空递归的CNN去模糊神经网络,得到最优去模糊模型,然后将摄像头采集到的后方路面视频图像作为输入,获得的输出便是去模糊处理后的动态视频。本发明提供了一种替代传统外后视镜的智能内后视镜实现,可以在减小风阻、消除大货车视觉盲区、适用于恶劣天气等方面有很好的表现；使用深度神经网络生成的网络,与传统图像处理方法比去模糊效果更佳；改进传统CNN网络去模糊架构,提出基于时空递归的思想,在不增加网络参数的情况下扩大网络接受域,使得去模糊的效率大大提高,可以实现实时性,达到与传统外后视镜一致的效果。</td>   <td>1.一种基于神经网络算法的智能内后视镜实时去模糊方法,其特征在于,所述方法包括：获得数据集,并划分训练集和测试集。获得数据集方法：采集道路交通视频,对间隔时间非常短的清晰图像求平均来获得模糊图像,合成交通运动模糊视频,产生模糊帧与清晰帧对；将合成的交通视频按帧输入到时空递归的CNN去模糊神经网络中,即前向传播依次通过编码器、动态时域混合网络以及解码器,获得去模糊处理后的去模糊帧；计算通过该时空递归CNN去模糊网络后的去模糊帧与清晰帧间的差距,得到损失值；根据损失值,利用反向传播算法对生成式CNN网络进行反向传播,更新网络权重,接着更新当前帧为下一帧图像；利用下一个模糊帧重复训练该网络,定期计算测试集数据的损失值大小,观察效果。直到训练集视频所有帧去模糊化完成或者测试集达到最小损失值,便得到最优去模糊模型；得到最优去模糊模型后,将摄像头采集到的后方路面视频图像作为输入,获得的输出便是去模糊处理后的动态视频。</td>   <td>G06T5/00;B60R1/00;B60R1/12</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈洪波;              任帅;              林润;              徐绍凯;                   黄勇慧       </td>   <td>桂林电子科技大学;中山大学</td>   <td>一种基于动态MRI信息融合的肝癌定量分析方法</td>   <td>广西壮族自治区</td>   <td>CN109461139A</td>   <td>2019-03-12</td>   <td>本发明公开了一种基于动态MRI信息融合的肝癌定量分析方法,该方法结合机器学习方法,利用XGboost模型算法,进行自学习,自优化,构建活性肝癌细胞识别最优模型,优化了活性肝癌区域的检测手段,为医生对肝癌的TACE治疗疗效进行评估提供更为精准的临床信息,从而为肝癌的精准治疗方案的制定提供技术支持和可靠依据。</td>   <td>1.一种基于动态MRI信息融合的肝癌定量分析方法,其特征在于,具体包括如下步骤：1)对采集到的肝癌患者的DCMRI图像,采用概率图谱与形状模型相结合的方法,同时引入适当的肝脏图像的特征,构造目标函数,实现对肝脏DCMRI图像的准确而且快速的分割,从DCMRI图像中分割得到肝脏图像区域；2)提取肝脏图像区域内各体素在DCMRI图像中的信号变化特征信息,作为活性肿瘤的识别特征；3)利用XGBoost模型算法构建活性肝癌区域识别模型,将步骤2)中提取的各体素的DCMRI特征信息输入该识别模型中进行识别；4)用训练集数据来对步骤3)构建的模型进行训练,并采用测试集数据对模型进行测试,获得模型的最优参数；5)利用训练好的识别模型对TACE治疗肝癌的疗效进行评估,得到肝脏活性肿瘤区域,并进行定量分析和三维显示,为制定肝癌进一步治疗方案以及预后预测提供精确可靠的信息。</td>   <td>G06T7/00;G06T7/11;G06T7/33;G16H30/20;G16H50/20;G16H50/30</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李熙莹;              吕硕;              袁敏贤;              江倩殷;                   李发文       </td>   <td>中山大学</td>   <td>一种基于散热器栅格背景消融的车标定位方法</td>   <td>广东省</td>   <td>CN105760876B</td>   <td>2019-03-08</td>   <td>本发明公开了一种基于散热器栅格背景消融的车标定位方法,包括：对车辆原始图像进行车标粗定位；判断车标粗定位图像中的背景为无栅格背景或有栅格背景；当车标粗定位图像中的背景为有栅格背景时,则对车标粗定位图像中车标的背景纹理进行分类,然后采用与车标的背景纹理类型相对应的算法对车标粗定位图像中车标的栅格背景进行消除,接着对车标粗定位图像进行车标物理特性判断,根据判断结果来实现车标的最终定位；反之,则对车标粗定位图像进行车标物理特性判断,从而根据判断结果来实现车标的最终定位。通过使用本发明的方法具有复杂度低、处理速度快、定位精确度高等优点。本发明的车标定位方法可广泛应用于车标定位领域中。</td>   <td>1.一种基于散热器栅格背景消融的车标定位方法,其特征在于：该方法包括：A、获取车辆原始图像；B、对车辆原始图像进行车标粗定位,从而得到车标粗定位图像；C、判断车标粗定位图像中的背景为无栅格背景或有栅格背景；当车标粗定位图像中的背景为有栅格背景时,则根据车标粗定位图像的灰度值变化趋势及车标粗定位图像的主方向,从而判断车标粗定位图像中车标的背景纹理为垂直背景纹理或水平背景纹理,然后执行步骤D；当车标粗定位图像中的背景为无栅格背景时,则执行步骤E；D、采用与车标的背景纹理类型相对应的算法对车标粗定位图像中车标的栅格背景进行消除；E、对车标粗定位图像进行车标物理特性判断,从而根据判断结果来实现车标的最终定位。</td>   <td>G06K9/46;G06K9/62;G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         纪庆革;                   李启运       </td>   <td>中山大学</td>   <td>一种实时鲁棒的人脸检测方法</td>   <td>广东省</td>   <td>CN109446922A</td>   <td>2019-03-08</td>   <td>本发明涉及计算机视觉识别技术,具体为实时鲁棒的人脸检测方法,包括步骤：获取要进行人脸检测的目标图像并进行预处理；建立并训练检测模型,检测模型包括多个卷积模块、多个Inception模块、多个带残差的Inception模块及多个检测模块,Inception模块是具有两条支路的通道分离卷积模块,带残差的Inception模块是带残差连接的多支路通道分离卷积模块,检测模块用卷积运算最终输出位置信息和分类信息；将目标图像输入训练好的检测模型中,分别获取指定层级上的卷积结果；对获取的卷积结果进行分类和回归；根据回归和分类结果计算出人脸的位置。该方法构建了简单高效的卷积神经网络,减少了检测过程中冗余操作,在CPU上能达到实时效果。</td>   <td>1.一种实时鲁棒的人脸检测方法,其特征在于,包括以下步骤：S1、获取要进行人脸检测的目标图像并进行预处理；S2、建立并训练检测模型；检测模型包括多个卷积模块、多个Inception模块、多个带残差的Inception模块及多个检测模块,第一卷积模块、第一Inception模块、第二Inception模块、第三Inception模块、第一带残差的Inception模块、第二卷积模块、第二带残差的Inception模块、第三卷积模块、第三带残差的Inception模块依次连接,第一带残差的Inception模块、第二带残差的Inception模块及第三带残差的Inception模块分别与第一检测模块、第二检测模块及第三检测模块连接,最终输出位置信息和分类信息；S3、将目标图像输入训练好的检测模型中,分别获取指定层级上的卷积结果；S4、对获取的卷积结果进行分类和回归；S5、根据回归和分类结果计算出人脸的位置。</td>   <td>G06K9/00;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈楚城;                   戴宪华       </td>   <td>中山大学</td>   <td>一种基于卷积神经网络的电力设备检测算法</td>   <td>广东省</td>   <td>CN109446925A</td>   <td>2019-03-08</td>   <td>本发明涉及一种基于卷积神经网络的电力设备检测算法,包括：(1)使用labelImg工具对拍摄的电力设备的可见光图像和红外图像进行标注,获取图像中关于电力设备的标签；(2)将训练集中的图像和设备级标签等信息输入到电力设备检测模型中进行训练,利用带动量的梯度下降算法反向传播,更新模型参数；(3)测试阶段对得到的预测框使用软的非极大值抑制来减少重叠的预测框,最后输出类别置信度高于阈值的预测框；(4)与现阶段的电力设备检测方法相比,该算法不需要在预训练模型上进行微调也可以收敛并得到优秀的检测性能,对于小的电力设备具有优秀的鲁棒性,可以有效减少错框漏框的现象,检测精度高,检测速度可以达到实时处理。</td>   <td>1.一种基于卷积神经网络的电力设备检测算法,其特征在于,包括如下步骤：(1)首先,采用labelImg工具对拍摄的关于电力设备的可见光图像和红外图像进行标注,获取图像中关于电力设备的标签。电力设备的标签包含了相关电力设备在图像中左上角的坐标(x1,y1),右下角的坐标(x2,y2)和电力设备的类型c。特别的,在一张图像中可能有多个不同种类的电力设备存在；(2)接着,将图像划分成训练集和测试集两部分,两个部分不存在相同的图像。训练集用来训练电力设备检测模型,测试集用来评估电力设备检测模型的性能,同时对图像进行预处理,使用高斯滤波器去除噪声后将图像分辨率变成300*300；(3)将训练集中的图像和设备级标签等信息输入到电力设备检测模型中进行训练,获取各图像中存在的电力设备的预测框和类别,并与实际的设备级标签信息进行对比,计算出定位误差和分类误差,然后采用多学习任务的方法,利用带动量的梯度下降算法进行训练；(4)将测试集中的图像输入到训练好的电力设备检测模型中进行检测,获取未训练的待测图像中电力设备的位置和类型。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘宁;              龙永超;              牛群;                   吴贺丰       </td>   <td>中山大学</td>   <td>基于注意力机制和可变形卷积神经网络的人群分析方法</td>   <td>广东省</td>   <td>CN109447008A</td>   <td>2019-03-08</td>   <td>本发明提供了一种基于注意力机制和可变形卷积神经网络的人群分析方法,包括：训练阶段,首先通过人群图像和背景图像训练注意力图生成器(AMG),并将训练好的注意力图生成器模型作为辅助密度图生成器(DME)训练的部件；测试阶段,仅使用训练好的密度图生成器对输入的人群图像生成对应的密度图。本发明通过结合注意力机制,生成注意力图检测人群的区域并在一定程度上反映人群区域的拥挤程度。注意力图作为人群的先验知识训练可变形卷积神经网络,使得网络能够克服人群场景中人群分布不均,环境噪声等问题,生成准确的人群密度图。</td>   <td>1.一种基于注意力机制和可变形卷积神经网络的人群分析方法,其特征在于包括：训练阶段,首先通过人群图像和背景图像训练注意力图生成器(AMG),并将训练好的注意力图生成器模型作为辅助密度图生成器(DME)训练的部件；测试阶段,仅使用训练好的密度图生成器对输入的人群图像生成对应的密度图。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陆广泉;              杨伟伟;              罗志平;              印鉴;                   高静       </td>   <td>中山大学;广东恒电信息科技股份有限公司</td>   <td>一种基于低秩约束图谱学习的无监督特征选择算法</td>   <td>广东省</td>   <td>CN109447116A</td>   <td>2019-03-08</td>   <td>本发明提出了一种基于低秩约束图谱学习的无监督特征选择算法,该算法在权重矩阵中使用低秩约束并能同时减少冗余特征对性能的影响,针对大部分特征选择方法的不足,本发明通过建立新的优化目标函数来完成特征选择,分析了超参数对算法的影响和低轶约束的必要性。同时分析了算法的收敛性和复杂度。另外,运用所选择出的特征在6个基准数据集进行比较分析,实验结果表明,本发明的算法在分类任务的准确率方面优于现有的的五种算法。</td>   <td>1.一种基于低秩约束图谱学习的无监督特征选择算法,其特征在于,包括以下步骤：S1：数据输入和标签输入,参数的初始化；S2：对矩阵A、B、S进行迭代求解；S3：使用所求解的矩阵A、B、S模型得到所选择的数据特征,通过SVM分类器进行试验比较分析。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李明;                   邹小兵       </td>   <td>昆山杜克大学;中山大学附属第三医院(中山大学肝脏病医院)</td>   <td>语音韵律异常评估方法、装置、计算机设备和存储介质</td>   <td>江苏省</td>   <td>CN109448758A</td>   <td>2019-03-08</td>   <td>本申请涉及一种孤独症语音韵律异常评估方法、装置、计算机设备和存储介质。所述方法包括：录音采集语音测试数据；提取所述语音测试数据中被测试者的语音数据,并进行语音片段划分；选出测试数据中对评估有效的语音片段；提取所述有效语音片段中的频谱特征；采用得出的评估模型对所述频谱特征进行评估。采用本方法能够能够提供客观的语音韵律异常自动量化评分,能够有效地增加诊断的客观性以及便捷性。</td>   <td>1.一种孤独症语音韵律异常评估方法,其特征在于,包括：录音采集语音测试数据；提取所述语音测试数据中被测试者的语音数据,并进行语音片段划分；选出测试数据中对评估有效的语音片段；提取所述有效语音片段中的频谱特征；采用得出的评估模型对所述频谱特征进行评估。</td>   <td>G10L25/66;G10L25/63;G10L25/12;G10L15/01;G10L15/04;G10L15/06;G10L15/10;G10L15/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈弟虎;              曹杨宇;              粟涛;                   衣杨       </td>   <td>中山大学</td>   <td>基于二次折射投影模型的鱼眼图像校正方法、装置和系统</td>   <td>广东省</td>   <td>CN105956996B</td>   <td>2019-03-01</td>   <td>本发明公开了一种基于二次折射投影模型的鱼眼图像校正方法、装置和系统,该校正方法包括：所述鱼眼图像一坐标点的光线沿第一方向进入第一球冠面；所述光线在所述第一球冠面折射后沿第一入射角进入第二球冠面,经所述第二球冠面折射后沿所述第二球冠面的半径方向投射在承接面上；根据所述所述发光点在所述承接面上的投影点的坐标计算发光点的坐标。本发明实施例提供的基于二次折射投影模型的鱼眼图像校正方法、装置和系统仅使用四则运算和开方运算,图像校正的运算量较少,校正速度快,便于用硬件实现,而且能实现较好的校正效果。在实际应用中,还可以采用不同的承接面来适应各种应用场合。</td>   <td>1.一种基于二次折射投影模型的鱼眼图像校正方法,其特征在于,包括如下步骤：所述鱼眼图像一坐标点的光线沿第一方向进入第一球冠面,所述坐标点为发光点；其中,所述第一方向与所述第一球冠面的顶点、第二球冠面的顶点之间的连线平行,所述鱼眼图像设于投影面内,所述投影面与所述第一球冠面的底面平行并且经过所述第一球冠面的球心；所述光线在所述第一球冠面折射后沿第一入射角进入第二球冠面,经所述第二球冠面折射后沿所述第二球冠面的半径方向投射在承接面,其中,所述光线经过所述第一球冠面折射后进入第二球冠面时形成第一折射光线,所述第一折射光线与所述第二球冠面形成第一交点,所述第一交点与所述第二球冠面的球心连接构成第一连线,所述第一折射光线与所述第一连线在第二球冠面内形成所述第一入射角；所述第二球冠面的顶点与第二球冠面的球心连接构成第二连线,所述第二连线与所述第一连线在第二球冠面内形成第一夹角；所述第一入射角为所述第一夹角的一半；根据所述发光点在所述承接面的投影点的坐标计算所述发光点的坐标。</td>   <td>G06T3/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡湛;              李裕龙;              任磊;                   苏敏       </td>   <td>中山大学</td>   <td>波浪与防波堤相互作用的预报方法、装置和计算机设备</td>   <td>广东省</td>   <td>CN109408863A</td>   <td>2019-03-01</td>   <td>本申请涉及一种波浪与防波堤相互作用的预报方法、装置和计算机设备。所述波浪与防波堤相互作用的预报方法包括：过将VOF函数对应的相界面以及Levelset函数对应的相界面进行耦合,得到耦合后的流体界面运动方程,基于浸入边界法,根据防波堤的物理参数为所述流体界面运动方程加上体积力项,得到虑及防波堤作用的流体运动方程,根据波浪的能量,以及所述虑及防波堤作用的流体运动方程,对波浪与防波堤之间的相互作用进行预报。上述波浪与防波堤相互作用的预报方法,能够提高了波浪与防波堤相互作用预报的准确率。</td>   <td>1.一种波浪与防波堤相互作用的预报方法,其特征在于,包括：通过将VOF函数对应的相界面以及Levelset函数对应的相界面进行耦合,得到耦合后的流体界面运动方程,并计算界面运动；所述VOF函数用于表征计算单元中波浪占据单元空间的体积分数；基于浸入边界法,根据防波堤的物理参数为所述流体界面运动方程加上体积力项,得到虑及防波堤作用的流体运动方程；根据波浪的能量,以及所述虑及防波堤作用的流体运动方程,对波浪与防波堤之间的相互作用进行预报。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李裕龙;              胡湛;              任磊;                   姚鹏       </td>   <td>中山大学</td>   <td>加载弹性液舱的船舶时域全耦合运动预报方法和装置</td>   <td>广东省</td>   <td>CN109408864A</td>   <td>2019-03-01</td>   <td>本申请涉及一种加载弹性液舱的船舶时域全耦合运动预报方法和装置。所述加载弹性液舱的船舶时域全耦合运动预报方法包括：根据有限元法构建弹性液舱的结构动力学方程,根据有限体积法构建液舱晃荡时液舱所承载液体的流体运动方程,根据结构动力学方程和流体运动方程构建液舱晃荡与弹性液舱的流固耦合问题求解模型,通过流固耦合问题求解模型对液舱晃荡问题与液舱弹性变形进行预报。上述加载弹性液舱的船舶时域全耦合运动预报方法,能够实现船舶运动的预报,并能保证预报的准确率。</td>   <td>1.一种加载弹性液舱的船舶时域全耦合运动预报方法,其特征在于,包括：根据有限元法构建弹性液舱的结构动力学方程,根据有限体积法构建液舱晃荡时液舱所承载液体的流体运动方程；其中有限元计算网格由弹性液舱划分得到,有限体积计算网格由所述弹性液舱所承载液体划分得到；根据所述结构动力学方程和所述流体运动方程构建液舱晃荡与弹性液舱的流固耦合问题求解模型；通过流固耦合问题求解模型对液舱晃荡问题与液舱弹性变形进行预报。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨晓;              胡音;                   李周越       </td>   <td>中山大学中山眼科中心</td>   <td>一种预测及优化光学干预对近视控制效果的方法</td>   <td>广东省</td>   <td>CN109408914A</td>   <td>2019-03-01</td>   <td>本发明公开了一种预测及优化光学干预对近视控制效果的方法,使用基于面积加权的角膜屈光力改变总和进行预测或优化近视控制效果。本发明建立了一个参数,叫基于面积加权的角膜屈光力改变总和(Areal Summed Corneal Power Shift,ASCPS),从而利用光学干预后早期的ASCPS来判断其长期的近视控制效果,从而更好的预测和优化光学干预的近视控制效果。</td>   <td>1.一种预测及优化光学干预对近视控制效果的方法,其特征在于,使用基于面积加权的角膜屈光力改变总和进行预测或优化近视控制效果。</td>   <td>G06F17/50;G02B27/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         田浩辰;                   吴贺俊       </td>   <td>中山大学</td>   <td>一种基于词袋模型的回环检测方法</td>   <td>广东省</td>   <td>CN109409418A</td>   <td>2019-03-01</td>   <td>本发明涉及一种基于词袋模型的回环检测方法。该词袋模型采用具有旋转不变性和尺度感知能力的二进制视觉特征ORB,这种视觉特征能够具有同SIFT特征和SURF特征相似的性能同时具有与由FAST关键点和BRIEF描述子构成的视觉特征相同的计算效率,是一种兼顾了低计算复杂度和高特征显著性的视觉特征。本发明采用依赖于该视觉特征的词袋模型,因而能够在具有平面旋转和尺度缩放的场景中有效的进行回环检测。同时改进了相似度分数的归一化方法,通过计算和保持一个归一化因子的均值并在归一化因子数值异常时代替该异常归一化因子执行归一化。这种归一化方法使得系统能够在主体运动过快或过慢以及发生转向的情况下有效的进行回环检测。</td>   <td>1.一种基于词袋模型的回环检测方法,其特征在于,包括以下步骤：S10.词袋模型向量转化：从系统所获取的图像中提取ORB视觉特征,根据ORB视觉特征在词袋模型视觉词典中的分布将图像转化为数值向量；S20.图像间相似度分数计算：依据当前图像和先前获取每幅图像的数值向量计算对应相似度分数；S30.相似度分数归一化：以当前图像同其前一幅图像间的相似度分数作为归一化因子对其他相似度分数致性归一化并计算归一化后的相似度分数η：          <Image id="icf0001" he="137" wi="446" file="FDA0001818148300000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,v<Sub>c</Sub>为当前图像,v<Sub>r</Sub>为参考图像；v<Sub>c-Δt</Sub>为与v<Sub>c</Sub>的上一幅图像,s(v<Sub>c</Sub>,v<Sub>c-Δt</Sub>)为当前图像同其上一幅图像间的相似度分数；s(v<Sub>c</Sub>,v<Sub>r</Sub>)为当前图像同参考图像间的相似度分数；S40.确定回环候选并分类：若归一化后的相似度分数η(v<Sub>c</Sub>,v<Sub>r</Sub>)达到给定阈值,可将图像v<Sub>r</Sub>作为当前图像v<Sub>c</Sub>的一个回环候选,再将相邻的回环候选组合到一起作为一类回环候选；S50.时间一致性验证：在时间一致性验证阶段,需验证一类回环候选是否在一段时间内持续的被检测到,若是则保留该类回环候选；若否则不保留该类回环候选；S60.几何一致性验证：在每类保留下来的回环候选中选取具有最大相似度分数的回环候选作为该类的代表进入几何一致性验证阶段,计算从当前图像对应相机坐标系到该回环候选对应相机坐标系的空间变换并验证其合理性,如果该空间变换合理,则该回环候选最终被确认为当前图像的真正回环。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑子彬;              马明杰;                   陈伟利       </td>   <td>中山大学</td>   <td>一种基于分层树状结构的区块链信息高效存储方法及装置</td>   <td>广东省</td>   <td>CN109410043A</td>   <td>2019-03-01</td>   <td>本发明公开一种基于分层树状结构的区块链信息高效存储方法,包括如下步骤：区块链账户的预设分类；桶状存储的划分；分层树状结构的构建；帐户信息的输入；帐户信息一致性的检验。提出了具有创新性的区块链账户存储结构,在保障了区块链信息不可篡改等特性的前提下,对账户按照交易频率进行分层存储,从总体上看,交易频率越高的账户将被存储在越接近根节点的节点中,以减少整体的Hash次数。</td>   <td>1.一种基于分层树状结构的区块链信息高效存储方法,其特征在于,包括如下步骤：S10区块链账户的预设分类：根据区块链场景的需要,将帐户信息预设分类成i组参数集合,i为自然数,其中帐户信息包括帐户的总数N,分层树状结构中桶的数目B,每个桶中帐户的数量x,分层树状结构中单个节点有一个或多少个子节点K,每种帐户数量的所占比A,每种帐户的交易频率R；S20对多组帐户信息的参数集合进行模拟演算,并根据模拟结果,选出最优参数集合；S30分层树状结构的构建：确定账户信息桶状存储的决定函数,输入最优参数集合及决定函数,每个桶的高度及每个高度上对应桶的数目,按一定的规则建立分层树状桶结构,根据分层树状桶结构对每个桶进行编号；S40将帐户信息经过函数计算出该帐户设置的对应的桶的序号,根据编号将帐户信息存储在在分层树状桶结构的相对应的桶中；S50信息一致性的检验：当交易发生时,帐户信息发生改变,先将桶中帐户信息拼接,进行一次哈希操作,再以此根据分层树状桶结构向上进行哈希操作,若桶中帐户信息未发生改变,则无需进行哈希操作；若桶中的帐户信息发生改变,则重新进行哈希操作,以获取新的根节点的哈希值；校验新的根节点的哈希值是否与其他可信节占的哈希值相一致,若一致,则视为帐户信息未遭到篡改,若不一致,则视为账户信息遭到篡改,则该节点不可信,需要从其他可信节点获取帐户信息以更新。</td>   <td>G06Q40/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              刘付康;              朱雄泳;              陈荣军;              谢舜道;                   吴炆芳       </td>   <td>中山大学;广东顺德中山大学卡内基梅隆大学国际联合研究院</td>   <td>一种细节增强与亮度自适应的高动态范围图像的色调映射方法</td>   <td>广东省</td>   <td>CN109410126A</td>   <td>2019-03-01</td>   <td>本发明公开了一种细节增强与亮度自适应的高动态范围图像色调映射方法。本发明对输入HDR图像亮度灰度图进行全局细节增强,计算HDR图像亮度灰度图细节增强后的对数,利用对数转换初步压缩原始场景中的亮度；再对其进行亮度直方图统计,并且计算其平均值与标准差,对直方图进行分段裁剪与补偿；再由亮度与标准差估算模型计算映射后中间低动态范围图像的平均亮度与标准差,从而求解HDR图像到中间LDR图像的亮度直方图全局映射曲线,其中由最大熵亮度估算方法自适应选出最优输出中间LDR图像亮度；接着,对中间LDR图像亮度过暗或过亮区域进行局部细节增强映射得到输出LDR图像亮度；最后将HDR图像色彩映射到输出LDR图像色彩,合并色彩空间获得输出LDR图像。本发明能将HDR图像映射到LDR图像,输出的LDR图像亮度自适应,细节增强,主观效果和谐。</td>   <td>1.一种细节增强与亮度自适应的高动态范围图像的色调映射方法,其特征在于包括有如下步骤：1)对输入HDR图像亮度灰度图进行全局细节增强,计算HDR图像亮度灰度图细节增强后的对数,利用对数转换初步压缩原始场景中的亮度；2)对全局细节增强的HDR图像亮度对数进行直方图统计,计算其平均值与标准差,对直方图进行分段裁剪与补偿；3)由亮度与标准差估算模型计算映射到中间LDR图像的平均亮度与标准差,从而求解HDR图像到中间LDR图像的亮度直方图全局映射曲线,其中由最大熵亮度估算方法自适应选出最优输出中间LDR图像亮度；4)对中间LDR图像亮度灰度图过暗或过亮区域进行局部细节增强映射得到输出LDR图像亮度；5)将HDR图像色彩通道映射到对应输出LDR图像色彩通道,合并色彩空间获得输出LDR图像。</td>   <td>G06T5/00;G06T5/40;G06T7/136</td>  </tr>        <tr>   <td>中国专利</td>   <td>         唐晓颖;              邓丽洁;              袁进;                   黄海香       </td>   <td>佛山市顺德区中山大学研究院;中山大学</td>   <td>荧光染色图像反光点识别与重定义的方法及系统</td>   <td>广东省</td>   <td>CN109410236A</td>   <td>2019-03-01</td>   <td>本发明为荧光染色图像反光点识别与重定义的方法及系统,提供的方法及系统根据反光区域的RGB颜色特征,设置特定阈值范围识别图片中的反光点、反光区域,根据反光点周围像素点的颜色信息进行迭代填充,最后根据反光点特定范围像素点的颜色信息对其进行颜色的重新定义。该方法精准地识别出荧光染色图像的反光区域,对于溃烂区域和非溃烂区域的反光点都能很好地区分以及重新定义,解决了染色图像中反光区域影响精准分割的问题,为后续的角膜溃烂区域分割减少冗余信息与干扰因素。</td>   <td>1.一种荧光染色图像反光点识别与重定义的方法,其特征在于：包括以下步骤：S1.定义具体的RGB颜色空间三个通道的阈值范围作为发光点的判断阈值范围H,并以此阈值范围来识别提取荧光染色图像中角膜的反光点及对应的反光区域；S2.以反光区域的每一个反光点为中心,在半径为m个像素点的圆形区域上随机抽取一个像素点,并把该像素点的RGB值赋值给对应的反光点,m为大于1的整数；以上过程由反光区域边缘向内部中心的方向依次进行,赋值后反光点的RGB颜色空间三个通道的值不在判断阈值范围H内；S3.在步骤S2的基础上,以反光点为中心,在半径为n个像素点的圆形区域上随机抽取k个像素点,其中n为大于1的整数,k为大于2的整数；k个像素点中,若某个像素点的RGB值满足阈值范围：0＜R＜200,100＜G＜255,0＜B＜200,则该像素点判定反光点原来的颜色为绿色；否则判定为蓝色；统计k个像素点中分别判定反光点原来的颜色为绿色、蓝色的像素点的数量,若判定反光点原来的颜色为绿色的像素点较多,则重定义反光点的RGB值为绿色[0,255,0],否则重定义反光点的RGB值为蓝色[0,0,255]。</td>   <td>G06T7/136;G06T7/11;G06T7/90;G06T7/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周翠英;              杜子纯;                   刘镇       </td>   <td>中山大学</td>   <td>一种三维地层模型内部自由漫游方法</td>   <td>广东省</td>   <td>CN109410326A</td>   <td>2019-03-01</td>   <td>本发明涉及一种三维地层模型内部自由漫游方法,属于地质与工程信息技术领域。其特征是：不直接绘制地层模型中的各个面,而是通过建立一组平板,计算平板与各地层的相互位置关系,在平板的对应位置绘制岩性花纹,来等效地展示视角附近的地层分布；此方法可以排除地层模型内部杂乱的面对视点绘制区域的遮挡等影响,使漫游过程直观简洁,因此也可满足在模型内部自由漫游的需求。</td>   <td>1.一种三维地层模型内部自由漫游方法,包括以下步骤：(1)隐去所有地层图像,使其只能参与计算,但不绘制图像；(2)根据视点位置,在视点前方、左侧、右侧、上方、下方的一定距离建立出5个平板,这5个平板的位置形成缺一个面的长方体形状,视点在长方体的缺面处的中点；(3)在各平板上确定若干条间距相等的线段,线段间距越小,漫游精度越高,同时漫游过程对计算机计算能力的要求越高；(4)计算每条线段的端点所在的地层,以及线段与各地层分界面的交点位置(与线段端点统称为关键节点),建立关键节点和所在地层的对应关系。将关键节点根据其在线段中的位置进行排序,相邻两点间的范围即为该线段上对应地层的范围；(5)在同一平板上,把各相邻的线段上相同的地层连接起来,即为平板与各地层的相交范围,在相交范围内,绘制该地层对应的岩性花纹；(6)当视角移动或旋转后,删除原有平板,重复步骤(2)到(5)。</td>   <td>G06T17/05</td>  </tr>        <tr>   <td>中国专利</td>   <td>         任江涛;                   殷明旺       </td>   <td>中山大学</td>   <td>电子病历命名实体识别的方法、装置及存储介质</td>   <td>广东省</td>   <td>CN109388807A</td>   <td>2019-02-26</td>   <td>本发明公开了一种电子病历命名实体识别的方法,包括：生成待识别命名实体的电子病历的字符序列对应的字向量矩阵和偏旁向量矩阵,将所述偏旁向量矩阵输入到卷积神经网络层进行处理,得到所述字符序列对应的偏旁卷积向量矩阵,根据所述字向量矩阵和偏旁卷积向量矩阵生成字特征向量矩阵,将所述字特征向量矩阵输入到双向长短期记忆网络中进行处理,得到所述电子病历的命名实体识别结果。本发明还公开了一种电子病历命名实体识别装置和存储介质。本发明通过提取电子病历字符内部的形态特征,将字符本身的特征和字符内部的形态特征依次输入到深度神经网络中对字符标签进行预测,提供了一种识别准确率高的电子病历命名实体识别的方法。</td>   <td>1.一种电子病历命名实体识别的方法,其特征在于,所述电子病历命名实体识别的方法包括以下步骤：生成待识别命名实体的电子病历的字符序列对应的字向量矩阵；生成所述字符序列对应的偏旁向量矩阵；将所述偏旁向量矩阵输入到第一神经网络进行处理,得到所述字符序列对应的偏旁卷积向量矩阵,其中,所述第一神经网络包括卷积神经网络层；根据所述字向量矩阵和所述偏旁卷积向量矩阵生成字特征向量矩阵；将所述字特征向量矩阵输入到第二神经网络中进行处理,得到所述电子病历的命名实体识别结果,其中,所述第二神经网络包括双向长短期记忆网络层；其中,所述第一神经网络和所述第二神经网络的参数根据已识别命名实体的电子病历训练得到。</td>   <td>G06F17/27;G16H10/60;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              施煜锴;              陈崇雨;              王可泽;                   成慧       </td>   <td>中山大学</td>   <td>一种基于上下文相关多任务深度学习的图像超分辨算法</td>   <td>广东省</td>   <td>CN109389552A</td>   <td>2019-02-26</td>   <td>本发明提供一种基于上下文相关多任务深度学习的图像超分辨算法,该算法设计了三个深度神经网络,分别用于捕捉图像的基本信息、主要边缘信息和微小细节信息,然后在一个多任务学习的框架中对这些神经网络进行上下文相关连接与统一训练。给定输入的低分辨率图像,训练好的神经网络将分别输出基本图像、主要边缘图像和微小细节图像,最终的高分辨率图像由基本图像和微小细节图像融合而成；该算法可以仅用静态低分辨率(LR)图像为输入,恢复出高分辨率(HR)的图像。并且,所恢复出来的HR图像的结构得到了很好的保持,能尽可能多地恢复出理想HR图像中的结构信息。</td>   <td>1.一种基于上下文相关多任务深度学习的图像超分辨算法,其特征在于,包括以下步骤：S1：收集图像数据；S2：建立神经网络模型；S3：利用收集图像数据对所建立的神经网络模型进行训练；S4：将训练好的神经网络处理静态低分辨率图像即得到高分辨率的图像。</td>   <td>G06T3/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         路永和;                   刘小桦       </td>   <td>中山大学</td>   <td>一种基于word2vec和语义相似度的专利文本建模方法</td>   <td>广东省</td>   <td>CN109376352A</td>   <td>2019-02-22</td>   <td>本发明涉及文本建模领域,提出一种基于word2vec和语义相似度的专利文本建模方法,包括以下步骤：爬取专利文本集并进行预处理；计算专利文本集每个词的TF-IDF值,排序选取得到特征词集；文本集导入word2vec模型通过训练得到词向量；计算余弦相似度得到相近词集wordC<sub>1</sub>；计算word2vec相似度得到相近词集textC<sub>1</sub>；文本集导入文本处理系统进行训练,得到语义相似度,选取相近词集wordC<sub>2</sub>；计算语义相似度得到相近词集textC<sub>2</sub>；计算混合相似度得到扩展词集textC_f；计算权重形成新的文本标识,完成建模。本发明从word2vec的统计学角度和语义相似度的语义角度为传统向量空间模型增加一部分词语间的信息,在一定程度上降低其文本矩阵的稀疏性,且聚类效果更显著稳定,具有更强的文本标识能力。</td>   <td>1.一种基于word2vec和语义相似度的专利文本建模方法,其特征在于,包括以下步骤：S1：爬取指定领域的专利文本集,并对专利文本集进行预处理；S2：统计专利文本集中每个词的词频,得到词频文档；S3：计算专利文本集中每个词的TF-IDF值,按TF-IDF值对每个词进行降序排序,选择前n个词作为特征词并组成专利文本集的特征词集；S4：将经过预处理的专利文本集导入word2vec模型,设置模型参数,专利文本集通过训练得到词向量；S5：计算每个特征词的词向量与专利文本集中其他所有特征词的词向量之间的余弦相似度,按余弦相似度对每个特征词进行降序排列,选择一系列特征词作为相近词并组成对应该特征词的相近词集wordC_1；S6：计算每个特征词所对应的相近词与该专利文本的word2vec相似度,按word2vec相似度对相近词进行降序排序,选取前m个相近词组成对应专利文本的相近词集textC_1；S7：将经过预处理的专利文本集和词频文档导入文本处理系统的训练模块中进行训练；S8：将每个特征词与专利文本集中其他特征词输入到文本处理系统的语义相似度计算模块中,获得对应的语义相似度,按语义相似度对特征词进行降序排列,选择一系列词组成该特征词的相近词集wordC_2；S9：计算每个特征词对应的每个基于语义相似度的相近词与该专利文本的语义相似度,按语义相似度对基于语义相似度的相近词进行降序排序,选取前m个相近词组成该专利文本基于语义相似度的相近词集textC_2；S10：计算每篇专利文本中基于word2vec的相近词集textC_1和基于语义相似度的相近词集textC_1中所有相近词与相应专利文本的混合相似度,按混合相似度对相应专利文本的所有相近词进行降序排序,选取前m个相近词组成该专利文本的扩展词集textC_f；S11：计算每篇专利文本中每个词与专利文本对应扩展词集textC_f中的词在专利文本中的权重,形成新的文本表示,完成专利文本基于word2vec和语义相似度的文本建模。</td>   <td>G06F17/27;G06F16/332;G06Q50/18</td>  </tr>        <tr>   <td>中国专利</td>   <td>         梁业恒;              邓孺孺;              秦雁;                   刘永明       </td>   <td>中山大学</td>   <td>一种水中重金属可遥感反演的下限浓度计算方法</td>   <td>广东省</td>   <td>CN109376424A</td>   <td>2019-02-22</td>   <td>本发明公开了一种水中重金属可遥感反演的下限浓度计算方法,其通过传感器对应的辐射定标公式确定遥感观测值和遥感反射率之间的关系,通过水质遥感模型确定金属浓度和遥感反射率间的关系,并将关系式联立进行求解,推导出重金属在水中能被传感器观测到的可遥感反演的下限浓度计算公式,可建立起重金属可遥感反演下限浓度值与传感器辐射灵敏度、传感器类型、背景水体类型、重金属种类之间的函数关系,可用于方便快捷地计算得出不同水体情况下,各种重金属的可遥感反演的下限浓度,适用性强。</td>   <td>1.一种水中重金属可遥感反演的下限浓度计算方法,其特征在于,包括以下步骤：S1、通过所述传感器对应的辐射定标公式f<Sub>1</Sub>标定：R<Sub>n</Sub>＝f<Sub>1</Sub>(DN<Sub>n</Sub>)；通过水质遥感模型f<Sub>2</Sub>标定：R<Sub>n</Sub>＝f<Sub>2</Sub>(D<Sub>n</Sub>)；其中,D<Sub>n</Sub>为水体中的重金属浓度,DN<Sub>n</Sub>为传感器对遥感观测值,R<Sub>n</Sub>为重金属浓度为D<Sub>n</Sub>的水体的遥感反射率；S2、通过所述传感器对应的辐射定标公式f<Sub>1</Sub>标定：R<Sub>n+1</Sub>＝f<Sub>1</Sub>(DN<Sub>n</Sub>-ε)；通过水质遥感模型f<Sub>2</Sub>标定：R<Sub>n+1</Sub>＝f<Sub>2</Sub>(D<Sub>n+1</Sub>)；其中,D<Sub>n+1</Sub>为水体中的变化后的重金属浓度,DN<Sub>n</Sub>-ε为所述传感器对所述水体的遥感观测值,R<Sub>n+1</Sub>为重金属浓度为D<Sub>n+1</Sub>的水体的遥感反射率；ε为所述传感器的辐射分辨率；S3、将R<Sub>n</Sub>和R<Sub>n+1</Sub>作比,可得<Image id="icf0001" he="151" wi="646" file="FDA0001834839680000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>由此可推导出D<Sub>n+1</Sub>和D<Sub>n</Sub>之间的关系D<Sub>n+1</Sub>＝g(D<Sub>n</Sub>)；S4、取n＝0,有D<Sub>0</Sub>＝0,此时D<Sub>1</Sub>＝g(0)；其中,D<Sub>1</Sub>为重金属在水中能被传感器观测到的可遥感反演的下限浓度。</td>   <td>G06F17/50;G01N21/25</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨猛;                   钟琴       </td>   <td>中山大学</td>   <td>基于主动半监督学习的图像分类方法</td>   <td>广东省</td>   <td>CN109376796A</td>   <td>2019-02-22</td>   <td>本发明涉及图像处理技术领域,提出一种基于主动半监督学习的图像分类方法,包括以下步骤：随机选择部分标记样本和所有未标记样本,用于训练模型中的半监督字典学习组件；基于预估计类概率的准则从未标记的数据集中迭代地选择含有信息量最大的未标记样本,即最翔实样本；引入一个用户来标记所述最翔实样本,然后将完成标记的最翔实样本添加到标记的数据集中,用于训练模型中的主动学习组件；重复上述步骤迭代更新模型直至算法最终收敛或达到某一迭代次数；对测试样本使用模型进行图像分类。本发明解决了类间表达能力差的问题,结合半监督学习和主动学习,有效地利用所有训练数据,提高了该算法模型的性能。</td>   <td>1.基于主动半监督学习的图像分类方法,其特征在于,包括以下步骤：S1：随机选择部分标记样本和所有未标记样本,用于训练模型中的半监督字典学习组件；S2：采用基于预估计类概率的准则从未标记的数据集中迭代地选择含有信息量最大的未标记样本,即最翔实样本；S3：引入一个用户来标记所述最翔实样本,然后将完成标记的最翔实样本添加到标记的数据集中,用于训练模型中的主动学习组件；S4：重复S1～S3步骤,通过新的标记样本和剩余未标记数据来迭代更新模型,直至算法最终收敛或达到某一迭代次数；S5：对测试样本使用模型进行图像分类。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              钟勇彬;              陈荣军;              罗维冰;              谢舜道;              朱雄泳;                   曾衍瀚       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院</td>   <td>一种基于可视化二维码的安全支付方法及系统</td>   <td>广东省</td>   <td>CN109377209A</td>   <td>2019-02-22</td>   <td>本发明公开了一种基于可视化二维码的安全支付方法及系统,该系统为闭源系统,包括收款方终端、后台服务器及支付方终端,在整个系统运行实现安全支付的过程中,均对传输隧道进行加密。该方法包括：对由收款方终端传来的支付信息和指定图像进行接收；根据接收到的支付信息和指定图像,生成可视化二维码；可视化二维码中显示有指定图像；将可视化二维码发送至收款方终端,以令收款方终端对可视化二维码显示；对显示的可视化二维码进行扫描；对扫描得到的图像信息进行识别处理从而获得支付跳转链接；根据支付跳转链接,跳转对应的支付界面以完成支付。本发明能实现防伪效果,提高交易支付的安全。本发明的方法和系统可广泛应用于支付交易领域中。</td>   <td>1.一种基于可视化二维码的安全支付方法,其特征在于,包括以下步骤：对由收款方终端传来的支付信息和指定图像进行接收；根据接收到的支付信息和指定图像,生成可视化二维码；所述可视化二维码中显示有指定图像；将生成的可视化二维码发送至收款方终端,以令收款方终端对接收到的可视化二维码进行显示。</td>   <td>G06Q20/32;G06K19/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              黄浩洸;                   陈崇雨       </td>   <td>中山大学</td>   <td>一种像素级物体分割方法及装置</td>   <td>广东省</td>   <td>CN109377499A</td>   <td>2019-02-22</td>   <td>本发明公开了一种像素级物体分割方法及装置,所述方法包括：步骤S1,对初始获得的深度图像和彩色图像进行预处理,获得粗糙的前景物体分割结果以及其所在的包围盒区域；步骤S2,对包围盒区域中的彩色图像和深度图像进行预设尺度下的下采样,得到金字塔分辨率下的多组图像；步骤S3,基于图像像素距离,结合不同分辨率下的深度图像和彩色图像,从低分辨率到高分辨率依次对物体分割结果进行联合双边滤波；步骤S4,将经联合双边滤波处理后得到的二值化物体分割结果与初始获得的深度图像和彩色图像进行融合处理,获得最终的像素级目标分割结果,本发明可在低资源损耗的同时实现输出图像中的前景物体的像素级别分割结果的目的。</td>   <td>1.一种像素级物体分割方法,包括如下步骤：步骤S1,对初始获得的深度图像和彩色图像进行预处理,获得粗糙的前景物体分割结果以及其所在的包围盒区域；步骤S2,对包围盒区域中的彩色图像和深度图像进行预设尺度下的下采样,得到金字塔分辨率下的多组图像；步骤S3,基于图像像素距离,结合不同分辨率下的深度图像和彩色图像,从低分辨率到高分辨率依次对物体分割结果进行联合双边滤波；步骤S4,将经联合双边滤波处理后得到的二值化物体分割结果与初始获得的深度图像和彩色图像进行融合处理,获得最终的像素级目标分割结果。</td>   <td>G06T7/11;G06T7/194;G06T5/20;G06T5/40;G06T5/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              邓铭豪;              陈荣军;              谢舜道;              苏宏雄;              朱雄泳;                   曾衍瀚       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院</td>   <td>一种基于改进ViBe算法的运动目标检测方法及系统</td>   <td>广东省</td>   <td>CN109377515A</td>   <td>2019-02-22</td>   <td>本发明公开了一种基于改进ViBe算法的运动目标检测方法及系统,其中,方法包括以下步骤：采用融合三帧差法的ViBe算法从输入视频图像中获得第一运动前景图像；采用融合边缘检测的ViBe算法对输入视频图像进行边缘提取,并获得第二运动前景图像；将第一运动前景图像、第二运动前景图像和预设的空白图像进行融合处理,并获得前景融合图像；将前景融合图像进行图像复原后,输出复原后的前景融合图像,从而完成运动目标检测。本发明采用三帧差算法和ViBe算法结合,降低了ViBe算法带来的“鬼影”对运动目标检测精度的影响,提高了检测准确度,满足运动目标的检测,降低监控成本,可广泛应用于计算机视频图像处理领域。</td>   <td>1.一种基于改进ViBe算法的运动目标检测方法,其特征在于,包括以下步骤：S1、采用融合三帧差法的ViBe算法从输入视频图像中获得第一运动前景图像；S2、采用融合边缘检测的ViBe算法对输入视频图像进行边缘提取,并获得第二运动前景图像；S3、将第一运动前景图像、第二运动前景图像和预设的空白图像进行融合处理,并获得前景融合图像；S4、将前景融合图像进行图像复原后,输出复原后的前景融合图像,从而完成运动目标检测。</td>   <td>G06T7/254;G06T7/13;G06T5/30;G06T5/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         符顺;              谢晓华;                   陈翔       </td>   <td>中山大学</td>   <td>基于单帧人脸图像的实时三维人脸重建方法</td>   <td>广东省</td>   <td>CN109377557A</td>   <td>2019-02-22</td>   <td>本发明公开了一种基于单帧人脸图像的实时三维人脸重建方法,步骤如下：从摄像头获取人脸图像,对图像进行人脸检测与人脸特征点定位与标注；根据人脸特征点的定位进行头部的姿态计算,获得图像中头部的旋转参数；使用特征点进行人脸归一化,计算归一化后的人脸深度信息；使用人脸深度信息,对标准头部三维网格模型进行变形；根据特征点与输入图像获得人体头部纹理图像；利用旋转参数与归一化获得变形后的头部网格点与纹理图像的直接映射关系；使用变形后的头部网格点、纹理图像与两者间的对应关系进行三维绘制与渲染,并展示给用户。本方法通过展示时纹理代替深度细节信息实现加速与纹理直接映射三维模型简化映射运算,达到三维重建的实时效果。</td>   <td>1.一种基于单帧人脸图像的实时三维人脸重建方法,其特征在于,所述的实时三维人脸重建方法包括以下步骤：S1、从摄像头获取人脸图像作为输入,对人脸图像进行人脸检测与人脸特征点定位与标注,如果存在人脸,则将人脸特征点标注在人脸图像上；S2、根据人脸特征点的定位进行头部的姿态计算,获得人脸图像中头部的旋转参数；S3、使用人脸特征点进行人脸归一化,计算归一化后的人脸深度信息；S4、使用人脸深度信息,对标准头部三维网格模型进行变形；S5、根据人脸特征点与摄像头输入的人脸图像获得人体头部纹理图像；S6、使用旋转参数与归一化,获得变形后的头部网格点与纹理图像的直接映射关系；S7、使用变形后的头部网格点、纹理图像与以上两者间的直接映射关系进行三维重建与渲染,并展示给用户。</td>   <td>G06T17/00;G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;              贾笑卿;                   李烨       </td>   <td>中山大学;广州市鑫广飞信息科技有限公司</td>   <td>一种基于边缘检测和颜色匹配的异常检测方法</td>   <td>广东省</td>   <td>CN109359513A</td>   <td>2019-02-19</td>   <td>本发明提出一种基于边缘检测和颜色匹配的异常检测方法,通过对不同时刻采集的两幅航拍图像进行配准、颜色平衡、边缘检测和颜色匹配,高效且准确地检测其中的异常区域。针对传统基于图像要素比对的异常检测方法的不足,本发明在图像配准步骤引入基于邻域的特征点匹配对筛选以及基于距离与面积准则的关键点选择策略,提高了图像配准的可靠性；在图像比对环节结合边缘检测与颜色匹配,解决了异常检测与颜色平衡相冲突的问题,提高了异常检测的准确率。</td>   <td>1.一种基于边缘检测和颜色匹配的异常检测方法,其特征在于：包括以下步骤：S1.对两幅图像A、B进行SIFT特征点的检测和匹配,获得特征点匹配对；S2.采用Lowe算法对得到的特征点匹配对进行初步的筛选,初步筛选完毕后进行基于邻域的特征点匹配对筛选；S3.从经过筛选的特征点匹配对中选取三对特征点匹配对,并基于三对特征点匹配对求取仿射变换矩阵,对图像A/B进行仿射变换；S4.对两幅图像A、B中的颜色异常进行初步的检测,将两幅图像转换为Lab颜色空间,然后进行Lab颜色空间下的颜色平衡；S5.对经历过颜色平衡的图像A、B分别进行边缘异常检测、颜色异常检测。</td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              程海杰;                   张权       </td>   <td>中山大学</td>   <td>一种高效的跨摄像头行人双向跟踪方法</td>   <td>广东省</td>   <td>CN109359552A</td>   <td>2019-02-19</td>   <td>本发明公开了一种高效的跨摄像头行人双向跟踪方法,用于获取行人在已出现过摄像头中的完整轨迹。包括步骤：(1)从监控视频中获取图像,对图像中的行人进行检测,根据检测结果建立候选行人库；(2)构建行人再识别模型,提取待查行人和候选行人库中所有图像的特征,一一计算待查行人特征与候选行人库中所有图像特征的距离特征,获取待查行人在其他摄像头下的最佳匹配块,并将其作为跟踪的起始位置；(3)进行目标跟踪,跟踪过程中通过正反向处理视频完成对行人的双向跟踪；(4)将不同摄像头下的轨迹进行整合,得到行人最终的轨迹输出。本发明对真实场景下的跨境跟踪具有速度快、精度高的优点,具有很强的工程意义。</td>   <td>1.一种高效的跨摄像头行人双向跟踪方法,其特征在于,包括步骤：(1)从监控视频中获取图像,对图像中的行人进行检测,根据检测结果建立候选行人库；(2)构建行人再识别模型,提取待查行人和候选行人库中所有图像的特征,一一计算待查行人特征与候选行人库中所有图像特征的距离特征,获取待查行人在其他摄像头下的最佳匹配块,并将其作为跟踪的起始位置；(3)进行目标跟踪,跟踪过程中通过正反向处理视频完成对行人的双向跟踪；(4)将不同摄像头下的轨迹进行整合,得到行人最终的轨迹输出。</td>   <td>G06K9/00;G06K9/62;G06T7/246</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              余伟浩;              陈添水;                   王青       </td>   <td>中山大学</td>   <td>一种图像场景图生成方法及装置</td>   <td>广东省</td>   <td>CN109359564A</td>   <td>2019-02-19</td>   <td>本发明公开了一种图像场景图生成方法及装置,所述方法包括：步骤S1,对输入图像进行处理,找出该图像中可能存在物体的各个物体候选区域；步骤S2,统计物体共存的概率,构建物体共存概率矩阵,并根据该图像候选框的数量,动态地构建嵌入了所述物体共存概率矩阵先验知识的知识图网络,利用该知识图网络对图像中各个候选区域进行物体分类；步骤S3,统计物体关系分布的概率,构建关系概率矩阵,并根据该图像候选框的数量和类别,动态地构建嵌入关系概率矩阵先验信息的知识图网络,利用该知识图网络对图像中候选区域两两之间的关系进行分类；步骤S4,整合步骤S3和S4的结果,生成场景图,本发明可提高图像场景图生成的准确率,提升小样本关系分类的准确率。</td>   <td>1.一种图像场景图生成方法,包括如下步骤：步骤S1,对输入图像进行处理,找出该图像中可能存在物体的各个物体候选区域；步骤S2,统计物体共存的概率,构建物体共存概率矩阵,并根据该图像的候选框的数量,动态地构建嵌入了所述物体共存概率矩阵先验知识的知识图网络,利用该知识图网络对图像中各个候选区域进行物体分类；步骤S3,统计物体关系分布的概率,构建关系概率矩阵,并根据该图像的候选框的数量和类别,动态地构建嵌入关系概率矩阵先验信息的知识图网络,利用该知识图网络对图像中候选区域两两之间的关系进行分类；步骤S4,整合步骤S3和S4的结果,生成场景图。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李明;                   邹小兵       </td>   <td>昆山杜克大学;中山大学附属第三医院(中山大学肝脏病医院)</td>   <td>基于联合学习身份和情感信息的面部表情识别方法</td>   <td>江苏省</td>   <td>CN109359599A</td>   <td>2019-02-19</td>   <td>本发明公开了一种基于联合学习身份信息和情感信息的面部表情识别方法,包括人脸识别图像数据库和面部表情图像数据库,利用人脸识别图像数据库独立训练面部身份信息网络支路,训练完毕后将最后的全连接层去掉,通过神经网络可以提取得到输入图像的身份特征向量；利用面部表情图像数据库训练面部表情信息网络支路,完毕后把全连接层去掉,通过神经网络可以提取得到输入图像的情感特征向量；将身份特征向量和情感特征向量串联在一起得到串联面部特征表达；将融合身份信息和面部信息的串联面部表达特征馈送给全连接层,后续训练仅使用面部表情图像数据库对合并网络进行联合学习和优化。本发明提升面部表情识别方法对于受试者个体间自身差异的鲁棒性。</td>   <td>1.一种基于联合学习身份信息和情感信息的面部表情识别方法,其特征在于,包括以下步骤：使用人脸识别图像数据库和面部表情图像数据库来联合训练神经网络和优化神经网络；所述人脸识别图像数据库用于独立训练和优化面部身份信息网络支路,训练完毕后将最后的人脸身份输出层去掉,只提取得到输入图像对应的身份特征向量；所述面部表情图像数据库用于独立训练和优化面部表情信息网络支路,训练完毕后把最后的面部表情输出层去掉,只提取得到输入图像对应的情感特征向量；将身份特征向量和情感特征向量串联在一起得到串联面部特征表达；最后将融合了身份信息和面部信息的串联面部表达特征馈送给随后的面部表情输出层；在后续网络训练过程中,仅使用面部表情图像数据库对合并网络进行联合学习和优化,并最终预测面部表情识别结果。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         任江涛;              陈兆鹏;                   梁华淇       </td>   <td>中山大学</td>   <td>基于深度学习的股票预测方法、装置、设备及存储介质</td>   <td>广东省</td>   <td>CN109360097A</td>   <td>2019-02-19</td>   <td>本发明公开了一种基于深度学习的股票预测方法,包括：获取目标股票和关联股票的最新交易数据,生成所述最新交易数据对应的多维特征矩阵,将所述最新交易数据对应的多维特征矩阵输入复合神经网络进行处理,得到所述目标股票的预测结果,本发明还公开了一种基于深度学习的股票预测装置、基于深度学习的股票预测设备和存储介质,本发明通过先利用复合神经网络中的卷积神经网络学习目标股票和关联股票的交易数据的特征,再将特征输入到复合神经网络中的长短期记忆网络进行处理,得到对股票涨跌的预测,提供了一种基于深度学习和群体智能的股票预测方法,可以准确地预测股票的涨跌。</td>   <td>1.一种基于深度学习的股票预测方法,其特征在于,所述基于深度学习的股票预测方法包括以下步骤：获取目标股票和关联股票的最新交易数据；生成所述最新交易数据对应的多维特征矩阵；将所述最新交易数据对应的多维特征矩阵输入复合神经网络进行处理,得到所述目标股票的预测结果,其中,所述复合神经网络的参数由所述目标股票和所述关联股票的历史交易数据训练得到,所述关联股票与所述目标股票相关联。</td>   <td>G06Q40/04;G06Q10/04;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周凡;              戴熹;              林淑金;                   苏卓       </td>   <td>中山大学</td>   <td>基于PBF的流体凝固模拟方法及系统</td>   <td>广东省</td>   <td>CN109344450A</td>   <td>2019-02-15</td>   <td>本发明公开了一种基于PBF的流体凝固模拟方法,包括：S1,初始化数据；S2,粒子固定半径邻域搜索；S3,更新粒子的凝固状态；S4,将速度衰减因子作用于粒子,计算粒子的位置中间量；S5,计算粒子密度及运动约束量；S6,计算粒子的位置变化量,并进行防穿透修正；S7,更新粒子速度及位置,并处理粒子与边界碰撞问题；S8,计算人造粘性和涡流约束。本发明还公开了一种基于PBF的流体凝固模拟系统。本发明,可通过不同的粒子速度衰减因子来模拟不同的流体模拟的快慢,还可通过位置变化量修正粒子穿透现象,在GPU的支持下实现更大规模更有效率的流体凝固模拟。</td>   <td>1.一种基于PBF的流体凝固模拟方法,其特征在于,包括：S1,初始化数据；S2,粒子固定半径邻域搜索；S3,更新粒子的凝固状态；S4,将速度衰减因子作用于粒子,计算粒子的位置中间量；S5,计算粒子密度及运动约束量；S6,计算粒子的位置变化量,并进行防穿透修正；S7,更新粒子速度及位置,并处理粒子与边界碰撞问题；S8,计算人造粘性和涡流约束。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李熙莹;              李国鸣;              江倩殷;                   邱铭凯       </td>   <td>中山大学</td>   <td>一种基于特征选择卷积神经网络的目标重识别方法和装置</td>   <td>广东省</td>   <td>CN109344695A</td>   <td>2019-02-15</td>   <td>本发明公开了一种基于特征选择卷积神经网络的目标重识别方法和装置,所述方法包括将待重识别的原始图像输入到设有特征图选择层的特征选择卷积神经网络中对目标进行重识别等步骤,所述特征图选择层分别设置在相邻的两个卷积层之间,所述特征图选择层用于接收上一层卷积层输出的特征图组,并对所接收的特征图组所包含的特征图进行筛选,并将经过筛选后的特征图组作为下一层卷积层的输入值。通过对特征图的筛选删除,对输出的特征图进行选择再将选择结果送入下一层卷积,可以减弱与重识别无关的、不具区分度的特征图在特征选择卷积神经网络中的传播,从而减少了无关信息的干扰,提高了网络提取鲁棒特征的能力。本发明广泛应用于图像识别技术领域。</td>   <td>1.一种基于特征选择卷积神经网络的目标重识别方法,其特征在于,包括以下步骤：S1.将待重识别的原始图像输入到特征选择卷积神经网络中；S2.特征选择卷积神经网络对原始图像进行处理,从而提取并输出原始图像的特征向量；S3.根据原始图像的特征向量,对目标进行重识别；所述特征选择卷积神经网络包括多个卷积层,所述每个卷积层分别用于对各自的输入值进行处理,从而输出与输入值相应的特征图组,所述特征图组包括多个特征图；所述特征选择卷积神经网络还包括至少一个特征图选择层,所述特征图选择层分别设置在相邻的两个卷积层之间,所述特征图选择层用于接收上一层卷积层输出的特征图组,并对所接收的特征图组所包含的特征图进行筛选,并将经过筛选后的特征图组作为下一层卷积层的输入值。</td>   <td>G06K9/00;G06K9/46;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林浩添;              杨雅涵;              李王婷;                   张凯       </td>   <td>中山大学中山眼科中心</td>   <td>一种基于深度学习的眼部图像处理系统</td>   <td>广东省</td>   <td>CN109344808A</td>   <td>2019-02-15</td>   <td>本发明涉及一种基于深度学习的眼部图像处理系统,包括图像初筛模块、图像定位模块：所述图像初筛模块,用于通过第一CNN模型判断眼部图像是否为眼部异常图像；所述图像定位模块,用于通过Faster-RCNN模型定位眼部图像中正常解剖结构及异常现象的位置,并对正常解剖结构及异常现象的类型进行标注,得到眼部标注图像。本发明可以获取眼部图像中正常解剖结构及异常现象的位置并且标注正常解剖结构及异常现象的类型,从而辅助系统使用者快速、准确地判断眼部图像上所包含的解剖结构及所存在的异常现象。</td>   <td>1.一种基于深度学习的眼部图像处理系统,其特征在于,包括图像初筛模块(10)、图像定位模块(20)：所述图像初筛模块(10),用于通过第一CNN模型判断眼部图像是否为眼部异常图像；所述图像定位模块(20),用于通过Faster-RCNN模型定位眼部图像中正常解剖结构及异常现象的位置,并对正常解剖结构及异常现象的类型进行标注,得到眼部标注图像。</td>   <td>G06K9/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王青;              赵惠;              陈添水;                   林倞       </td>   <td>中山大学</td>   <td>一种基于图片蒸馏的通用物体检测框架及其实现方法</td>   <td>广东省</td>   <td>CN109344897A</td>   <td>2019-02-15</td>   <td>本发明公开了一种基于图片蒸馏的通用物体检测框架及其实现方法,该框架包括：Faster RCNN模型,构建Faster RCNN的网络结构,并进行训练,得到训练好的Faster RCNN模型；Wae Faster RCNN检测模型,将输入图像分解成两个分辨率只有原图一半的子图,构建并利用Wae Faster RCNN网络结构分别对低频和高频子图进行物体检测,将两个子图的检测结果进行融合得到最终检测结果；训练指导单元,对Wae Faster RCNN检测模型进行训练,并在训练时引入知识蒸馏机制,利用已训练好的Faster RCNN模型的输出作为软目标来指导Wae Faster RCNN模型的训练。</td>   <td>1.一种基于图片蒸馏的通用物体检测框架,包括：Faster RCNN模型,用于构建Faster RCNN的网络结构,并进行训练,得到训练好的Faster RCNN模型；Wae Faster RCNN检测模型,用于将输入图像分解成两个分辨率只有原图一半的子图,构建Wae Faster RCNN网络结构,利用Wae Faster RCNN网络结构分别对低频子图和高频子图进行物体检测,然后将两个子图的检测结果进行融合得到最终检测结果；训练指导单元,用于对所述Wae Faster RCNN检测模型进行训练,并在所述Wae FasterRCNN检测模型训练时引入知识蒸馏机制,利用训练好的Faster RCNN模型的输出作为软目标来指导所述Wae Faster RCNN检测模型的训练。</td>   <td>G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>              孙雪冬       </td>   <td>中山大学</td>   <td>一种基于知识拓扑关系的群体教学内容量化优化方法</td>   <td>广东省</td>   <td>CN109345047A</td>   <td>2019-02-15</td>   <td>本发明公开了一种基于知识拓扑关系的群体教学内容量化优化方法,首先,对研究对象进行限定,再给出研究对象群体的折中优化模型；然后,创建相关知识图谱描述,并由此图谱获得学习个体的个性化知识图谱；试设定群体的知识目标和知识背景,并进行个体知识目标及知识背景划分；最后,进行群体补充知识及冗余知识求解,并计算相应的补充量及冗余量,并根据优化目标、重复前面步骤进行群体教学内容优化,给出此时个体的补充知识及冗余知识。本发明可以根据群体中个体的知识目标和知识背景,给出优化的群体知识目标、知识背景及教学内容,并给出每个个体需要补充的知识元及冗余知识元,解决了传统教学计划制定中教学内容无法进行量化优化的难题。</td>   <td>1.一种基于知识拓扑关系的群体教学内容量化优化方法,其特征在于：该方法包括以下步骤：步骤1,限定研究对象,所述研究对象包括由知识目标和知识背景相近的多个学习个体构成的学习群体；通过分析所述学习群体与所述学习个体的关系,给出所述学习群体教学内容优化模型,所述模型为基于学习群体冗余知识量和补充知识量的折中优化模型；根据知识特点及学习群体与学习个体的学习实际情况给出进行优化求解的基本前提；步骤2,根据所述学习群体中所述学习个体的知识目标进行相关知识图谱选择或创建,并基于有向超图对所述相关知识图谱进行描述,用有向超图的节点描述某一领域所包含的知识元,用有向超图的超边描述所述知识元之间的关系；所述知识元是指不可再分割的、具有完备知识表达的知识单位；在所述相关知识图谱上进行学习个体原知识目标和原知识背景描述,获得个性化知识图谱；步骤3,根据所述学习群体中包含的所述学习个体的知识目标和知识背景试设定所述学习群体的知识目标和知识背景；步骤4,根据试设定的所述学习群体知识目标和知识背景,进行所述学习个体知识目标及知识背景划分,其中,所述学习个体目标知识划分为：公共目标知识集、多余目标知识集、补充目标知识集；所述学习个体知识背景划分为：公共背景知识集、重复背景知识集、补充背景知识集；并给出所述学习个体知识目标和知识背景的的形式化描述；步骤5,进行所述学习个体的公共知识、补充知识及冗余知识求解,在此基础上,给出群体教学内容、个体学习内容的求解公式,进而进行相应的学习群体的补充知识量、冗余知识量计算；根据优化目标,重复前面步骤,进行群体教学内容优选,并给出此时学习个体的补充知识集、冗余知识集。</td>   <td>G06Q10/04;G06Q50/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘永红;              林晓芳;                   林颖       </td>   <td>中山大学</td>   <td>基于我国机动车登记制度的车辆存活曲线模型优化方法</td>   <td>广东省</td>   <td>CN109345136A</td>   <td>2019-02-15</td>   <td>本发明公开了一种基于我国机动车登记制度的车辆存活曲线模型优化方法,该方法包括：在威布尔分布存活曲线模型基础上,利用我国机动车登记制度对单车进行全生命周期记录的特点,建立曲线模型的特征参数新车增长量n<Sub>0,j</Sub>(k)、报废量s<Sub>i,j</Sub>(k+i)的直接标定方法,从而优化适用于我国国情的车辆存活曲线模型。与现有技术相比,本发明的优点在于充分利用了我国机动车登记制度业务数据对单车进行全生命周期记录的数据特点,实现对车辆存活规律特征参数的准确获取,与已有的间接标定方法相比,本发明具有更高的可靠性；由于发明所涉及的业务数据时间跨度包含我国机动车污染防治工作的实施前后阶段,因此本优化模型可反映车辆管控措施对车辆存活曲线的影响。</td>   <td>1.一种基于我国机动车登记制度的车辆存活曲线模型优化方法,其特征在于,所述方法包括获取机动车业务登记信息,以获得新车增长量n<Sub>0,j</Sub>(k)和报废量s<Sub>i,j</Sub>(k+i)；采用威布尔概率分布来构建车辆存活曲线模型,该分布表达如下：          <Image id="icf0001" he="66" wi="700" file="FDA0001837405000000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中,<Image id="icf0002" he="69" wi="191" file="FDA0001837405000000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>表示车型j在车龄i的存活概率；b<Sub>j</Sub>为方程待定系数,表示失效陡度(b<Sub>j</Sub>＞1)；T<Sub>j</Sub>为方程待定系数,表示车型j的服务寿命；k为年份；又,存活概率<Image id="icf0003" he="71" wi="188" file="FDA0001837405000000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>满足以下方程：          <Image id="icf0004" he="35" wi="700" file="FDA0001837405000000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        n<Sub>i,j</Sub>(k+i)＝1-s<Sub>i,j</Sub>(k+i)                   (3)其中,n<Sub>0,j</Sub>(k)新车增长量,指k年j车型新车数(车龄i＝0)；n<Sub>i,j</Sub>(k+i)指j车型车辆在k+i年仍正常使用的车辆数；s<Sub>i,j</Sub>(k+i)报废量,指j车型车辆在k+i年报废的车辆数；故,公式(1)换算为：          <Image id="icf0005" he="68" wi="700" file="FDA0001837405000000015.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        通过以上i组新车增长量n<Sub>0,j</Sub>(k)、报废量s<Sub>i,j</Sub>(k+i),依据公式(4)进行拟合,以最小二乘法迭代,确定在误差平方和最小时的分车型曲线方程的待定系数b<Sub>j</Sub>、T<Sub>j</Sub>,建立存活曲线的数学模型。</td>   <td>G06Q10/06;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         翁建平;              郑雪瑛;              许宇彤;              严晋华;              杨黛稚;                   骆斯慧       </td>   <td>中山大学附属第三医院(中山大学肝脏病医院)</td>   <td>基于传统食物频率问卷法的电子问卷调查方法及装置</td>   <td>广东省</td>   <td>CN109345278A</td>   <td>2019-02-15</td>   <td>本发明公开了一种基于传统食物频率问卷法的电子问卷调查方法、装置、设备及介质,所述方法包括通过接收管理终端录入的食物品种分类内容和食用频率参数,根据所述食物品种分类内容和所述食用频率参数构建食物频率数据模型,将所述食物频率数据模型按照跳转规则生成膳食调查问卷,并在接收到用户终端接受电子问卷调查邀请的响应信息后,向所述用户终端发送所述膳食调查问卷,实现了食物频率调查问卷的电子化,有效地实现调查文件的无纸化,从而降低成本和操作难度,且有利于远程问卷调查的实现。</td>   <td>1.一种基于传统食物频率问卷法的电子问卷调查方法,其特征在于,所述方法至少包括：接收管理终端录入的食物品种分类内容和食用频率参数；其中,所述食物品种分类内容包括依据食物能量和营养进行分类的食物种类内容,所述食用频率参数包括预设数量的食物食用频率的阈值参数；根据所述食物品种分类内容和所述食用频率参数构建食物频率数据模型；其中,所述食物频率数据模型包括预设的问题以及与所述预设的问题对应的预设数量的可选答案；将所述食物频率数据模型按照跳转规则生成膳食调查问卷；其中,所述跳转规则表示所述预设的问题的跳转逻辑；在接收到用户终端接受电子问卷调查邀请的响应信息后,向所述用户终端发送所述膳食调查问卷。</td>   <td>G06Q30/02;G16H20/60</td>  </tr>        <tr>   <td>中国专利</td>   <td>         覃晓逸;              张东;                   李明       </td>   <td>中山大学</td>   <td>一种基于LSTM的独立说话人语音发音逆求解的方法</td>   <td>广东省</td>   <td>CN109346107A</td>   <td>2019-02-15</td>   <td>本发明涉及试验系统及其方法,更具体地涉及一种基于LSTM的独立说话人语音发音逆求解的方法,具体步骤如下：(1)首先对指定4个人音频信号以及同步的轨迹信号进行采集,通过安放传感器对上唇(Upper lip,UL)、下唇(Lower lip,LL)、下齿龈(Lower incisor,LI)、舌尖(Tongue tip,TP)、舌中(Tongue body,TB),舌根(Tongue dorsum,TD)六个点的数据进行采集；(2)在步骤(1)之后,选定鼻梁(RF)为参考点,在参考点处也放置传感器进行数据的采集。本发明第一：预测了未在训练集中出现说话人的语音发音轨迹；第二：改变输入特征,选取了效果更好、更合适的的声学特征作为网络输入,提升了RMSE和相关系数；第三：克服了轨迹采集时不连续、不平滑的特性。</td>   <td>1.一种基于LSTM的独立说话人语音发音逆求解的方法,其特征在于,具体步骤如下：(1)首先对指定4个人音频信号以及同步的轨迹信号进行采集,通过安放传感器对上唇(Upper lip,UL)、下唇(Lower lip,LL)、下齿龈(Lower incisor,LI)、舌尖(Tongue tip,TP)、舌中(Tongue body,TB),舌根(Tongue dorsum,TD)六个点的数据进行采集；(2)在步骤(1)之后,选定鼻梁(RF)为参考点,在参考点处也放置传感器进行数据的采集；(3)在步骤(2)之后,选定其中三个记为A、B、C作为训练人,D作为测试人；(4)将训练人的语音信号进行特征提取,提取梅尔频率倒谱系数(Mel FrequencyCepstrum Coefficient,MFCC)以及音素后验概率(phoneme posterior probabilities,PPP)；并且将梅尔频率倒谱系数(Mel Frequency Cepstrum Coefficient,MFCC)以及音素后验概率(phoneme posterior probabilities,PPP)作为联合输入特征(tandem),输入到长短期记忆网络(Long Short-Term Memory,LSTM)网络中；获取MFCC步骤中Mel滤波的公式为,          <Image id="icf0001" he="99" wi="694" file="FDA0001824590870000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        (5)选定训练好的模型,将D的联合输入特征(tandem)作为输入,发音轨迹作为输出；对轨迹与采集到的轨迹计算RMSE和相关系数r进行数据推测,并与参考数据对比；RMSE和相关系数r是衡量系统的两个指标；RMSE越小,误差越小,r越大,预测的轨迹与真实值的轨迹趋势越接近；公式如下：          <Image id="icf0002" he="184" wi="539" file="FDA0001824590870000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中e<Sub>i</Sub>是网络预测的输出,t<Sub>i</Sub>是在i时间的真实值；          <Image id="icf0003" he="200" wi="626" file="FDA0001824590870000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        其中e’是预测值的均值,t’是实际值的均值；语音音频和语音轨迹同步数据是利用NDI公司的WAVE系统采集。</td>   <td>G10L25/24;G10L15/06;G10L25/30;G10L17/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄定帮;                   潘嵘       </td>   <td>中山大学</td>   <td>一种多任务模型生成词向量的方法</td>   <td>广东省</td>   <td>CN109325231A</td>   <td>2019-02-12</td>   <td>本发明涉及计算机领域中的自然语言处理的技术领域,更具体地,涉及一种多任务模型生成词向量的方法。该方法通过集成无监督任务,分类任务,词性标注等多个任务模型的信息,增强其产生的词向量所蕴涵的信息。同时在多任务集成上使用高效且足够优秀的模型,以便能在大规模数据集上进行使用。该方法通过GloVe模型(基于全局信息的词向量,Global vectors for word representation)训练无监督任务,获取语言模型相关的信息。通过Fasttext模型训练分类任务,来获取文本中的类别信息。通过逻辑回归模型训练词性任务,获取词性相关信息。该方法能够在大规模数据集上快速得到蕴含丰富词义的优质词向量,从而应用于自然语言处理任务场景中。</td>   <td>1.一种多任务模型生成词向量的方法,其特征在于,包括以下步骤：S1.数据预处理与词向量初始化；S2.分类任务；S3.词性任务；S4.无监督任务；S5.优化目标与参数优化。</td>   <td>G06F17/27;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;              张泳翔;              黄鹏;              李仕仁;                   王德明       </td>   <td>广州智慧城市发展研究院;中山大学</td>   <td>一种基于PCA和SRC的高清图像人脸快速识别方法</td>   <td>广东省</td>   <td>CN109325416A</td>   <td>2019-02-12</td>   <td>本发明公开了一种基于PCA和SRC的高清图像人脸快速识别方法,主要提高高清图像的人脸识别速度。该方法主要包括以下步骤：首先,获取标准图像,输入数据库；接着,结合主成分分析(PCA,Principal Component Analysis)和Adaboost算法思想,使用数据库中的人脸图像对模型进行训练,获得多个特征矩阵。通过每个特征矩阵均能对应得到待分类图像的分类结果,再进行加权投票获得最终的分类预测,这样可在降低维度提高速度的同时保证分类识别的准确率；然后,通过对待分类图像进行仿射变换,降低SRC算法对人像姿态的要求；最后,使用改进的SRC对待分类图像进行分类。通过实施本发明实例,依次获取图片输入数据库、对图片进行模型训练、对待分类图像进行仿射变换、使用改进的SRC对图片进行分类识别,本专利所提出的方法具有很好的效果。</td>   <td>1.一种基于PCA和SRC的高清图像人脸快速识别方法,其特征在于所述方法包括：获取人脸图像,输入数据库；结合主成分分析和Adaboost算法思想,使用数据库中的人脸图像对模型进行训练,获得多个特征矩阵；通过对待分类图像进行仿射变换,降低SRC算法对人像姿态的要求；使用改进的SRC对待分类图像进行分类,减少算法的计算量。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赖剑煌;              黄培根;              王广聪;                   谢晓华       </td>   <td>中山大学</td>   <td>一种结合表观特征和时空分布的双流网络行人重识别方法</td>   <td>广东省</td>   <td>CN109325471A</td>   <td>2019-02-12</td>   <td>本发明公开了一种结合表观特征和时空分布的双流网络行人重识别方法,方法主要包括下述步骤：使用深度神经网络提取行人图像的表观特征并计算图像对的表观相似度；通过基于高斯平滑的统计方法学习训练数据集的时空分布模型；通过基于逻辑平滑的联合度量方法对表观相似度和时空概率进行联合计算得出最终相似度；将最终相似度进行排序得到行人重识别结果。主要贡献包括：(1)提出一种结合表观特征和时空分布的双流网络行人重识别框架；(2)提出新的基于高斯平滑的时空分布学习方法。(3)提出新的基于逻辑平滑的相似性联合度量方法。实验结果表明,本方法在DukeMTMC-reID和Market1501数据集上的Rank1准确率分别从83.8％和91.2％提高到94.4％和98.0％,较其他方法有非常大的性能提升。</td>   <td>1.一种结合表观特征和时空分布的双流网络行人重识别方法,其特征在于,包括步骤：表观特征上,使用深度神经网络算法提取每个行人图像的表观特征向量,计算出所有行人图像对之间的表观相似度,所述图像对是指检索图像和数据库图像；时空分布上,对于训练数据集,以一时间差单位区间统计每组摄像头对的原始时间差概率分布模型,得到n*n个时间差概率统计直方图,n为摄像头个数,然后对每个时间差概率统计直方图进行高斯平滑,得到时空分布模型；由时空分布模型求出检索图像和数据库图像之间的时空概率；对表观相似度和时空概率进行逻辑平滑,得到平滑后的表观相似度和时空概率；将平滑后的表观相似度和时空概率进行联合计算得到最终的行人图像对相似度,对行人图像对相似度排序得到行人重识别结果。</td>   <td>G06K9/00;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘冶;              李宏浩;              陈宇恒;              刘春鹏;              吕梦瑶;                   印鉴       </td>   <td>中山大学;广州赫炎大数据科技有限公司</td>   <td>用户价值预测方法、装置、存储介质及设备</td>   <td>广东省</td>   <td>CN109325640A</td>   <td>2019-02-12</td>   <td>本发明涉及一种用户价值预测方法、装置、存储介质及设备,包括：将预处理后的行为数据和对应的用户价值存储为数据集；通过数据集分别训练SVM模型、随机森林模型和决策树模型,再用训练好的模型做预测,获得各模型预测的用户价值；通过各模型预测的用户价值以及对应的实际的用户价值训练逻辑回归模型,确定最优的SVM模型、随机森林模型和决策树模型以及逻辑回归模型；获取目标产品中待分析用户的行为数据,并对行为数据进行预处理；且分别输入至最优的SVM模型、随机森林模型和决策树模型中,获得各模型预测的用户价值；将各模型预测的用户价值输入至最优的逻辑回归模型中,获得最终预测的用户价值。本发明提高了预测的准确性,提高了预测速度。</td>   <td>1.一种用户价值预测方法,其特征在于,包括如下步骤：获取目标产品中用户的行为数据以及对应的用户价值,并对所述行为数据进行预处理,且将预处理后的行为数据和对应的用户价值存储为数据集；通过数据集分别训练SVM模型、随机森林模型和决策树模型,再用训练好的模型做预测,获得各模型预测的用户价值；通过各模型预测的用户价值以及对应的实际的用户价值训练逻辑回归模型,确定最优的SVM模型、随机森林模型和决策树模型以及逻辑回归模型；获取目标产品中待分析用户的行为数据,并对所述行为数据进行预处理；将预处理后的行为数据分别输入至最优的SVM模型、随机森林模型和决策树模型中,获得各模型预测的用户价值；将各模型预测的用户价值输入至最优的逻辑回归模型中,获得最终预测的用户价值。</td>   <td>G06Q10/04;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         赵鹏;              郑贵锋;                   周凡       </td>   <td>中山大学</td>   <td>一种基于联盟链的多模型溯源方法</td>   <td>广东省</td>   <td>CN109325785A</td>   <td>2019-02-12</td>   <td>本发明公开了一种基于联盟链的多模型溯源方法。本发明首先为商品线上各企业创建联盟,然后首个企业记录商品相关信息到区块链,之后下一企业通过查询模型、消息模型、交易模型之一获取相关信息,之后流转商品并重复该操作直至到达最后零售环节供消费者查询。本发明通过综合三种不同的信息提取模型,提供可供选择的溯源方式,实现针对不同产品不同成本的防伪溯源控制,通过统一接口的方式,降低各企业使用门槛和不一致性；在保证区块链的安全特性的优势下,联盟链提供了更独立的方式,解决了区块链最关键的性能问题；提供了允许特定企业间的隐私信息保护功能,在最大程度让企业便捷操作提升影响力的同时,保证企业的隐私数据安全,保护企业利益。</td>   <td>1.一种基于联盟链的多模型溯源方法,其特征在于,所述方法包括：各企业在系统上注册企业信息,获取企业数字证书,并管理企业信息,如企业资质、介绍等；待溯源商品的商品线上各企业针对该商品在系统中创建一个联盟,联盟内各企业相互信任；联盟内各企业设定联盟内信任策略,满足该策略的交易方可写入到区块链；依据信任策略,商品线首个企业记录该商品信息、环境信息、运输信息等到区块链,生成信息hash,并通知商品线的下个企业,如此时存在商品唯一码,一并记录在固定字段中；依据信任策略,商品线的下个企业获取上一企业针对该商品所上传的信息hash,与商品信息、环境信息、运输信息等一起记录到区块链中,如此时存在商品唯一码,一并记录在固定字段中；流转商品并重复上一步操作,直至到达最后零售环节；消费者通过零售环节信息hash或商品唯一码查询该商品的各环节信息,进行溯源。</td>   <td>G06Q30/00;G06F16/901;G06F16/903</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李伟彬;                   潘嵘       </td>   <td>中山大学</td>   <td>一种基于词向量的检测词义随时间变化的方法</td>   <td>广东省</td>   <td>CN109308356A</td>   <td>2019-02-05</td>   <td>本发明涉及自然语言处理的技术领域,更具体地,涉及一种基于词向量的检测词义随时间变化的方法。本发明在普通词向量的基础上考虑上了时间维度,相比普通的词向量,丰富了词向量在不同时间上的语义表示。有助于自然语言处理更高层次上的应用。而不像现有方法,将所有数据都认为是在同一个时间段内,忽视了时间对词义的影响。另一方面,通过对比词在不同时代的词向量的变化,可以观察到词随时代变迁的词义变化情况,有助于语言学家,历史学家的考察。</td>   <td>1.一种基于词向量的检测词义随时间变化的方法,包括以下步骤：S1: 将数据集按照每十年的间隔时间分开；S2: 在每个数据集中,初始的词向量除了第一个时间段是随机产生的外,其它时间的词向量都是以上一个时间段的词向量作为初始词向量；S3: 对于数据集中的每一行,以窗口大小为n,将窗口内的词与窗口的中心词组成pair；所以一个窗口总共有n-1对pair；S4: 对于其中的每对pair,先索引到它们各自的词向量,然后将中心词的词向量作为输入；另一个词是当成label；S5: 搭建全连接神经网络,将中心词的词向量作为输入；S6: 利用交叉熵函数计算loss；S7: 重复S2到S6,直到loss收敛,不再下降；S8: 对于下一个时间段的训练,将词向量初始化为上一个时间段已经训练好的词向量,然后重复S2到S7；S9: 所有时间段的词向量都训练完成之后,通过计算同一个词在不同的时间段的词向量的余弦相似度,就可以判断这个词在不同时代的词义是否发生了变化。</td>   <td>G06F17/27;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         邓倩倩;              王若梅;              周凡;              林格;                   陈湘萍       </td>   <td>中山大学</td>   <td>基于控制网格变形的部件网格融合方法及系统</td>   <td>广东省</td>   <td>CN109308732A</td>   <td>2019-02-05</td>   <td>本发明公开了一种基于控制网格变形的部件网格融合方法,包括：S1,对源网格和目标网格的融合边界进行预处理；S2,生成源网格与目标网格的控制网格；S3,对控制网格进行变形及融合处理；S4,对变形及融合后的控制网格进行渗透处理。本发明还公开了一种基于控制网格变形的部件网格融合系统。采用本发明,可有效处理具有任意拓扑结构的融合边界的网格之间的融合问题,并且在保持网格几何特征的基础上自适应的变形。</td>   <td>1.一种基于控制网格变形的部件网格融合方法,其特征在于,包括：S1,对源网格和目标网格的融合边界进行预处理；S2,生成源网格与目标网格的控制网格；S3,对控制网格进行变形及融合处理；S4,对变形及融合后的控制网格进行渗透处理。</td>   <td>G06T15/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘凯;              刘洋;              朱远辉;                   柳林       </td>   <td>中山大学</td>   <td>基于梯度滤波和PCA的无人机影像与多光谱影像融合方法</td>   <td>广东省</td>   <td>CN106023130B</td>   <td>2019-02-01</td>   <td>本发明公开一种基于梯度滤波和PCA的无人机影像与多光谱影像融合方法。其首先经过影像配准、重采样至相同像元尺寸,裁剪得到相同空间范围的两套独立的多波段影像,分别为波段数目大但空间分辨率低的多光谱遥感影像与三波段高空间分辨率的无人机影像；然后对两套多波段影像采用相关系数矩阵分别进行主成分变换；进一步地,对无人机影像的全部主成分选取特定的滤波算子进行梯度滤波获取其三个主成分的纹理信息,并将其以一定权重叠加至波段数目大但空间分辨率低的多光谱影像的前三个主成分中进行增强；最后对增强后的主成分进行主成分逆变换获得波段数目大同时空间分辨率高的多光谱融合结果。本方法扩展了传统融合方法的单波段全色数据与多光谱影像融合的局限性,可以使更多样的数据参与影像融合,实现融合结果更丰富的空间细节信息。</td>   <td>1.一种基于梯度滤波和主成分变换的三波段无人机光学影像与多光谱遥感影像融合方法,其特征在于,包括：S11.经过影像配准、重采样至相同像元尺寸,裁剪得到相同空间范围的两套独立的多波段影像,分别为波段数目大但空间分辨率低的多光谱遥感影像与三波段高空间分辨率的无人机影像；S12.对两套多波段影像采用相关系数矩阵分别进行主成分变换；S13.对无人机影像的全部主成分选取预设的滤波算子进行梯度滤波获取其三个主成分的纹理信息；S14.对无人机影像主成分的纹理信息以预设的权重叠加至波段数目大但空间分辨率低的多光谱影像的前三个主成分中进行增强；S15.对增强后的主成分进行主成分逆变换获得波段数目大同时空间分辨率高的多光谱融合结果。</td>   <td>G06T5/50;G06T7/44</td>  </tr>        <tr>   <td>中国专利</td>   <td>         蔡铭;                   高慧敏       </td>   <td>中山大学</td>   <td>结合声线跟踪法与声束跟踪法的室内外声音传播模拟方法</td>   <td>广东省</td>   <td>CN106096321B</td>   <td>2019-02-01</td>   <td>本发明提出一种结合声线跟踪法与声束跟踪法的室内外声音传播模拟方法,具体是：首先对室外空间进行三维空间剖分,并利用声束跟踪法模拟声音在室外的传播,当声束遇到门窗时,根据门窗的形状将该声束分割为若干子声束,并记录下即将进入室内的子声束,而其余子声束则在室外继续传播,不断重复上述过程,直到达到预先设定的声音传播阈值；然后利用所记录的子声束,通过几何运算可以获得每个声源点的若干个搜索夹角范围,对于每个声源点,只在其记录的各个搜索夹角范围内发射声线,则可以利用声线跟踪法来模拟声音在室内的传播。本发明应用于三维室内外空间的声场计算,可以有效提高区域声场的计算效率,并确保室内声场的计算精度。</td>   <td>1.一种结合声线跟踪法与声束跟踪法的室内外声音传播模拟方法,其特征在于,包含以下步骤：步骤1：根据建筑物外围边界及计算区域的边界,将室外空间剖分为若干个三棱柱状的子空间,作为加速声束跟踪法模拟室外声音传播过程的基础；步骤2：若噪声源为线声源或面声源,则对噪声源进行离散化,形成一系列的点声源；若噪声源为点声源,则无需离散化,直接进行下一步；步骤3：从点声源出发,利用声束跟踪法模拟其在室外三维空间的传播,当声束遇到墙面,且与门窗有重叠部分,则根据门窗的形状将该声束分割为若干子声束,每个子声束在墙面上的投影均为矩形,并记录下即将进入室内的子声束,而其余子声束则继续利用声束跟踪法模拟其在室外的传播；步骤4：步骤3中产生的子声束,若在后续的室外传播过程当中又遇到门窗,则根据门窗的形状将子声束进一步分割,不断重复步骤3的处理过程,直到达到预先设定的声音传播阈值；步骤5：利用步骤3、步骤4中所记录的即将进入室内的子声束,通过几何运算获得每个声源点的若干个搜索夹角范围,对于每个点声源,只在其记录的各个搜索夹角范围内发射声线,并利用声线跟踪法来模拟该点声源在室内的声音传播。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘玉葆;                   吴杰锋       </td>   <td>中山大学</td>   <td>一种基于最小顶点覆盖的新闻文本去重方法</td>   <td>广东省</td>   <td>CN109299443A</td>   <td>2019-02-01</td>   <td>本发明涉及一种基于最小顶点覆盖的新闻文本去重方法,通过去除尽量少的文本,使得剩余的文本数据集中不存在重复的文本,本发明使用了树状的动态规划和基于优先队列的贪心算法,在保证在有效去除重复文本的同时,使得需要去除的文本数量尽量地少。这种方法降低了去除的文本数量,增大了去重后可用的数据量,使后续的数据分析过程更为准确和有效。</td>   <td>1.一种基于最小顶点覆盖的新闻文本去重方法,其特征在于,包括以下步骤：步骤S1：计算两篇新闻文本两两之间的相似度sim(ti,tj),ti,tj分别表示两篇新闻文本；判断相似度sim(ti,tj)是否达到阈值α,若达到阈值α,则进行步骤S2,若相似度没有达到阈值α,则认为两篇文章不重复；步骤S2：为新闻文本之间的相似关系建立一个相似图G＝&lt;V,E&gt;,并用邻接表进行存储,将文本看做图论中的顶点,在这两个顶点之间连着一条无向边；步骤S3：创建一个相似图G中需要去除的顶点列表list,接着判断G的每一个连通分量G’＝&lt;V’,E’&gt;是不是一棵树,即判断有没有环,若G’无环,则转到步骤S4进行处理；若G’有环,则转到步骤S5处理；步骤S4：对相似图G中的每一棵树执行树状的动态规划算法以筛选出需要去除的节点,将需要去除的节点加入list中；步骤S5：对相似图G中的每一个有环的连通分量执行基于优先队列的贪心算法筛选出需要去除的节点,将需要去除的节点加入list中；步骤S6：在新闻文本数据集中删除list中所有需要去除的顶点所对应的文本,则剩余的新闻文本均为不重复,将这些不重复的文本作为输出。</td>   <td>G06F17/22;G06F16/9535</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘冶;              张允聪;              陈宇恒;              吕梦瑶;              傅自豪;              潘炎;                   印鉴       </td>   <td>中山大学;广州赫炎大数据科技有限公司</td>   <td>基于人脸识别的数据推荐方法、装置、服务器端及客户端</td>   <td>广东省</td>   <td>CN109299709A</td>   <td>2019-02-01</td>   <td>本发明涉及一种基于人脸识别的数据推荐方法、装置、服务器端及客户端,其中,方法包括：获取小程序提供的用户标识以及人脸图像；根据用户标识,获取与用户标识对应存储的人脸特征向量；从人脸图像提取获得人脸特征向量,判断提取的人脸特征向量与存储的人脸特征向量是否匹配,若匹配,则获取与存储的人脸特征向量相对应的推荐数据,并将推荐数据发送到小程序显示。通过人脸识别作为数据推荐的依据,使用方便,有效防止信息泄露的问题,能更好的刻画用户画像,通过结合小程序进行数据推荐,无需安装卸载应用,可实现数据的快速分析和处理,减少了资源的占用,降低了开发成本和开发门槛,能够在生活服务场景得到广泛应用。</td>   <td>1.一种基于人脸识别的数据推荐方法,其特征在于,包括如下步骤：获取小程序提供的用户标识以及人脸图像；其中,用户访问小程序时将触发一触发指令,小程序根据所述触发指令获取存储的用户标识,并根据所述触发指令调用小程序所在的设备的摄像头,获取用户的人脸图像；根据所述用户标识,获取与用户标识对应存储的人脸特征向量；从所述人脸图像提取获得人脸特征向量,判断提取的人脸特征向量与存储的人脸特征向量是否匹配,若匹配,则获取与存储的人脸特征向量相对应的推荐数据,并将所述推荐数据发送到小程序显示。</td>   <td>G06K9/00;G06F16/9535</td>  </tr>        <tr>   <td>中国专利</td>   <td>              戴小款       </td>   <td>中山大学</td>   <td>一种过滤垃圾用户和抽取短文本话题的方法</td>   <td>广东省</td>   <td>CN109284507A</td>   <td>2019-01-29</td>   <td>本发明提供一种过滤垃圾用户和抽取短文本话题的方法,该方法对原始数据进行垃圾用户过滤处理,很大程度上避免了检测出来的突发性话题为没有实际意义话题这个问题；采用BTM(Biterm Topic Model)主题模型进行话题抽取,模型清晰易懂,并且抽取话题的效率高效。</td>   <td>1.一种过滤垃圾用户和抽取短文本话题的方法,其特征在于,包括以下步骤：S1：对微博数据流进行垃圾用户过滤处理,过滤掉在短时间内发布大量相似微博的用户及其所发微博数据；S2：在进行过步骤S1处理后的数据上,计算某个时间窗口内微博数据词对的突发性数值；S3：用步骤S2所计算的突发性数值跟所设阈值进行比较；若大于所设阈值,则进入步骤S4,若小于所设阈值,则进入下一个时间窗口,进入步骤S2；S4：使用BTM模型对该时间窗口内的微博数据进行主题抽取并输出突发性主题。</td>   <td>G06F17/27;G06F17/22</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王学孟;                   吴洲华       </td>   <td>顺德中山大学太阳能研究院</td>   <td>光伏组件热流计算方法及装置</td>   <td>广东省</td>   <td>CN109284566A</td>   <td>2019-01-29</td>   <td>本申请实施例提供一种光伏组件热流计算方法及装置,通过获取采集到的光伏组件所在区域的实时气象数据和所述光伏组件的安装参数,并根据所述实时气象数据和所述安装参数,计算所述光伏组件的温度分布信息。由此,可以准确地获得光伏组件在实际运行过程中的温度分布信息,从而更加全面地了解温度对于光伏组件的影响,有效地提高光伏组件评估工作的准确性,进一步帮助电站运维人员分析组件受热情况,从而针对性地改进评估、运维方式,提升电站整体发电性能,增加收益。</td>   <td>1.一种光伏组件热流计算方法,其特征在于,所述方法包括：获取采集到的光伏组件所在区域的实时气象数据和所述光伏组件的安装参数,其中,所述实时气象数据包括辐照度数据、风向数据、风速数据、湿度数据、气压数据、环境温度数据,所述安装参数包括组件安装倾角、安装方位角、安装海拔、组件材料属性；根据所述实时气象数据和所述安装参数,计算所述光伏组件的温度分布信息。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周永章;              徐述腾;              沈文杰;                   张彦龙       </td>   <td>中山大学</td>   <td>矿石矿物图像自动识别与分类方法</td>   <td>广东省</td>   <td>CN109284780A</td>   <td>2019-01-29</td>   <td>本发明公开了一种矿石矿物图像自动识别与分类方法,本发明运用计算机视觉技术和深度卷积神经网络理论,利用大数据平台Tensorflow框架,建立卷积人工神经网络模型,并针对来自吉林夹皮沟金矿等不同地区黄铁矿石镜下照片进行图像数据输入模型训练学习,从而实现镜下黄铁矿石图片中不同矿石矿物的自动识别与分类。本发明可以辅助地质工作者来对矿石矿物的镜下照片进行识别与分类,提高地质工作者的工作效率。</td>   <td>1.矿石矿物图像自动识别与分类方法,其特征是包括以下步骤：步骤一：收集涵盖多种类型的岩石矿物镜下照片,并对图像参数统一调节及图像分割标注；步骤二：将输入照片的像素位置发生变化,得到更多的输入图像数据,实现图像数据增加处理；步骤三：根据岩石矿物的图像特征,运用Unet分割网络模型将镜下全岩照片按照矿物种类经过机器模型的有监督训练学习,该模型的岩石矿物自动识别与鉴定框架的构建与训练所对应的框架层次的具体解释如下：Step1:在输入层中直接将原始岩石矿物图像数据输入网络进行训练,并将原始岩石矿物镜下照片转换为572*572大小的特征图像；Step2：输入层在接收到原始的572*572岩石数据图像之后传播至第一个卷积层,然后经过两层的3*3卷积操作和欧拉激活函数转换之后变成64幅568*568大小的特征图像,在进入step3的同时,部分特征图像被截取为64个392*392大小的特征图像至step1；Step3:将上一步中64副568*568大小的特征图像进行2*2的最大池化操作得到64个248*248大小的特征图像；然后再经过两层的3*3卷积操作和欧拉激活函数转换之后变成128副280*280大小的特征图像；再进入step4的同时,部分特征图像被截取为step9中的256个200*200大小的特征图像；Step4:将经过上一层操作处理的特征图像进行2*2的最大池化操作得到128个140*140大小的特征图像,然后再将图像数据进行两层的3*3卷积操作和欧拉激活函数转换之后变成512个64*64大小的特征图像,然后部分特征图像被截取为1024个56*56大小的特征图像至step8；Step5：将上层中处理过的特征图像进行2*2的最大池化操作得到256个68*68大小的特征图像,再将图像数据进行两层的3*3卷积操作和欧拉激活函数转换之后变成512个64*64大小的特征图像,部分特征图像被截取为1024个56*56的图像数据至step7；Step6:将上一步操作中512个64*64大小的特征图像经过一次2*2的最大池化操作之后得到512个32*32大小的图像数据；然后再经过两层的3*3卷积操作和欧拉激活函数转换之后得到1024个8*8大小的特征图像；Step7:经过上一层操作处理的图像数据经过2*2的上卷积处理之后与经过step5中图像复制与截取处理得到的1024个56*56的图像数据共同得到1024个56*56大小的图像数据,并在此基础上再经过两层3*3卷积操作和欧拉激活函数转换之后得到512副52*52大小的特征图像；Step8:512副52*52大小的特征图像经过2*2的向上卷积处理之后结合step4中图像复制与截取处理得到的结果共同组成512个104*104大小的特征图像；再经过两层3*3卷积操作和欧拉激活函数转换之后得到256个100*100大小的特征图像；Step9:先经过向上卷积处理和step3中图像复制与截取处理后得到256个200*200大小的特征图像,然后经过两层3*3卷积操作和欧拉激活函数转换之后得到128个196*196大小的特征图像；Step10:先经过向上卷积处理和step2中图像复制与截取处理后得到128个392*392大小的特征图像；然后再经过两层的3*3卷积操作和欧拉激活函数转换后得到64个388*388大小的特征图像；最后再将64*388*388个神经元节点分别与五层572*572个神经元节点进行全连接,最终输出为五层572*572大小的输出层,分别标记为0-4,即分别代表其他矿物、黄铜矿、黄铁矿、方铅矿及闪锌矿,并分别得出了各矿物的准确识别率,再结合统计学的方法可得出每种矿物所占比例。</td>   <td>G06K9/62;G06K9/34;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         詹羽荣;              赵常均;              李博;              万磊;                   肖定坤       </td>   <td>广州智能装备研究院有限公司;中山大学肿瘤防治中心</td>   <td>一种头颈部肿瘤检测方法、装置及计算机可读存储介质</td>   <td>广东省</td>   <td>CN109285142A</td>   <td>2019-01-29</td>   <td>本发明公开了一种头颈部肿瘤检测方法、装置及计算机可读存储介质,所述方法包括：采集头颈部CT数据,并根据所述头颈部CT数据中的窗宽和窗位调整Hu值,形成头颈部图片；将所述头颈部图片输入预设的区域分割模型,输出像素分割图；对所述像素分割图进行图形膨胀和腐蚀处理,分割出头颈部各个部位所在区域的区域图片；将所述区域图片输入预设的目标检测模型,检测出肿瘤的位置和种类。本发明能够基于深度学习,融合区域分割技术和目标检测技术对头颈部肿瘤进行检测,使得检测结果更加稳定,检测精度也更高。</td>   <td>1.一种头颈部肿瘤检测方法,适于在计算设备中执行,其特征在于,包括如下步骤：采集头颈部CT数据,并根据所述头颈部CT数据中的窗宽和窗位调整Hu值,形成头颈部图片；将所述头颈部图片输入预设的区域分割模型,输出像素分割图；对所述像素分割图进行图形膨胀和腐蚀处理,分割出头颈部各个部位所在区域的区域图片；将所述区域图片输入预设的目标检测模型,检测出肿瘤的位置和种类。</td>   <td>G06T7/00;G06T7/11;G06T7/155;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周永章;              高乐;              唐沐阳;                   王若嘉       </td>   <td>中山大学</td>   <td>矿床三维地质建模方法</td>   <td>广东省</td>   <td>CN109285221A</td>   <td>2019-01-29</td>   <td>本发明公开了一种矿床三维地质建模方法,本发明在现代成矿预测理论研究的基础上,结合已有的地形地质图、勘探线剖面图、钻孔数据等资料,运用三维地质建模技术建立了径口矿段的地表模型、断裂模型、地层实体模型、矿体模型。本发明可以清晰表达矿体品位值变化特征,进而加深对矿体、矿床的空间分布规律的认识。</td>   <td>1.矿床三维地质建模方法,其特征是包括以下步骤：步骤一：分析矿区地质环境,构建矿区地质简图；步骤二：建立地质数据库：收集矿区的钻孔、勘探线剖面图、探槽、地形地质图的地质数据,建立完善的三维地质空间数据库；步骤三：使用Gocad软件结合编程完成三维地质建模,三维地质建模包括地表模型、断层模型、地层实体模型和矿体模型；所述的地表模型利用收集矿区已知的连续坐标点构建数字地形模型,并对模型表面进行相应地离散光滑插值,使数字地形模型更加完善与接近实际地表；所述的断层模型通过结合钻孔编录数据与勘探线剖面图完成对断层立体模型的构建,钻孔编录数据与勘探线剖面图包括断层在地下深处延伸的长度、厚度、倾向、倾角,在Gocad软件中利用断层面解译工具将大量断层面上的离散点进行组合处理,得到断层模型；所述的地层实体模型为多个层状分布的地层面的层数据集合,将两相邻地层间充填地质体即形成区内地层实体形成地层实体模型；所述的矿体模型通过勘探线地质剖面图及钻孔编录数据,并参照矿体轮廓圈定原则对矿体进行圈定形成矿体模型。</td>   <td>G06T17/05</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑永川;              王若梅;              周凡;              林谋广;                   林格       </td>   <td>中山大学</td>   <td>基于体素模型的影像重建四边形网格方法及系统</td>   <td>广东省</td>   <td>CN109285223A</td>   <td>2019-01-29</td>   <td>本发明公开了一种基于体素模型的影像重建四边形网格方法,包括：S1,将CT影像转换为文本格式并进行平滑处理；S2,选取灰度值并使用插值法提取出等值点作为点云数据；S3,根据堆叠立方体的方法构建初始体素模型,并对体素模型进行体素优化,通过深度优先搜索提取出体素模型外表面；S4,计算体素模型外表面每个顶点的法向量,通过领域搜索方法查找与法向量最近的一个点云点作为顶点的映射点；S5,将四边形网格的顶点调整到重心后进行重新映射并对四边形网格进行网格塌缩处理。本发明还公开了一种基于体素模型的影像重建四边形网格系统。采用本发明,可直接从点云重建出四边形网格,并可选取不同的体素立方体大小来控制生成的四边形网格精细程度。</td>   <td>1.一种基于体素模型的影像重建四边形网格方法,其特征在于,包括：S1,CT影像预处理：将CT影像转换为文本格式,并进行平滑处理过滤噪声；S2,提取点云数据：根据要重建的组织选取灰度值,并使用插值法提取出等值点作为点云数据；S3,构建体素模型：根据堆叠立方体的方法构建初始体素模型,并对体素模型进行体素优化,通过深度优先搜索提取出体素模型外表面；S4,顶点映射：计算体素模型外表面每个顶点的法向量,通过领域搜索方法查找与法向量最近的一个点云点作为顶点的映射点；S5,网格优化：将四边形网格的顶点调整到重心后进行重新映射,并对四边形网格进行网格塌缩处理。</td>   <td>G06T17/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李雅美;                   潘嵘       </td>   <td>中山大学</td>   <td>基于强化学习的生成式文本摘要方法</td>   <td>广东省</td>   <td>CN109271629A</td>   <td>2019-01-25</td>   <td>本发明涉及人工智能自然语言处理的技术领域,更具体地,涉及基于强化学习的生成式文本摘要方法。基于强化学习的生成式文本摘要方法,包括Actor部分与Critic部分,其中,包括以下步骤：S1.Actor部分用Seq2Seq方法生成摘要序列,Seq2Seq由编码器和解码器组成,同时应用了Attention机制；S2.Critic部分通过监督学习的方式估计Actor部分的状态价值V(s)；S3.不断重复步骤S1和步骤S2,使得Actor部分与Critic部分的网络参数不断优化,直到收敛；S4.最终Actor部分的模型即为文本摘要生成模型。本发明将Rouge评估指标通过强化学习方法融入到训练目标中,即最终的训练目标是最大似然和Rouge指标的加权平均。</td>   <td>1.基于强化学习的生成式文本摘要方法,包括Actor部分与Critic部分,其特征在于,包括以下步骤：S1.Actor部分用Seq2Seq方法生成摘要序列,Seq2Seq由编码器和解码器组成,同时应用了Attention机制；S2.Critic部分通过监督学习的方式估计Actor部分的状态价值V(s)；S3.不断重复步骤S1和步骤S2,使得Actor部分与Critic部分的网络参数不断优化,直到收敛；S4.最终Actor部分的模型即为文本摘要生成模型。</td>   <td>G06F17/27;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄淼鑫;                   潘嵘       </td>   <td>中山大学</td>   <td>一种嵌入外部词典信息的词向量改进方法</td>   <td>广东省</td>   <td>CN109271635A</td>   <td>2019-01-25</td>   <td>本发明涉及自然语言处理的技术领域,更具体地,涉及一种嵌入外部词典信息的词向量改进方法。本发明在普通词向量的基础上融合了相似词词典和相关词词典的信息,相比于普通词向量,本发明可以较好的分离共现词的影响,同时缩小词义相近的词的词向量距离,使得最终的词向量更接近词的客观词义；另一方面,因为词向量是很多自然语言处理任务的底层技术,更接近客观词义的词向量有助于下游任务的提升。外部预训练的高质量词向量在一些任务中还能缓解标注数据不足的问题。</td>   <td>1.一种嵌入外部词典信息的词向量改进方法,其特征在于,包括以下步骤：S1: 准备一个大型语料库和一个电子词典；S2: 相似词词典：电子词典的每个词可能会附带有近义词和同义词,利用脚本将其抽取出来并记录；S3: 相关词词典：在大型语料库中,使用统计方法寻找相关词对,按照两个相关词的联合概率远大于两个词的单独概率乘积的原则,将相关词对识别出来并记录；S4: 针对语料库,统计出现的所有词及其词频,构建一个词汇表；S5: 在语料库中设定一个滑动窗口,窗口大小为n,取窗口的中间词为中心词,将中心词和其他词构成正例pair；S6: 在词汇表中依据词频确定被采样概率,采样出若干个词,和中心词一起构成负例pair；S7: 如果S5的中心词出现在相似词词典中,则分别把中心词和相似词典记录的对应词构成pair,加入到正例pair中；S8: 如果S5的中心词出现在相关词词典中,则分别把中心词和相关词典记录的对应词构成pair,加入到负例pair中；S9: 搭建一个单层且无偏置参数的全连接神经网络,将正例pair和负例pair作为输入,利用sigmoid函数输出pair是正例或负例的概率；S10: 利用均方差计算输出loss,使用梯度下降法使loss下降；S11: 重复S5到S10,直到loss收敛；S12: 全连接网络的权重矩阵即是所有词语的词向量构成的矩阵。</td>   <td>G06F17/27</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈旭游;              王若梅;              周凡;              陈小燕;                   林格       </td>   <td>中山大学</td>   <td>基于MPM的血液凝固模拟方法及系统</td>   <td>广东省</td>   <td>CN109271696A</td>   <td>2019-01-25</td>   <td>本发明公开了一种基于MPM的血液凝固模拟方法,包括：S1,初始化通用数据；S2,将血液离散化成粒子和网格；S3,根据物质点法模拟血液凝固。本发明还公开了一种基于MPM的血液凝固模拟方法系统。采用本发明,可基于MPM框架,根据凝固的特点结合连续介质力学以及流体力学N-S方程,并且根据血液凝固速度设计拉梅系数函数,来精确地模拟血液凝固过程。</td>   <td>1.一种基于MPM的血液凝固模拟方法,其特征在于,包括：S1,初始化通用数据；S2,将血液离散化成粒子和网格；S3,根据物质点法模拟血液凝固。</td>   <td>G06F17/50;G06T13/20</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林冠豪;                   吴贺俊       </td>   <td>中山大学</td>   <td>一种基于双层LSTM神经网络的动作识别方法</td>   <td>广东省</td>   <td>CN109271889A</td>   <td>2019-01-25</td>   <td>本发明涉及传感器领域,提出一种基于双层LSTM神经网络的动作识别方法,包括以下步骤：收集原始数据,对原始数据进行去噪处理；对数据进行分片；计算分片数据的频谱图,将两两相邻的频谱图进行减法运算,得出频谱图差值；对分片数据打标签；将打好标签的数据划分为训练集、交叉验证集和测试集；将频谱图差值导入双层单向LSTM神经网络模型对数据进行训练；在交叉验证集上不断调整学习率参数,选择准确度最高的模型对应的学习率作为最终参数值；将最终参数值导入测试集中,将模型运行在测试集数据上,其运行结果即为本算法模型的最终结果。本发明能够记忆所有输入数据,准确划分不同的人体动作的运动数据,提取不同动作在时间上的依赖性。</td>   <td>1.一种基于双层LSTM神经网络的动作识别方法,其特征在于,包括以下步骤：S1：收集原始数据,对原始数据进行去噪处理；S2：根据分片标准对经去噪处理的数据进行分片；S3：计算分片数据的频谱图,将两两相邻的频谱图进行减法运算,得出频谱图差值；S4：对分片数据打标签；S5：将打好标签的数据划分为训练集、交叉验证集和测试集；S6：设计双层单向LSTM神经网络模型,并将频谱图差值输入双层单向LSTM神经网络模型中对数据进行训练；S7：在交叉验证集上不断调整学习率参数,选择准确度最高的模型对应的学习率作为最终参数值；S8：确定双层单向LSTM神经网络模型的最终参数值之后,将模型运行在测试集数据上,其运行结果即为动作识别的结果。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑慧诚;                   林浩文       </td>   <td>中山大学</td>   <td>一种轻量化人脸联合检测与识别方法及其系统</td>   <td>广东省</td>   <td>CN109271974A</td>   <td>2019-01-25</td>   <td>本发明公开了一种轻量化人脸联合检测与识别方法及其系统,应用于人脸识别领域。其具体思路为：首先从摄像头读取的一张RGB图像,把该图像输入到特征提取网络中,生成检测与识别网络共享的特征图；特征图首先用于输入到区域候选网络得到人脸候选区域及这些区域在RGB图像中的位置信息,这些候选区域与特征图经过处理后再经过人脸特征提取网络提取到人脸特征向量。通过对比人脸特征向量之间的距离,可以得到人脸之间的相似度。本发明能够共享特征图计算结果,减少计算量；并且由于检测与识别用同一个网络实现,训练过程中检测能向有利于识别的方向学习,有利于提高识别质量,同时实现轻量化的目的。</td>   <td>1.一种基于轻量化人脸联合检测与识别的方法,其特征在于,包括以下步骤：S1：将训练图像进行初始化；S2：将训练图像输入到人脸联合检测与识别网络中并对其中的主干网络模块、区域候选模块RPN以及人脸特征提取模块进行训练；S3：将待测图像输入到训练好的人脸联合检测与识别网络中实现人脸的识别。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;              全小虎;              丁颜玉;              李仕仁;                   谭德志       </td>   <td>广州智慧城市发展研究院;中山大学</td>   <td>一种基于区块链网络下的区块实现方法及系统</td>   <td>广东省</td>   <td>CN109272316A</td>   <td>2019-01-25</td>   <td>本发明实施例公开了一种基于区块链网络下的区块实现方法及系统,区块链系统与应用端建立交易节点和共识节点,所述方法包括如下步骤：所述区块链系统控制着交易节点在非交付共识下与应用端实现交易互动,并控制着共识节点在与应用端存在交付共识时产生区块,并将所产生的区块发送给交易节点,所述交易节点将所述区块存储在本地账本中。在本发明实施例中,将负责交易的节点和负责共识的节点区分开来,交易节点与应用紧紧相关,而共识节点只需获得应用的背书交易,然后独立的运行复杂的共识环节,可以使得共识节点独立采用算力大的终端,减少交易直接相关节点的负担,从而提升整个区块链的效率。</td>   <td>1.一种基于区块链网络下的区块实现方法,其特征在于,区块链系统与应用端建立交易节点和共识节点,所述方法包括如下步骤：所述区块链系统控制着交易节点在非交付共识下与应用端实现交易互动,并控制着共识节点在与应用端存在交付共识时产生区块,并将所产生的区块发送给交易节点,所述交易节点将所述区块存储在本地账本中。</td>   <td>G06Q20/38</td>  </tr>        <tr>   <td>中国专利</td>   <td>         罗玲;                   饶洋辉       </td>   <td>中山大学</td>   <td>融合评分和标签信息的个性化推荐方法</td>   <td>广东省</td>   <td>CN109272390A</td>   <td>2019-01-25</td>   <td>本发明涉及计算机数据处理的技术领域,更具体地,涉及融合评分和标签信息的个性化推荐方法。融合评分和标签信息的个性化推荐方法,其中,包括三个部分,一个是基于评分信息和标签频率信息求解得到用户的标签偏好部分,一个是基于标签频率信息得到标签对商品的相关程度部分,还有一个是通过加入用户的标签偏好信息和标签对商品的相关程度信息来进行协同矩阵分解部分。</td>   <td>1.融合评分和标签信息的个性化推荐方法,其特征在于,包括三个部分,一个是基于评分信息和标签频率信息求解得到用户的标签偏好部分,一个是基于标签频率信息得到标签对商品的相关程度部分,还有一个是通过加入用户的标签偏好信息和标签对商品的相关程度信息来进行协同矩阵分解部分。</td>   <td>G06Q30/06;G06F16/9535</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陆遥;                   霍焯亮       </td>   <td>中山大学</td>   <td>一种基于主题类的跨语言生物医学类学术论文信息推荐方法</td>   <td>广东省</td>   <td>CN109255121A</td>   <td>2019-01-22</td>   <td>本发明涉及信息检索以及推荐系统技术领域,更具体地,涉及一种基于主题类的跨语言生物医学类学术论文信息推荐方法。本方法主要包括对文本数据进行数据预处理、应用PLAS模型进行文本聚类、计算每个主题分组的词向量信息、获得每一个主题最相关的跨语言主题编号、读取用户输入的检索词组、判断用户的检索词组、得到中文文章的推荐结果以及得到英文文献的推荐等步骤。本发明实现了将文本的分析从词频空间降维至空间主题空间；通过数据降维的方法能够有效地减少模型对翻译方法的依赖,有利于实现跨语言的文献特征分析；同时主题模型能够有效地挖掘文档中的语义信息,发现文档之间的潜在关联,能够有效地解决一词多义和一义多词的问题。</td>   <td>1.一种基于主题聚类的跨语言生物医学类学术论文信息推荐方法,其特征在于,包括以下步骤：S1：首先对文本数据进行数据预处理；S2：根据数据预处理得到的词频信息应用PLAS模型进行文本聚类并得到每个学术文献的主题分组；S3：计算每个主题分组的词向量信息并得到每个主题分组的向量信息；S4：利用翻译关系将每个主题分组的向量信息中的词组进行对应,通过加权计算的方法统一不同语言下的主题信息向量的维度,然后利用向量间的余弦距离计算主题之间的相似度,得到每一个主题最相关的跨语言主题编号；S5：读取用户输入的检索词组,利用结巴分词进行分词,同样去除停用词；S6：判断用户的检索词是否能够在系统的总词表中检索到,如果检索不到检索词,则输出无法得到推荐结果,推出系统,否则进入到下一步；S7：通过字符串匹配,计算检索词在中文主题下文本中出现的TF-IDF值,根据TF-IDF值排序得到与检索词最相关的中文主题,根据中文主题对应的p(z|d)矩阵,对主题内的中文文章进行排序,得到中文文章的推荐结果；S8：根据步骤S7中找到的中文主题,访问数据库得到对应最相关的英文主题,判断检索词是否能在翻译词表中找到对应的英文翻译,如果能找到英文翻译,则计算该英文翻译相对于该英文主题下所有英文文章的TF-IDF值,根据TF-IDF值对英文文章进行排序,得到英文文章的推荐结果；若无法找到对应的英文翻译,则直接根据该英文主题对应的p(z|d)矩阵,对主题内的英文文章进行排序,得到英文文章的推荐结果。</td>   <td>G06F17/27;G06F16/35</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              王嘉奇;              曾衍瀚;              陈熙衡;              方魏;                   谈磊       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院</td>   <td>基于追踪编码和遗传算法的电路结构优化方法</td>   <td>广东省</td>   <td>CN109255163A</td>   <td>2019-01-22</td>   <td>本发明公开了基于追踪编码和遗传算法的电路结构优化方法包括以下步骤：对电路中各器件及其参数进行追踪编码,从而获得初代种群；对初代种群中所有染色体依次进行选择、交叉和变异操作,从而获得下一代种群；对下一代种群进行解码仿真,从而获得每个染色体的适应度,比较全部染色体的适应度是否均满足设置要求的适应度函数,若是,则输出该种群以作为电路优化结构,否则返回至上一步骤。相比于传统技术,本发明步骤简洁,设计合理,方便了解到电路中各器件及其参数,能够对电路进行精确优化,有利于提高优化精确率。</td>   <td>1.基于追踪编码和遗传算法的电路结构优化方法,其特征在于,包括以下步骤：S1、对电路中各器件及其参数进行追踪编码,从而获得初代种群；S2、对初代种群中所有染色体依次进行选择、交叉和变异操作,从而获得下一代种群；S3、对下一代种群进行解码仿真,从而获得每个染色体的适应度；比较全部染色体的适应度是否均满足设置要求的适应度函数,若是,则输出该种群以作为电路优化结构,否则返回至步骤S2。</td>   <td>G06F17/50;G06N3/12</td>  </tr>        <tr>   <td>中国专利</td>   <td>         闫志强;              苏蕾;              夏北成;              吴迪;              李志今;              萧海敬;              黄秀玲;              张晓健;              周刚;                   王广义       </td>   <td>广州资源环保科技股份有限公司;中山大学</td>   <td>一种预测和模拟浅水湖泊生态系统总磷循环的方法</td>   <td>广东省</td>   <td>CN109255168A</td>   <td>2019-01-22</td>   <td>本发明公开了一种预测和模拟浅水湖泊生态系统总磷循环的方法,包括,A、构建水体中的磷循环模型；B、构建底泥中的磷循环模型；C、构建浅水湖泊生态系统总磷循环模型。本发明的方法可以对湖泊所存在的磷污染问题做出科学的评价,并预测不同管理方法的使用所达到的环境效益、风险以及不确定性,并利用环境条件对不同管理方法的使用做出情景模拟,从而做出预测,为环境管理者提供科学的理论参考和指导。</td>   <td>1.一种预测和模拟浅水湖泊生态系统总磷循环的方法,其特征在于包括,A、构建水体中的磷循环模型：根据水体中的磷循环的过程构建湖泊水体中的磷循环模型,过程如下：a、外界输入；b、底泥的扩散；c、碎屑的分解和来源；d、沉水植物的生长吸收；e、浮游植物的生长吸收；f、附生藻的生长吸收；g、沉降；h、输出；B、构建底泥中的磷循环模型：根据底泥中的磷循环的过程构建底泥中的磷循环模型,过程如下：i、水体中磷的沉降；j、底栖生物的死亡分解；k、扩散；l、沉水植物的生长吸收；C、构建浅水湖泊生态系统总磷循环模型：利用系统动力学模拟软件,将水体中的磷循环模型和底泥中的磷循环模型输入系统动力模拟软件,得到浅水湖泊生态系统总磷循环模型。</td>   <td>G06F17/50;G06Q10/06;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周永章;              高乐;              张彦龙;                   王若嘉       </td>   <td>中山大学</td>   <td>矿床三维地质储量估算方法</td>   <td>广东省</td>   <td>CN109255834A</td>   <td>2019-01-22</td>   <td>本发明公开了一种矿床三维地质储量估算方法,本发明在现代成矿预测理论研究的基础上,结合已有的地形地质图、勘探线剖面图、钻孔数据等资料,运用三维地质建模技术建立了径口矿段的地表模型、断裂模型、地层实体模型、矿体模型。在此基础上分别应用地质块段法和块体模型统计法进行了矿体的矿产资源储量估算,并将两种储量估算结果进行了分析。分析结果表明,块体模型统计法比地质块段法应用范围更广,效果更精准,可以清晰表达矿体品位值变化特征,进而加深对矿体、矿床的空间分布规律的认识。</td>   <td>1.矿床三维地质储量估算方法,其特征是包括以下步骤：在三维地质建模的基础上建立一个空白矿块模型,矿块模型由一系列尺寸相同的小长方体单元块组成,利用单元块近似地表达矿体,每一个单元块都具有相应的属性来表示矿体内部某一位置的品位值,单元块彼此之间属性的不同即代表了矿体内部品位变化规律,采用距离反比加权法进行块体插值,插值的距离根据搜索椭球体的半径来确定,通过搜索椭球体的半径确定参与插值运算的块体单元的个数,根据区内钻孔与勘探线间距等实际情况,插值一次后,查看块体模型中插值情况,再依次进行第2、3、4次搜索插值,插值半径依次,通过设置不同的颜色来代表矿石品位的不同范围,最终得到矿段内矿体块体模型。</td>   <td>G06T17/05</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陆遥;                   余丹填       </td>   <td>中山大学</td>   <td>一种基于引用关系的科技文献高关注度句子提取方法</td>   <td>广东省</td>   <td>CN109241521A</td>   <td>2019-01-18</td>   <td>本发明涉及句子级知识抽取的技术领域,更具体地,涉及一种基于引用关系的科技文献高关注度句子提取方法。本发明的主要步骤包括：对领域文档集预处理工作、统计高被引文章、提取高关注度句子以及CNN句子分类器训练。本发明提出一种更具客观性和适用性的科技论文高关注度句子提取方法,以所要研究的某一学科领域的大量科技文献为对象,基于引文分析研究方法,统计高被引文献,从中进行相应引证句子的相似度匹配,提取出相似度高的句子组成高关注度句子集；接着,对高关注度句子进行词性标注,使用标注序列集作为训练语料,通过CNN的训练,得到能自动识别论文中创新点句子的分类器。</td>   <td>1.一种基于引用关系的科技文献高关注度句子提取方法,其特征在于,包括以下步骤：S1：对领域文档集做相关预处理工作,所述预处理工作包括核对以及统一正文中的引用标注,保证领域文档集与参考文献一一匹配,同时便于后续试验的统一处理,所述预处理工作还包括对领域文档集中论文进行分句处理；S2：根据参考文献列表对文章被引次数进行统计,根据被引次数分布情况确定一个被引量阈值,取被引量大于阈值的高被引文章作为进一步抽取高关注度句子的文本对象；同时提取出高被引文章对应的引述句子,用于下一步高被引文章中高关注度句子的提取；S3：基于LSI潜在语义索引模型训练文本并计算句子的相似度,设定相似度阈值,从高被引文章中找出与对应引述句子有最高句子相似度且该相似度达到相似度阈值的句子,加入到高关注度句子训练集；S4：利用自然语言处理工具包NLTK中的词性标注器对高关注度句子和非高关注度句子进行词性标注,对高关注句子和非高关注句子分别加以标签1和0；输入词性符号序列和对应标签,进行CNN训练得到高关注度句子的分类器。</td>   <td>G06F17/27;G06F16/35</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;              李本超;                   唐永毅       </td>   <td>中山大学</td>   <td>一种基于隐变量的嵌入的群体行为识别方法</td>   <td>广东省</td>   <td>CN109241834A</td>   <td>2019-01-18</td>   <td>本发明公开了一种基于隐变量的嵌入的群体行为识别方法,包括下述步骤：S1、对于群体行为识别,利用数据集中提供的个体标注框进行个体图片分割；S2、构建隐变量嵌入模型,对具有节点和边的图模型利用隐嵌入的变量进行表征；S3、基于隐变量嵌入模型的群体行为识别,通过构造一组隐变量,使其能够对于人与群体交互行为进行表达的特征；S4、引入注意力机制对每个与当下群体行为有关的个体和场景信息进行特征嵌入,通过在编码段加入注意力机制,对编码段的源数据进行加权变换,从而对目标数据进行加权变换。本发明能够刻画更全局的群体行为特征,从而获得一个更加整体的群体行为描述进而完成识别任务。</td>   <td>1.一种基于隐变量的嵌入的群体行为识别方法,其特征在于,包括下述步骤：S1、对于群体行为识别,利用数据集中提供的个体标注框进行个体图片分割,通过对图片中的个体特征进行双流卷积神经网络的特征提取,得到群体行为场景中每个个体的特征表达,同样通过双流卷积神经网络,以视频帧图片进行输入,得到当前群体行为场景下的特征表达；S2、构建隐变量嵌入模型,对具有节点和边的图模型利用隐嵌入的变量进行表征,从而使得每个节点嵌入特征具备与该节点相关节点的信息,对表征人与群体之间的关系进行隐变量嵌入,将迭代的参数更新方式展开成递归神经网络,从而对视频中的每个个体进行关系建模,根据学习得到的隐变量进行群体行为识别；S3、基于隐变量嵌入模型的群体行为识别,通过构造一组隐变量,使其能够对于人与群体交互行为进行表达的特征,通过对个体外观和运动等信息进行编码,得到具有群体行为语义的中层隐变量表达,对于当前场景中的所有人,利用提出的隐变量嵌入模型分别对其进行人与群体交互关系的提取,并且对场景进行总体的群体行为隐变量表达,然后将隐变量通过特征嵌入的方法嵌入到语义特征空间,利用监督信号使得具有相近群体行为语义信息的隐变量在特征空间中具有较近的距离,从而便于后续根据隐变量对群体行为进行分类和识别；S4、引入注意力机制对每个与当下群体行为有关的个体和场景信息进行特征嵌入,所述注意力机制是一种被验证过的,能够有效地提升序列学习任务效果的一种方法,在编解码器框架内,通过在编码段加入注意力机制,对编码段的源数据进行加权变换,或者在解码段引入注意力机制,从而对目标数据进行加权变换,有效地提高模型对信息的获取能力和筛选能力。</td>   <td>G06K9/00;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         方锡鑫;              潘炎;              赖韩江;              印鉴;                   潘文杰       </td>   <td>中山大学;广州中大南沙科技创新产业园有限公司</td>   <td>一种基于深度强化学习的金融时序数据预测方法</td>   <td>广东省</td>   <td>CN109242207A</td>   <td>2019-01-18</td>   <td>本发明公开了一种基于深度强化学习的金融时序数据预测方法。本方法包括三个主要的子系统：数据处理子系统,此子系统的功能是对从WindAPI获取到的原始数据进行数据处理；特征提取子系统,此子系统的功能是构造一个深度神经网络来提取数据特征；强化学习子系统,此子系统的功能是基于Actor-Critic算法,构造策略网络和评估网络,分别进行交易动作的选取与评价,然后不断迭代更新来保证整个系统获取到市场最新的动态信息,并根据获取到的状态信息作出最优的交易动作,最终获取较好的交易效果。本发明能够通过金融市场的一些基本信息,不断去学习这个复杂的金融市场,及时捕获可能获利的交易动作,实现盈利目的。</td>   <td>1.一种基于深度强化学习的金融时序数据预测方法,其特征在于：包括以下步骤：S1：从外部系统的WindAPI下载原始的金融时序数据；S2：构建数据处理子系统,将下载后的原始的金融时序数据进行数据预处理,并输出预处理后的数据；S3：构建特征提取子系统,将预处理后的数据进行深度特征的提取,输出提取到的深度特征信息；S4：构建强化学习子系统,将深度特征信息与交易环境进行比对,进行强化学习,输出交易动作；S5：根据输入到系统中的数据产生的交易动作,实时调整金融市场的目标仓位,达到交易目的。</td>   <td>G06Q10/04;G06Q40/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建芳;              朱海昇;              谢佳锋;                   郑伟诗       </td>   <td>中山大学</td>   <td>基于层次信息传递的图片人体动作识别方法</td>   <td>广东省</td>   <td>CN109214346A</td>   <td>2019-01-15</td>   <td>本发明公开了一种基于层次信息传递的图片人体动作识别方法,于,包括下述步骤：S1、将人体分割为一个层次结构,该层次结构是自顶向下由粒度越来越细的局部身体区域构成,即将人体递归地分解为更小的身体部分；S2、构建层次传播网络,递归地对步骤S1中层次结构的信息进行传递与整合,从而得到最终的动作描述子；S3、将步骤S2得到的动作描述子与额外的全图信息结合,输入最后的全连接层进行分类,使用sigmoid函数来计算该置信度的概率分布,用二元交叉熵来计算分类损失。本发明定义了抽象的人体分割框架与分割规则,使得人体分割方案的选定更加灵活,降低单一性或者不合理性。</td>   <td>1.一种基于层次信息传递的图片人体动作识别方法,其特征在于,包括下述步骤：S1、将人体分割为一个层次结构,该层次结构是自顶向下由粒度越来越细的局部身体区域构成,即将人体递归地分解为更小的身体部分；围绕这些身体部分,从图像中截取对应区域,利用卷积神经网络提取图像特征；S2、构建层次传播网络,递归地对步骤S1中层次结构的特征信息进行传递与整合,从而得到最终的动作描述子；S3、将步骤S2得到的动作描述子与额外的全图信息结合,输入最后的全连接层进行分类。</td>   <td>G06K9/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;              宋日辉;              江志华;                   李洋灏       </td>   <td>中山大学</td>   <td>一种事件触发相机与三维激光雷达的配准方法</td>   <td>广东省</td>   <td>CN109215063A</td>   <td>2019-01-15</td>   <td>本发明涉及图像处理、点云处理以及传感器数据配准的技术领域,更具体地,涉及一种事件触发相机与三维激光雷达的配准方法。步骤1：设计一种适用于事件触发相机与三维激光雷达配准的标定物；步骤2：同时启动事件触发相机以及三维激光雷达,得到两种传感器在同一个时刻的数据；步骤3：使用边缘提取、特定图案识别等图象处理方法,在图象中定位到标定物的一个点；步骤4：使用基于RANSAC的点云分割等方法,在点云中也定位到标定物的同一个点；步骤5：通过步骤3和步骤4得到的结果推算出空间中六个自由度上的变换矩阵；步骤6：通过本发明提出的配准误差和基于边缘的价值函数对配准结果进行评价。</td>   <td>1.一种事件触发相机与三维激光雷达的配准方法,其特征在于,包括以下步骤：步骤1：设计一种适用于事件触发相机与三维激光雷达配准的标定物,并得到事件触发相机的内参矩阵；步骤2：同时启动事件触发相机以及三维激光雷达,得到两种传感器在同一个时刻的数据；步骤3：使用边缘提取、特定图案识别等图象处理方法,在图象中定位到标定物的一个点；步骤4：使用基于RANSAC的点云分割等方法,在点云中也定位到标定物的同一个点；步骤5：通过步骤3和步骤4得到的结果推算出空间中六个自由度上的变换矩阵；步骤6：通过配准误差和基于边缘的价值函数对配准结果进行评价。</td>   <td>G06T7/33;G06T7/80;G06T7/155</td>  </tr>        <tr>   <td>中国专利</td>   <td>         郑伟诗;              李翔;              吴岸聪;              曹玫;                   游瑾洁       </td>   <td>中山大学</td>   <td>基于多尺度联合学习的无交叠视域摄像头之间的低分辨率行人匹配方法</td>   <td>广东省</td>   <td>CN105404871B</td>   <td>2019-01-11</td>   <td>本发明公开了一种基于多尺度联合学习的无交叠视域摄像头之间的低分辨率行人匹配方法,包括以下步骤：(1)从原始的行人训练数据中分别生成多个不同尺度的行人训练数据；(2)提出不同尺度上同一行人差异最小化准则；(3)建立多尺度联合学习模型；(4)实现不同摄像头间低分辨率行人的匹配。本发明采用多个不同的图像尺度来有效地保持低分辨率行人的表观信息,进一步利用不同尺度上同一行人差异最小化准则来传递行人在不同分辨率下的判别信息,在此基础上建立多尺度联合学习模型,学习每个尺度的最优距离度量。本发明的方法相比于直接将行人图像缩放到单一尺度建模的现有行人匹配方法,能够获得更高的不同摄像头之间低分辨行人的匹配准确率。</td>   <td>1.一种基于多尺度联合学习的无交叠视域摄像头之间的低分辨率行人匹配方法,其特征在于包括以下步骤：(1)生成多个不同图像尺度的行人训练数据；(2)提出不同尺度上同一行人差异最小化准则；(3)建立多尺度联合学习模型；(4)实现不同摄像头间低分辨率行人的匹配；所述步骤(1)中,不同尺度的行人训练数据是通过将原始分辨率各异的行人训练图像同时缩放到多个不同的尺度得到的；步骤(1)具体为：行人训练图像被缩放到两个尺度,即一个标准尺度和一个其他的小尺度,对于一个行人训练集,通过缩放图像可以分别得到一个标准尺度的行人训练集和一个小尺度的行人训练集其中和分别表示同一张行人图像调整到标准尺度和小尺度后提取的特征向量,其所属的行人/类别记为y-i,N是训练集的行人图像样本总数,和分别表示行人图像调整到标准尺度和小尺度提取的特征向量空间,d-h和d-s分别表示行人图像调整到标准尺度和小尺度后提取的特征向量维度。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘强;                   黄翊豪       </td>   <td>中山大学</td>   <td>六旋翼无人机机身铺层设计方法</td>   <td>广东省</td>   <td>CN109190262A</td>   <td>2019-01-11</td>   <td>本发明涉及复合材料无人机结构设计技术领域,特别是一种六旋翼无人机机身铺层设计方法；首先将无人机机身分为旋翼臂、上翼缘、机身侧壁以及机身腹板四大区域；上述区域分别进行铺层优化设计,由以下三阶段铺层优化方法获得,首先进行自由尺寸优化获得各区域上每一层铺层的形状以及每层的厚度,并按照划分的区域形状进行调整,再对其进行尺寸优化得到采用实际材料厚度的各铺层的角度、层数和顺序,最后进行铺层顺序优化通过调整铺层顺序来进一步提高机身整体刚度,最终获得各区域的铺层方案；本发明将无人机机身分成若干区域,各区域再进行铺层优化设计,可以提高设计效率,获得最轻量化、最合理的铺层方案。</td>   <td>1.一种六旋翼无人机机身铺层设计方法,其特征在于：包括以下步骤：步骤1、机身分区,将无人机机身分为旋翼臂、上翼缘、机身侧壁以及机身腹板四大区域；步骤2、建立机身有限元分析模型；步骤3、自由尺寸优化,初步得到旋翼臂、上翼缘、机身侧壁以及机身腹板的铺层形状和厚度；步骤4、对步骤3中得到的各个区域的每一层铺层的厚度和形状进行调整,删除不符合实际制造工艺要求厚度的铺层,并跟据已划分的机身四大区域的形状对其余铺层形状进行调整；步骤5、对经步骤4调整后的铺层进行尺寸优化,各区域中每一层的铺层厚度优化成实际采用的复合材料厚度,并确定各角度铺层所占百分比,以获得各区域各均匀层的角度、整体层数和顺序；步骤6、对各区域中铺层顺序进行优化。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李熙莹;              李国鸣;              江倩殷;                   李鑫       </td>   <td>中山大学</td>   <td>结合图像显著性检测和神经网络的车辆重识别方法与系统</td>   <td>广东省</td>   <td>CN109190513A</td>   <td>2019-01-11</td>   <td>本发明公开了一种结合图像显著性检测和神经网络的车辆重识别方法与系统,方法包括：采用SIM算法对车辆的原始图像进行显著性检测,得到车辆的显著外观特征图像；将车辆的显著外观特征图像和原始图像一起输入神经网络进行训练；根据神经网络的训练结果,提取车辆特征并对车辆进行重识别；系统包括显著性检测模块、训练模块和重识别模块；系统还包括存储器和处理器。本发明能够根据车辆的显著外观特征图像来对车辆进行重识别,增强了提取特征的鲁棒性；另外,本发明使神经网络能针对性地对车辆图像的显著特征区域进行特征学习,效率较高,可广泛应用于图像识别技术领域。</td>   <td>1.结合图像显著性检测和神经网络的车辆重识别方法,其特征在于：包括以下步骤：采用SIM算法对车辆的原始图像进行显著性检测,得到车辆的显著外观特征图像；将车辆的显著外观特征图像和原始图像一起输入神经网络进行训练；根据神经网络的训练结果,提取车辆特征并对车辆进行重识别。</td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              邱志林;              张雨浓;              张冬雨;                   王青       </td>   <td>中山大学</td>   <td>一种区域间出行需求预测方法及装置</td>   <td>广东省</td>   <td>CN109190795A</td>   <td>2019-01-11</td>   <td>本发明公开了一种区域间出行需求预测方法及装置,所述方法包括如下步骤：步骤S1,构建多重上下文信息抽取的深度模型,利用历史多个时间段的交通出行需求矩阵序列作为输入和对应序列下一个时间段的实际交通出行需求矩阵作为目标输出,并利用神经网络的反向传播算法来训练深度模型；步骤S2,构建上下文信息丰富的交通出行需求矩阵序列；步骤S3,将步骤S1经训练得到的深度模型参数和深度模型一起作为最终的预测器,输入连续的交通出行需求矩阵序列,预测未知的下一个时间段的交通出行需求矩阵,本发明可提高区域间出行需求预测的准确性。</td>   <td>1.一种区域间出行需求预测方法,包括如下步骤：步骤S1,构建多重上下文信息抽取的深度模型,利用历史多个时间段的交通出行需求矩阵序列作为输入和对应序列下一个时间段的实际交通出行需求矩阵作为目标输出,并利用神经网络的反向传播算法来训练深度模型；步骤S2,构建上下文信息丰富的交通出行需求矩阵序列；步骤S3,将步骤S1经训练得到的深度模型参数和深度模型一起作为最终的预测器,输入连续的交通出行需求矩阵序列,预测未知的下一个时间段的交通出行需求矩阵。</td>   <td>G06Q10/04;G06Q10/06;G06Q50/30</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈晓宏;              郑炎辉;              何艳虎;              张建云;                   王高旭       </td>   <td>中山大学;水利部交通运输部国家能源局南京水利科学研究院</td>   <td>考虑供需不确定的水资源优化配置报童模型</td>   <td>广东省</td>   <td>CN109190902A</td>   <td>2019-01-11</td>   <td>本发明涉及一种考虑供需不确定的水资源优化配置报童模型。包括：S1.利用实测水文资料分析寻求径流变化规律,通过统计分析得到径流的概率分布；S2.按照经典报童理论,根据农业、工业、生活用水特点,把这三个类型需水量均假设为均匀分布,以前三年的数据作为历史数据分析进行需水滚动预报；S3.利用来水、需水的概率分布函数,引入经济学报童模型,构建考虑供需水不确定性的水资源优化配置报童模型,求解得到初步配置方案；判断初步配置方案总配置水量是否超过最大可供水量,若没有,则初步配置方案就是最优方案,若超过,采用两阶段启发式算法重新进行求解,得到最优方案。本发明提供的方法能够获得更接近来需水实际情况的水资源配置方案。</td>   <td>1.一种考虑供需不确定的水资源优化配置报童模型,其特征在于,包括以下步骤：S1.来水不确定性分析：利用实测水文资料分析寻求径流变化规律,通过统计分析得到径流的概率分布函数；S2.需水不确定性分析：按照经典报童理论,根据农业、工业、生活用水特点,把农业、工业、生活三个类型需水量均假设为均匀分布,以预测年份前三年的数据作为历史数据分析进行需水滚动预报；S3.利用S1、S2步骤分析得到的来水、需水的概率分布函数,引入经济学报童模型,构建考虑供水需水不确定性的水资源优化配置报童模型,求解得到初步配置方案；判断初步配置方案总配置水量是否超过最大可供水量,若无超过则初步配置方案就是最优方案,若超过则采用两阶段启发式算法重新进行求解,得到最优方案。</td>   <td>G06Q10/06;G06Q10/04;G06Q50/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         康乐;                   潘嵘       </td>   <td>中山大学</td>   <td>一种基于无监督特征点检测的商品对齐方法</td>   <td>广东省</td>   <td>CN109191255A</td>   <td>2019-01-11</td>   <td>本发明涉及人工智能的技术领域,更具体地,涉及一种基于无监督特征点检测的商品对齐方法。一种基于无监督特征点检测的商品对齐方法,其中,包括以下步骤：S1.特征点检测训练数据准备；S2.检测框模型训练；S3.特征点检测；S4.根据特征点坐标进行仿射变换对齐。本发明经过特征点检测对齐之后用在商品后续的识别网络上,相较于没做对齐直接识别准确率会明显更高,因为网络对于正向的物体比倾斜的物体更容易识别；对于现有的有监督特征点对齐,这个方法能节省标注成本。</td>   <td>1.一种基于无监督特征点检测的商品对齐方法,其特征在于,包括以下步骤：S1.特征点检测训练数据准备；S2.检测框模型训练；S3.特征点检测；S4.根据特征点坐标进行仿射变换对齐。</td>   <td>G06Q30/06;G06F16/953</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈进财;              刘永红;                   罗银萍       </td>   <td>中山大学</td>   <td>面向交通环境污染模拟的城市三维建筑物快速建模方法</td>   <td>广东省</td>   <td>CN109191561A</td>   <td>2019-01-11</td>   <td>面向交通环境污染模拟的城市三维建筑物快速建模方法,其特征在于,具体步骤如下：步骤1通过数字化城市软件导出模拟区域的建筑物数据；步骤2通过程序语言软件对建筑物数据进行编程读取,并对建筑物外形数据特征进行优化,使其在保证建筑物主要形状特征的前提下,利于获取质量更好的数值计算网格；步骤3根据建筑物外形数据以及计算流动力学模拟的边界条件,设计数值模拟的计算区域；步骤4通过建模软件脚本语言,生成对应的建筑物、计算域的建模脚本；优点是,解决了真实城市场景大气污染扩散模拟过程中快速生成建筑几何模型的难题,提高了城市三维建筑建模的效率和准确度,简化城市大气污染扩散模拟的前处理过程。</td>   <td>1.面向交通环境污染模拟的城市三维建筑物快速建模方法,其特征在于,具体步骤如下：步骤1 通过数字化城市软件导出模拟区域的建筑物数据；步骤2 通过程序语言软件对建筑物数据进行编程读取,并对建筑物外形数据特征进行优化,使其在保证建筑物主要形状特征的前提下,利于获取质量更好的数值计算网格；步骤3 根据建筑物外形数据以及计算流动力学模拟的边界条件,设计数值模拟的计算区域；步骤4 通过建模软件脚本语言,生成对应的建筑物、计算域的建模脚本；步骤5 在建模软件中运行建模脚本,生成建筑几何模型。</td>   <td>G06T17/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周永章;              张彦龙;              沈文杰;                   张焱       </td>   <td>中山大学</td>   <td>地球化学异常信息提取方法</td>   <td>广东省</td>   <td>CN109166050A</td>   <td>2019-01-08</td>   <td>本发明公开了一种地球化学异常信息提取方法,本发明是根据双对数图中各拟合直线段的交点可以确定区域异常和局部异常的异常下限值,从而为划分地球化学背景、区域异常与局部异常提供依据。基于研究区复杂的多重地球化学背景,多重分形滤波技术能够克服高背景,从中提取出弱异常,这些异常不仅与多数已发现矿床吻合,在未知区得到的弱异常也有进一步开展工作的意义,这为该区矿床勘查提供了新的靶区,为未知矿床的探寻提供了新的启示。</td>   <td>1.地球化学异常信息提取方法,其特征在于包括以下步骤：步骤一：设定A(&gt;S)∝S<Sup>-β</Sup>其中S为能谱密度,A为能谱密度大于阀值S时的面积,β为分形模型指数系数；步骤二：将S-A数值绘制在双对数坐标图上,不同的幂律关系通过不同斜率的直线关系表达出来。由此可根据不同线段的拟合来确定具有不同S-A关系的能谱密度范围；由两条直线段拟合的S-A关系的交点确定阀值S<Sub>0</Sub>,阀值S<Sub>0</Sub>可将能谱密度的分布范围分成两个区域,即两个滤波器：          <Image id="icf0001" he="144" wi="475" file="FDA0001796691090000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0002" he="143" wi="467" file="FDA0001796691090000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image></td>   <td>G06Q50/02</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李熙莹;              吕硕;              江倩殷;              罗东华;              袁敏贤;                   余志       </td>   <td>中山大学;广东方纬科技有限公司</td>   <td>一种基于选择性搜索算法的车标检测识别方法及系统</td>   <td>广东省</td>   <td>CN106529424B</td>   <td>2019-01-04</td>   <td>本发明公开了一种基于选择性搜索算法的车标检测识别方法及系统,方法包括：对原始车辆图像进行车牌定位,获取车牌位置；根据车牌位置、车牌与车标空间位置关系和车窗边缘信息在原始车辆图像中对车标进行粗定位,得到车标粗定位图像；基于车辆中轴线在车标粗定位图像中选取车标候选区；采用选择性搜索算法对车标候选区进行目标定位,得到定位目标集；采用线性约束编码算法训练车标判断分类器来对定位目标集进行车标的判别,得到车标的位置；采用线性约束编码算法训练多类车标识别分类器来对车标进行具体的类型识别,得到车标识别结果。本发明具有适用性广、鲁棒性强和检测速度快的优点,可广泛应用于图像处理领域。</td>   <td>1.一种基于选择性搜索算法的车标检测识别方法,其特征在于：包括以下步骤：对原始车辆图像进行车牌定位,获取车牌位置；根据车牌位置、车牌与车标空间位置关系和车窗边缘信息在原始车辆图像中对车标进行粗定位,得到车标粗定位图像；基于车辆中轴线在车标粗定位图像中选取车标候选区；采用选择性搜索算法对车标候选区进行目标定位,得到定位目标集,所述选择性搜索算法综合根据颜色相似度、纹理相似度、大小相似度和吻合度相似度来进行区域合并；采用线性约束编码算法训练车标判断分类器来对定位目标集进行车标的判别,得到车标的位置；采用线性约束编码算法训练多类车标识别分类器来对车标进行具体的类型识别,得到车标识别结果；所述采用线性约束编码算法训练车标判断分类器来对定位目标集进行车标的判别,得到车标的位置这一步骤,其包括：将样本集的样本划分为正样本和负样本,其中,正样本包括单个字符样本、样本集中的小型车样本和样本集中的大型车标样本,负样本为样本集中大小随机选取的且与车标重合度小于20％的样本；以正样本作为训练样本,采用线性约束编码分类器进行迭代训练直至收敛,最终训练出车标判断分类器,其中,迭代训练过程在每次完成训练后会将车标判断分类器中错分为负样本的样本加入训练样本中形成新训练样本集,然后再以新训练样本集重新进行训练；根据训练出的车标判断分类器对定位目标集进行车标的判别,得到车标的位置。</td>   <td>G06K9/00;G06K9/32;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         叶宏彪;                   潘嵘       </td>   <td>中山大学</td>   <td>一种基于字的中文观点要素情感分析方法</td>   <td>广东省</td>   <td>CN109145304A</td>   <td>2019-01-04</td>   <td>本发明涉及自然语言处理的技术领域,更具体地,涉及一种基于字的中文观点要素情感分析方法。一种基于字的中文观点要素情感分析方法,其中,包括以下步骤：S1.选取数据集；S2.数据清洗和提取；S3.字嵌入；S4.训练初始化的模型；S5.测试。本发明的优点在于,相比较于传统的中文分词并获得词向量,把句子划分到字级别改用字嵌入方式获得字向量,避免了因中文分词的不准确带来的歧义困扰。</td>   <td>1.一种基于字的中文观点要素情感分析方法,其特征在于,包括以下步骤：S1.选取数据集；S2.数据清洗和提取；S3.字嵌入；S4.训练初始化的模型；S5.测试。</td>   <td>G06F17/27</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈晓宏;              汪丽娜;                   张建云       </td>   <td>中山大学;水利部交通运输部国家能源局南京水利科学研究院</td>   <td>洪水全要素异变的诊断方法</td>   <td>广东省</td>   <td>CN109145967A</td>   <td>2019-01-04</td>   <td>本发明涉及水文水资源领域,更具体地,涉及一种洪水全要素异变的诊断方法。包括以下步骤：S1.获取洪水全要素的指标数据；S2.对各个指标进行归一化处理；S3.从分类的角度对洪水全要素的异变诊断进行界定,利用AFS-FCM模型确定分类数；S4.确定各个要素的权重值大小；S5.依据归一化的指标、分类数、各要素的权重值大小,依据云模型的计算方式,计算得到洪水全要素异变的诊断结果；S6.根据分类结果解读洪水全要素的异变性。本发明利用AFS-FCM模型中的平均目标函数值,为分类数的划分寻找到了科学的依据,能够用于比较不同分类数分类方案的优劣,在确定权重时,依据量化结果为各洪水要素提供权重值,为洪水全要素的异变性分析提供了有益的支撑。</td>   <td>1.一种洪水全要素异变的诊断方法,其特征在于,包括以下步骤：S1.获取洪水全要素的指标数据；S2.对各个指标进行归一化处理；S3.从分类的角度对洪水全要素的异变诊断进行界定,利用AFS-FCM模型确定分类数；S4.确定各个要素的权重值大小；S5.依据归一化的指标、分类数、各要素的权重值大小,依据云模型的计算方式,计算得到洪水全要素异变的诊断结果；S6.根据分类结果解读洪水全要素的异变性。</td>   <td>G06K9/62;G06Q50/26</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周永章;              高乐;              王俊;              沈文杰;                   唐沐阳       </td>   <td>中山大学</td>   <td>证据权模型预测矿产资源方法</td>   <td>广东省</td>   <td>CN109146200A</td>   <td>2019-01-04</td>   <td>本发明公开了一种证据权模型预测矿产资源方法,本发明根据证据图层对预测对象的显著性大小顺序逐步加入到模型中,所得到的效果是显而易见的,其预测结果对进一步开展预测区优选评价具有非常重要的参考意义。最后根据改进证据权法得到的后验概率图圈定成矿有利地段,本次预测结果对下一步开展找矿工作部署具有重要的指导作用。本发明可以降低由于证据图层不满足条件独立性假设对预测结果产生的影响。</td>   <td>1.证据权模型预测矿产资源方法,其特征在于包括以下步骤：步骤一：用D表示S内事件D已出现的训练层。假设A<Sub>1</Sub>,A<Sub>2</Sub>,…,A<Sub>n</Sub>为与D相关的证据层,那么D在证据层A<Sub>1</Sub>,A<Sub>2</Sub>,…,A<Sub>n</Sub>条件下发生的后验概率模型为          <Image id="icf0001" he="156" wi="700" file="FDA0001796692920000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        步骤二：将上式取对数得logit模型：Logit(D|A<Sub>1</Sub>A<Sub>2</Sub>…A<Sub>n</Sub>)＝log(O(D|A<Sub>1</Sub>A<Sub>2</Sub>…A<Sub>n</Sub>))＝W<Sub>0</Sub>+W<Sub>1</Sub>+…+W<Sub>n</Sub>     (2)式(2)中,          <Image id="icf0002" he="52" wi="700" file="FDA0001796692920000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        S等同于A<Sub>0</Sub>,从式(3)可以得知W<Sub>i</Sub>与A<Sub>1</Sub>,A<Sub>2</Sub>,…,A<Sub>i-1</Sub>,A<Sub>n</Sub>,D均有关；步骤三：在遵循条件独立性的假设下,由上式变为          <Image id="icf0003" he="121" wi="700" file="FDA0001796692920000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image></td>   <td>G06Q10/04;G06Q50/02</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陶良乐;              陈湘萍;                   周凡       </td>   <td>中山大学</td>   <td>一种基于内容的多版本App更新评价方法及系统</td>   <td>广东省</td>   <td>CN109146625A</td>   <td>2019-01-04</td>   <td>本发明实施例公开了一种基于内容的多版本App更新评价方法及系统,其中,该方法包括：通过自动遍历App,获取信息并存储到数据库；获取数据库中的信息进行分析、识别处理,获得不同版本App的差异；对App的评论信息及评论时间等相关信息进行预处理,获得修改后与每个版本的App相对应的用户评论信息；获取所述修改后与每个版本的App相对应的用户评论信息,结合不同版本App的差异进行比较评分处理,获得每个App更新的综合情感分析数值。实施本发明实施例,能够为开发者提供更加全面的反馈,提高工作效率；还能为开发者提供特定功能生命周期的有关信息。</td>   <td>1.一种基于内容的多版本App更新评价方法,其特征在于,所述方法包括：通过自动遍历App,获取App信息并存储到数据库；获取数据库中的信息进行分析、识别处理,获得不同版本App的差异；获取对App应用商店的评论信息及评论时间等相关信息进行预处理,获得修改后与每个版本的App相对应的用户评论信息；获取所述修改后与每个版本的App相对应的用户评论信息,结合不同版本App的差异进行比较评分处理,获得每个App更新的综合情感分析数值。</td>   <td>G06Q30/06;G06Q10/06;G06F8/65;G06F17/27</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王建峰;              王若梅;              苏卓;              周凡;                   林淑金       </td>   <td>中山大学</td>   <td>一种基于用户动态兴趣分析的时尚服装搭配推荐方法</td>   <td>广东省</td>   <td>CN109146626A</td>   <td>2019-01-04</td>   <td>本发明公开了一种基于用户动态兴趣分析的时尚服装搭配推荐方法。本发明首先通过为服装商品的特征属性建立树结构,之后根据用户设定的时间因子,把“用户—商品”评分矩阵分解为“用户—隐含特征”矩阵和“物品—隐含特征”矩阵并提取对应的关键词,然后分别根据各自出现的高频词和低频词构建特征向量,算出概率进行排序得到排序模型,根据排序模型形成物品排序列表推荐给用户。本发明可以根据用户的长期兴趣爱好和短期兴趣漂移,准确的预测出用户在一定时间范围内对服装搭配的兴趣,可以根据用户的购买记录以及对物品的评分,精确的为用户推荐喜欢的服装以及与之搭配的配件或饰品。</td>   <td>1.一种基于用户动态兴趣分析的时尚服装搭配推荐方法,其特征在于,所述方法包括：从网络以及服装商品数据库中获取服装商品的信息(包括用户评分与商品图片),以及对服装图片的分类的标记信息,组成照片库；从照片库中选取一万条数据信息作为一组样本,在样本数据集合中,分为训练集与测试集；提取服装商品的特征属性信息,依据这些信息对服装商品进行分类表示,并确定服装商品的层数；依据服装商品属性的分类与层数建立树结构,以服装配饰为总的大类即树的根节点,服装商品属性的每一层通过各自附属的属性进行相连；利用所建立的服装商品属性树,采用隐语义模型,根据用户设定的时间因子,把“用户-商品”评分矩阵分解为“用户-隐含特征”矩阵和“物品-隐含特征”矩阵；根据前述两个矩阵,分别提取“用户-特征”关键词,以及提取“物品-特征”关键词,再分别根据各自出现的高频词和低频词构建特征向量；通过“用户-物品”评分矩阵作为训练集进行学习,根据构建出的特征向量进行划分,算出概率进行排序得到排序模型,再利用测试集根据排序模型形成物品排序列表推荐给用户。</td>   <td>G06Q30/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         黄凯;              宋日辉;              李洋灏;                   江志华       </td>   <td>中山大学</td>   <td>一种基于事件触发相机与三维激光雷达融合系统下的物体识别与配准方法</td>   <td>广东省</td>   <td>CN109146929A</td>   <td>2019-01-04</td>   <td>本发明涉及深度学习、图象处理以及三维点云处理的技术领域,更具体地,涉及一种基于事件触发相机与三维激光雷达融合系统下的物体识别与配准方法。本发明提出的基于事件触发相机数据和激光雷达数据融合的系统。通过YOLO3深度学习神经网络对图像进行通用物体检测。对被检测出物体的图像使用极小值滤波器融合激光雷达的深度信息,实现实时准确地检测图像中的物体及其深度信息的目的。</td>   <td>1.一种基于事件触发相机与三维激光雷达融合系统下的物体识别与配准方法,其特征在于,包括以下步骤：步骤1、搭建和配准事件触发相机与三维激光雷达构成的融合系统；得到事件触发相机的内参矩阵以及事件触发相机与三维激光雷达之间的外参矩阵,利用这两个矩阵配准上述两个传感器得到的数据,使这两种数据融合；步骤2、针对事件触发相机得到的数据,对YOLO3深度学习神经网络进行微调；因为原YOLO3是对普通RGB相机得到的图片进行物体分类识别,而事件触发相机得到的图片为黑白二值图；黑白二值图相对于彩色照片来说更容易被处理,因此能够相应地对YOLO3深度学习网络进行微调,减少系统的运行负担；另外,用事件触发相机得到的数据对微调过后的YOLO3深度学习神经网络进行训练；步骤3、实现极小值滤波器；在使用微调过的YOLO3对事件触发相机得到的图片进行物体分类检测之后,每一个被识别到的物体都会有一个对应的数据结构,包括被检测到物体的可能种类的置信度以及物体在图像中的位置和大小；根据步骤1,图像中的某些像素点会被赋予深度信息；通过使用极小值滤波器,对被识别到的物体范围内的带有深度信息的像素点进行筛选,以其中深度的最小值作为该物体的深度；步骤4、启动事件触发相机与三维激光传感器的融合系统,并运行算法,能够实时分类识别物体及对物体进行定位。</td>   <td>G06T7/30;G06T7/80</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林浩添;              黄凯;              王东妮;              汪瑞昕;                   康德开       </td>   <td>中山大学中山眼科中心;中山大学</td>   <td>一种基于细粒度识别的混杂细胞种类鉴定方法</td>   <td>广东省</td>   <td>CN109117703A</td>   <td>2019-01-01</td>   <td>本发明具体涉及一种基于细粒度识别的混杂细胞种类鉴定方法,包括如下步骤：预先建立细粒度识别卷积神经网络模型和细胞图像数据库,细胞图像数据库中包括有混种细胞图像,混种细胞图像为包括多种类型细胞的图像；S1、收集混种细胞图像；S2、将混种细胞图像输入细粒度识别卷积神经网络模型中,得出细胞种类热图；S3、对混种细胞图像进行阈值化,得到细胞区域二值图像；S4、结合细胞区域二值图像和细胞种类热图,得到细胞种类鉴定结果。本发明根据细胞形态特征的特异性进行细胞种类的精确识别,避免了传统细胞种类鉴定方法耗时较长、过程繁琐的缺点。模型能够学习到细粒度细胞形态特征,通过纹理等信息识别细胞种类,具有较高的识别准确率及鲁棒性。</td>   <td>1.一种基于细粒度识别的混杂细胞种类鉴定方法,其特征在于,包括如下步骤：预先建立细粒度识别卷积神经网络模型和细胞图像数据库,细胞图像数据库中包括有混种细胞图像,混种细胞图像为包括多种类型细胞的图像；S1、收集混种细胞图像；S2、将混种细胞图像输入细粒度识别卷积神经网络模型中,得出细胞种类热图；S3、对混种细胞图像进行阈值化,得到细胞区域二值图像；S4、结合细胞区域二值图像和细胞种类热图,得到细胞种类鉴定结果。</td>   <td>G06K9/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈晓宏;              苏程佳;              张建云;                   叶海霞       </td>   <td>中山大学;水利部交通运输部国家能源局南京水利科学研究院</td>   <td>一种水文序列变异尺度的识别方法</td>   <td>广东省</td>   <td>CN109117878A</td>   <td>2019-01-01</td>   <td>本发明涉及水文水资源领域,具体地,涉及一种水文序列变异尺度的识别方法。包括以下步骤：S1.收集水文序列数据；S2.利用有序聚类法对水文序列进行突变点检测；S3.根据突变点将水文序列分成前后两个子序列,其中突变点前的子序列称为参考序列,突变点后的序列称为被测序列；S4.采用相关积分法并结合两个子序列即可计算水文序列的变异度。本发明提供的一种水文序列变异尺度的识别方法,方法原理简单,计算复杂度较低、效率高,可对水文序列的变异尺度进行识别,可广泛应用于各种水文要素序列变异程度的研究。</td>   <td>1.一种水文序列变异尺度的识别方法,其特征在于,包括以下步骤：S1.收集水文序列数据；S2.利用有序聚类法对水文序列进行突变点检测；S3.根据突变点将水文序列分成前后两个子序列,其中突变点前的子序列称为参考序列,突变点后的序列称为被测序列；S4.采用相关积分法并结合两个子序列即可计算水文序列的变异度。</td>   <td>G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈晓宏;              刘德地;              张建云;              叶海霞;                   王高旭       </td>   <td>中山大学;水利部交通运输部国家能源局南京水利科学研究院</td>   <td>一种多智能体配置方法</td>   <td>广东省</td>   <td>CN109117998A</td>   <td>2019-01-01</td>   <td>本发明涉及一种多智能体配置方法。包括：S1.根据人类用水的特点,确立水资源配置系统的层次结构；S2.建立基于多智能体系统的水资源优化配置模型框架：确立水资源配置系统中智能体的组成与系统结构,在智能体组成与层次结构的基础上构建水资源优化配置模型框架结构,其次确定智能体之间的协作关系、协商机制；S3.分析水资源优化配置模型中各智能体Agent的模型行为,确定各智能体的约束和边界条件；S4.利用优化算法对模型进行求解。本发明将智能体系统的理论与方法应用于水资源配置领域,确立了一套比较完整的多智能体模型,有助于分析水资源配置系统中各层次的相互关系,描述各主体行为与整体演化的有机联系,模拟水资源分配过程,进行水资源的优化配置。</td>   <td>1.一种多智能体配置方法,其特征在于,包括以下步骤：S1.根据研究区人类用水的特点,确立水资源配置系统的层次结构；S2.建立基于多智能体系统MAS的水资源优化配置模型框架：首先确立水资源配置系统中智能体的组成与系统结构,并在智能体组成与层次结构的基础上构建水资源优化配置模型框架结构,其次确定智能体之间的协作关系、协商机制；S3.分析水资源优化配置模型中各智能体Agent的模型行为,确定各智能体的约束和边界条件；S4.利用优化算法对模型进行求解。</td>   <td>G06Q10/04;G06Q10/06;G06Q50/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         周永章;              沈文杰;                   张彦龙       </td>   <td>中山大学</td>   <td>蔬菜土壤中Pb的健康安全预测预警方法</td>   <td>广东省</td>   <td>CN109118021A</td>   <td>2019-01-01</td>   <td>本发明公开了一种蔬菜土壤中Pb的健康安全预测预警方法,本发明从健康安全的角度对土壤重金属Pb的累积趋势进行预测预警,该分析方法和思路在现有技术领域中并未运用于土壤金属中进行预测预警中,从而填补了国内对土壤重金属预测预警的空白。</td>   <td>1.蔬菜土壤中Pb的健康安全预测预警方法,其特征在于：所述的Pb的健康安全预测预警方法包括以下步骤：步骤一：土壤样品采集分析,样品在室温下风干,除去石子或其它碎片,然后过2mm的聚乙烯筛,混匀玛瑙研钵中研磨过筛；步骤二：采用电感耦合等离子体质谱仪测定所有样品,土壤Pb全量和形态分析过程中分别插入8％的GSS-1、GSS-2、GSS-3、GSS-8和10％的GSF-2、GSF-3、GSF-4、GSF-5国家一级标准物质及5％的密码重复样监控分析质量；监控样分析数据显示样品分析质量是否符合《生态地球化学评价样品分析技术要求(试行)》的规定要求；步骤三：建立土地安全质量预警预测的数学基础①单位重量土壤重金属污染元素现累积量(Q)计算公式Q＝a-b                                    (1)式(1)中,a为浅层土壤某元素含量值,以Hazen概率曲线得到污染叠加含量值表示；b为元素土壤背景值,以Hazen概率曲线得到背景值表示；②累积加速率及现速率；设Hazen概率曲线得到的背景值为1973年土壤含量值,叠加含量值为2007年土壤含量值,1973～2007年的土壤重金属是加速累积,其间土壤累积增加量<Image id="icf0001" he="111" wi="379" file="FDA0001793027820000011.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>设V<Sub>1973</Sub>为零,则<Image id="icf0002" he="103" wi="200" file="FDA0001793027820000012.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>加速率A及2007年的累积速率如下：          <Image id="icf0003" he="42" wi="700" file="FDA0001793027820000013.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>                  <Image id="icf0004" he="47" wi="700" file="FDA0001793027820000014.GIF" imgContent="drawing" imgFormat="GIF" orientation="portrait" inline="no"></Image>        式(2)、(3)中,A为累积加速率；V<Sub>0</Sub>为某种污染元素的累积现速率；T＝35；步骤四：通过建立缓变型地球化学灾害数学模型,用多项式表示如下,Q＝a<Sub>0</Sub>+a<Sub>1</Sub>C+a<Sub>2</Sub>C<Sup>2</Sup>+a<Sub>3</Sub>C<Sup>3</Sup>+…    (4)在一个演化周期内,该多项式的最高次数为3,式中一阶、二阶导数为零处分别代表缓变型地球化学灾害爆发的临界点、爆发点。</td>   <td>G06Q10/04;G06Q10/06;G06Q50/02;G06F17/18;G01N27/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         闫志强;              苏蕾;              夏北成;              李志今;              石敏球;              吴迪;              林荣斌;              李永怿;              周刚;              张晓健;                   王广义       </td>   <td>广州资源环保科技股份有限公司;中山大学</td>   <td>一种用于模拟浅水湖泊生态系统模型的方法</td>   <td>广东</td>   <td>CN109101707A</td>   <td>2018-12-28</td>   <td>本发明公开了一种用于模拟浅水湖泊生态系统模型的方法,利用生态模拟软件构建模型,包括,A、构建初级生产者生态过程方程；B、构建消费者生态过程方程；C、构建初级生产者和消费者生态子模型；D、构建生态系统模型。本发明的模拟浅水湖泊生态系统模型的方法,利用生态模拟软件构建了浅水湖泊生态系统模型,更精确的考虑了生物的生态过程,提高了模型模拟的准确度,为浅水湖泊生态系统研究与管理提供了理论参考与技术支持。</td>   <td>1.一种用于模拟浅水湖泊生态系统模型的方法,其特征在于,利用生态模拟软件构建模型,包括,A、构建初级生产者生态过程方程：根据初级生产者的光合作用生长过程、呼吸损失过程以及死亡过程构建初级生产者生态过程方程；B、构建消费者生态过程方程：根据消费者的捕食生长过程、基础代谢损失以及死亡损失构建消费者生态过程方程；C、构建初级生产者和消费者生态子模型：根据初级生产者生态过程方程,构建作为初级生产者的沉水植物生态动力学模型、浮游植物生态动力学模型和附生藻类生态动力学模型；根据消费者生态过程方程,构建作为消费者的浮游动物生态动力学模型、底栖动物生态动力学模型、滤食性鱼类生态动力学模型、草食性鱼类生态动力学模型和肉食性鱼类生态动力学模型；D、构建生态系统模型：根据限制函数,将沉水植物生态动力学模型、浮游植物生态动力学模型和附生藻类生态动力学模型与浮游动物生态动力学模型、底栖动物生态动力学模型、滤食性鱼类生态动力学模型、草食性鱼类生态动力学模型和肉食性鱼类生态动力学模型相应耦合,得到生态系统模型。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;                   杨焕       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>基于目标检测的多任务及临近信息融合的深度学习算法</td>   <td>广东</td>   <td>CN109101932A</td>   <td>2018-12-28</td>   <td>本发明公开了基于目标检测的多任务及临近信息融合的深度学习算法,包括输入图片,利用卷积神经网络提取图像特征,并生成目标候选框；利用图像特征,将图片经过区域候选网络,提取出目标预测框；将目标预测框进行特征提取和特征池化,再进行边框回归、方向预测、目标检测分类,得到初步检测结果；将初步检测结果与目标候选框融合并进入ROI池化层和通过第二全连接层,得到最终检测结果；其中,目标检测分类是利用一个目标预测框与其临近的其他目标预测框的信息关系而重新定义该目标预测框的置信度分数；算法采用多任务输出模式。本发明在提高目标检测的速度的同时,确保了目标检测的准确性,达到实时目标检测的要求。</td>   <td>1.基于目标检测的多任务及临近信息融合的深度学习算法,其特征在于,包括以下步骤：输入经初始化带有真实框的图片,利用预训练好的卷积神经网络提取图像特征,并生成目标候选框；利用所述的图像特征,将图片经过区域候选网络,提取出目标预测框；将目标预测框经过卷积层进行特征提取和经过池化层进行特征池化,再经过第一全连接层进行初步边框回归、目标预测框与真实框之间的方向预测、初步目标检测分类,得到包含经筛选后的目标预测框的初步检测结果；将所述初步检测结果和目标候选框融合并进入ROI池化层,并通过第二全连接层来进行最终边框回归和最终目标检测分类,得到包含已分类图片的最终检测结果；其中,所述初步目标检测分类和最终目标检测分类是利用一个目标预测框与其临近的其他目标预测框的信息关系而重新定义该目标预测框的置信度分数。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林浩添;                   龙尔平       </td>   <td>中山大学中山眼科中心</td>   <td>一种婴幼儿视功能智能评估方法及装置</td>   <td>广东</td>   <td>CN109101991A</td>   <td>2018-12-28</td>   <td>本发明提供一种婴幼儿视功能智能评估方法,包括：构建基于时空双轨框架神经网络的视功能智能评估模型,并采用Temporal#Segment#Network技术训练该视功能智能评估模型；接收监护人持有的移动设备拍摄的待评测视频数据,并通过视功能智能评估模型从待评测视频数据中提取与视功能评估相关的至少一个特征值；对每个特征值进行迭代变换处理,得到与每个特征值对应的视功能损伤概率值；以视功能智能评估模型和视功能损伤概率值为依据,对待评测视频数据进行视功能评估,得到视功能评估结果,并发送视功能评估结果至移动设备。可见,实施这种实施方式,能够根据待检测对象的相关视频数据,快速准确地对该待检测对象的视功能进行评估,检测精度高,有利于减少误诊率。</td>   <td>1.一种婴幼儿视功能智能评估方法,其特征在于,包括：构建基于时空双轨框架神经网络的视功能智能评估模型,并采用Temporal#Segment#Network技术训练所述视功能智能评估模型；接收来自监护人持有的移动设备拍摄的待评测视频数据,并通过所述视功能智能评估模型从所述待评测视频数据中提取与视功能评估相关的至少一个特征值；对每个所述特征值进行迭代变换处理,得到与每个所述特征值对应的视功能损伤概率值；以所述视功能智能评估模型和所述视功能损伤概率值为依据,对所述待评测视频数据进行视功能评估,得到视功能评估结果,并发送所述视功能评估结果至所述移动设备。</td>   <td>G06K9/62;G16H50/30</td>  </tr>        <tr>   <td>中国专利</td>   <td>         聂琳;              吴文熙;              陈添水;                   王青       </td>   <td>中山大学</td>   <td>一种用于物体精细识别的层次语义嵌入模型及其实现方法</td>   <td>广东</td>   <td>CN109102024A</td>   <td>2018-12-28</td>   <td>本发明公开了一种用于物体精细识别的层次语义嵌入模型及其实现方法,所述层次语义嵌入模型包括：主干网络,用于对输入图像的浅层特征进行提取,以特征图的形式输出至各分支网络；若干分支网络,用于对主干网络输出的图像浅层特征图进行进一步的深层特征提取,使其输出的特征图适用于分支网络所对应层级的识别任务,并通过引入语义知识嵌入机制,实现上层语义知识对下层分支网络特征学习的指导,本发明解决依赖额外信息引导学习的物体精细化识别技术方案中的额外信息标注成本高的问题。</td>   <td>1.一种用于物体精细识别的层次语义嵌入模型,包括：主干网络,用于对输入图像的浅层特征进行提取,以特征图的形式输出至各分支网络；若干分支网络,用于对主干网络输出的图像浅层特征图进行进一步的深层特征提取,使其输出的特征图适用于分支网络所对应层级的识别任务,并通过引入语义知识嵌入机制,实现上层语义知识对下层分支网络特征学习的指导。</td>   <td>G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林浩添;                   龙尔平       </td>   <td>中山大学中山眼科中心</td>   <td>一种后发性白内障图像分析方法及装置</td>   <td>广东</td>   <td>CN109102494A</td>   <td>2018-12-28</td>   <td>本发明提供一种后发性后发性白内障图像分析方法及装置,包括：接收后发性白内障眼部图片；定位并处理后发性后发性白内障裂隙灯后照法图像的后囊增殖区域；以人工智能模型包括的检测区域为依据,在后囊增殖区域中提取后发性白内障区域；其中,人工智能模型为预先训练好的；以人工智能模型包括的特征库为依据,提取后发性白内障区域包括的至少一个后发性白内障特征；对至少一个后发性白内障特征进行处理得到分类概率结果；以人工智能模型包括的分析结果比对库为依据,匹配得到分类概率结果对应的分析结果。可见,上述方法能够简单、快速、准确地确定出患者的后发性白内障情况,并给予用户相应的分析结果,从而提高了用户就医的效率。</td>   <td>1.一种后发性白内障图像分析方法,其特征在于,所述方法包括：获取患有后发性白内障的眼部图片；在所述眼部图片中定位晶状体区域,并对所述晶状体区域进行去噪处理；以预先训练好的人工智能模型中包括的检测区域为依据,在所述晶状体区域中提取后发性白内障区域；以所述人工智能模型包括的特征库为依据,提取所述后发性白内障区域包括的至少一个后发性白内障特征；对所述至少一个后发性白内障特征进行处理得到分类概率结果；以所述人工智能模型包括的分析结果比对库为依据,匹配得到所述分类概率结果对应的分析结果。</td>   <td>G06T7/00;G06K9/62;H04L29/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈晓宏;              陈莎;              苏程佳;              杨志峰;                   苏美蓉       </td>   <td>中山大学;东莞理工学院</td>   <td>一种咸潮中长期预报方法</td>   <td>广东</td>   <td>CN109086933A</td>   <td>2018-12-25</td>   <td>本发明涉及水文水利工程领域,更具体地,涉及一种咸潮中长期预报的方法。包括以下步骤：S1.收集河口区同期的实测潮位数据、含氯度数据以及上游的径流量数据；S2.对收集的原始数据进行归一化处理,并将数据分为训练期和验证期；S3.以训练期的潮位数据和径流量数据作为输入,以同期的含氯度作为模型输出,构建咸潮中长期预报模型；S4.验证期的潮位和径流量输入到构建好的模型中,模型输出采用反归一化处理得到预测含氯度,并选用评价指标评价模型的可靠性。本发明提供的一种咸潮中长期预报的方法,计算复杂度较低、计算效率高、预测准确度高,操作简单,可实现对咸潮中长期进行准确预报。</td>   <td>1.一种咸潮中长期预报的方法,其特征在于,包括以下步骤：S1.收集河口区同期的实测潮位数据、含氯度数据以及上游的径流量数据；S2.对收集的原始数据进行归一化处理,并将数据分为训练期和验证期；S3.以训练期的潮位数据和径流量数据作为输入,以同期的含氯度作为模型输出,构建咸潮中长期预报模型；S4.将验证期的潮位和径流量输入到构建好的模型中,模型输出采用反归一化处理得到预测含氯度,选用评价指标评价模型的可靠性。</td>   <td>G06Q10/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         程梦成;              苏卓;                   郑贵锋       </td>   <td>中山大学</td>   <td>一种基于深度卷积生成对抗网络的视频超分辨率生成方法</td>   <td>广东</td>   <td>CN109087243A</td>   <td>2018-12-25</td>   <td>本发明公开了一种基于深度卷积生成对抗网络的视频超分辨率生成方法。其中,该方法包括：获取目标视频,将同一场景的视频片段进行转换成连续的图像序列信息作为原图组以PNG格式保存；将所述原图组进行采样缩放,获取缩小的低分辨率图像,进行运动补偿处理,获得补偿后的低分辨率图像；构建深度卷积生成对抗网络模型,将所述补偿后的低分辨率图像输入进行计算,输出重建后的高分辨率图像后再进行放大处理,获得超分辨率图像；将所述超分辨率图像,按照所述目标视频进行重新组合,获得超分辨率目标视频。实施本发明实施例,能够有效地提高低分辨率视频的重建质量,快速生成纹理细节更加丰富、效果更加自然真实的图像。</td>   <td>1.一种基于深度卷积生成对抗网络的视频超分辨率生成方法,其特征在于,所述方法包括：获取目标视频,将同一场景的视频片段进行转换成连续的图像序列信息作为原图组以PNG格式保存；将所述原图组进行采样缩放,获取缩小的低分辨率图像,进行运动补偿处理,获得补偿后的低分辨率图像；构建深度卷积生成对抗网络模型,将所述补偿后的低分辨率图像输入进行计算,输出重建后的高分辨率图像后再进行放大处理,获得超分辨率图像；将所述超分辨率图像,按照所述目标视频进行重新组合,获得超分辨率目标视频。</td>   <td>G06T3/40</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              李冠彬;              张雨浓;              何翔;                   王青       </td>   <td>中山大学</td>   <td>一种基于深度学习的图像去雨方法及装置</td>   <td>广东</td>   <td>CN109087258A</td>   <td>2018-12-25</td>   <td>本发明公开了一种基于深度学习的图像去雨方法及装置,所述方法包括：步骤S1,利用浅层卷积神经网络产生所有有雨图像的浅层特征图；步骤S2,将获得的浅层特征图输入至一个逐级下采样的多层编码器以进行编码；步骤S3,通过一个与上游的编码器结构对称、逐级上采样的多层解码器对编码后的特征图进行解码操作；步骤S4,对解码后的特征图进行细化处理,并预测有雨图像的雨条负残差信息；步骤S5,对所述有雨图像与雨条负残差信息求和,最终获得高质量无雨图像,本发明在有效地去除雨条信息的同时,能良好地保留场景细节信息。</td>   <td>1.一种基于深度学习的图像去雨方法,包括如下步骤：步骤S1,利用浅层卷积神经网络产生所有有雨图像的浅层特征图；步骤S2,将获得的浅层特征图输入至一个逐级下采样的多层编码器以进行编码；步骤S3,通过一个与上游的编码器结构对称、逐级上采样的多层解码器对编码后的特征图进行解码操作；步骤S4,对解码后的特征图进行细化处理,并预测有雨图像的雨条负残差信息；步骤S5,对所述有雨图像与雨条负残差信息求和,最终获得高质量无雨图像。</td>   <td>G06T5/00;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谢佳锋;              胡建芳;              钟逸;              朱海昇;                   郑伟诗       </td>   <td>中山大学</td>   <td>基于迁移学习提升语义分割模型效果的框架</td>   <td>广东</td>   <td>CN109087303A</td>   <td>2018-12-25</td>   <td>本发明公开了一种基于迁移学习提升语义分割模型效果的框架,包括下述内容：1)将迁移学习引入到语义分割领域,使得快速语义分割网络可以通过教师模型提升学生模型分割效果；2)提出一致性映射度量教师和学生模型的轮廓和纹路信息,并通过构造一致性损失函数来使得快速语义分割在细节处分割得更好；3)利用老师模型和条件随机场(CRF)模型为无标签数据生成辅助标签,并把数据加入到训练集,提升模型的泛化能力和分割效果。本发明在不引入额外模型参数,降低模型速度的情况下,提升了快速语义分割模型的准确率。</td>   <td>1.基于迁移学习提升语义分割模型效果的框架,其特征在于,包括下述步骤：构建一个新的语义分割模型基础框架,所述语义分割模型基础框架由两个不同的网络组成,分别为老师网络和学生网络,所述老师网络为学生网路提供有益于分割的知识指导,使得学生网路能学到老师网络的知识来帮助其拥有更好的分割效果,所述学生网络用于在保证其分割的速度的同时从老师网络提供的知识中学习到有益于其分割效果的知识；通过目标函数将老师网络和学生网络连接起来,所述目标函数是由基于逻辑分布变换出来的信息形式构造的,该目标函数的具体内容如下：用S和T来分别表示公式中的学生网络和老师网络：L＝Ls+r(S,T)上述公式中,Ls是交叉熵损失函数,其实由图片的标签与学生网络的概率分布之间求交叉熵得到的损失函数；r(S,T)代表的是老师网络与学生网络之间的知识偏差,其作为一个正则化项来正则化学生网络的学习过程,通过r(S,T)这一项,学生网络和老师网络被连接起来,并且通过最小化L目标函数可以把老师网络的知识传递到学生网络；把r(S,T)函数定义为：r(S,T)＝αLp(S,T)+βLc(S,T)Lp(S,T)是老师网络与学生网络之间的概率分布损失函数,定义为<img file="FDA0001766122630000011.TIF" wi="700" he="74"/>函数中的I表示batch#size的数量,G表示图片的像素集合,PS(x),PT(x)分别是学生和老师网络在图片区域每个像素点的概率分布输出,这个损失函数的定义是学生网络的输出概率分布跟老师网络的概率分布是相似的,这个函数可以捕抓到不同分割输出的零阶知识；为了补充LP损失函数捕捉到的零阶知识,LC函数被用于捕获学生网络和老师网络输出的一阶知识,定义LC函数为：<img file="FDA0001766122630000021.TIF" wi="700" he="75"/>其中函数中的I表示batch#size的数量,G表示图片的像素集合,一致性矩阵C(x)定义为<img file="FDA0001766122630000022.TIF" wi="613" he="94"/>B(x)意味着像素x的8个临近的像素,I(x)是对应网络像素点的逻辑分布输出；利用网络结构中的老师网络对无标签数据进行标签预测生成伪标签,并把生成标签数据加入模型的训练集中,再通过框架训练提升学生网络的分割效果。</td>   <td>G06T7/10</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林倞;              杨猛;              王可泽;                   王青       </td>   <td>中山大学</td>   <td>基于深度网络的人体三维关节点估计框架及其定位方法</td>   <td>广东</td>   <td>CN109087329A</td>   <td>2018-12-25</td>   <td>本发明公开了一种基于深度网络的人体三维关节点估计框架及其定位方法,该框架包括：二维姿势子网络,用于在二维姿势数据集上进行预训练,以提取二维姿势特征传入二维#三维转换模块,并生成精确的二维预测姿势；二维#三维转换模块,用于接收所述二维姿势子网络提取的二维姿势特征转换到三维姿势特征空间中,并生成时序一致的三维姿势粗估计；三维#二维投影模块,用于将所述二维#三维转换模块估计的中间级三维姿势粗估计投影回二维空间,生成二维投影姿势,并通过优化二维投影姿势和二维预测姿势之间的一致性,修正估计的三维姿势,最终输出具有时空一致性、二维三维几何一致性的精确三维姿势估计,本发明可提高人体三维关节点预测定位的精度。</td>   <td>1.一种基于深度网络的人体三维关节点估计框架,包括：二维姿势子网络,用于在二维姿势数据集上进行预训练,以提取二维姿势特征传入二维#三维转换模块,并生成精确的二维预测姿势；二维#三维转换模块,用于接收所述二维姿势子网络提取的二维姿势特征转换到三维姿势特征空间中,并生成时序一致的三维姿势粗估计；三维#二维投影模块,用于将所述二维#三维转换模块估计的中间级三维姿势粗估计投影回二维空间,生成二维投影姿势,并通过优化二维投影姿势和二维预测姿势之间的一致性,修正估计的三维姿势,最终输出具有时空一致性、二维三维几何一致性的精确三维姿势估计。</td>   <td>G06T7/207</td>  </tr>        <tr>   <td>中国专利</td>   <td>         蔡铭;                   姚逸璠       </td>   <td>中山大学</td>   <td>一种城市区域交通噪声分布并行计算方法</td>   <td>广东</td>   <td>CN109063288A</td>   <td>2018-12-21</td>   <td>本发明提供一种城市区域交通噪声分布并行计算方法,该方法在不失精确度的前提下大幅提高了交通噪声分布计算的速度。在进行网格化划分时,考虑将网格周围的噪声源以及噪声遮挡物纳入缓冲区进行噪声计算,并通过动态扩展缓冲区大小保证每个网格中的噪声源数目足够多且数目大致相同。在进行并行化计算时,采取一个控制节点控制若干计算节点的计算方案,并在计算过程中对每个网格进行修正以保证整个区域的噪声分布连续性。</td>   <td>1.一种城市区域交通噪声分布并行计算方法,其特征在于,包括以下步骤：S1：对城市区域进行网格化划分,得到一系列正方形的网格分块,其中,网格化划分以等间距进行划分,所用坐标系为高斯平面坐标；S2：对每个网格分块进行缓冲区生成；S3：进行城市交通噪声分布的并行计算。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         汪飞;              林淑金;              罗笑南;                   周凡       </td>   <td>中山大学</td>   <td>基于保密度和无散度的不可压缩流体的模拟方法及系统</td>   <td>广东</td>   <td>CN109063375A</td>   <td>2018-12-21</td>   <td>本发明公开了一种基于保密度和无散度的不可压缩流体的模拟方法,包括：S1,初始化粒子信息；S2,计算粒子的中间速度和中间位置；S3,计算粒子的密度；S4,计算粒子的约束因子；S5,计算粒子的位移；S6,更新粒子的位置；S7,重复步骤S4～S6,直到迭代次数达到最大值并且约束因子大于阈值。本发明还公开了一种基于保密度和无散度的不可压缩流体的模拟系统。采用本发明,不但提高了模拟的时间效率,而且流体模拟的分布比先前的方法更加的均匀和稳定。</td>   <td>1.一种基于保密度和无散度的不可压缩流体的模拟方法,其特征在于,包括：S1,初始化粒子信息；S2,计算粒子的中间速度和中间位置；S3,计算粒子的密度；S4,计算粒子的约束因子；S5,计算粒子的位移；S6,更新粒子的位置；S7,重复步骤S4～S6,直到迭代的约束因子大于阈值。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李熙莹;              邱铭凯;                   江倩殷       </td>   <td>中山大学</td>   <td>一种考虑局部形变的视频车辆重识别方法、系统及装置</td>   <td>广东</td>   <td>CN109063543A</td>   <td>2018-12-21</td>   <td>本发明公开了一种考虑局部形变的视频车辆重识别方法、系统及装置,方法包括：构建车辆的三维边界框；根据构建的三维边界框,对车辆的图像进行多面分割；根据分割结果,计算候选车辆与目标车辆之间对应面的局部相似度；根据对应面的局部相似度,生成候选车辆与目标车辆之间的整体相似度；根据候选车辆与目标车辆之间的整体相似度,生成车辆识别结果；系统包括构建模块、分割模块、计算模块、生成模块和识别模块；装置包括存储器和处理器。本发明在车辆识别的过程中考虑了车辆的局部形变因素,通过局部的相似度来生成整体的相似度,车辆识别的精度高,可广泛应用于车辆识别技术领域。</td>   <td>1.一种考虑局部形变的视频车辆重识别方法,其特征在于：包括以下步骤：构建车辆的三维边界框；根据构建的三维边界框,对车辆的图像进行多面分割；根据分割结果,计算候选车辆与目标车辆之间对应面的局部相似度；根据对应面的局部相似度,生成候选车辆与目标车辆之间的整体相似度；根据候选车辆与目标车辆之间的整体相似度,生成车辆识别结果。</td>   <td>G06K9/00;G06K9/46;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         林浩添;              黄凯;              王东妮;              汪瑞昕;                   康德开       </td>   <td>中山大学中山眼科中心;中山大学</td>   <td>一种基于深度学习的细胞种类鉴定方法</td>   <td>广东</td>   <td>CN109063547A</td>   <td>2018-12-21</td>   <td>本发明具体涉及一种基于深度学习的细胞种类鉴定方法,该方法基于深度学习来预设神经网络模型,通过滑动窗口得到多个局部图像输入神经网络模型后,再将得出来的结果进行整合,而不是直接将细胞图像输入到神经网络模型中,提高了得到细胞种类热图的精度。上述过程均为计算机的图像处理过程,相比于现有常用的细胞鉴定方法,不需要人工操作,也不需要对细胞进行侵入性的检测,具有快速、方便、非侵入、全局检测等优点。</td>   <td>1.一种基于深度学习的细胞种类鉴定方法,其特征在于,包括如下步骤：预先建立神经网络模型；采集细胞图像；得出细胞种类热图：S11、对细胞图像进行初步处理；S12、采用滑动窗口的方式使细胞图像分为多个局部图像；S13、多个局部图像输入神经网络模型中,得到局部图像的细胞种类标签；S14、将所有细胞种类标签整合为细胞种类热图；得出二值化图像及细胞密度热图：S21、对经步骤S11处理后的细胞图像进行阈值化处理,得到二值化图像；S22、采用滑动窗口的方式使二值化图像分为多个局部二值化图像,计算每个局部二值化图像中细胞区域所占总区域的比例,整合计算得到的所有比例,得到细胞密度热图；得出细胞种类鉴定结果：S31、整合细胞种类热图和二值化图像,得到二值化细胞种类热图；S32、结合二值化细胞种类热图和细胞密度热图,得到细胞图像的细胞种类鉴定结果。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         杨子琳;              苏卓;              周凡;                   郑贵锋       </td>   <td>中山大学</td>   <td>一种基于特征个性化修改的服装推荐优化方法及其系统</td>   <td>广东</td>   <td>CN109064249A</td>   <td>2018-12-21</td>   <td>本发明公开了一种基于特征个性化修改的服装推荐优化方法及其系统。其中,该方法包括：用户根据简单的词语进行检索,获得推荐类似服装图片；根据用户想要修改装饰、花纹、风格等美学特征选择目标图片,自定义修改获得自定义目标图片；提取推荐类似服装图片的属性特征；提取自定义目标图片的美学特征；结合前述两者进行拼接获得完整的服装特征；对完整的服装特征进行计算余弦相似度处理,获得与拼接后完整的服装特征的服装图片最相似的服装,作为最新的推荐反馈给用户。实施本发明实施例,能够弥补了现有属性反馈检索技术中需要准确描述出需要修改属性的名称关键词,以及将从图像中提取出的视觉特征向量与文字关键词映射到同一空间内的不足,以实现对服装美学特征的修改和再推荐。</td>   <td>1.一种基于特征个性化修改的服装推荐优化方法,其特征在于,所述方法包括：用户根据简单的词语进行检索处理,获得推荐类似服装图片；根据用户想要修改装饰、花纹、风格等美学特征另外选择目标图片,进行自定义修改处理,获得自定义目标图片；获取推荐类似服装图片,进行提取,获得推荐类似服装图片的属性特征；获取自定义目标图片,进行提取处理,获得自定义目标图片的美学特征；结合所述的推荐类服装图片的属性特征及所述的自定义目标图片的美学特征,进行拼接处理,获得完整的服装特征；获取完整的服装特征,进行计算余弦相似度处理,获得与拼接后完整的服装特征的服装图片最相似的服装,作为最新的推荐反馈给用户。</td>   <td>G06Q30/06;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>              赖韩江       </td>   <td>腾讯科技(深圳)有限公司;中山大学</td>   <td>图像检索方法和装置、计算机设备和存储介质</td>   <td>广东</td>   <td>CN109033107A</td>   <td>2018-12-18</td>   <td>本发明提供一种图像检索方法和装置、计算机设备和存储介质,包括：获取检索图像；将检索图像输入预先对具有相似关系的训练图像样本训练得到的卷积神经网络,通过输入层和前置卷积层得到第一输出结果；第一输出结果输入到的分支网络,得到每个像素点为物体的概率对应的物体的位置概率图；第一输出结果输入到卷积神经网络的中间卷积层,得到检索图像的特征图；对特征图与位置概率图进行点乘处理得到融合位置信息的特征图,并将融合位置信息的特征图输入至卷积神经网络的后置卷积层,通过后置卷积层和全连接层,输出检索图像的特征向量；将检索图像的特征向量与数据库的各图像的特征向量进行比较,得到检索图像的检索结果。该方法精确度高。</td>   <td>1.一种图像检索方法,其特征在于,包括：获取检索图像；将所述检索图像输入预先对具有相似关系的训练图像样本训练得到的卷积神经网络,通过所述卷积神经网络的输入层和前置卷积层得到第一输出结果；所述第一输出结果输入到所述卷积神经网络的分支网络,得到每个像素点为物体的概率对应的物体的位置概率图；所述第一输出结果输入到所述卷积神经网络的中间卷积层,得到所述检索图像的特征图；对所述特征图与所述位置概率图进行点乘处理得到融合位置信息的特征图,并将所述融合位置信息的特征图输入至所述卷积神经网络的后置卷积层,通过后置卷积层和全连接层,输出所述检索图像的特征向量；将所述检索图像的特征向量与数据库的各图像的特征向量进行比较,得到所述检索图像的检索结果。</td>   <td>G06F17/30;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         马争鸣;              王鑫;              车航健;              陈映宏;                   黎伟浚       </td>   <td>中山大学</td>   <td>基于最短路径覆盖的测地距离保持算法</td>   <td>广东</td>   <td>CN109033349A</td>   <td>2018-12-18</td>   <td>本发明涉及机器学习领域中的数据降维问题,提出了一种基于最短路径覆盖的测地距离保持算法。首先从由输入样本构造的邻域图中选取一系列与真实测地线尽量接近的最短路径来覆盖所有的数据点。用MDS保持选取的测地线的起点与终点之间的测地距离将所有的起点和终点映射到低维空间。由每一对起点和终点的低维坐标可以确定低维空间中的一条直线。本发明的主要思想是将高维空间中处于一条测地线上的点映射到低维空间中的一条直线上,故高维空间中数据点的低维坐标可以根据它们与起点之间的测地距离来预测,最后通过最小化数据点的低维坐标与其预测值之间的误差来得到所有数据点的低维坐标。</td>   <td>1.一种基于最短路径覆盖的测地距离保持算法,其主要特征在于：A.构造输入数据集X的邻域图G。从数据集X中随机选取一个数据点xp,计算出以该点为起点的所有最短路径以及每条最短路径偏离真实测地线的程度；B.从以xp为起点的所有最短路径中选出一条包含数据点尽量多而且尽量接近真实测地线的最短路径；C.从数据集中剔除C中得到的最短路径所包含的点并重复B到C直到数据集为空,得到一系列能覆盖所有数据点的最短路径集合；D.将所有最短路径的起点和终点收集起来,记为<img file="FSA0000167759260000011.TIF" wi="546" he="93"/>对X<sub>F</sub>应用MDS保持所有起点和终点之间的测地距离得到<img file="FSA0000167759260000012.TIF" wi="536" he="92"/>E.每条最短路径的起点和终点在低维空间中的坐标可以确定低维空间中的一条直线,算法的基本思想是将高维空间中处于一条测地线上的点映射为低维空间中一条直线上的点,因此处于最短路径上的其它点的低维坐标可以由该条最短路径的起点和终点的低维坐标来预测；F.最小化所有数据点的实际低维坐标和它们由上述起点和终点预测得到的低维坐标之间的误差来获得所有数据点的低维坐标。</td>   <td>G06F17/30</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              梁耀淦;              谢舜道;              陈荣军;              朱雄泳;              曾衍瀚;                   路崇       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>基于遗传算法的三维集成电路互连线长度优化方法及装置</td>   <td>广东</td>   <td>CN109033510A</td>   <td>2018-12-18</td>   <td>本发明公开了基于遗传算法的三维集成电路互连线长度优化方法及装置,方法包括以下步骤：初始化遗传算法参数,以偶数个满足固定边框约束的三维集成电路布局为输入；对输入的三维集成电路布局进行两两配对,按交叉概率对每一对执行交叉操作；按变异概率对所有三维集成电路布局执行变异操作；根据互连线长度为所有三维集成电路布局分配保存概率,按保存概率保留三维集成电路布局并以之作为新解；判断是否达到终止操作次数,若达到,则以新解作为优化完成的三维集成电路布局输出,否则返回至第一步,并以新解替代输入。因此,本发明可以更大概率地获取满足互连线长度要求的三维集成电路,并且优化精度较高,得到结果较为稳定。</td>   <td>1.基于遗传算法的三维集成电路互连线长度优化方法,其特征在于,包括以下步骤：S1、初始化遗传算法参数,以偶数个满足固定边框约束的三维集成电路布局为输入；所述遗传算法参数包括交叉概率、变异概率和终止操作次数；S2、对输入的三维集成电路布局进行两两配对,按交叉概率对每一对执行交叉操作；S3、按变异概率对所有三维集成电路布局执行变异操作；S4、根据三维集成电路布局的互连线长度为所有三维集成电路布局分配保存概率,按保存概率保留三维集成电路布局并以之作为新解；其中,所述保存概率与互连线长度呈反比关系；S5、判断对所有三维集成电路布局执行交叉操作、变异操作的次数是否达到终止操作次数,若达到,则以新解作为优化完成的三维集成电路布局输出,否则返回执行步骤S1,并以新解替代输入。</td>   <td>G06F17/50;G06N3/12</td>  </tr>        <tr>   <td>中国专利</td>   <td>         王凯;                   邹泽为       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种快速的指纹连续分布方向图平滑方法及装置</td>   <td>广东</td>   <td>CN109033982A</td>   <td>2018-12-18</td>   <td>本发明公开了一种快速的指纹连续分布方向图平滑方法及装置,涉及指纹识别领域,在对指纹连续分布方向图上每一个方向点多次平滑算法的基础上进行改进,在除第一次的后续平滑过程中只对位于方向交界处的点进行平滑处理,能在不损失平滑效果的基础上有效地减少连续分布方向图多次平滑所需的时间,提高连续分布方向图的计算速度。</td>   <td>1.一种快速的指纹连续分布方向图平滑方法,其特征在于,包括以下步骤：获取未经平滑的指纹点方向图；遍历指纹点方向图上的每一个方向点,对每一个方向点进行平滑操作后得到新的方向点,由所有新的方向点构成指纹连续分布方向图；按预设次数对指纹连续分布方向图进行如下操作：遍历指纹连续分布方向图上的每一个方向点,对自身的方向与周围的点的方向不完全相同的方向点进行平滑操作后得到新的方向点,由所有新的方向点构成指纹连续分布方向图；输出指纹连续分布方向图。</td>   <td>G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         陈晓宏;              李深林;              何艳虎;                   王浩       </td>   <td>中山大学;中国水利水电科学研究院</td>   <td>基于反馈机制的水资源优化配置方法</td>   <td>广东</td>   <td>CN109034485A</td>   <td>2018-12-18</td>   <td>本发明涉及水资源管理领域,更具体地,涉及一种基于反馈机制的水资源优化配置方法。包括以下步骤：S1.构建水资源配置基础模型,运用常规水资源配置方法得到水资源初始配置方案；S2.将初始配置方案应用于实际水文情境中,由于需水不可准确预测,初始配置水量与实际需水量存在偏差,得到配置偏差序列；S3.构建反馈增益函数,利用偏差序列得到增益量；S4.利用增益量对未来时段的初始配置方案进行调整,得到反馈配置方案。本方法可以节约高效利用水资源,有效减少水资源管理的工程量,对于水资源管理具有十分重要的意义。</td>   <td>1.一种基于反馈机制的水资源优化配置方法,其特征在于,包括以下步骤：S1.构建水资源配置基础模型,运用常规水资源配置方法得到水资源初始配置方案；S2.将初始配置方案应用于实际水文情境中,由于需水不可准确预测,初始配置水量与实际需水量存在偏差,得到配置偏差序列；S3.构建反馈增益函数,利用偏差序列得到增益量；S4.利用增益量对未来时段的初始配置方案进行调整,得到反馈配置方案。</td>   <td>G06Q10/04;G06Q10/06;G06Q50/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         瞿文政;              许志明;              王嘉茵;              肖泽彬;              廖嘉凯;              邱泽敏;                   万智萍       </td>   <td>中山大学新华学院</td>   <td>一种基于深度卷积神经网络的行人重识别监控系统</td>   <td>广东</td>   <td>CN109002761A</td>   <td>2018-12-14</td>   <td>本发明公开了一种基于深度卷积神经网络的行人重识别监控系统,所述系统包括：视频采集装置,识别监控装置和行人数据库；所述视频采集装置用于采集被测行人图像；所述行人数据库用于存储目标行人图像；所述识别监控装置用于从所述视频采集装置获取被测行人图像；从所述行人数据库中获取所述目标行人图像,利用所述目标行人图像应用深度卷积神经网络学习得出目标行人特征；并根据所述目标行人特征匹配判断所述被测行人图像是否为目标行人。通过本技术方案,能够有效减少多种对识别稳健性的不利影响因素,从而提高对行人目标进行相似度匹配正确率。</td>   <td>1.一种基于深度卷积神经网络的行人重识别监控系统,其特征在于,所述系统包括：视频采集装置,识别监控装置和行人数据库；所述视频采集装置用于采集被测行人图像；所述行人数据库用于存储目标行人图像；所述识别监控装置用于从所述视频采集装置获取被测行人图像；从所述行人数据库中获取所述目标行人图像,利用所述目标行人图像应用深度卷积神经网络学习得出目标行人特征；并根据所述目标行人特征匹配判断所述被测行人图像是否为目标行人。</td>   <td>G06K9/00;G06K9/62;G06N3/04;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李禹源;              张东;              吴增程;                   李骁       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种面向资源受限系统的紧凑卷积神经网络</td>   <td>广东</td>   <td>CN109002863A</td>   <td>2018-12-14</td>   <td>本发明公开了一种面向资源受限系统的紧凑卷积神经网络,包括依次连接设置的原始输入层、特征卷积结构、若干个具有多层感知的微型结构和平均池化层；所述微型结构包括依次连接设置的紧凑模块、中间卷积层和最大池化层,所述中间卷积层通过ReLU与最大池化层构建连接；所述紧凑模块基于多尺度滤波、多位置池化、滤波器分解和参数缩减构建若干条支路而成。相比于传统技术,本发明能够加强对输入图像的抽象表征,有利于提高识别效率,并且兼容资源受限、适当降低网络深度的计算机系统,可减小计算机资源消耗。</td>   <td>1.一种面向资源受限系统的紧凑卷积神经网络,其特征在于,包括：依次连接设置的原始输入层、特征卷积结构、若干个微型结构和平均池化层；所述微型结构包括依次连接设置的紧凑模块、中间卷积层和最大池化层,所述中间卷积层通过ReLU与最大池化层构建连接；所述紧凑模块基于多尺度滤波、多位置池化、滤波器分解和参数缩减构建若干条平行支路而成；所述紧凑模块基于多尺度滤波构建,包括：将具有不同尺度的卷积核的卷积层分别分配为各平行支路上输入层的下一层；所述紧凑模块基于多位置池化构建,包括：增加新的平行支路,用于补充提取输入图像中不同位置的特征；所述紧凑模块基于滤波器分解构建,包括：选择将各支路单层卷积层替换为多层卷积核数更小的卷积层；所述紧凑模块基于参数缩减构建,包括：减少各支路的输入通道数以及缩小各支路各层上的卷积核。</td>   <td>G06K9/66;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         余阳;              吕京泽;                   严有强       </td>   <td>中山大学</td>   <td>基于随机回归森林模型的容器资源供给方法及系统</td>   <td>广东</td>   <td>CN108984269A</td>   <td>2018-12-11</td>   <td>本发明提供了一种基于随机回归森林模型的容器资源供给方法,其在负载动态变化的情况下,以用户实际响应时间和访问量为依据,迅速对变化为高负载的微服务进行扩容以解决大量用户一起访问同一个服务所造成的负载过重、响应时间过长的问题；并且当服务器为高负载服务集群提供大量容器资源后,负载突然递减时,能够对处于低负载的微服务快速回收资源,以达到保证用户服务质量和优化资源利用的目的。</td>   <td>1.一种基于随机回归森林模型的容器资源供给方法,其特征在于：包括以下步骤：S1.数据采集器周期性从网关处统计用户访问的记录详情,统计出各服务的响应时间和访问量；S2.服务调度器周期性地向数据采集器请求统计到的各个服务的响应时间和访问量,然后使用自回归时间序列方法分别基于各个服务的响应时间和访问量构建相应的预测模型,对各个服务未来一段时间内的响应时间和访问量进行预测；S3.服务调度器根据预测的响应时间判断各个服务在未来一段时间内的负载状态,形成高负载列表、低负载列表、正常负载列表,交由调度执行器处理；S4.调度执行器获得负载列表,将高负载列表和低负载列表交给有针对性弹性收缩器进行处理；所述有针对性弹性收缩器的分析处理数据模型为基于随机回归森林的模型；S5.有针对性弹性收缩器对高、低负载列表中的服务分别进行遍历处理,对于每一个服务,根据统计到的在一个周期内服务的响应和访问量得到一个数值X,在此期间保护冷却时间调度,将高、低负载列表中的服务对应的X值交由调度执行器处理,最后执行相应的调度。</td>   <td>G06F9/455;G06F9/50;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;              郑慧琳;                   李仕仁       </td>   <td>广州智慧城市发展研究院;中山大学;广州中大微电子有限公司</td>   <td>一种基于多类别联合软聚类的推荐方法及系统</td>   <td>广东</td>   <td>CN108984551A</td>   <td>2018-12-11</td>   <td>本发明公开了一种基于多类别联合软聚类的推荐方法及系统,其中,所述推荐方法包括：获取用户#物品交互信息,根据所述用户#物品交互信息构建评分矩阵和分类矩阵；对所述评分矩阵和所述分类矩阵进行多类别软聚类处理,获取多类别软聚类结果；采用加权非负矩阵分解对所述多类别软聚类结果进行用户喜好度预测,获取预测结果；根据所述预测结果向用户推荐预测分数最高的物品。在本发明实施例中,可以根据用户对物品的喜爱程度对进行评分预测,根据评分预测向用户推荐物品,预测准确度较高。</td>   <td>1.一种基于多类别联合软聚类的推荐方法,其特征在于,所述推荐方法包括：获取用户#物品交互信息,根据所述用户#物品交互信息构建评分矩阵和分类矩阵；对所述评分矩阵和所述分类矩阵进行多类别软聚类处理,获取多类别软聚类结果；采用加权非负矩阵分解对所述多类别软聚类结果进行用户喜好度预测,获取预测结果；根据所述预测结果向用户推荐预测分数最高的物品。</td>   <td>G06F17/30;G06Q10/04;G06Q30/06</td>  </tr>        <tr>   <td>中国专利</td>   <td>         唐晓颖;                   周检根       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>用于判别角膜溃烂的深度学习模型的构建方法及其系统</td>   <td>广东</td>   <td>CN108985328A</td>   <td>2018-12-11</td>   <td>本发明公开了用于判别角膜溃烂的深度学习模型的构建方法及其系统,对角膜测试图像进行处理,并且利用AlexNet和VGGNet来获取不同的深度学习模型,并在此之后,通过不断调整深度学习模型的各类参数来获取新的深度学习模型,在每次调整后,均以识别率更高的深度学习模型作为基础与下次调整后的进行比较,因此能够针对深度学习模型的各项参数分别进行优化,总体上藉由这些优化,能够大大提升识别率。因此,本发明给出了优化后的深度学习模型的构建方法,此深度学习模型能够代替人眼判别且具有较高的识别率,方便医务人员及研究者使用。</td>   <td>1.用于判别角膜溃烂的深度学习模型的构建方法,其特征在于,包括以下步骤：对角膜测试图像进行处理,得到角膜处理图像；基于AlexNet和VGGNet训练得到两个深度学习模型并利用此两个深度学习模型分别对角膜处理图像进行识别,将识别率更高的作为第一深度学习模型；调整第一深度学习模型的卷积层数,由调整前与调整后的第一深度学习模型分别对角膜处理图像进行识别,将识别率更高的作为第二深度学习模型；用于调整第二深度学习模型的卷积核数,由调整前与调整后的第二深度学习模型分别对角膜处理图像进行识别,将识别率更高的作为第三深度学习模型；用于调整第三深度学习模型的优化器模型,由调整前与调整后的第三深度学习模型分别对角膜处理图像进行识别,将识别率更高的作为最终判别模型。</td>   <td>G06K9/62;G06N3/08</td>  </tr>        <tr>   <td>中国专利</td>   <td>         蔡志岗;              朱晓强;              邱宇民;              李子健;              王福娟;              李佼洋;              陈建宇;                   陈梓艺       </td>   <td>中山大学</td>   <td>一种新型OCT图像显示方法</td>   <td>广东</td>   <td>CN108986084A</td>   <td>2018-12-11</td>   <td>本发明涉及一种新型OCT图像显示方法,包括有以下步骤：S1.将OCT设备所获得的原始数据进行傅里叶变换,转换成A#扫描三维图像并实时显示输出到信号瀑布图中；S2.将OCT设备所获得的原始数据进行傅里叶变换后与手动转动的角度θ结合转换成B#扫描二维图像并实时显示输出到扇形图中；S3.结合信号瀑布图和扇形图同时显示,让医护相关人员实现定点精准检测组织。</td>   <td>1.一种新型OCT图像显示方法,其特征在于：包括有以下步骤：S1.将OCT设备所获得的原始数据进行傅里叶变换,转换成A#扫描三维图像并实时显示输出到信号瀑布图中；S2.将OCT设备所获得的原始数据进行傅里叶变换后与手动转动的角度θ结合转换成B#扫描二维图像并实时显示输出到扇形图中；S3.结合信号瀑布图和扇形图同时显示,让医护相关人员实现定点精准检测组织。</td>   <td>G06T7/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>              李璇       </td>   <td>中山大学</td>   <td>一种基于卷积神经网络的图像隐写分析方法及系统</td>   <td>广东</td>   <td>CN108961137A</td>   <td>2018-12-07</td>   <td>本发明公开了一种基于卷积神经网络的图像隐写分析方法及系统,包括三个模块：图像预处理部分、特征提取部分、特征分类模块。在图像预处理部分,本发明通过实验选择了一组具有多方向、多尺度参数的Gabor滤波器与输入图像卷积获得高信噪比的图像残差；特征提取部分使用了快捷连接结构将浅层的输出与后面层直接相连来减轻过拟合现象。本发明基于卷积神经网络的隐写方法不需要大量关于隐写、隐写分析的领域知识,特征提取和特征分类过程是联合的优化过程,设计简单易于实施；其次,利用Gabor滤波器的尺度性和方向性可以帮助网络提取更有效的图像残差；最后,对J#UNIWARD和UED两种内容自适应的隐写算法都能取得比较好的检测效果。</td>   <td>1.一种基于卷积神经网络的图像隐写分析方法,其特征在于,包括以下步骤：S1：设计实验所需的图像数据库,对图像数据库中的图像进行裁剪、压缩和隐秘信息嵌入,得到载密图像；将原始图像和载密图像分为互不相交的训练集、验证集和测试集；S2：设计基于卷积神经网络的网络模型,所述的网络模型包括预处理层、使用卷积层和池化层的特征提取层、全连接层和Softmax函数的特征分类层；S3：根据实验结果设计预处理层滤波器的初始化参数；S4：将训练集的图像进行数据增强后输入S2的基于卷积神经网络的网络模型进行训练；S5：选取步骤S4中训练后得到的最优的N个基于卷积神经网络的网络模型对测试集的图像进行分析,所述的N是正整数。</td>   <td>G06T1/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         罗祥辉;              苏卓;              陈强;                   林格       </td>   <td>中山大学</td>   <td>一种基于精细化网络模型的多标签服装解析方法</td>   <td>广东</td>   <td>CN108932517A</td>   <td>2018-12-04</td>   <td>本发明公开了一种基于精细化网络模型的多标签服装解析方法,其中,该方法包括：获取人体服装数据集,进行初始化处理,获得有标签的初始化数据集；建立精细化网络模型,通过融入聚焦损失函数进行训练,获得训练好的精细化网络模型；获取训练好的精细化网络模型,进行测试与评估处理,获得分割好的服装图像。在本发明实施例中,能够在进行解析服装标签时抑制标签对损失函数值产生的贡献数量,尽最大程度保留精细化语义信息,对解析结果提高精确性,为程序设计员提供便捷,减少不必要的人工操作步骤。</td>   <td>1.一种基于精细化网络模型的多标签服装解析方法,其特征在于,所述方法包括：获取人体服装数据集,进行初始化处理,获得有标签的初始化数据集；建立精细化网络模型,通过融入聚焦损失函数进行训练,获得训练好的精细化网络模型；获取训练好的精细化网络模型,进行测试与评估处理,获得分割好的服装图像。</td>   <td>G06K9/34;G06K9/62;G06N3/04</td>  </tr>        <tr>   <td>中国专利</td>   <td>         谭洪舟;              梁耀淦;              谢舜道;              陈荣军;              朱雄泳;              曾衍瀚;                   路崇       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种集成电路布局规划的预压缩方法及系统</td>   <td>广东</td>   <td>CN108920746A</td>   <td>2018-11-30</td>   <td>本发明公开了一种集成电路布局规划的预压缩方法及系统,其中,方法包括以下步骤：获取数组信息和包含集成电路的多个模块之间相互关系的序列对信息,以及各模块的参数信息；依次结合序列对信息和数组信息在布局框内布局各模块后,结合参数信息和预设判断方式判断是否存有遮挡模块,若存有,采用第一方式更新数组信息；反之,采用第二方式更新数组信息；根据序列对信息判断是否布局完所有模块,若是,完成预压缩；反之继续执行上一步。本发明通过判断是否存有遮挡模块,并在存有遮挡模块时更新数组信息后,结合序列对信息和数组信息布局下一模块,从而提高了工作效率和布局面积的利用率,降低了生产成本,可广泛应用于集成电路物理设计领域。</td>   <td>1.一种集成电路布局规划的预压缩方法,其特征在于,包括以下步骤：S1、获取预设的数组信息和包含集成电路的多个模块之间相互关系的序列对信息,以及各模块的参数信息；S2、依次结合序列对信息和数组信息在预设的布局框内布局各模块后,结合参数信息和预设判断方式判断是否存有遮挡模块,若存有,采用第一方式更新数组信息；反之,采用第二方式更新数组信息；S3、根据序列对信息判断是否布局完所有模块,若是,完成预压缩；反之继续执行S2。</td>   <td>G06F17/50</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;              马媛;              李元新;              杨焕;                   吴明华       </td>   <td>佛山市顺德区中山大学研究院;广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种基于判别式目标方程的人脸识别方法</td>   <td>广东</td>   <td>CN108921088A</td>   <td>2018-11-30</td>   <td>本发明公开了一种基于判别式目标方程的人脸识别方法,首先用第一训练样本来线性表示测试样本,并构建第一目标方程,然后计算测试样本的最小残差所对应的第一类别,将最小残差所对应的第一类别作为第二训练样本来重新线性表示测试样本,然后构建第二目标方程,通过计算第二目标方程得到测试样本的最小残差所对应的第二类别,最小残差所对应的第二类别即为测试样本所属的类别,本发明的方法通过对测试样本的二次表示以及两次残差的计算,降低了图片分类过程中的设计复杂度,可以更精确的分类出人脸,并且在不同光照以及姿势下都有很强的鲁棒性。</td>   <td>1.一种基于判别式目标方程的人脸识别方法,其特征在于：包括以下步骤：A、将人脸样本分为第一训练样本和测试样本,并用第一训练样本线性表示测试样本；B、构建第一目标方程并求解第一表示系数；C、根据第一表示系数计算测试样本的残差得到最小残差所对应的第一类别,最小残差所对应的第一类别为第二训练样本；D、用第二训练样本线性表示测试样本；E、构建第二目标方程并求解第二表示系数；F、根据第二表示系数计算测试样本的残差得到最小残差所对应的第二类别,最小残差所对应的第二类别为测试样本所属的类别。</td>   <td>G06K9/00;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         康显桂;              资涵琪;                   刘凯       </td>   <td>中山大学</td>   <td>一种基于生成对抗网络的图像隐写方法及系统</td>   <td>广东</td>   <td>CN108921764A</td>   <td>2018-11-30</td>   <td>本发明公开了一种基于生成对抗网络的图像隐写方法及系统,系统包括框架设计模块、网络训练模块和性能估计模块；通过设计网络框架并构造损失函数,输入原始的随机噪声,训练整个网络,使得生成的图片尽可能地拟合原始图片库的数据分布,对最终训练得到的模型性能进行检测,分析由该模型生成的图片的视觉质量以及抗隐写分析的安全性能。本发明的抗隐写分析的安全性能较其他基于生成对抗网络的方法有2～5个多百分点的提升,和原始图片库相比,生成图像视觉质量较好且更为安全。</td>   <td>1.一种基于生成对抗网络的图像隐写方法,其特征在于,包括以下步骤：S1：在待拟合的数据库中设计网络框架,其中网络框架由三个子网络构成,包括生成器(G)、判别器(D)和隐写分析器(S)；S2：构建各子网络的损失函数并进行优化函数,三个子网络的损失函数均基于交叉熵损失函数,其中生成器(G)的损失函数设计为：<img file="FDA0001598434540000011.TIF" wi="387" he="74"/>其中α＝0.02,β＝1,<img file="FDA0001598434540000012.TIF" wi="563" he="180"/><img file="FDA0001598434540000013.TIF" wi="700" he="65"/>判别器(D)的损失函数设计为：<img file="FDA0001598434540000014.TIF" wi="700" he="125"/>隐写分析器(S)的损失函数设计为：<img file="FDA0001598434540000015.TIF" wi="700" he="98"/>如上公式中G(z(i))表示有生成器(G)生成的图片,D(x)表示输入x时判别器(D)0的输出,S(x)表示输入x时隐写分析器(S)的输出,stego(x)表示对x嵌入信息后得到的结果；S3：选择随机噪声作为生成器(G)的输入,选择需要拟合的数据作为判别器(D)的输入之一,对网络进行训练,得到最终的训练模型；S4：利用基于卷积神经网络的隐写分析器(S)对训练模型产生的图片进行性能评估；S5：将本发明的输出结果与原始图片库以及其他生成对抗网络模型的对抗结果进行比较。</td>   <td>G06T1/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         常扣;              朝红阳;                   郭怡适       </td>   <td>中山大学</td>   <td>基于图像检测的商品货架分割和层信息统计方法</td>   <td>广东</td>   <td>CN108898141A</td>   <td>2018-11-27</td>   <td>本发明涉及人工智能的技术领域,更具体地,涉及基于图像检测的商品货架分割和层信息统计方法。基于图像检测的商品货架分割和层信息统计方法,其中,包括训练数据收集、检测模型训练和模型测试及上线使用。销售代表只需要将商品信息图片进行拍摄,货架划分和层数统计信息由电脑自动处理,大大节省了人力成本。工作人员只需要进行拍照,传输到后台之后,立即就可以给出货架信息,工作效率大大提高。统计信息没有人为因素的制约,数据的准确度。</td>   <td>1.基于图像检测的商品货架分割和层信息统计方法,其特征在于,包括训练数据收集、检测模型训练和模型测试及上线使用。</td>   <td>G06K9/34;G06K9/62;G06K9/00</td>  </tr>        <tr>   <td>中国专利</td>   <td>         刘冶;              桂进军;              叶思聪;              李宏浩;              周婷;              陈宇恒;              徐振涛;                   印鉴       </td>   <td>火烈鸟网络(广州)股份有限公司;中山大学</td>   <td>一种监控数据库的方法及系统</td>   <td>广东</td>   <td>CN108874964A</td>   <td>2018-11-23</td>   <td>本发明涉及一种监控数据库的方法及系统,包括：实时获取待监控数据库中的二进制数据流；实时解析所述二进制数据流,获得待监控数据库的所有操作事件；过滤所述所有操作事件,获得待监控数据库中数据表的变更事件；从所述待监控数据库中数据表的变更事件中提取需记录的数据,并标记所述需记录的数据；存储需记录的数据、二进制数据流对应的二进制文件名称及二进制数据流在二进制文件中的位置信息。本发明实现了采集和存储待监控数据库中数据表的变更信息,降低了数据库信息安全的维护成本,避免了通过对数据库中大规模数据的变更操作进行数据篡改的行为,提升了数据库审计的效率。</td>   <td>1.一种监控数据库的方法,其特征在于,包括如下步骤：实时获取待监控数据库中的二进制数据流；实时解析所述二进制数据流,获得待监控数据库的所有操作事件；过滤所述所有操作事件,获得待监控数据库中数据表的变更事件；从所述待监控数据库中数据表的变更事件中提取需记录的数据,并标记所述需记录的数据；存储所述需记录的数据、二进制数据对应的二进制文件名称以及二进制数据流在二进制文件的位置。</td>   <td>G06F17/30</td>  </tr>        <tr>   <td>中国专利</td>   <td>         胡建国;              林培祥;              晏斌;              黄家诚;              邓成谦;                   李凯祥       </td>   <td>广东顺德中山大学卡内基梅隆大学国际联合研究院;中山大学</td>   <td>一种流量检测的方法及系统</td>   <td>广东</td>   <td>CN108875447A</td>   <td>2018-11-23</td>   <td>本发明公开一种流量检测的方法,包括以下步骤：提取训练样本的梯度向量直方图HOG特征向量的各种特征,根据所述训练样本的HOG特征向量的各种特征与加性交叉核函数构造支持向量机SVM分类器；获取采样图像中候选样本的HOG特征向量的各种特征；根据所述SVM分类器对所述样本的HOG特征向量的各种特征进行识别以得到流量检测结果。本发明还公开一种流量检测的系统,用于实现上述方法。本发明技术方案提高了流量检测的识别精度。</td>   <td>1.一种流量检测的方法,其特征在于,包括以下步骤：提取训练样本的梯度向量直方图HOG特征向量的各种特征,根据所述训练样本的HOG特征向量的各种特征与加性交叉核函数构造支持向量机SVM分类器；获取采样图像中候选样本的HOG特征向量的各种特征；根据所述SVM分类器对所述样本的HOG特征向量的各种特征进行识别以得到流量检测结果。</td>   <td>G06K9/00;G06K9/46;G06K9/62</td>  </tr>        <tr>   <td>中国专利</td>   <td>         李熙莹;              朱肯钢;                   江倩殷       </td>   <td>中山大学</td>   <td>一种图像快速去雾方法、系统、终端及存储介质</td>   <td>广东</td>   <td>CN108876743A</td>   <td>2018-11-23</td>   <td>本发明公开了一种图像快速去雾方法、系统、终端及存储介质,该方法包括：采用OTSU对原始有雾图像进行天空区域和非天空区域划分后,采用最大值滤波和引导滤波对暗通道图优化,根据优化后的暗通道图来确定第一透射率；采用自适应性参数调整方法对第一透射率调整后得到第二透射率；根据第一透射率和第二透射率,按照大气散射模型分别对非天空区域和天空区域进行去雾复原,得到去雾复原图像；采用CLAHE对去雾复原图像进行色调调整。该系统包括获取模块、划分模块、优化模块、调整模块、去雾模块及调色模块。通过使用本发明,能够有效精细地实现有雾图像的去雾处理,去雾效果优且处理运行效率高。本发明可广泛应用于图像处理领域中。</td>   <td>1.一种图像快速去雾方法,其特征在于,包括以下步骤：获取原始有雾图像；采用OTSU对获取得到原始有雾图像进行天空区域和非天空区域的划分；采用最大值滤波方式和引导滤波方式对暗通道图进行优化处理,从而根据优化处理后的暗通道图来确定得出第一透射率；其中,所述暗通道图指的是与原始有雾图像对应的暗通道图,所述第一透射率是用于对原始有雾图像中的非天空区域进行去雾复原处理的透射率；采用自适应性参数调整方法对第一透射率进行调整处理,从而得到第二透射率；其中,所述第二透射率是用于对原始有雾图像中的天空区域进行去雾复原处理的透射率；根据第一透射率和第二透射率,按照大气散射模型分别对原始有雾图像中的非天空区域和天空区域进行去雾复原处理,从而得到原始有雾图像所对应的去雾复原图像；采用CLAHE对去雾复原图像进行色调调整处理,从而得到最终去雾图像。</td>   <td>G06T5/00;G06T7/11;G06T7/12</td>  </tr> </table></body></html>